{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d00a58e-4a8d-475c-b918-0e3a2a6fc561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries imported successfully\n",
      "Analysis date: 2026-01-26\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data fetching\n",
    "import pandas_datareader as pdr\n",
    "from fredapi import Fred\n",
    "\n",
    "# Statistical modeling\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from hmmlearn import hmm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, adfuller, kpss\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy import stats\n",
    "from scipy.stats import jarque_bera, shapiro\n",
    "\n",
    "# Plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")\n",
    "print(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fcf1b617-8287-4ee2-9fae-dea4532f3053",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"1990-01-01\"\n",
    "end_date   = \"2025-12-31\"\n",
    "\n",
    "# --- FRED series codes ---\n",
    "FRED_SERIES = {\n",
    "    \"M2\":   \"M2SL\",      # M2 money stock (monthly, SA)\n",
    "    \"FED_BS\":\"WALCL\",     # Fed balance sheet total assets (weekly)\n",
    "    \"FFR\": \"DFF\",  \n",
    "    \"Real_Rate\": \"REAINTRATREARAT1YE\",\n",
    "    \"T3M\":   \"DTB3\",     # 3-Month T-Bill rate (monthly)\n",
    "    \"T10Y\":  \"DGS10\",     # 10-Year Treasury yield (daily -> resample monthly)\n",
    "    \"BAA\":    \"DBAA\",       # Moody's Baa corporate yield (monthly)\n",
    "    \"AAA\":    \"DAAA\",       # Moody's Aaa corporate yield (monthly)\n",
    "    \"CPI\":    \"CPIAUCSL\",  # CPI index (monthly, SA)\n",
    "    \"CORE_CPI\": \"CPILFESL\",    # Core CPI\n",
    "    \"GDP\":    \"GDP\",       # Nominal GDP (quarterly, SAAR)\n",
    "    \n",
    "    'IORB': 'IORB',                    # Interest on Reserve Balances\n",
    "    'RRP': 'RRPONTSYD',                # Overnight Reverse Repo\n",
    "    'TGA': 'WTREGEN',                  # Treasury General Account\n",
    "    \n",
    "    'Bank_Reserves': 'TOTRESNS',       # Total Reserves\n",
    "    'CI_Loans': 'TOTBKCR',             # Bank Credit, All Commercial Banks\n",
    "    'Consumer_Credit': 'TOTALSL',      # Consumer Credit Outstanding\n",
    "    'Mortgage_Rate': 'MORTGAGE30US',   # 30Y Mortgage Rate\n",
    "    'Prime_Rate': 'DPRIME',            # Bank Prime Loan Rate\n",
    "    'STLFSI': 'STLFSI',                # St. Louis Fed Financial Stress\n",
    "    'Dollar_Index': 'DTWEXBGS',        # # Global Liquidity: Trade-Weighted Dollar\n",
    "    'Household_Debt_Service': 'TDSP',  # Household Liquidity: Household Debt Service Ratio\n",
    "}\n",
    "\n",
    "# For equity factors: Fama-French via pandas_datareader (famafrench)\n",
    "FF_FACTORS_DATASET = \"F-F_Research_Data_5_Factors_2x3\"\n",
    "\n",
    "def download_fred_series(series_dict, start, end):\n",
    "    \"\"\"\n",
    "    Download FRED series into a single monthly DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series_dict : dict\n",
    "        Mapping logical_name -> FRED code.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for name, code in series_dict.items():\n",
    "        print(f\"Downloading {name} ({code}) from FRED...\")\n",
    "        s = pdr.DataReader(code, \"fred\", start, end)\n",
    "        s = s.rename(columns={code: name})\n",
    "        dfs.append(s)\n",
    "\n",
    "    df = pd.concat(dfs, axis=1)\n",
    "    # Ensure monthly freq by end-of-month sampling\n",
    "    df = df.resample(\"M\").last()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1f53cd67-b4d1-46a0-9035-320c8d68d278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading M2 (M2SL) from FRED...\n",
      "Downloading FED_BS (WALCL) from FRED...\n",
      "Downloading FFR (DFF) from FRED...\n",
      "Downloading Real_Rate (REAINTRATREARAT1YE) from FRED...\n",
      "Downloading T3M (DTB3) from FRED...\n",
      "Downloading T10Y (DGS10) from FRED...\n",
      "Downloading BAA (DBAA) from FRED...\n",
      "Downloading AAA (DAAA) from FRED...\n",
      "Downloading CPI (CPIAUCSL) from FRED...\n",
      "Downloading CORE_CPI (CPILFESL) from FRED...\n",
      "Downloading GDP (GDP) from FRED...\n",
      "Downloading IORB (IORB) from FRED...\n",
      "Downloading RRP (RRPONTSYD) from FRED...\n",
      "Downloading TGA (WTREGEN) from FRED...\n",
      "Downloading Bank_Reserves (TOTRESNS) from FRED...\n",
      "Downloading CI_Loans (TOTBKCR) from FRED...\n",
      "Downloading Consumer_Credit (TOTALSL) from FRED...\n",
      "Downloading Mortgage_Rate (MORTGAGE30US) from FRED...\n",
      "Downloading Prime_Rate (DPRIME) from FRED...\n",
      "Downloading STLFSI (STLFSI) from FRED...\n",
      "Downloading Dollar_Index (DTWEXBGS) from FRED...\n",
      "Downloading Household_Debt_Service (TDSP) from FRED...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M2</th>\n",
       "      <th>FED_BS</th>\n",
       "      <th>FFR</th>\n",
       "      <th>Real_Rate</th>\n",
       "      <th>T3M</th>\n",
       "      <th>T10Y</th>\n",
       "      <th>BAA</th>\n",
       "      <th>AAA</th>\n",
       "      <th>CPI</th>\n",
       "      <th>CORE_CPI</th>\n",
       "      <th>...</th>\n",
       "      <th>RRP</th>\n",
       "      <th>TGA</th>\n",
       "      <th>Bank_Reserves</th>\n",
       "      <th>CI_Loans</th>\n",
       "      <th>Consumer_Credit</th>\n",
       "      <th>Mortgage_Rate</th>\n",
       "      <th>Prime_Rate</th>\n",
       "      <th>STLFSI</th>\n",
       "      <th>Dollar_Index</th>\n",
       "      <th>Household_Debt_Service</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-31</th>\n",
       "      <td>22108.3</td>\n",
       "      <td>6603384.0</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1.523005</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.23</td>\n",
       "      <td>6.03</td>\n",
       "      <td>5.42</td>\n",
       "      <td>323.364</td>\n",
       "      <td>329.793</td>\n",
       "      <td>...</td>\n",
       "      <td>77.898</td>\n",
       "      <td>589998.0</td>\n",
       "      <td>3281.9</td>\n",
       "      <td>18720.1524</td>\n",
       "      <td>5059896.38</td>\n",
       "      <td>6.56</td>\n",
       "      <td>7.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.6028</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-30</th>\n",
       "      <td>22212.4</td>\n",
       "      <td>6608395.0</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1.149868</td>\n",
       "      <td>3.86</td>\n",
       "      <td>4.16</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.22</td>\n",
       "      <td>324.368</td>\n",
       "      <td>330.542</td>\n",
       "      <td>...</td>\n",
       "      <td>49.071</td>\n",
       "      <td>804856.0</td>\n",
       "      <td>3068.1</td>\n",
       "      <td>18759.8099</td>\n",
       "      <td>5071365.99</td>\n",
       "      <td>6.30</td>\n",
       "      <td>7.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.5624</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-31</th>\n",
       "      <td>22298.0</td>\n",
       "      <td>6587034.0</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.973381</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.11</td>\n",
       "      <td>5.80</td>\n",
       "      <td>5.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>51.802</td>\n",
       "      <td>957990.0</td>\n",
       "      <td>2944.9</td>\n",
       "      <td>18820.7752</td>\n",
       "      <td>5080601.87</td>\n",
       "      <td>6.17</td>\n",
       "      <td>7.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.7715</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-30</th>\n",
       "      <td>22322.4</td>\n",
       "      <td>6552419.0</td>\n",
       "      <td>3.89</td>\n",
       "      <td>1.001027</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.80</td>\n",
       "      <td>5.18</td>\n",
       "      <td>325.031</td>\n",
       "      <td>331.068</td>\n",
       "      <td>...</td>\n",
       "      <td>7.561</td>\n",
       "      <td>903394.0</td>\n",
       "      <td>2879.3</td>\n",
       "      <td>18925.1502</td>\n",
       "      <td>5084831.24</td>\n",
       "      <td>6.23</td>\n",
       "      <td>7.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.4288</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>6640618.0</td>\n",
       "      <td>3.64</td>\n",
       "      <td>-0.135174</td>\n",
       "      <td>3.57</td>\n",
       "      <td>4.18</td>\n",
       "      <td>5.90</td>\n",
       "      <td>5.35</td>\n",
       "      <td>326.030</td>\n",
       "      <td>331.860</td>\n",
       "      <td>...</td>\n",
       "      <td>105.993</td>\n",
       "      <td>837306.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18934.1126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.1166</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 M2     FED_BS   FFR  Real_Rate   T3M  T10Y   BAA   AAA  \\\n",
       "DATE                                                                      \n",
       "2025-08-31  22108.3  6603384.0  4.33   1.523005  4.05  4.23  6.03  5.42   \n",
       "2025-09-30  22212.4  6608395.0  4.09   1.149868  3.86  4.16  5.83  5.22   \n",
       "2025-10-31  22298.0  6587034.0  3.86   0.973381  3.73  4.11  5.80  5.22   \n",
       "2025-11-30  22322.4  6552419.0  3.89   1.001027  3.73  4.02  5.80  5.18   \n",
       "2025-12-31      NaN  6640618.0  3.64  -0.135174  3.57  4.18  5.90  5.35   \n",
       "\n",
       "                CPI  CORE_CPI  ...      RRP       TGA  Bank_Reserves  \\\n",
       "DATE                           ...                                     \n",
       "2025-08-31  323.364   329.793  ...   77.898  589998.0         3281.9   \n",
       "2025-09-30  324.368   330.542  ...   49.071  804856.0         3068.1   \n",
       "2025-10-31      NaN       NaN  ...   51.802  957990.0         2944.9   \n",
       "2025-11-30  325.031   331.068  ...    7.561  903394.0         2879.3   \n",
       "2025-12-31  326.030   331.860  ...  105.993  837306.0            NaN   \n",
       "\n",
       "              CI_Loans  Consumer_Credit  Mortgage_Rate  Prime_Rate  STLFSI  \\\n",
       "DATE                                                                         \n",
       "2025-08-31  18720.1524       5059896.38           6.56        7.50     NaN   \n",
       "2025-09-30  18759.8099       5071365.99           6.30        7.25     NaN   \n",
       "2025-10-31  18820.7752       5080601.87           6.17        7.00     NaN   \n",
       "2025-11-30  18925.1502       5084831.24           6.23        7.00     NaN   \n",
       "2025-12-31  18934.1126              NaN           6.15        6.75     NaN   \n",
       "\n",
       "            Dollar_Index  Household_Debt_Service  \n",
       "DATE                                              \n",
       "2025-08-31      120.6028                     NaN  \n",
       "2025-09-30      120.5624                     NaN  \n",
       "2025-10-31      121.7715                     NaN  \n",
       "2025-11-30      121.4288                     NaN  \n",
       "2025-12-31      120.1166                     NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro_data = download_fred_series(FRED_SERIES, start_date, end_date)\n",
    "\n",
    "# 1. Start from the original GDP values only (drop NaNs)\n",
    "gdp_series = macro_data[\"GDP\"].dropna()\n",
    "# 2. Collapse to unique quarter-end values\n",
    "#    - If GDP is timestamped at quarter *start* (e.g. 2025-04-01),\n",
    "#      this will move it to the quarter end (2025-06-30) and give unique labels.\n",
    "gdp_q = gdp_series.resample(\"Q\").last()   # quarterly, at quarter-end (e.g. 2025-03-31, 2025-06-30, ...)\n",
    "\n",
    "# 3. Downsample to monthly and forward-fill within the quarter\n",
    "gdp_m = gdp_q.resample(\"M\").ffill()\n",
    "\n",
    "# 4. Assign back into macro_raw, aligning on the monthly index\n",
    "macro_data[\"GDP\"] = gdp_m\n",
    "\n",
    "macro_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6668372-a3b6-4779-8943-95281ae64ea0",
   "metadata": {},
   "source": [
    "### 3.2 Fama-French Factors and Valuation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90242e1e-f95a-4636-82dc-8495aa43298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Fama-French factors (famafrench)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mkt_excess</th>\n",
       "      <th>smb</th>\n",
       "      <th>hml</th>\n",
       "      <th>rmw</th>\n",
       "      <th>cma</th>\n",
       "      <th>rf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-07-31</th>\n",
       "      <td>0.0198</td>\n",
       "      <td>-0.0015</td>\n",
       "      <td>-0.0127</td>\n",
       "      <td>-0.0029</td>\n",
       "      <td>-0.0207</td>\n",
       "      <td>0.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-31</th>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>-0.0067</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-30</th>\n",
       "      <td>0.0339</td>\n",
       "      <td>-0.0218</td>\n",
       "      <td>-0.0105</td>\n",
       "      <td>-0.0203</td>\n",
       "      <td>-0.0222</td>\n",
       "      <td>0.0033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-31</th>\n",
       "      <td>0.0196</td>\n",
       "      <td>-0.0131</td>\n",
       "      <td>-0.0309</td>\n",
       "      <td>-0.0522</td>\n",
       "      <td>-0.0403</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-30</th>\n",
       "      <td>-0.0013</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            mkt_excess     smb     hml     rmw     cma      rf\n",
       "date                                                          \n",
       "2025-07-31      0.0198 -0.0015 -0.0127 -0.0029 -0.0207  0.0034\n",
       "2025-08-31      0.0185  0.0488  0.0442 -0.0067  0.0208  0.0038\n",
       "2025-09-30      0.0339 -0.0218 -0.0105 -0.0203 -0.0222  0.0033\n",
       "2025-10-31      0.0196 -0.0131 -0.0309 -0.0522 -0.0403  0.0037\n",
       "2025-11-30     -0.0013  0.0147  0.0376  0.0143  0.0068  0.0030"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_ff_factors(start, end):\n",
    "    \"\"\"\n",
    "    Download monthly Fama-French 5 factors (2x3) using pandas_datareader.\n",
    "    \"\"\"\n",
    "    print(\"Downloading Fama-French factors (famafrench)...\")\n",
    "    ff_raw = pdr.DataReader(\n",
    "        name=FF_FACTORS_DATASET,\n",
    "        data_source=\"famafrench\",\n",
    "        start=start,\n",
    "        end=end\n",
    "    )[0]  # table [0] contains the data\n",
    "\n",
    "    ff = (ff_raw\n",
    "          .divide(100)  # convert from % to decimal\n",
    "          .reset_index(names=\"date\")\n",
    "          .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n",
    "          .rename(str.lower, axis=\"columns\")\n",
    "          .rename(columns={\"mkt-rf\": \"mkt_excess\"}))\n",
    "\n",
    "    ff = ff.set_index(\"date\")\n",
    "    return ff\n",
    "\n",
    "ff_factors = download_ff_factors(start_date, end_date)\n",
    "ff_adj = ff_factors.copy()\n",
    "ff_adj.index = ff_adj.index.to_period(\"M\").to_timestamp(\"M\")\n",
    "ff_adj.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8cc449-8b6f-4883-a539-08c4f60b5b21",
   "metadata": {},
   "source": [
    "### 3.3 Shiller CAPE Data\n",
    "https://shillerdata.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "702ef36f-2a5c-4bd9-ae62-df5d64fd18d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>CAPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-08-31</th>\n",
       "      <td>6408.95</td>\n",
       "      <td>37.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-30</th>\n",
       "      <td>6584.02</td>\n",
       "      <td>38.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-31</th>\n",
       "      <td>6735.69</td>\n",
       "      <td>39.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-30</th>\n",
       "      <td>6740.89</td>\n",
       "      <td>39.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-12-31</th>\n",
       "      <td>6812.63</td>\n",
       "      <td>39.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  P   CAPE\n",
       "date                      \n",
       "2025-08-31  6408.95  37.85\n",
       "2025-09-30  6584.02  38.58\n",
       "2025-10-31  6735.69  39.21\n",
       "2025-11-30  6740.89  39.12\n",
       "2025-12-31  6812.63  39.42"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def parse_yyyy_mm_to_date(s: str):\n",
    "    \"\"\"\n",
    "    Convert 'YYYY.MM' to a proper datetime.\n",
    "    Handles the special case where October appears as YYYY.1 instead of YYYY.10.\n",
    "    \"\"\"\n",
    "    s = str(s).strip()\n",
    "    parts = s.split(\".\")\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(f\"Unexpected date format: {s}\")\n",
    "    \n",
    "    year = int(parts[0])\n",
    "    mm = parts[1]\n",
    "\n",
    "    # Fix: \"1\" should be \"10\" (October)\n",
    "    if mm == \"1\":\n",
    "        month = 10\n",
    "    else:\n",
    "        month = int(mm)\n",
    "\n",
    "    return pd.Timestamp(year=year, month=month, day=1)\n",
    "\n",
    "# --- Load CAPE CSV ---\n",
    "sp500_cape_raw = pd.read_csv(\"data/sp500-cape-1990-2025.csv\")\n",
    "\n",
    "# Clean + parse\n",
    "sp500_cape_raw[\"date\"] = sp500_cape_raw[\"Date\"].apply(parse_yyyy_mm_to_date)\n",
    "\n",
    "sp500_cape_raw = (\n",
    "    sp500_cape_raw[[\"date\", \"SP500\", \"CAPE\"]]\n",
    "    .set_index(\"date\")\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# Convert to month-end aligned CAPE series\n",
    "sp500_cape_m = sp500_cape_raw.resample(\"M\").last()\n",
    "sp500_cape_m.rename(columns={\"CAPE\": \"CAPE\", \"SP500\": \"P\"}, inplace=True)\n",
    "\n",
    "sp500_cape_m.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3821ca6f-5db9-478f-a18a-cd0714572d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            VIXCLS\n",
      "DATE              \n",
      "2025-12-25     NaN\n",
      "2025-12-26   13.60\n",
      "2025-12-29   14.20\n",
      "2025-12-30   14.33\n",
      "2025-12-31   14.95\n"
     ]
    }
   ],
   "source": [
    "import pandas_datareader as pdr\n",
    "\n",
    "# Fetch the VIXCLS series from FRED\n",
    "vix_data = pdr.get_data_fred('VIXCLS', start=start_date, end=end_date)\n",
    "\n",
    "# Display the results\n",
    "print(vix.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e4f9e483-3022-4e31-962c-fddddca04acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M2</th>\n",
       "      <th>FED_BS</th>\n",
       "      <th>FFR</th>\n",
       "      <th>Real_Rate</th>\n",
       "      <th>T3M</th>\n",
       "      <th>T10Y</th>\n",
       "      <th>BAA</th>\n",
       "      <th>AAA</th>\n",
       "      <th>CPI</th>\n",
       "      <th>CORE_CPI</th>\n",
       "      <th>...</th>\n",
       "      <th>Household_Debt_Service</th>\n",
       "      <th>mkt_excess</th>\n",
       "      <th>smb</th>\n",
       "      <th>hml</th>\n",
       "      <th>rmw</th>\n",
       "      <th>cma</th>\n",
       "      <th>rf</th>\n",
       "      <th>P</th>\n",
       "      <th>CAPE</th>\n",
       "      <th>VIX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-07-31</th>\n",
       "      <td>22028.7</td>\n",
       "      <td>6642578.0</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.838904</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.37</td>\n",
       "      <td>6.04</td>\n",
       "      <td>5.41</td>\n",
       "      <td>322.132</td>\n",
       "      <td>328.656</td>\n",
       "      <td>...</td>\n",
       "      <td>11.256338</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>-0.0015</td>\n",
       "      <td>-0.0127</td>\n",
       "      <td>-0.0029</td>\n",
       "      <td>-0.0207</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>6296.50</td>\n",
       "      <td>37.47</td>\n",
       "      <td>16.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-31</th>\n",
       "      <td>22108.3</td>\n",
       "      <td>6603384.0</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1.523005</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.23</td>\n",
       "      <td>6.03</td>\n",
       "      <td>5.42</td>\n",
       "      <td>323.364</td>\n",
       "      <td>329.793</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>-0.0067</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>6408.95</td>\n",
       "      <td>37.85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-30</th>\n",
       "      <td>22212.4</td>\n",
       "      <td>6608395.0</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1.149868</td>\n",
       "      <td>3.86</td>\n",
       "      <td>4.16</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.22</td>\n",
       "      <td>324.368</td>\n",
       "      <td>330.542</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>-0.0218</td>\n",
       "      <td>-0.0105</td>\n",
       "      <td>-0.0203</td>\n",
       "      <td>-0.0222</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>6584.02</td>\n",
       "      <td>38.58</td>\n",
       "      <td>16.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-31</th>\n",
       "      <td>22298.0</td>\n",
       "      <td>6587034.0</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.973381</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.11</td>\n",
       "      <td>5.80</td>\n",
       "      <td>5.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>-0.0131</td>\n",
       "      <td>-0.0309</td>\n",
       "      <td>-0.0522</td>\n",
       "      <td>-0.0403</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>6735.69</td>\n",
       "      <td>39.21</td>\n",
       "      <td>17.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-30</th>\n",
       "      <td>22322.4</td>\n",
       "      <td>6552419.0</td>\n",
       "      <td>3.89</td>\n",
       "      <td>1.001027</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.80</td>\n",
       "      <td>5.18</td>\n",
       "      <td>325.031</td>\n",
       "      <td>331.068</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0013</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>6740.89</td>\n",
       "      <td>39.12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 M2     FED_BS   FFR  Real_Rate   T3M  T10Y   BAA   AAA  \\\n",
       "2025-07-31  22028.7  6642578.0  4.33   0.838904  4.24  4.37  6.04  5.41   \n",
       "2025-08-31  22108.3  6603384.0  4.33   1.523005  4.05  4.23  6.03  5.42   \n",
       "2025-09-30  22212.4  6608395.0  4.09   1.149868  3.86  4.16  5.83  5.22   \n",
       "2025-10-31  22298.0  6587034.0  3.86   0.973381  3.73  4.11  5.80  5.22   \n",
       "2025-11-30  22322.4  6552419.0  3.89   1.001027  3.73  4.02  5.80  5.18   \n",
       "\n",
       "                CPI  CORE_CPI  ...  Household_Debt_Service  mkt_excess  \\\n",
       "2025-07-31  322.132   328.656  ...               11.256338      0.0198   \n",
       "2025-08-31  323.364   329.793  ...                     NaN      0.0185   \n",
       "2025-09-30  324.368   330.542  ...                     NaN      0.0339   \n",
       "2025-10-31      NaN       NaN  ...                     NaN      0.0196   \n",
       "2025-11-30  325.031   331.068  ...                     NaN     -0.0013   \n",
       "\n",
       "               smb     hml     rmw     cma      rf        P   CAPE    VIX  \n",
       "2025-07-31 -0.0015 -0.0127 -0.0029 -0.0207  0.0034  6296.50  37.47  16.72  \n",
       "2025-08-31  0.0488  0.0442 -0.0067  0.0208  0.0038  6408.95  37.85    NaN  \n",
       "2025-09-30 -0.0218 -0.0105 -0.0203 -0.0222  0.0033  6584.02  38.58  16.28  \n",
       "2025-10-31 -0.0131 -0.0309 -0.0522 -0.0403  0.0037  6735.69  39.21  17.44  \n",
       "2025-11-30  0.0147  0.0376  0.0143  0.0068  0.0030  6740.89  39.12    NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all data\n",
    "combined_df = pd.concat([\n",
    "    macro_data,\n",
    "    ff_adj,\n",
    "    sp500_cape_m,\n",
    "    vix_data.rename(columns={\"VIXCLS\": \"VIX\"})\n",
    "], axis=1)\n",
    "\n",
    "# Drop rows with too many missing values\n",
    "combined_df = combined_df.dropna(thresh=len(combined_df.columns) * 0.7)\n",
    "combined_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49c792-4a83-431c-bdcf-2cd4c7cee92c",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "### 4.1 Construct Liquidity Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "65679d83-092e-480d-b60d-e49d71222d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Feature engineering complete\n",
      "\n",
      "New features created:\n",
      "  - M2_growth\n",
      "  - FED_BS_growth\n",
      "  - Term_Spread\n",
      "  - Credit_Spread\n",
      "  - CPI_inflation\n",
      "  - log_CAPE\n",
      "  - CAPE_zscore\n",
      "  - Risk_Adj_Spread\n",
      "  - Net_Liq\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M2</th>\n",
       "      <th>FED_BS</th>\n",
       "      <th>FFR</th>\n",
       "      <th>Real_Rate</th>\n",
       "      <th>T3M</th>\n",
       "      <th>T10Y</th>\n",
       "      <th>BAA</th>\n",
       "      <th>AAA</th>\n",
       "      <th>CPI</th>\n",
       "      <th>CORE_CPI</th>\n",
       "      <th>...</th>\n",
       "      <th>VIX</th>\n",
       "      <th>M2_growth</th>\n",
       "      <th>FED_BS_growth</th>\n",
       "      <th>Term_Spread</th>\n",
       "      <th>Credit_Spread</th>\n",
       "      <th>CPI_inflation</th>\n",
       "      <th>log_CAPE</th>\n",
       "      <th>CAPE_zscore</th>\n",
       "      <th>Risk_Adj_Spread</th>\n",
       "      <th>Net_Liq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-07-31</th>\n",
       "      <td>22028.7</td>\n",
       "      <td>6642578.0</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.838904</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.37</td>\n",
       "      <td>6.04</td>\n",
       "      <td>5.41</td>\n",
       "      <td>322.132</td>\n",
       "      <td>328.656</td>\n",
       "      <td>...</td>\n",
       "      <td>16.72</td>\n",
       "      <td>4.480153</td>\n",
       "      <td>-7.464249</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.63</td>\n",
       "      <td>2.731801</td>\n",
       "      <td>3.623541</td>\n",
       "      <td>1.242869</td>\n",
       "      <td>0.239234</td>\n",
       "      <td>6271856.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-31</th>\n",
       "      <td>22108.3</td>\n",
       "      <td>6603384.0</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1.523005</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.23</td>\n",
       "      <td>6.03</td>\n",
       "      <td>5.42</td>\n",
       "      <td>323.364</td>\n",
       "      <td>329.793</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.427283</td>\n",
       "      <td>-7.298001</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.61</td>\n",
       "      <td>2.939220</td>\n",
       "      <td>3.633631</td>\n",
       "      <td>1.306865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6013308.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-30</th>\n",
       "      <td>22212.4</td>\n",
       "      <td>6608395.0</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1.149868</td>\n",
       "      <td>3.86</td>\n",
       "      <td>4.16</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.22</td>\n",
       "      <td>324.368</td>\n",
       "      <td>330.542</td>\n",
       "      <td>...</td>\n",
       "      <td>16.28</td>\n",
       "      <td>4.492553</td>\n",
       "      <td>-6.661865</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.61</td>\n",
       "      <td>3.022700</td>\n",
       "      <td>3.652734</td>\n",
       "      <td>1.461587</td>\n",
       "      <td>0.429975</td>\n",
       "      <td>5803489.929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-31</th>\n",
       "      <td>22298.0</td>\n",
       "      <td>6587034.0</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.973381</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.11</td>\n",
       "      <td>5.80</td>\n",
       "      <td>5.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17.44</td>\n",
       "      <td>4.645651</td>\n",
       "      <td>-6.080511</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.58</td>\n",
       "      <td>2.789925</td>\n",
       "      <td>3.668932</td>\n",
       "      <td>1.575675</td>\n",
       "      <td>1.433486</td>\n",
       "      <td>5628992.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-30</th>\n",
       "      <td>22322.4</td>\n",
       "      <td>6552419.0</td>\n",
       "      <td>3.89</td>\n",
       "      <td>1.001027</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.80</td>\n",
       "      <td>5.18</td>\n",
       "      <td>325.031</td>\n",
       "      <td>331.068</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.271300</td>\n",
       "      <td>-5.108093</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2.711969</td>\n",
       "      <td>3.666634</td>\n",
       "      <td>1.490940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5649017.439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 M2     FED_BS   FFR  Real_Rate   T3M  T10Y   BAA   AAA  \\\n",
       "2025-07-31  22028.7  6642578.0  4.33   0.838904  4.24  4.37  6.04  5.41   \n",
       "2025-08-31  22108.3  6603384.0  4.33   1.523005  4.05  4.23  6.03  5.42   \n",
       "2025-09-30  22212.4  6608395.0  4.09   1.149868  3.86  4.16  5.83  5.22   \n",
       "2025-10-31  22298.0  6587034.0  3.86   0.973381  3.73  4.11  5.80  5.22   \n",
       "2025-11-30  22322.4  6552419.0  3.89   1.001027  3.73  4.02  5.80  5.18   \n",
       "\n",
       "                CPI  CORE_CPI  ...    VIX  M2_growth  FED_BS_growth  \\\n",
       "2025-07-31  322.132   328.656  ...  16.72   4.480153      -7.464249   \n",
       "2025-08-31  323.364   329.793  ...    NaN   4.427283      -7.298001   \n",
       "2025-09-30  324.368   330.542  ...  16.28   4.492553      -6.661865   \n",
       "2025-10-31      NaN       NaN  ...  17.44   4.645651      -6.080511   \n",
       "2025-11-30  325.031   331.068  ...    NaN   4.271300      -5.108093   \n",
       "\n",
       "            Term_Spread  Credit_Spread  CPI_inflation  log_CAPE  CAPE_zscore  \\\n",
       "2025-07-31         0.04           0.63       2.731801  3.623541     1.242869   \n",
       "2025-08-31        -0.10           0.61       2.939220  3.633631     1.306865   \n",
       "2025-09-30         0.07           0.61       3.022700  3.652734     1.461587   \n",
       "2025-10-31         0.25           0.58       2.789925  3.668932     1.575675   \n",
       "2025-11-30         0.13           0.62       2.711969  3.666634     1.490940   \n",
       "\n",
       "            Risk_Adj_Spread      Net_Liq  \n",
       "2025-07-31         0.239234  6271856.555  \n",
       "2025-08-31              NaN  6013308.102  \n",
       "2025-09-30         0.429975  5803489.929  \n",
       "2025-10-31         1.433486  5628992.198  \n",
       "2025-11-30              NaN  5649017.439  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def engineer_liquidity_features(df):\n",
    "    \"\"\"\n",
    "    Create liquidity-related features from raw macro data\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Growth rates\n",
    "    df['M2_growth'] = df['M2'].pct_change(12) * 100  # YoY % change\n",
    "    df['FED_BS_growth'] = df['FED_BS'].pct_change(12) * 100\n",
    "    \n",
    "    # 2. Term spread\n",
    "    df['Term_Spread'] = df['T10Y'] - df['FFR']\n",
    "    \n",
    "    # 3. Credit spread\n",
    "    df['Credit_Spread'] = df['BAA'] - df['AAA']\n",
    "    \n",
    "    # 4. Real interest rate (ex-post using CPI)\n",
    "    df['CPI_inflation'] = df['CPI'].pct_change(12) * 100  # YoY inflation\n",
    "    #df['Real_Rate'] = df['FFR'] - df['CPI_inflation']\n",
    "    \n",
    "    # 5. Valuation metrics\n",
    "    df['log_CAPE'] = np.log(df['CAPE'])\n",
    "    df['CAPE_zscore'] = (df['CAPE'] - df['CAPE'].rolling(60).mean()) / df['CAPE'].rolling(60).std()\n",
    "    \n",
    "    # 6. Risk-adjusted term spread\n",
    "    df['Risk_Adj_Spread'] = df['Term_Spread'] / (df['VIX'] / 100)\n",
    "\n",
    "    #7 Net liquidity\n",
    "    df['Net_Liq'] = df['FED_BS'] - (df['TGA'] + df['RRP'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df_features = engineer_liquidity_features(combined_df)\n",
    "\n",
    "print(\"✅ Feature engineering complete\")\n",
    "print(f\"\\nNew features created:\")\n",
    "new_cols = [c for c in df_features.columns if c not in combined_df.columns]\n",
    "for col in new_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "df_features.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88629d42-1d4c-46b1-bdb4-78f5868c492b",
   "metadata": {},
   "source": [
    "### 4.2 Construct Valuation Spread Measure\n",
    "\n",
    "We define valuation spread as the return differential between cheap (high B/M) and expensive (low B/M) stocks.\n",
    "This is proxied by the HML (High Minus Low) factor from Fama-French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8ea77c44-03b9-475c-a830-eef5db04dea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Valuation spread measures constructed\n",
      "\n",
      "V_spread (HML 12m) stats:\n",
      "count    420.000000\n",
      "mean       0.018185\n",
      "std        0.141636\n",
      "min       -0.417500\n",
      "25%       -0.078100\n",
      "50%        0.016600\n",
      "75%        0.095125\n",
      "max        0.579800\n",
      "Name: V_spread, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M2</th>\n",
       "      <th>FED_BS</th>\n",
       "      <th>FFR</th>\n",
       "      <th>Real_Rate</th>\n",
       "      <th>T3M</th>\n",
       "      <th>T10Y</th>\n",
       "      <th>BAA</th>\n",
       "      <th>AAA</th>\n",
       "      <th>CPI</th>\n",
       "      <th>CORE_CPI</th>\n",
       "      <th>...</th>\n",
       "      <th>CPI_inflation</th>\n",
       "      <th>log_CAPE</th>\n",
       "      <th>CAPE_zscore</th>\n",
       "      <th>Risk_Adj_Spread</th>\n",
       "      <th>Net_Liq</th>\n",
       "      <th>HML_cumret</th>\n",
       "      <th>HML_12m</th>\n",
       "      <th>V_spread</th>\n",
       "      <th>V_spread_z</th>\n",
       "      <th>V_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-07-31</th>\n",
       "      <td>22028.7</td>\n",
       "      <td>6642578.0</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.838904</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.37</td>\n",
       "      <td>6.04</td>\n",
       "      <td>5.41</td>\n",
       "      <td>322.132</td>\n",
       "      <td>328.656</td>\n",
       "      <td>...</td>\n",
       "      <td>2.731801</td>\n",
       "      <td>3.623541</td>\n",
       "      <td>1.242869</td>\n",
       "      <td>0.239234</td>\n",
       "      <td>6271856.555</td>\n",
       "      <td>1.005492</td>\n",
       "      <td>-0.0558</td>\n",
       "      <td>-0.0558</td>\n",
       "      <td>-0.522355</td>\n",
       "      <td>1.242869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-08-31</th>\n",
       "      <td>22108.3</td>\n",
       "      <td>6603384.0</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1.523005</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.23</td>\n",
       "      <td>6.03</td>\n",
       "      <td>5.42</td>\n",
       "      <td>323.364</td>\n",
       "      <td>329.793</td>\n",
       "      <td>...</td>\n",
       "      <td>2.939220</td>\n",
       "      <td>3.633631</td>\n",
       "      <td>1.306865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6013308.102</td>\n",
       "      <td>1.005936</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.132625</td>\n",
       "      <td>1.306865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-30</th>\n",
       "      <td>22212.4</td>\n",
       "      <td>6608395.0</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1.149868</td>\n",
       "      <td>3.86</td>\n",
       "      <td>4.16</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.22</td>\n",
       "      <td>324.368</td>\n",
       "      <td>330.542</td>\n",
       "      <td>...</td>\n",
       "      <td>3.022700</td>\n",
       "      <td>3.652734</td>\n",
       "      <td>1.461587</td>\n",
       "      <td>0.429975</td>\n",
       "      <td>5803489.929</td>\n",
       "      <td>1.005831</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>-0.011187</td>\n",
       "      <td>1.461587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-10-31</th>\n",
       "      <td>22298.0</td>\n",
       "      <td>6587034.0</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.973381</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.11</td>\n",
       "      <td>5.80</td>\n",
       "      <td>5.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.789925</td>\n",
       "      <td>3.668932</td>\n",
       "      <td>1.575675</td>\n",
       "      <td>1.433486</td>\n",
       "      <td>5628992.198</td>\n",
       "      <td>1.005520</td>\n",
       "      <td>-0.0229</td>\n",
       "      <td>-0.0229</td>\n",
       "      <td>-0.290070</td>\n",
       "      <td>1.575675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-11-30</th>\n",
       "      <td>22322.4</td>\n",
       "      <td>6552419.0</td>\n",
       "      <td>3.89</td>\n",
       "      <td>1.001027</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.80</td>\n",
       "      <td>5.18</td>\n",
       "      <td>325.031</td>\n",
       "      <td>331.068</td>\n",
       "      <td>...</td>\n",
       "      <td>2.711969</td>\n",
       "      <td>3.666634</td>\n",
       "      <td>1.490940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5649017.439</td>\n",
       "      <td>1.005898</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>-0.035192</td>\n",
       "      <td>1.490940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 M2     FED_BS   FFR  Real_Rate   T3M  T10Y   BAA   AAA  \\\n",
       "2025-07-31  22028.7  6642578.0  4.33   0.838904  4.24  4.37  6.04  5.41   \n",
       "2025-08-31  22108.3  6603384.0  4.33   1.523005  4.05  4.23  6.03  5.42   \n",
       "2025-09-30  22212.4  6608395.0  4.09   1.149868  3.86  4.16  5.83  5.22   \n",
       "2025-10-31  22298.0  6587034.0  3.86   0.973381  3.73  4.11  5.80  5.22   \n",
       "2025-11-30  22322.4  6552419.0  3.89   1.001027  3.73  4.02  5.80  5.18   \n",
       "\n",
       "                CPI  CORE_CPI  ...  CPI_inflation  log_CAPE  CAPE_zscore  \\\n",
       "2025-07-31  322.132   328.656  ...       2.731801  3.623541     1.242869   \n",
       "2025-08-31  323.364   329.793  ...       2.939220  3.633631     1.306865   \n",
       "2025-09-30  324.368   330.542  ...       3.022700  3.652734     1.461587   \n",
       "2025-10-31      NaN       NaN  ...       2.789925  3.668932     1.575675   \n",
       "2025-11-30  325.031   331.068  ...       2.711969  3.666634     1.490940   \n",
       "\n",
       "            Risk_Adj_Spread      Net_Liq  HML_cumret  HML_12m  V_spread  \\\n",
       "2025-07-31         0.239234  6271856.555    1.005492  -0.0558   -0.0558   \n",
       "2025-08-31              NaN  6013308.102    1.005936  -0.0006   -0.0006   \n",
       "2025-09-30         0.429975  5803489.929    1.005831   0.0166    0.0166   \n",
       "2025-10-31         1.433486  5628992.198    1.005520  -0.0229   -0.0229   \n",
       "2025-11-30              NaN  5649017.439    1.005898   0.0132    0.0132   \n",
       "\n",
       "            V_spread_z   V_level  \n",
       "2025-07-31   -0.522355  1.242869  \n",
       "2025-08-31   -0.132625  1.306865  \n",
       "2025-09-30   -0.011187  1.461587  \n",
       "2025-10-31   -0.290070  1.575675  \n",
       "2025-11-30   -0.035192  1.490940  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_valuation_spread(df):\n",
    "    \"\"\"\n",
    "    Construct valuation spread measure\n",
    "    \n",
    "    Primary measure: HML (Fama-French) represents value premium\n",
    "    Secondary: CAPE z-score as market-level valuation\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Cumulative HML performance (value vs growth)\n",
    "    df['HML_cumret'] = (1 + df['hml'] / 100).cumprod()\n",
    "    \n",
    "    # Rolling 12-month HML performance\n",
    "    df['HML_12m'] = df['hml'].rolling(12).sum()\n",
    "    \n",
    "    # Define valuation spread as:\n",
    "    # Positive = cheap stocks outperforming (value regime)\n",
    "    # Negative = expensive stocks outperforming (growth regime)\n",
    "    df['V_spread'] = df['HML_12m']  # 12-month rolling HML\n",
    "    \n",
    "    # Standardize\n",
    "    df['V_spread_z'] = (df['V_spread'] - df['V_spread'].mean()) / df['V_spread'].std()\n",
    "    \n",
    "    # Also use CAPE z-score as alternative valuation measure\n",
    "    df['V_level'] = df['CAPE_zscore']\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_features = construct_valuation_spread(df_features)\n",
    "\n",
    "print(\"✅ Valuation spread measures constructed\")\n",
    "print(f\"\\nV_spread (HML 12m) stats:\")\n",
    "print(df_features['V_spread'].describe())\n",
    "df_features.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56699b84-92aa-4d4a-977f-354454b2ab5a",
   "metadata": {},
   "source": [
    "## 5. Liquidity Index Construction via Sparse PCA\n",
    "\n",
    "### 5.1 Methodology\n",
    "\n",
    "We construct a composite liquidity index using Sparse PCA (Zou et al. 2006, Witten et al. 2009) which:\n",
    "\n",
    "1. Maximizes explained variance like standard PCA\n",
    "2. Enforces sparsity via L1 penalty for interpretability\n",
    "3. Selects only the most relevant liquidity indicators\n",
    "\n",
    "**Liquidity proxies:**\n",
    "- Fed balance sheet growth (QE intensity)\n",
    "- Real interest rates (opportunity cost)\n",
    "- Term spread (yield curve steepness)\n",
    "- Credit spreads (credit conditions)\n",
    "- VIX (risk appetite)\n",
    "\n",
    "All variables are sign-adjusted so higher values = easier liquidity conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dbf3243a-d1ae-4c4d-92d9-83a454c1e0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M2</th>\n",
       "      <th>FED_BS</th>\n",
       "      <th>FFR</th>\n",
       "      <th>Real_Rate</th>\n",
       "      <th>T3M</th>\n",
       "      <th>T10Y</th>\n",
       "      <th>BAA</th>\n",
       "      <th>AAA</th>\n",
       "      <th>CPI</th>\n",
       "      <th>CORE_CPI</th>\n",
       "      <th>...</th>\n",
       "      <th>Credit_Spread</th>\n",
       "      <th>CPI_inflation</th>\n",
       "      <th>log_CAPE</th>\n",
       "      <th>CAPE_zscore</th>\n",
       "      <th>Risk_Adj_Spread</th>\n",
       "      <th>HML_cumret</th>\n",
       "      <th>HML_12m</th>\n",
       "      <th>V_spread</th>\n",
       "      <th>V_spread_z</th>\n",
       "      <th>V_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-11-30</th>\n",
       "      <td>22322.4</td>\n",
       "      <td>6552419.0</td>\n",
       "      <td>3.89</td>\n",
       "      <td>1.001027</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.18</td>\n",
       "      <td>325.031</td>\n",
       "      <td>331.068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2.711969</td>\n",
       "      <td>3.666634</td>\n",
       "      <td>1.49094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.005898</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>-0.035192</td>\n",
       "      <td>1.49094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 M2     FED_BS   FFR  Real_Rate   T3M  T10Y  BAA   AAA  \\\n",
       "2025-11-30  22322.4  6552419.0  3.89   1.001027  3.73  4.02  5.8  5.18   \n",
       "\n",
       "                CPI  CORE_CPI  ...  Credit_Spread  CPI_inflation  log_CAPE  \\\n",
       "2025-11-30  325.031   331.068  ...           0.62       2.711969  3.666634   \n",
       "\n",
       "            CAPE_zscore  Risk_Adj_Spread  HML_cumret  HML_12m  V_spread  \\\n",
       "2025-11-30      1.49094              NaN    1.005898   0.0132    0.0132   \n",
       "\n",
       "            V_spread_z  V_level  \n",
       "2025-11-30   -0.035192  1.49094  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9baca787-f092-4c5b-a15f-181d063b9599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Liquidity Index Construction (SPARSE)\n",
      "======================================================================\n",
      "\n",
      "Variance explained: 44.98%\n",
      "\n",
      "Loadings:\n",
      "                    PC1\n",
      "FED_BS_growth  0.525122\n",
      "M2_growth      0.379190\n",
      "Term_Spread    0.370752\n",
      "Real_Rate      0.244985\n",
      "Credit_Spread -0.414407\n",
      "VIX           -0.459623\n",
      "\n",
      "Liquidity index range: -3.15 to 7.11\n",
      "Mean: 0.00, Std: 1.65\n"
     ]
    }
   ],
   "source": [
    "def build_liquidity_index(df, method='sparse', n_components=1, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Build liquidity index using PCA or Sparse PCA\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input data\n",
    "    method : str\n",
    "        'pca' or 'sparse'\n",
    "    n_components : int\n",
    "        Number of components\n",
    "    alpha : float\n",
    "        Sparsity penalty (only for sparse PCA)\n",
    "    \"\"\"\n",
    "    # Select liquidity variables\n",
    "    liq_vars = [\n",
    "        'M2_growth',\n",
    "        'FED_BS_growth',\n",
    "        'Term_Spread',\n",
    "        'Real_Rate',\n",
    "        'Credit_Spread',\n",
    "        'VIX'\n",
    "    ]\n",
    "    \n",
    "    # Extract data and drop NaN\n",
    "    X = df[liq_vars].copy()\n",
    "    X = X.dropna()\n",
    "    \n",
    "    # Sign adjustments: higher = easier liquidity\n",
    "    X['Real_Rate'] = -X['Real_Rate']  # Lower real rates = easier\n",
    "    X['Credit_Spread'] = -X['Credit_Spread']  # Tighter spreads = easier\n",
    "    X['VIX'] = -X['VIX']  # Lower VIX = easier\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n",
    "    \n",
    "    # Fit PCA/Sparse PCA\n",
    "    if method == 'sparse':\n",
    "        model = SparsePCA(n_components=n_components, alpha=alpha, random_state=42)\n",
    "    else:\n",
    "        model = PCA(n_components=n_components)\n",
    "    \n",
    "    # Fit and transform\n",
    "    L = model.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create series\n",
    "    L_series = pd.Series(L[:, 0], index=X.index, name='L')\n",
    "    \n",
    "    # Get loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        model.components_.T,\n",
    "        index=X.columns,\n",
    "        columns=['PC1']\n",
    "    )\n",
    "    \n",
    "    # Calculate explained variance\n",
    "    if method == 'pca':\n",
    "        explained_var = model.explained_variance_ratio_[0]\n",
    "    else:\n",
    "        # For sparse PCA, compute manually\n",
    "        var_explained = np.var(L[:, 0]) / np.sum(np.var(X_scaled, axis=0))\n",
    "        explained_var = var_explained\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Liquidity Index Construction ({method.upper()})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nVariance explained: {explained_var:.2%}\")\n",
    "    print(f\"\\nLoadings:\")\n",
    "    print(loadings.sort_values('PC1', ascending=False))\n",
    "    print(f\"\\nLiquidity index range: {L_series.min():.2f} to {L_series.max():.2f}\")\n",
    "    print(f\"Mean: {L_series.mean():.2f}, Std: {L_series.std():.2f}\")\n",
    "    \n",
    "    return L_series, loadings, model, X_scaled_df\n",
    "\n",
    "# Build liquidity index\n",
    "L_index, loadings, pca_model, scaled_features = build_liquidity_index(\n",
    "    df_features,\n",
    "    method='sparse',\n",
    "    alpha=0.5  # Moderate sparsity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999399c6-112c-4e65-bd0d-f879d68296b0",
   "metadata": {},
   "source": [
    "#### Liquidity Index Augmentation: Two-layer liquidity: Flow vs Stock / Excess\n",
    "\n",
    "#### Add level / excess variables\n",
    "\n",
    "Examples (all can be pulled from FRED):\n",
    "\n",
    "**Log level vs pre-2008 trend**\n",
    "\n",
    "Let $m_t = \\log M2_t$. Fit a linear trend on a pre-QE baseline (say 1985–2007):\n",
    "\n",
    "$m_t \\approx a + bt \\quad (t \\le 2007)$\n",
    "\n",
    "Then define excess money stock:\n",
    "\n",
    "$EM_t = m_t - (a + bt)$\n",
    "\n",
    "After 2008, $EM_t$ becomes increasingly positive if M2 grows above its historical trend.\n",
    "\n",
    "Similarly for the Fed Balance Sheet, let $b_t = \\log(\\text{FedBal}_t)$:\n",
    "\n",
    "$EB_t = b_t - (\\alpha + \\beta t)$  (pre-2007 trend)\n",
    "\n",
    "---\n",
    "\n",
    "**Excess liquidity versus real economy**\n",
    "\n",
    "Use real GDP series (e.g., GDPC1) and define:\n",
    "\n",
    "$EL_t = \\log M2_t - \\log GDP_t$\n",
    "\n",
    "Or measure multi-year excess growth:\n",
    "\n",
    "$EL_t(3y) = \\log M2_t - \\log M2_{t-36} - (\\log GDP_t - \\log GDP_{t-36})$\n",
    "\n",
    "---\n",
    "\n",
    "**ZIRP / negative real-rate indicator**\n",
    "\n",
    "$D_t^{ZIRP} = 1(r_t^{real} < 0)$\n",
    "\n",
    "From 2009–2015, this should be mostly 1.\n",
    "\n",
    "---\n",
    "\n",
    "#### Build an augmented feature vector\n",
    "\n",
    "Instead of only:\n",
    "\n",
    "$x_t = \\{ \\Delta \\log M2_t,\\; \\Delta \\log FedBal_t,\\; TS_t,\\; r_t^{real},\\; CS_t \\}$\n",
    "\n",
    "Let's expand to:\n",
    "\n",
    "$x_t^{aug} = \\{ \n",
    "\\Delta \\log M2_t,\\;\n",
    "\\Delta \\log FedBal_t,\\;\n",
    "TS_t,\\;\n",
    "r_t^{real},\\;\n",
    "CS_t,\\;\n",
    "EM_t,\\;\n",
    "EB_t,\\;\n",
    "EL_t,\\;\n",
    "EL_t(3y),\\;\n",
    "D_t^{ZIRP}\n",
    "\\}$\n",
    "\n",
    "Then standardize and perform PCA / SparsePCA on this.\n",
    "\n",
    "\n",
    "- **PC1** (or a combination) loading heavily on $EM$, $EB$, $EL$, $D_t^{ZIRP}$ → structural easy-money regime  \n",
    "- **PC2** (or your current PC1) loading on short-horizon changes → flow / shock liquidity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "71c92649-812b-4970-92dd-78833fb3b67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['M2', 'FED_BS', 'FFR', 'Real_Rate', 'T3M', 'T10Y', 'BAA', 'AAA', 'CPI',\n",
       "       'CORE_CPI', 'GDP', 'IORB', 'RRP', 'TGA', 'Bank_Reserves', 'CI_Loans',\n",
       "       'Consumer_Credit', 'Mortgage_Rate', 'Prime_Rate', 'STLFSI',\n",
       "       'Dollar_Index', 'Household_Debt_Service', 'mkt_excess', 'smb', 'hml',\n",
       "       'rmw', 'cma', 'rf', 'P', 'CAPE', 'VIX', 'M2_growth', 'FED_BS_growth',\n",
       "       'Term_Spread', 'Credit_Spread', 'CPI_inflation', 'log_CAPE',\n",
       "       'CAPE_zscore', 'Risk_Adj_Spread', 'Net_Liq', 'HML_cumret', 'HML_12m',\n",
       "       'V_spread', 'V_spread_z', 'V_level'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac411b0c-a6ef-4984-92ad-23143b1a97a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M2</th>\n",
       "      <th>FED_BS</th>\n",
       "      <th>FFR</th>\n",
       "      <th>Real_Rate</th>\n",
       "      <th>T3M</th>\n",
       "      <th>T10Y</th>\n",
       "      <th>BAA</th>\n",
       "      <th>AAA</th>\n",
       "      <th>CPI</th>\n",
       "      <th>CORE_CPI</th>\n",
       "      <th>...</th>\n",
       "      <th>V_spread_z</th>\n",
       "      <th>V_level</th>\n",
       "      <th>dlog_M2</th>\n",
       "      <th>dlog_FED_BS</th>\n",
       "      <th>log_M2</th>\n",
       "      <th>log_FED_BS</th>\n",
       "      <th>EM</th>\n",
       "      <th>EB</th>\n",
       "      <th>log_GDP</th>\n",
       "      <th>EL_3y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-11-30</th>\n",
       "      <td>22322.4</td>\n",
       "      <td>6552419.0</td>\n",
       "      <td>3.89</td>\n",
       "      <td>1.001027</td>\n",
       "      <td>3.73</td>\n",
       "      <td>4.02</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.18</td>\n",
       "      <td>325.031</td>\n",
       "      <td>331.068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035192</td>\n",
       "      <td>1.49094</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>-0.005269</td>\n",
       "      <td>10.013346</td>\n",
       "      <td>15.695345</td>\n",
       "      <td>0.19326</td>\n",
       "      <td>0.685233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 M2     FED_BS   FFR  Real_Rate   T3M  T10Y  BAA   AAA  \\\n",
       "2025-11-30  22322.4  6552419.0  3.89   1.001027  3.73  4.02  5.8  5.18   \n",
       "\n",
       "                CPI  CORE_CPI  ...  V_spread_z  V_level   dlog_M2  \\\n",
       "2025-11-30  325.031   331.068  ...   -0.035192  1.49094  0.001094   \n",
       "\n",
       "            dlog_FED_BS     log_M2  log_FED_BS       EM        EB  log_GDP  \\\n",
       "2025-11-30    -0.005269  10.013346   15.695345  0.19326  0.685233      NaN   \n",
       "\n",
       "            EL_3y  \n",
       "2025-11-30    NaN  \n",
       "\n",
       "[1 rows x 53 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def augment_liquidity_index(df_features):\n",
    "    df = df_features.copy()\n",
    "\n",
    "    # Flow proxies\n",
    "    df[\"dlog_M2\"]      = np.log(df[\"M2\"]).diff()\n",
    "    df[\"dlog_FED_BS\"]  = np.log(df[\"FED_BS\"]).diff()\n",
    "\n",
    "    # Levels\n",
    "    df[\"log_M2\"]      = np.log(df[\"M2\"])\n",
    "    df[\"log_FED_BS\"] = np.log(df[\"FED_BS\"])\n",
    "\n",
    "    # Use data up to 2007-12-31 to fit trends\n",
    "    pre = df.loc[: \"2007-12-31\"].copy()\n",
    "\n",
    "    # --- Trend for M2 ---\n",
    "    pre_m2 = pre[\"log_M2\"].dropna()\n",
    "    t_m2   = np.arange(len(pre_m2))\n",
    "    coefs_M2 = np.polyfit(t_m2, pre_m2.values, deg=1)\n",
    "\n",
    "    t_full = np.arange(len(df))\n",
    "    trend_M2 = np.polyval(coefs_M2, t_full)\n",
    "    df[\"EM\"] = df[\"log_M2\"] - trend_M2\n",
    "\n",
    "    # --- Trend for Fed balance sheet ---\n",
    "    pre_fb = pre[\"log_FED_BS\"].dropna()\n",
    "    t_fb   = np.arange(len(pre_fb))\n",
    "    coefs_FB = np.polyfit(t_fb, pre_fb.values, deg=1)\n",
    "\n",
    "    trend_FB = np.polyval(coefs_FB, t_full)\n",
    "    df[\"EB\"] = df[\"log_FED_BS\"] - trend_FB\n",
    "\n",
    "    # 3. Excess liquidity vs GDP over 3y\n",
    "    df[\"log_GDP\"] = np.log(df[\"GDP\"])\n",
    "    df[\"EL_3y\"] = (df[\"log_M2\"] - df[\"log_M2\"].shift(36)) - \\\n",
    "                  (df[\"log_GDP\"] - df[\"log_GDP\"].shift(36))\n",
    "\n",
    "    return df\n",
    "\n",
    "df_features_aug = augment_liquidity_index(df_features)\n",
    "df_features_aug.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c1f00771-299b-4f74-b39e-c96073925ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Liquidity Index Construction (SPARSE)\n",
      "======================================================================\n",
      "\n",
      "Variance explained: 37.40%\n",
      "\n",
      "Loadings:\n",
      "                    PC1\n",
      "EL_3y          0.479785\n",
      "FED_BS_growth  0.414107\n",
      "M2_growth      0.386169\n",
      "Term_Spread    0.365477\n",
      "Real_Rate      0.348663\n",
      "EB             0.135796\n",
      "EM             0.091370\n",
      "Credit_Spread -0.242676\n",
      "VIX           -0.329204\n",
      "\n",
      "Liquidity index range: -4.23 to 4.97\n",
      "Mean: -0.00, Std: 1.84\n"
     ]
    }
   ],
   "source": [
    "def build_liquidity_index_aug(df, method='sparse', n_components=1, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Build liquidity index using PCA or Sparse PCA\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input data\n",
    "    method : str\n",
    "        'pca' or 'sparse'\n",
    "    n_components : int\n",
    "        Number of components\n",
    "    alpha : float\n",
    "        Sparsity penalty (only for sparse PCA)\n",
    "    \"\"\"\n",
    "    # Select liquidity variables\n",
    "    liq_vars = [\n",
    "        'M2_growth',\n",
    "        'FED_BS_growth',\n",
    "        'EM',\n",
    "        'EB',\n",
    "        'EL_3y',\n",
    "        'Term_Spread',\n",
    "        'Real_Rate',\n",
    "        'Credit_Spread',\n",
    "        'VIX'\n",
    "    ]\n",
    "    \n",
    "    # Extract data and drop NaN\n",
    "    X = df[liq_vars].copy()\n",
    "    X = X.dropna()\n",
    "    \n",
    "    # Sign adjustments: higher = easier liquidity\n",
    "    X['Real_Rate'] = -X['Real_Rate']  # Lower real rates = easier\n",
    "    X['Credit_Spread'] = -X['Credit_Spread']  # Tighter spreads = easier\n",
    "    X['VIX'] = -X['VIX']  # Lower VIX = easier\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n",
    "    \n",
    "    # Fit PCA/Sparse PCA\n",
    "    if method == 'sparse':\n",
    "        model = SparsePCA(n_components=n_components, alpha=alpha, random_state=42)\n",
    "    else:\n",
    "        model = PCA(n_components=n_components)\n",
    "    \n",
    "    # Fit and transform\n",
    "    L = model.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create series\n",
    "    L_series = pd.Series(L[:, 0], index=X.index, name='L')\n",
    "    \n",
    "    # Get loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        model.components_.T,\n",
    "        index=X.columns,\n",
    "        columns=['PC1']\n",
    "    )\n",
    "    \n",
    "    # Calculate explained variance\n",
    "    if method == 'pca':\n",
    "        explained_var = model.explained_variance_ratio_[0]\n",
    "    else:\n",
    "        # For sparse PCA, compute manually\n",
    "        var_explained = np.var(L[:, 0]) / np.sum(np.var(X_scaled, axis=0))\n",
    "        explained_var = var_explained\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Liquidity Index Construction ({method.upper()})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nVariance explained: {explained_var:.2%}\")\n",
    "    print(f\"\\nLoadings:\")\n",
    "    print(loadings.sort_values('PC1', ascending=False))\n",
    "    print(f\"\\nLiquidity index range: {L_series.min():.2f} to {L_series.max():.2f}\")\n",
    "    print(f\"Mean: {L_series.mean():.2f}, Std: {L_series.std():.2f}\")\n",
    "    \n",
    "    return L_series, loadings, model, X_scaled_df\n",
    "\n",
    "# Build liquidity index\n",
    "L_index, loadings, pca_model, scaled_features = build_liquidity_index_aug(\n",
    "    df_features_aug,\n",
    "    method='sparse',\n",
    "    alpha=0.5  # Moderate sparsity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb212392-193c-4382-9abd-0166bcd04878",
   "metadata": {},
   "source": [
    "### Explained variance is too small = ~45%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "42d3b88a-a9bf-4fc0-9b6d-955956682e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "➖ Flipping (higher = tighter):\n",
      "   Real_Rate                : Higher real rates = tighter → FLIP\n",
      "   Credit_Spread            : Wider spreads = tighter → FLIP\n",
      "   VIX                      : Higher VIX = tighter → FLIP\n",
      "\n",
      "✅ Keeping (higher = easier):\n",
      "   M2_growth                : Higher growth = easier → KEEP\n",
      "   FED_BS_growth            : Higher growth = easier → KEEP\n",
      "   EL_3y                    : Higher excess liquidity = easier → KEEP\n",
      "   Term_Spread              : Positive slope = easier → KEEP\n",
      "\n",
      "======================================================================\n",
      "Liquidity Index Construction (SPARSE)\n",
      "======================================================================\n",
      "\n",
      "Variance explained: 47.21%\n",
      "\n",
      "Loadings:\n",
      "                    PC1\n",
      "EL_3y          0.472608\n",
      "FED_BS_growth  0.428525\n",
      "Term_Spread    0.385863\n",
      "M2_growth      0.381727\n",
      "Real_Rate      0.334564\n",
      "Credit_Spread -0.269403\n",
      "VIX           -0.337477\n",
      "\n",
      "Liquidity index range: -4.46 to 5.37\n",
      "Mean: -0.00, Std: 1.82\n"
     ]
    }
   ],
   "source": [
    "def build_liquidity_index_aug_1(df, method='sparse', n_components=1, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Build liquidity index using PCA or Sparse PCA\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Input data\n",
    "    method : str\n",
    "        'pca' or 'sparse'\n",
    "    n_components : int\n",
    "        Number of components\n",
    "    alpha : float\n",
    "        Sparsity penalty (only for sparse PCA)\n",
    "    \"\"\"\n",
    "    # Select liquidity variables\n",
    "    # liq_vars = [\n",
    "    #     'Net_Liq',\n",
    "    #     'M2_growth',\n",
    "    #     'FED_BS_growth',\n",
    "    #     'EM',\n",
    "    #     'EB',\n",
    "    #     'EL_3y',\n",
    "    #     'Term_Spread',\n",
    "    #     'Real_Rate',\n",
    "    #     'Credit_Spread',\n",
    "    #     'VIX',\n",
    "    #     'Dollar_Index',\n",
    "    #     'Household_Debt_Service',\n",
    "    #     'STLFSI'\n",
    "    # ]\n",
    "    liq_vars = [\n",
    "        'M2_growth',\n",
    "        'FED_BS_growth',\n",
    "        'Term_Spread',\n",
    "        'Real_Rate',\n",
    "        'Credit_Spread',\n",
    "        'VIX',\n",
    "        'EL_3y'\n",
    "    ]\n",
    "    \n",
    "    # Extract data and drop NaN\n",
    "    X = df[liq_vars].copy()\n",
    "    X = X.dropna()\n",
    "    \n",
    "    # ========================================\n",
    "    # CORRECT SIGN ADJUSTMENTS\n",
    "    # Rule: Higher value = EASIER liquidity\n",
    "    # ========================================\n",
    "    \n",
    "    # Variables to FLIP (higher originally means tighter)\n",
    "    flip_vars = {\n",
    "        'Real_Rate': 'Higher real rates = tighter → FLIP',\n",
    "        'Credit_Spread': 'Wider spreads = tighter → FLIP',\n",
    "        'VIX': 'Higher VIX = tighter → FLIP',\n",
    "        'STLFSI': 'Higher stress = tighter → FLIP',\n",
    "        'Dollar_Index': 'Stronger dollar = tighter → FLIP',\n",
    "        'Household_Debt_Service': 'Higher burden = tighter → FLIP',\n",
    "    }\n",
    "    \n",
    "    # Variables to KEEP (higher already means easier)\n",
    "    keep_vars = {\n",
    "        'Net_Liq': 'Higher net liquidity = easier → KEEP',\n",
    "        'M2_growth': 'Higher growth = easier → KEEP',\n",
    "        'FED_BS_growth': 'Higher growth = easier → KEEP',\n",
    "        'EM': 'Higher excess M2 = easier → KEEP',\n",
    "        'EB': 'Higher excess Fed BS = easier → KEEP',\n",
    "        'EL_3y': 'Higher excess liquidity = easier → KEEP',\n",
    "        'Term_Spread': 'Positive slope = easier → KEEP',\n",
    "    }\n",
    "    \n",
    "    print(\"\\n➖ Flipping (higher = tighter):\")\n",
    "    for var, reason in flip_vars.items():\n",
    "        if var in X.columns:\n",
    "            X[var] = -X[var]\n",
    "            print(f\"   {var:25s}: {reason}\")\n",
    "    \n",
    "    print(\"\\n✅ Keeping (higher = easier):\")\n",
    "    for var, reason in keep_vars.items():\n",
    "        if var in X.columns:\n",
    "            print(f\"   {var:25s}: {reason}\")\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n",
    "    \n",
    "    # Fit PCA/Sparse PCA\n",
    "    if method == 'sparse':\n",
    "        model = SparsePCA(n_components=n_components, alpha=alpha, random_state=42)\n",
    "    else:\n",
    "        model = PCA(n_components=n_components)\n",
    "    \n",
    "    # Fit and transform\n",
    "    L = model.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create series\n",
    "    L_series = pd.Series(L[:, 0], index=X.index, name='L')\n",
    "    \n",
    "    # Get loadings\n",
    "    loadings = pd.DataFrame(\n",
    "        model.components_.T,\n",
    "        index=X.columns,\n",
    "        columns=['PC1']\n",
    "    )\n",
    "    \n",
    "    # Calculate explained variance\n",
    "    if method == 'pca':\n",
    "        explained_var = model.explained_variance_ratio_[0]\n",
    "    else:\n",
    "        # For sparse PCA, compute manually\n",
    "        var_explained = np.var(L[:, 0]) / np.sum(np.var(X_scaled, axis=0))\n",
    "        explained_var = var_explained\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Liquidity Index Construction ({method.upper()})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nVariance explained: {explained_var:.2%}\")\n",
    "    print(f\"\\nLoadings:\")\n",
    "    print(loadings.sort_values('PC1', ascending=False))\n",
    "    print(f\"\\nLiquidity index range: {L_series.min():.2f} to {L_series.max():.2f}\")\n",
    "    print(f\"Mean: {L_series.mean():.2f}, Std: {L_series.std():.2f}\")\n",
    "    \n",
    "    return L_series, loadings, model, X_scaled_df\n",
    "\n",
    "# Build liquidity index\n",
    "L_index, loadings, pca_model, scaled_features = build_liquidity_index_aug_1(\n",
    "    df_features_aug,\n",
    "    method='sparse',\n",
    "    alpha=0.5  # Moderate sparsity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b908b1d-1773-45bb-866c-af6c550e0f69",
   "metadata": {},
   "source": [
    "## 6. Regime Detection via Hidden Markov Models\n",
    "\n",
    "### 6.1 HMM Specification\n",
    "\n",
    "We employ a 3-state Gaussian HMM to identify:\n",
    "- **State 0 (Tight):** Low liquidity, restrictive monetary policy\n",
    "- **State 1 (Neutral):** Normal conditions\n",
    "- **State 2 (High):** Easy liquidity, expansionary policy\n",
    "\n",
    "The model assumes:\n",
    "$$L_t | s_t = k \\sim \\mathcal{N}(\\mu_k, \\sigma_k^2)$$\n",
    "\n",
    "With transition matrix capturing regime persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ee7343d6-08b5-408d-a088-6ad80c53c4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HMM Regime Detection (3 states)\n",
      "======================================================================\n",
      "\n",
      "Model converged: True\n",
      "Log-likelihood: -301.76\n",
      "\n",
      "State statistics:\n",
      "  State 0 (Tight): 80 obs (43.2%), L = -1.62 ± 0.99\n",
      "  State 1 (Neutral): 53 obs (28.6%), L = +1.21 ± 1.24\n",
      "  State 2 (High): 52 obs (28.1%), L = +1.26 ± 1.29\n",
      "\n",
      "Transition Matrix:\n",
      "              To_Tight  To_Neutral  To_High\n",
      "From_Tight       0.001       0.999    0.000\n",
      "From_Neutral     0.935       0.009    0.056\n",
      "From_High        0.022       0.004    0.974\n",
      "\n",
      "Regime persistence (prob. of staying):\n",
      "  Tight: 0.1%\n",
      "  Neutral: 0.9%\n",
      "  High: 97.4%\n"
     ]
    }
   ],
   "source": [
    "def fit_hmm_regimes(L_series, n_states=3, n_iter=1000):\n",
    "    \"\"\"\n",
    "    Fit Hidden Markov Model to identify liquidity regimes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    L_series : Series\n",
    "        Liquidity index\n",
    "    n_states : int\n",
    "        Number of hidden states\n",
    "    n_iter : int\n",
    "        Maximum EM iterations\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    X = L_series.dropna().values.reshape(-1, 1)\n",
    "    \n",
    "    # Fit HMM\n",
    "    model = hmm.GaussianHMM(\n",
    "        n_components=n_states,\n",
    "        covariance_type='full',\n",
    "        n_iter=n_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X)\n",
    "    \n",
    "    # Predict states (Viterbi algorithm)\n",
    "    states = model.predict(X)\n",
    "    \n",
    "    # Get posterior probabilities\n",
    "    posteriors = model.predict_proba(X)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'L': L_series.dropna(),\n",
    "        'state': states\n",
    "    })\n",
    "    \n",
    "    for i in range(n_states):\n",
    "        results[f'prob_state_{i}'] = posteriors[:, i]\n",
    "    \n",
    "    # Sort states by mean (0=Tight, 1=Neutral, 2=High)\n",
    "    state_means = results.groupby('state')['L'].mean().sort_values()\n",
    "    state_mapping = {old: new for new, old in enumerate(state_means.index)}\n",
    "    results['state'] = results['state'].map(state_mapping)\n",
    "    \n",
    "    # Add labels\n",
    "    state_labels = {0: 'Tight', 1: 'Neutral', 2: 'High'}\n",
    "    results['state_label'] = results['state'].map(state_labels)\n",
    "    \n",
    "    # Print diagnostics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"HMM Regime Detection ({n_states} states)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nModel converged: {model.monitor_.converged}\")\n",
    "    print(f\"Log-likelihood: {model.score(X):.2f}\")\n",
    "    \n",
    "    print(f\"\\nState statistics:\")\n",
    "    for state in range(n_states):\n",
    "        mask = results['state'] == state\n",
    "        n_obs = mask.sum()\n",
    "        pct = n_obs / len(results) * 100\n",
    "        mean_L = results.loc[mask, 'L'].mean()\n",
    "        std_L = results.loc[mask, 'L'].std()\n",
    "        label = state_labels[state]\n",
    "        print(f\"  State {state} ({label}): {n_obs} obs ({pct:.1f}%), \"\n",
    "              f\"L = {mean_L:+.2f} ± {std_L:.2f}\")\n",
    "    \n",
    "    print(f\"\\nTransition Matrix:\")\n",
    "    trans_matrix = pd.DataFrame(\n",
    "        model.transmat_,\n",
    "        index=[f\"From_{state_labels[i]}\" for i in range(n_states)],\n",
    "        columns=[f\"To_{state_labels[i]}\" for i in range(n_states)]\n",
    "    )\n",
    "    print(trans_matrix.round(3))\n",
    "    \n",
    "    # Calculate regime persistence (diagonal elements)\n",
    "    persistence = np.diag(model.transmat_)\n",
    "    print(f\"\\nRegime persistence (prob. of staying):\")\n",
    "    for i, label in state_labels.items():\n",
    "        print(f\"  {label}: {persistence[i]:.1%}\")\n",
    "    \n",
    "    return results, model, state_labels\n",
    "\n",
    "# Fit HMM\n",
    "regimes_df, hmm_model, state_labels = fit_hmm_regimes(L_index, n_states=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef6af1-8c05-4da0-8804-d8bb91fb0a23",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fc03397b-2f09-4a61-8bb5-0c37a461c302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "REGIME VALIDATION: HMM vs Known Fed Policy\n",
      "======================================================================\n",
      "\n",
      "📊 Overlap period: 2003-12 to 2024-12\n",
      "   Total months: 166\n",
      "\n",
      "======================================================================\n",
      "Confusion Matrix: HMM vs Known Regimes\n",
      "======================================================================\n",
      "\n",
      "                HMM_Tight  HMM_Neutral  HMM_High\n",
      "Known_Tight           44           11         9\n",
      "Known_Neutral         21           12        13\n",
      "Known_High             2           28        26\n",
      "\n",
      "Percentages (row-wise):\n",
      "               HMM_Tight  HMM_Neutral  HMM_High\n",
      "Known_Tight         68.8         17.2      14.1\n",
      "Known_Neutral       45.7         26.1      28.3\n",
      "Known_High           3.6         50.0      46.4\n",
      "\n",
      "📈 Overall Accuracy: 49.4%\n",
      "\n",
      "======================================================================\n",
      "Detailed Regime Concordance\n",
      "======================================================================\n",
      "\n",
      "TIGHT REGIME (64 months):\n",
      "   ⚠️ Classified as High    :  14.1%\n",
      "   ⚠️ Classified as Neutral :  17.2%\n",
      "   ✅ Classified as Tight   :  68.8%\n",
      "\n",
      "NEUTRAL REGIME (46 months):\n",
      "   ⚠️ Classified as High    :  28.3%\n",
      "   ✅ Classified as Neutral :  26.1%\n",
      "   ⚠️ Classified as Tight   :  45.7%\n",
      "\n",
      "HIGH REGIME (56 months):\n",
      "   ✅ Classified as High    :  46.4%\n",
      "   ⚠️ Classified as Neutral :  50.0%\n",
      "   ⚠️ Classified as Tight   :   3.6%\n",
      "\n",
      "======================================================================\n",
      "Key Historical Periods Validation\n",
      "======================================================================\n",
      "\n",
      "✅ QE1 (Financial Crisis)\n",
      "   Period: 2008-09 to 2009-12\n",
      "   Expected: High\n",
      "   HMM Mode: High (50% of period)\n",
      "   Distribution: Neutral=5, High=5, \n",
      "\n",
      "✅ QE2\n",
      "   Period: 2010-11 to 2011-06\n",
      "   Expected: High\n",
      "   HMM Mode: High (50% of period)\n",
      "   Distribution: Neutral=3, High=3, \n",
      "\n",
      "✅ QE3\n",
      "   Period: 2012-09 to 2014-10\n",
      "   Expected: High\n",
      "   HMM Mode: High (50% of period)\n",
      "   Distribution: High=9, Neutral=9, \n",
      "\n",
      "✅ Fed Hiking + QT\n",
      "   Period: 2015-12 to 2018-12\n",
      "   Expected: Tight\n",
      "   HMM Mode: Tight (58% of period)\n",
      "   Distribution: Tight=15, Neutral=6, High=5, \n",
      "\n",
      "✅ COVID QE (Unlimited)\n",
      "   Period: 2020-03 to 2021-06\n",
      "   Expected: High\n",
      "   HMM Mode: High (50% of period)\n",
      "   Distribution: High=5, Neutral=5, \n",
      "\n",
      "✅ Aggressive Rate Hikes\n",
      "   Period: 2022-03 to 2023-12\n",
      "   Expected: Tight\n",
      "   HMM Mode: Tight (53% of period)\n",
      "   Distribution: Tight=8, Neutral=4, High=3, \n",
      "\n",
      "======================================================================\n",
      "Significant Mismatches (Known ≠ HMM)\n",
      "======================================================================\n",
      "\n",
      "Found 1 significant mismatch periods (≥3 months):\n",
      "\n",
      "   2007-12 to 2008-02 (3 months)\n",
      "      Known: Neutral  | HMM: Tight   \n",
      "      Context: Initial Crisis Response, Rate Cuts Begin\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SUMMARY STATISTICS\n",
      "======================================================================\n",
      "\n",
      "📊 Agreement Rates by Regime:\n",
      "   Tight   :  68.8% correct\n",
      "   Neutral :  26.1% correct\n",
      "   High    :  46.4% correct\n",
      "\n",
      "📊 Overall Performance:\n",
      "   Total Accuracy:        49.4%\n",
      "   Mismatch Rate:         50.6%\n",
      "   Consecutive Mismatches: 1\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "\n",
      "❌ POOR: HMM does not align well with Fed policy regimes\n",
      "   Major revisions needed to liquidity index or regime detection.\n"
     ]
    }
   ],
   "source": [
    "def map_to_known_liquidity_regimes():\n",
    "    \"\"\"\n",
    "    Define known Fed policy regimes based on historical record\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format: (start_date, end_date, regime_type, description)\n",
    "    known_regimes = [\n",
    "        # =====================================\n",
    "        # Pre-Crisis Era (1990-2007)\n",
    "        # =====================================\n",
    "        ('1990-01', '1992-12', 'Neutral', 'Post-1990 Recession, Gradual Easing'),\n",
    "        ('1993-01', '1994-01', 'High', 'Accommodative Policy, Economic Expansion'),\n",
    "        ('1994-02', '1995-12', 'Tight', 'Fed Tightening Cycle (3.0% → 6.0%)'),\n",
    "        ('1996-01', '1998-12', 'Neutral', 'Goldilocks Economy'),\n",
    "        ('1999-01', '2000-05', 'Tight', 'Dot-com Bubble, Y2K Tightening'),\n",
    "        ('2000-06', '2003-06', 'High', 'Dot-com Crash Response, Aggressive Easing'),\n",
    "        ('2003-07', '2004-05', 'High', 'Ultra-Low Rates (1%), Housing Boom'),\n",
    "        ('2004-06', '2006-06', 'Tight', 'Fed Hiking Cycle (1% → 5.25%)'),\n",
    "        ('2006-07', '2007-07', 'Neutral', 'Pause at Peak, Pre-Crisis'),\n",
    "        \n",
    "        # =====================================\n",
    "        # Financial Crisis & QE Era (2007-2015)\n",
    "        # =====================================\n",
    "        ('2007-08', '2008-08', 'Neutral', 'Initial Crisis Response, Rate Cuts Begin'),\n",
    "        ('2008-09', '2010-03', 'High', 'QE1: Emergency Response ($1.7T)'),\n",
    "        ('2010-04', '2010-10', 'Neutral', 'QE1 Taper, Brief Pause'),\n",
    "        ('2010-11', '2011-06', 'High', 'QE2: $600B Purchase Program'),\n",
    "        ('2011-07', '2012-08', 'Neutral', 'Operation Twist, Pre-QE3'),\n",
    "        ('2012-09', '2014-10', 'High', 'QE3: $85B/month, then taper'),\n",
    "        ('2014-11', '2015-11', 'Neutral', 'Post-QE3, ZIRP Maintained'),\n",
    "        \n",
    "        # =====================================\n",
    "        # Normalization Attempt (2015-2019)\n",
    "        # =====================================\n",
    "        ('2015-12', '2018-12', 'Tight', 'Fed Hiking + QT (0.25% → 2.50%)'),\n",
    "        ('2019-01', '2019-07', 'Neutral', 'Pause, Economic Slowdown'),\n",
    "        ('2019-08', '2019-12', 'High', 'Insurance Cuts + Repo Crisis Response'),\n",
    "        \n",
    "        # =====================================\n",
    "        # COVID Era (2020-2021)\n",
    "        # =====================================\n",
    "        ('2020-01', '2020-02', 'Neutral', 'Pre-COVID'),\n",
    "        ('2020-03', '2021-10', 'High', 'COVID QE: Unlimited purchases, ZIRP'),\n",
    "        ('2021-11', '2022-02', 'High', 'Taper Announcement but still easy'),\n",
    "        \n",
    "        # =====================================\n",
    "        # Modern Tightening (2022-2025)\n",
    "        # =====================================\n",
    "        ('2022-03', '2023-06', 'Tight', 'Aggressive Hikes (0% → 5.25%)'),\n",
    "        ('2023-07', '2024-08', 'Tight', 'Higher for Longer, QT Active'),\n",
    "        ('2024-09', '2025-01', 'Neutral', 'Easing Cycle Begins'),\n",
    "    ]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    regime_df = pd.DataFrame(known_regimes, columns=['start', 'end', 'regime', 'description'])\n",
    "    regime_df['start'] = pd.to_datetime(regime_df['start'])\n",
    "    regime_df['end'] = pd.to_datetime(regime_df['end'])\n",
    "    \n",
    "    return regime_df\n",
    "\n",
    "\n",
    "def validate_hmm_against_known_regimes(regimes_df, known_regimes_df):\n",
    "    \"\"\"\n",
    "    Compare HMM-detected regimes to known Fed policy periods\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"REGIME VALIDATION: HMM vs Known Fed Policy\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Expand known regimes to monthly observations\n",
    "    known_monthly = []\n",
    "    for _, row in known_regimes_df.iterrows():\n",
    "        date_range = pd.date_range(row['start'], row['end'], freq='M')\n",
    "        for date in date_range:\n",
    "            known_monthly.append({\n",
    "                'date': date,\n",
    "                'known_regime': row['regime'],\n",
    "                'period_description': row['description']\n",
    "            })\n",
    "    \n",
    "    known_df = pd.DataFrame(known_monthly).set_index('date')\n",
    "    \n",
    "    # Merge with HMM results\n",
    "    comparison = regimes_df.join(known_df, how='inner')\n",
    "    comparison = comparison.dropna(subset=['known_regime', 'state_label'])\n",
    "    \n",
    "    print(f\"\\n📊 Overlap period: {comparison.index[0].strftime('%Y-%m')} to {comparison.index[-1].strftime('%Y-%m')}\")\n",
    "    print(f\"   Total months: {len(comparison)}\")\n",
    "    \n",
    "    # =====================================\n",
    "    # Confusion Matrix\n",
    "    # =====================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Confusion Matrix: HMM vs Known Regimes\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(comparison['known_regime'], comparison['state_label'], \n",
    "                         labels=['Tight', 'Neutral', 'High'])\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, \n",
    "                        index=['Known_Tight', 'Known_Neutral', 'Known_High'],\n",
    "                        columns=['HMM_Tight', 'HMM_Neutral', 'HMM_High'])\n",
    "    \n",
    "    print(\"\\n\", cm_df)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    cm_pct = cm_df.div(cm_df.sum(axis=1), axis=0) * 100\n",
    "    print(\"\\nPercentages (row-wise):\")\n",
    "    print(cm_pct.round(1))\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(comparison['known_regime'], comparison['state_label'])\n",
    "    print(f\"\\n📈 Overall Accuracy: {accuracy:.1%}\")\n",
    "    \n",
    "    # =====================================\n",
    "    # Regime-by-Regime Analysis\n",
    "    # =====================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Detailed Regime Concordance\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for known_regime in ['Tight', 'Neutral', 'High']:\n",
    "        mask = comparison['known_regime'] == known_regime\n",
    "        subset = comparison[mask]\n",
    "        \n",
    "        print(f\"\\n{known_regime.upper()} REGIME ({mask.sum()} months):\")\n",
    "        \n",
    "        # How HMM classified these periods\n",
    "        hmm_dist = subset['state_label'].value_counts(normalize=True) * 100\n",
    "        for hmm_regime, pct in hmm_dist.sort_index().items():\n",
    "            status = \"✅\" if hmm_regime == known_regime else \"⚠️\"\n",
    "            print(f\"   {status} Classified as {hmm_regime:8s}: {pct:5.1f}%\")\n",
    "    \n",
    "    # =====================================\n",
    "    # Key Historical Periods Check\n",
    "    # =====================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Key Historical Periods Validation\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    key_periods = [\n",
    "        ('2008-09', '2009-12', 'High', 'QE1 (Financial Crisis)'),\n",
    "        ('2010-11', '2011-06', 'High', 'QE2'),\n",
    "        ('2012-09', '2014-10', 'High', 'QE3'),\n",
    "        ('2015-12', '2018-12', 'Tight', 'Fed Hiking + QT'),\n",
    "        ('2020-03', '2021-06', 'High', 'COVID QE (Unlimited)'),\n",
    "        ('2022-03', '2023-12', 'Tight', 'Aggressive Rate Hikes'),\n",
    "    ]\n",
    "    \n",
    "    for start, end, expected, description in key_periods:\n",
    "        try:\n",
    "            mask = (comparison.index >= start) & (comparison.index <= end)\n",
    "            subset = comparison[mask]\n",
    "            \n",
    "            if len(subset) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Modal HMM regime\n",
    "            hmm_mode = subset['state_label'].mode()[0]\n",
    "            hmm_pct = (subset['state_label'] == hmm_mode).sum() / len(subset) * 100\n",
    "            \n",
    "            # Check if matches expectation\n",
    "            match = \"✅\" if hmm_mode == expected else \"❌\"\n",
    "            \n",
    "            print(f\"\\n{match} {description}\")\n",
    "            print(f\"   Period: {start} to {end}\")\n",
    "            print(f\"   Expected: {expected}\")\n",
    "            print(f\"   HMM Mode: {hmm_mode} ({hmm_pct:.0f}% of period)\")\n",
    "            \n",
    "            # Show distribution\n",
    "            dist = subset['state_label'].value_counts()\n",
    "            print(f\"   Distribution: \", end=\"\")\n",
    "            for regime, count in dist.items():\n",
    "                print(f\"{regime}={count}, \", end=\"\")\n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  {description}: Data not available\")\n",
    "    \n",
    "    # =====================================\n",
    "    # Mismatches Analysis\n",
    "    # =====================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Significant Mismatches (Known ≠ HMM)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    mismatches = comparison[comparison['known_regime'] != comparison['state_label']]\n",
    "    \n",
    "    if len(mismatches) > 0:\n",
    "        # Find consecutive mismatch periods\n",
    "        mismatch_periods = []\n",
    "        current_start = None\n",
    "        \n",
    "        for i, (date, row) in enumerate(mismatches.iterrows()):\n",
    "            if current_start is None:\n",
    "                current_start = date\n",
    "                current_known = row['known_regime']\n",
    "                current_hmm = row['state_label']\n",
    "            \n",
    "            # Check if next row continues the mismatch pattern\n",
    "            is_last = i == len(mismatches) - 1\n",
    "            if is_last or (not is_last and mismatches.index[i+1] != date + pd.DateOffset(months=1)):\n",
    "                mismatch_periods.append({\n",
    "                    'start': current_start,\n",
    "                    'end': date,\n",
    "                    'known': current_known,\n",
    "                    'hmm': current_hmm,\n",
    "                    'months': (date.year - current_start.year) * 12 + date.month - current_start.month + 1\n",
    "                })\n",
    "                current_start = None\n",
    "        \n",
    "        # Show significant mismatches (>= 3 months)\n",
    "        significant = [p for p in mismatch_periods if p['months'] >= 3]\n",
    "        \n",
    "        if significant:\n",
    "            print(f\"\\nFound {len(significant)} significant mismatch periods (≥3 months):\\n\")\n",
    "            for p in significant:\n",
    "                print(f\"   {p['start'].strftime('%Y-%m')} to {p['end'].strftime('%Y-%m')} ({p['months']} months)\")\n",
    "                print(f\"      Known: {p['known']:8s} | HMM: {p['hmm']:8s}\")\n",
    "                \n",
    "                # Get description for this period\n",
    "                desc_mask = (known_regimes_df['start'] <= p['start']) & (known_regimes_df['end'] >= p['end'])\n",
    "                if desc_mask.any():\n",
    "                    desc = known_regimes_df[desc_mask]['description'].iloc[0]\n",
    "                    print(f\"      Context: {desc}\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"\\n✅ No significant consecutive mismatches (all discrepancies < 3 months)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n🎉 Perfect match! HMM exactly matches known regimes.\")\n",
    "    \n",
    "    # =====================================\n",
    "    # Summary Statistics\n",
    "    # =====================================\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\n📊 Agreement Rates by Regime:\")\n",
    "    for regime in ['Tight', 'Neutral', 'High']:\n",
    "        mask = comparison['known_regime'] == regime\n",
    "        if mask.sum() > 0:\n",
    "            agreement = (comparison[mask]['state_label'] == regime).sum() / mask.sum() * 100\n",
    "            print(f\"   {regime:8s}: {agreement:5.1f}% correct\")\n",
    "    \n",
    "    print(f\"\\n📊 Overall Performance:\")\n",
    "    print(f\"   Total Accuracy:        {accuracy:.1%}\")\n",
    "    print(f\"   Mismatch Rate:         {(1-accuracy):.1%}\")\n",
    "    print(f\"   Consecutive Mismatches: {len([p for p in mismatch_periods if p['months'] >= 3])}\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"INTERPRETATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if accuracy >= 0.80:\n",
    "        print(\"\\n✅ EXCELLENT: HMM successfully captures Fed policy regimes\")\n",
    "        print(\"   The model shows strong alignment with known monetary policy history.\")\n",
    "    elif accuracy >= 0.65:\n",
    "        print(\"\\n✔️  GOOD: HMM generally aligns with Fed policy regimes\")\n",
    "        print(\"   Some discrepancies exist but the model captures major policy shifts.\")\n",
    "    elif accuracy >= 0.50:\n",
    "        print(\"\\n⚠️  MODERATE: HMM shows partial alignment with Fed policy\")\n",
    "        print(\"   Consider adjusting liquidity index composition or HMM parameters.\")\n",
    "    else:\n",
    "        print(\"\\n❌ POOR: HMM does not align well with Fed policy regimes\")\n",
    "        print(\"   Major revisions needed to liquidity index or regime detection.\")\n",
    "    \n",
    "    return comparison, cm_df, accuracy\n",
    "\n",
    "\n",
    "# Run validation\n",
    "known_regimes_df = map_to_known_liquidity_regimes()\n",
    "comparison_df, confusion_matrix, accuracy = validate_hmm_against_known_regimes(\n",
    "    regimes_df, \n",
    "    known_regimes_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a3a608ae-11c2-4171-aba8-0262d6d78d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ADVANCED REGIME CLASSIFICATION SYSTEM\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "STEP 0: DATA ALIGNMENT\n",
      "================================================================================\n",
      "\n",
      "📊 Data Coverage:\n",
      "   df range:       1990-01-31 00:00:00 to 2025-11-30 00:00:00 (431 obs)\n",
      "   L_series range: 2003-12-31 00:00:00 to 2025-09-30 00:00:00 (185 obs)\n",
      "   Overlap range:  2003-12-31 00:00:00 to 2025-09-30 00:00:00 (185 obs)\n",
      "\n",
      "✅ Data aligned: 185 observations\n",
      "\n",
      "================================================================================\n",
      "STEP 1: CRISIS DETECTION\n",
      "================================================================================\n",
      "\n",
      "📊 Crisis Features:\n",
      "   ret_vol_6m          : 183/185 valid\n",
      "   ret_vol_12m         : 180/185 valid\n",
      "   L_vol_3m            : 184/185 valid\n",
      "   L_vol_6m            : 183/185 valid\n",
      "   abs_ret             : 185/185 valid\n",
      "   cum_ret_3m          : 184/185 valid\n",
      "   cum_ret_6m          : 183/185 valid\n",
      "   max_dd_6m           : 183/185 valid\n",
      "   VIX_level           : 185/185 valid\n",
      "   VIX_zscore          : 156/185 valid\n",
      "\n",
      "📊 Crisis detection dataset: 168 observations\n",
      "\n",
      "✅ Crisis Detection Complete\n",
      "   Crisis periods:     0 months (0.0%)\n",
      "   Non-crisis periods: 185 months (100.0%)\n",
      "\n",
      "================================================================================\n",
      "STEP 2: REGIME CLASSIFICATION (NON-CRISIS PERIODS)\n",
      "================================================================================\n",
      "\n",
      "📊 Liquidity Quantiles:\n",
      "   33rd percentile (q33): -0.80\n",
      "   67th percentile (q67): +0.69\n",
      "\n",
      "📊 Non-Crisis Regime Distribution:\n",
      "   Tight   :   61 months ( 33.0%), Mean L = -1.95\n",
      "   Neutral :   63 months ( 34.1%), Mean L = +0.00\n",
      "   High    :   61 months ( 33.0%), Mean L = +1.95\n",
      "\n",
      "================================================================================\n",
      "STEP 3: FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "✅ Selected 16 features:\n",
      "    1. dlog_M2             : 185/185 valid\n",
      "    2. dlog_FED_BS         : 185/185 valid\n",
      "    3. EM                  : 185/185 valid\n",
      "    4. EB                  : 185/185 valid\n",
      "    5. EL_3y               : 185/185 valid\n",
      "    6. Net_Liq             : 118/185 valid\n",
      "    7. Term_Spread         : 185/185 valid\n",
      "    8. Credit_Spread       : 185/185 valid\n",
      "    9. Real_Rate           : 185/185 valid\n",
      "   10. VIX                 : 185/185 valid\n",
      "   11. M2_growth           : 185/185 valid\n",
      "   12. FED_BS_growth       : 185/185 valid\n",
      "   13. L_ma_3              : 185/185 valid\n",
      "   14. L_ma_6              : 183/185 valid\n",
      "   15. L_momentum          : 182/185 valid\n",
      "   16. V_spread            : 185/185 valid\n",
      "\n",
      "📊 Final ML dataset: 155 samples × 16 features\n",
      "\n",
      "================================================================================\n",
      "STEP 4: MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "Training Random Forest\n",
      "======================================================================\n",
      "\n",
      "📊 CV Accuracy: 0.800 (±0.104)\n",
      "\n",
      "📈 Performance:\n",
      "   Accuracy: 0.935\n",
      "   ROC-AUC:  0.995\n",
      "\n",
      "📋 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Tight       0.95      0.95      0.95        42\n",
      "     Neutral       0.89      0.93      0.91        54\n",
      "        High       0.96      0.93      0.95        59\n",
      "\n",
      "    accuracy                           0.94       155\n",
      "   macro avg       0.94      0.94      0.94       155\n",
      "weighted avg       0.94      0.94      0.94       155\n",
      "\n",
      "\n",
      "🎯 Confusion Matrix (%):\n",
      "              Pred_Tight  Pred_Neutral  Pred_High\n",
      "True_Tight          95.2           4.8        0.0\n",
      "True_Neutral         3.7          92.6        3.7\n",
      "True_High            0.0           6.8       93.2\n",
      "\n",
      "======================================================================\n",
      "Training XGBoost\n",
      "======================================================================\n",
      "\n",
      "📊 CV Accuracy: 0.744 (±0.151)\n",
      "\n",
      "📈 Performance:\n",
      "   Accuracy: 1.000\n",
      "   ROC-AUC:  1.000\n",
      "\n",
      "📋 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Tight       1.00      1.00      1.00        42\n",
      "     Neutral       1.00      1.00      1.00        54\n",
      "        High       1.00      1.00      1.00        59\n",
      "\n",
      "    accuracy                           1.00       155\n",
      "   macro avg       1.00      1.00      1.00       155\n",
      "weighted avg       1.00      1.00      1.00       155\n",
      "\n",
      "\n",
      "🎯 Confusion Matrix (%):\n",
      "              Pred_Tight  Pred_Neutral  Pred_High\n",
      "True_Tight         100.0           0.0        0.0\n",
      "True_Neutral         0.0         100.0        0.0\n",
      "True_High            0.0           0.0      100.0\n",
      "\n",
      "================================================================================\n",
      "STEP 5: FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "📊 Top 10 Features:\n",
      "   L_ma_6              : 0.3007\n",
      "   L_ma_3              : 0.2085\n",
      "   Term_Spread         : 0.1153\n",
      "   FED_BS_growth       : 0.0552\n",
      "   EL_3y               : 0.0481\n",
      "   EM                  : 0.0375\n",
      "   L_momentum          : 0.0371\n",
      "   Real_Rate           : 0.0302\n",
      "   Credit_Spread       : 0.0288\n",
      "   VIX                 : 0.0272\n",
      "\n",
      "================================================================================\n",
      "STEP 6: FINAL REGIME ASSIGNMENT\n",
      "================================================================================\n",
      "\n",
      "📊 Final Distribution:\n",
      "   High    :   59 ( 31.9%)\n",
      "   Neutral :   54 ( 29.2%)\n",
      "   Tight   :   42 ( 22.7%)\n",
      "   Unknown :   30 ( 16.2%)\n",
      "\n",
      "================================================================================\n",
      "✅ COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def build_advanced_regime_classification_system(df, L_series):\n",
    "    \"\"\"\n",
    "    State-of-the-art regime classification with proper index alignment\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "    import shap\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ADVANCED REGIME CLASSIFICATION SYSTEM\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 0: DATA ALIGNMENT\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 0: DATA ALIGNMENT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Find common index\n",
    "    common_idx = df.index.intersection(L_series.index)\n",
    "    \n",
    "    if len(common_idx) == 0:\n",
    "        raise ValueError(\"No overlapping dates between df and L_series!\")\n",
    "    \n",
    "    print(f\"\\n📊 Data Coverage:\")\n",
    "    print(f\"   df range:       {df.index[0]} to {df.index[-1]} ({len(df)} obs)\")\n",
    "    print(f\"   L_series range: {L_series.index[0]} to {L_series.index[-1]} ({len(L_series)} obs)\")\n",
    "    print(f\"   Overlap range:  {common_idx[0]} to {common_idx[-1]} ({len(common_idx)} obs)\")\n",
    "    \n",
    "    # Use only overlapping data\n",
    "    df_aligned = df.loc[common_idx].copy()\n",
    "    L_aligned = L_series.loc[common_idx].copy()\n",
    "    \n",
    "    # Add L to dataframe\n",
    "    df_aligned['L'] = L_aligned\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['mkt_excess']\n",
    "    missing_cols = [c for c in required_cols if c not in df_aligned.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    print(f\"\\n✅ Data aligned: {len(df_aligned)} observations\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 1: Crisis Detection (Isolation Forest)\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 1: CRISIS DETECTION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create crisis features\n",
    "    df_crisis = df_aligned.copy()\n",
    "    \n",
    "    # Volatility features\n",
    "    df_crisis['ret_vol_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).std()\n",
    "    df_crisis['ret_vol_12m'] = df_crisis['mkt_excess'].rolling(12, min_periods=6).std()\n",
    "    df_crisis['L_vol_3m'] = df_crisis['L'].rolling(3, min_periods=2).std()\n",
    "    df_crisis['L_vol_6m'] = df_crisis['L'].rolling(6, min_periods=3).std()\n",
    "    \n",
    "    # Return features\n",
    "    df_crisis['abs_ret'] = df_crisis['mkt_excess'].abs()\n",
    "    df_crisis['cum_ret_3m'] = df_crisis['mkt_excess'].rolling(3, min_periods=2).sum()\n",
    "    df_crisis['cum_ret_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).sum()\n",
    "    \n",
    "    # Drawdown\n",
    "    cumret = (1 + df_crisis['mkt_excess']/100).cumprod()\n",
    "    running_max = cumret.expanding().max()\n",
    "    df_crisis['drawdown'] = (cumret / running_max - 1) * 100\n",
    "    df_crisis['max_dd_6m'] = df_crisis['drawdown'].rolling(6, min_periods=3).min()\n",
    "    \n",
    "    # VIX if available\n",
    "    crisis_features = ['ret_vol_6m', 'ret_vol_12m', 'L_vol_3m', 'L_vol_6m', \n",
    "                       'abs_ret', 'cum_ret_3m', 'cum_ret_6m', 'max_dd_6m']\n",
    "    \n",
    "    if 'VIX' in df_crisis.columns:\n",
    "        df_crisis['VIX_level'] = df_crisis['VIX']\n",
    "        df_crisis['VIX_zscore'] = (df_crisis['VIX'] - df_crisis['VIX'].rolling(60, min_periods=30).mean()) / \\\n",
    "                                   df_crisis['VIX'].rolling(60, min_periods=30).std()\n",
    "        crisis_features.extend(['VIX_level', 'VIX_zscore'])\n",
    "    \n",
    "    # Check data availability\n",
    "    print(f\"\\n📊 Crisis Features:\")\n",
    "    for feat in crisis_features:\n",
    "        if feat in df_crisis.columns:\n",
    "            n_valid = df_crisis[feat].notna().sum()\n",
    "            print(f\"   {feat:20s}: {n_valid}/{len(df_crisis)} valid\")\n",
    "    \n",
    "    # Extract crisis features and drop NaN\n",
    "    crisis_data = df_crisis[crisis_features].copy()\n",
    "    \n",
    "    # Fill forward initial NaN from rolling windows (up to 12 months)\n",
    "    crisis_data = crisis_data.fillna(method='bfill', limit=12)\n",
    "    crisis_data = crisis_data.dropna()\n",
    "    \n",
    "    print(f\"\\n📊 Crisis detection dataset: {len(crisis_data)} observations\")\n",
    "    \n",
    "    if len(crisis_data) < 20:\n",
    "        print(f\"\\n⚠️  WARNING: Not enough data for crisis detection ({len(crisis_data)} obs)\")\n",
    "        print(f\"   Skipping crisis detection, will use all data for regime classification\")\n",
    "        \n",
    "        df_crisis['is_crisis'] = False\n",
    "        df_noncrisis = df_crisis.copy()\n",
    "        \n",
    "    else:\n",
    "        # Fit Isolation Forest\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=0.05,\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_samples='auto'\n",
    "        )\n",
    "        \n",
    "        outlier_labels = iso_forest.fit_predict(crisis_data.values)\n",
    "        \n",
    "        # Crisis = outlier + negative return\n",
    "        is_crisis_candidate = (outlier_labels == -1)\n",
    "        is_negative_return = (df_crisis.loc[crisis_data.index, 'mkt_excess'] < -2)\n",
    "        \n",
    "        df_crisis['is_crisis'] = False\n",
    "        df_crisis.loc[crisis_data.index, 'is_crisis'] = is_crisis_candidate & is_negative_return\n",
    "        \n",
    "        n_crisis = df_crisis['is_crisis'].sum()\n",
    "        crisis_pct = n_crisis / len(df_crisis) * 100\n",
    "        \n",
    "        print(f\"\\n✅ Crisis Detection Complete\")\n",
    "        print(f\"   Crisis periods:     {n_crisis} months ({crisis_pct:.1f}%)\")\n",
    "        print(f\"   Non-crisis periods: {len(df_crisis) - n_crisis} months ({100-crisis_pct:.1f}%)\")\n",
    "        \n",
    "        # Show crisis periods\n",
    "        if n_crisis > 0:\n",
    "            crisis_periods = df_crisis[df_crisis['is_crisis']].index\n",
    "            print(f\"\\n📅 Detected Crisis Months:\")\n",
    "            for i, date in enumerate(crisis_periods[:10], 1):\n",
    "                ret = df_crisis.loc[date, 'mkt_excess']\n",
    "                print(f\"   {i:2d}. {date.strftime('%Y-%m')}: Return = {ret:+.2f}%\")\n",
    "            if len(crisis_periods) > 10:\n",
    "                print(f\"   ... and {len(crisis_periods) - 10} more\")\n",
    "        \n",
    "        # Filter to non-crisis\n",
    "        df_noncrisis = df_crisis[~df_crisis['is_crisis']].copy()\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 2: Regime Classification Setup\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 2: REGIME CLASSIFICATION (NON-CRISIS PERIODS)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Create regime labels based on liquidity quantiles\n",
    "    L_noncrisis = df_noncrisis['L']\n",
    "    \n",
    "    if len(L_noncrisis) < 30:\n",
    "        raise ValueError(f\"Not enough non-crisis data: {len(L_noncrisis)} observations\")\n",
    "    \n",
    "    q33 = L_noncrisis.quantile(0.33)\n",
    "    q67 = L_noncrisis.quantile(0.67)\n",
    "    \n",
    "    print(f\"\\n📊 Liquidity Quantiles:\")\n",
    "    print(f\"   33rd percentile (q33): {q33:+.2f}\")\n",
    "    print(f\"   67th percentile (q67): {q67:+.2f}\")\n",
    "    \n",
    "    def assign_regime(L_val):\n",
    "        if L_val < q33:\n",
    "            return 0  # Tight\n",
    "        elif L_val < q67:\n",
    "            return 1  # Neutral\n",
    "        else:\n",
    "            return 2  # High\n",
    "    \n",
    "    df_noncrisis['regime'] = L_noncrisis.apply(assign_regime)\n",
    "    df_noncrisis['regime_label'] = df_noncrisis['regime'].map({0: 'Tight', 1: 'Neutral', 2: 'High'})\n",
    "    \n",
    "    print(f\"\\n📊 Non-Crisis Regime Distribution:\")\n",
    "    for regime, label in [(0, 'Tight'), (1, 'Neutral'), (2, 'High')]:\n",
    "        mask = df_noncrisis['regime'] == regime\n",
    "        count = mask.sum()\n",
    "        pct = count / len(df_noncrisis) * 100\n",
    "        mean_L = df_noncrisis[mask]['L'].mean()\n",
    "        print(f\"   {label:8s}: {count:4d} months ({pct:5.1f}%), Mean L = {mean_L:+.2f}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 3: Feature Engineering\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 3: FEATURE ENGINEERING\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Build feature set\n",
    "    feature_cols = []\n",
    "    \n",
    "    # Core liquidity components\n",
    "    liq_features = ['dlog_M2', 'dlog_FED_BS', 'EM', 'EB', 'EL_3y']\n",
    "    if 'Net_Liq' in df_noncrisis.columns:\n",
    "        liq_features.append('Net_Liq')\n",
    "    \n",
    "    for feat in liq_features:\n",
    "        if feat in df_noncrisis.columns:\n",
    "            feature_cols.append(feat)\n",
    "    \n",
    "    # Rates and spreads\n",
    "    rate_features = ['Term_Spread', 'Credit_Spread']\n",
    "    if 'Real_Rate' in df_noncrisis.columns:\n",
    "        rate_features.append('Real_Rate')\n",
    "    \n",
    "    for feat in rate_features:\n",
    "        if feat in df_noncrisis.columns:\n",
    "            feature_cols.append(feat)\n",
    "    \n",
    "    # Market indicators\n",
    "    if 'VIX' in df_noncrisis.columns:\n",
    "        feature_cols.append('VIX')\n",
    "    \n",
    "    # Growth rates\n",
    "    if 'M2_growth' in df_noncrisis.columns:\n",
    "        feature_cols.append('M2_growth')\n",
    "    if 'FED_BS_growth' in df_noncrisis.columns:\n",
    "        feature_cols.append('FED_BS_growth')\n",
    "    \n",
    "    # Technical features on L\n",
    "    df_noncrisis['L_ma_3'] = df_noncrisis['L'].rolling(3, min_periods=1).mean()\n",
    "    df_noncrisis['L_ma_6'] = df_noncrisis['L'].rolling(6, min_periods=3).mean()\n",
    "    df_noncrisis['L_momentum'] = df_noncrisis['L'].diff(3)\n",
    "    \n",
    "    feature_cols.extend(['L_ma_3', 'L_ma_6', 'L_momentum'])\n",
    "    \n",
    "    # Valuation if available\n",
    "    if 'V_spread' in df_noncrisis.columns:\n",
    "        feature_cols.append('V_spread')\n",
    "    \n",
    "    print(f\"\\n✅ Selected {len(feature_cols)} features:\")\n",
    "    for i, feat in enumerate(feature_cols, 1):\n",
    "        if feat in df_noncrisis.columns:\n",
    "            n_valid = df_noncrisis[feat].notna().sum()\n",
    "            print(f\"   {i:2d}. {feat:20s}: {n_valid}/{len(df_noncrisis)} valid\")\n",
    "    \n",
    "    # Prepare feature matrix\n",
    "    X = df_noncrisis[feature_cols].copy()\n",
    "    y = df_noncrisis['regime'].copy()\n",
    "    \n",
    "    # Forward fill initial NaN from rolling windows\n",
    "    X = X.fillna(method='ffill', limit=6)\n",
    "    \n",
    "    # Drop remaining NaN\n",
    "    valid_idx = X.notna().all(axis=1) & y.notna()\n",
    "    X = X[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "    \n",
    "    print(f\"\\n📊 Final ML dataset: {len(X)} samples × {len(feature_cols)} features\")\n",
    "    \n",
    "    if len(X) < 30:\n",
    "        raise ValueError(f\"Not enough valid data for ML: {len(X)} samples\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 4: Train ML Models\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 4: MODEL TRAINING\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Time series split\n",
    "    tscv = TimeSeriesSplit(n_splits=min(5, len(X) // 20))\n",
    "    \n",
    "    # Models\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=8,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',\n",
    "            use_label_encoder=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Training {name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        try:\n",
    "            cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy', n_jobs=-1)\n",
    "            print(f\"\\n📊 CV Accuracy: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n⚠️  CV failed: {e}\")\n",
    "            cv_scores = np.array([])\n",
    "        \n",
    "        # Train full model\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X)\n",
    "        y_pred_proba = model.predict_proba(X)\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy = (y_pred == y).mean()\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "        except:\n",
    "            roc_auc = np.nan\n",
    "        \n",
    "        print(f\"\\n📈 Performance:\")\n",
    "        print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "        if not np.isnan(roc_auc):\n",
    "            print(f\"   ROC-AUC:  {roc_auc:.3f}\")\n",
    "        \n",
    "        # Classification report\n",
    "        print(f\"\\n📋 Classification Report:\")\n",
    "        print(classification_report(y, y_pred, target_names=['Tight', 'Neutral', 'High'], zero_division=0))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "        \n",
    "        print(f\"\\n🎯 Confusion Matrix (%):\")\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm_pct,\n",
    "            index=['True_Tight', 'True_Neutral', 'True_High'],\n",
    "            columns=['Pred_Tight', 'Pred_Neutral', 'Pred_High']\n",
    "        )\n",
    "        print(cm_df.round(1))\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'cv_scores': cv_scores,\n",
    "            'accuracy': accuracy,\n",
    "            'roc_auc': roc_auc,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 5: Feature Importance\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 5: FEATURE IMPORTANCE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    best_model = results['XGBoost']['model']\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n📊 Top 10 Features:\")\n",
    "    for i, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"   {row['feature']:20s}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 6: Final Labels\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 6: FINAL REGIME ASSIGNMENT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df_final = df_crisis.copy()\n",
    "    df_final['regime'] = np.nan\n",
    "    df_final['regime_label'] = 'Unknown'\n",
    "    \n",
    "    # Crisis\n",
    "    if 'is_crisis' in df_final.columns:\n",
    "        df_final.loc[df_final['is_crisis'], 'regime'] = 3\n",
    "        df_final.loc[df_final['is_crisis'], 'regime_label'] = 'Crisis'\n",
    "    \n",
    "    # ML predictions\n",
    "    ml_idx = X.index\n",
    "    df_final.loc[ml_idx, 'regime'] = results['XGBoost']['y_pred']\n",
    "    df_final.loc[ml_idx, 'regime_label'] = pd.Series(\n",
    "        results['XGBoost']['y_pred'],\n",
    "        index=ml_idx\n",
    "    ).map({0: 'Tight', 1: 'Neutral', 2: 'High'})\n",
    "    \n",
    "    print(f\"\\n📊 Final Distribution:\")\n",
    "    dist = df_final['regime_label'].value_counts()\n",
    "    for regime, count in dist.items():\n",
    "        pct = count / len(df_final) * 100\n",
    "        print(f\"   {regime:8s}: {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✅ COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return df_final, results, feature_importance\n",
    "\n",
    "\n",
    "# Run with proper inputs\n",
    "df_regimes_advanced, ml_results, feature_importance = build_advanced_regime_classification_system(\n",
    "    df_features_aug,\n",
    "    L_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5c187b9a-5ef9-40d4-a007-4a54f6bd380c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M2</th>\n",
       "      <th>FED_BS</th>\n",
       "      <th>FFR</th>\n",
       "      <th>Real_Rate</th>\n",
       "      <th>T3M</th>\n",
       "      <th>T10Y</th>\n",
       "      <th>BAA</th>\n",
       "      <th>AAA</th>\n",
       "      <th>CPI</th>\n",
       "      <th>CORE_CPI</th>\n",
       "      <th>...</th>\n",
       "      <th>abs_ret</th>\n",
       "      <th>cum_ret_3m</th>\n",
       "      <th>cum_ret_6m</th>\n",
       "      <th>drawdown</th>\n",
       "      <th>max_dd_6m</th>\n",
       "      <th>VIX_level</th>\n",
       "      <th>VIX_zscore</th>\n",
       "      <th>is_crisis</th>\n",
       "      <th>regime</th>\n",
       "      <th>regime_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-07-31</th>\n",
       "      <td>22028.7</td>\n",
       "      <td>6642578.0</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.838904</td>\n",
       "      <td>4.24</td>\n",
       "      <td>4.37</td>\n",
       "      <td>6.04</td>\n",
       "      <td>5.41</td>\n",
       "      <td>322.132</td>\n",
       "      <td>328.656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>-0.041738</td>\n",
       "      <td>-0.110072</td>\n",
       "      <td>16.72</td>\n",
       "      <td>-0.536469</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-09-30</th>\n",
       "      <td>22212.4</td>\n",
       "      <td>6608395.0</td>\n",
       "      <td>4.09</td>\n",
       "      <td>1.149868</td>\n",
       "      <td>3.86</td>\n",
       "      <td>4.16</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.22</td>\n",
       "      <td>324.368</td>\n",
       "      <td>330.542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.1023</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>-0.007852</td>\n",
       "      <td>-0.110072</td>\n",
       "      <td>16.28</td>\n",
       "      <td>-0.613043</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 M2     FED_BS   FFR  Real_Rate   T3M  T10Y   BAA   AAA  \\\n",
       "2025-07-31  22028.7  6642578.0  4.33   0.838904  4.24  4.37  6.04  5.41   \n",
       "2025-09-30  22212.4  6608395.0  4.09   1.149868  3.86  4.16  5.83  5.22   \n",
       "\n",
       "                CPI  CORE_CPI  ...  abs_ret  cum_ret_3m  cum_ret_6m  drawdown  \\\n",
       "2025-07-31  322.132   328.656  ...   0.0198      0.0600     -0.0002 -0.041738   \n",
       "2025-09-30  324.368   330.542  ...   0.0339      0.1023      0.0057 -0.007852   \n",
       "\n",
       "            max_dd_6m  VIX_level  VIX_zscore  is_crisis  regime  regime_label  \n",
       "2025-07-31  -0.110072      16.72   -0.536469      False     0.0         Tight  \n",
       "2025-09-30  -0.110072      16.28   -0.613043      False     0.0         Tight  \n",
       "\n",
       "[2 rows x 68 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_regimes_advanced.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "79b91dda-f3b3-4ebe-948e-a4a2a5baa5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor_returns_by_regime = (\n",
    "#     df_regimes_advanced.groupby(\"regime_label\")[[\"mkt_excess\", \"hml\", \"rmw\", \"cma\"]]\n",
    "#     .mean() * 100\n",
    "# )\n",
    "# print(factor_returns_by_regime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "93f8d34f-7efb-4897-a3c0-7e7baa16c775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DIAGNOSIS: Unknown Regime Periods\n",
      "================================================================================\n",
      "\n",
      "📊 Unknown Periods: 30 months (16.2%)\n",
      "\n",
      "📅 Unknown Date Range:\n",
      "   First: 2003-12\n",
      "   Last:  2012-10\n",
      "\n",
      "🔍 Missing Data Analysis:\n",
      "   Net_Liq             :  29/30 missing ( 96.7%)\n",
      "\n",
      "📍 Unknown Period Distribution:\n",
      "   Found 14 clusters of unknown periods:\n",
      "   Cluster 1: 2003-12 to 2003-12 (1 months)\n",
      "   Cluster 2: 2004-03 to 2004-04 (2 months)\n",
      "   Cluster 3: 2004-12 to 2005-03 (4 months)\n",
      "   Cluster 4: 2005-05 to 2005-06 (2 months)\n",
      "   Cluster 5: 2005-08 to 2005-11 (4 months)\n",
      "   Cluster 6: 2006-01 to 2006-03 (3 months)\n",
      "   Cluster 7: 2006-05 to 2006-08 (4 months)\n",
      "   Cluster 8: 2006-10 to 2006-11 (2 months)\n",
      "   Cluster 9: 2007-01 to 2007-02 (2 months)\n",
      "   Cluster 10: 2008-02 to 2008-02 (1 months)\n",
      "   Cluster 11: 2009-11 to 2009-11 (1 months)\n",
      "   Cluster 12: 2012-05 to 2012-05 (1 months)\n",
      "   Cluster 13: 2012-07 to 2012-08 (2 months)\n",
      "   Cluster 14: 2012-10 to 2012-10 (1 months)\n"
     ]
    }
   ],
   "source": [
    "def diagnose_unknown_regimes(df_regimes_advanced, df_features_aug, L_index):\n",
    "    \"\"\"\n",
    "    Understand why some periods are marked as Unknown\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"DIAGNOSIS: Unknown Regime Periods\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Find Unknown periods\n",
    "    unknown_mask = df_regimes_advanced['regime_label'] == 'Unknown'\n",
    "    unknown_dates = df_regimes_advanced[unknown_mask].index\n",
    "    \n",
    "    print(f\"\\n📊 Unknown Periods: {len(unknown_dates)} months ({len(unknown_dates)/len(df_regimes_advanced)*100:.1f}%)\")\n",
    "    \n",
    "    if len(unknown_dates) > 0:\n",
    "        print(f\"\\n📅 Unknown Date Range:\")\n",
    "        print(f\"   First: {unknown_dates[0].strftime('%Y-%m')}\")\n",
    "        print(f\"   Last:  {unknown_dates[-1].strftime('%Y-%m')}\")\n",
    "        \n",
    "        # Check which features are missing\n",
    "        print(f\"\\n🔍 Missing Data Analysis:\")\n",
    "        \n",
    "        feature_cols = ['dlog_M2', 'dlog_FED_BS', 'EM', 'EB', 'EL_3y', 'Net_Liq',\n",
    "                       'Term_Spread', 'Credit_Spread', 'Real_Rate', 'VIX',\n",
    "                       'M2_growth', 'FED_BS_growth', 'V_spread']\n",
    "        \n",
    "        for feat in feature_cols:\n",
    "            if feat in df_features_aug.columns:\n",
    "                # Check missing in unknown periods\n",
    "                unknown_data = df_features_aug.loc[unknown_dates, feat]\n",
    "                n_missing = unknown_data.isna().sum()\n",
    "                pct_missing = n_missing / len(unknown_dates) * 100\n",
    "                \n",
    "                if n_missing > 0:\n",
    "                    print(f\"   {feat:20s}: {n_missing:3d}/{len(unknown_dates)} missing ({pct_missing:5.1f}%)\")\n",
    "        \n",
    "        # Check if Unknown periods cluster at start/end\n",
    "        print(f\"\\n📍 Unknown Period Distribution:\")\n",
    "        \n",
    "        # Group consecutive unknowns\n",
    "        unknown_groups = []\n",
    "        current_group = [unknown_dates[0]]\n",
    "        \n",
    "        for i in range(1, len(unknown_dates)):\n",
    "            if (unknown_dates[i] - unknown_dates[i-1]).days < 60:  # Within 2 months\n",
    "                current_group.append(unknown_dates[i])\n",
    "            else:\n",
    "                unknown_groups.append(current_group)\n",
    "                current_group = [unknown_dates[i]]\n",
    "        unknown_groups.append(current_group)\n",
    "        \n",
    "        print(f\"   Found {len(unknown_groups)} clusters of unknown periods:\")\n",
    "        for i, group in enumerate(unknown_groups, 1):\n",
    "            print(f\"   Cluster {i}: {group[0].strftime('%Y-%m')} to {group[-1].strftime('%Y-%m')} ({len(group)} months)\")\n",
    "\n",
    "# Run diagnosis\n",
    "diagnose_unknown_regimes(df_regimes_advanced, df_features_aug, L_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5267697a-9d40-4e1b-ac1f-34699c083497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['M2', 'FED_BS', 'FFR', 'Real_Rate', 'T3M', 'T10Y', 'BAA', 'AAA', 'CPI',\n",
       "       'CORE_CPI', 'GDP', 'IORB', 'RRP', 'TGA', 'Bank_Reserves', 'CI_Loans',\n",
       "       'Consumer_Credit', 'Mortgage_Rate', 'Prime_Rate', 'STLFSI',\n",
       "       'Dollar_Index', 'Household_Debt_Service', 'mkt_excess', 'smb', 'hml',\n",
       "       'rmw', 'cma', 'rf', 'P', 'CAPE', 'VIX', 'M2_growth', 'FED_BS_growth',\n",
       "       'Term_Spread', 'Credit_Spread', 'CPI_inflation', 'log_CAPE',\n",
       "       'CAPE_zscore', 'Risk_Adj_Spread', 'Net_Liq', 'HML_cumret', 'HML_12m',\n",
       "       'V_spread', 'V_spread_z', 'V_level', 'dlog_M2', 'dlog_FED_BS', 'log_M2',\n",
       "       'log_FED_BS', 'EM', 'EB', 'log_GDP', 'EL_3y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features_aug.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3fb02079-76f2-415f-bead-fd9e66ad8394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HML REGIME CLASSIFICATION (WITH CRISIS DETECTION)\n",
      "================================================================================\n",
      "\n",
      "✅ Aligned: 185 observations\n",
      "\n",
      "================================================================================\n",
      "STEP 1: CRISIS DETECTION\n",
      "================================================================================\n",
      "\n",
      "📊 Crisis Features Created: 10\n",
      "   Valid crisis detection samples: 168/185\n",
      "\n",
      "✅ Crisis Detection Complete:\n",
      "   Crisis periods:     0 months (0.0%)\n",
      "   Non-crisis periods: 185 months (100.0%)\n",
      "\n",
      "📊 Non-Crisis Data: 185 observations\n",
      "\n",
      "================================================================================\n",
      "STEP 2: DEFINE HML REGIMES (NON-CRISIS PERIODS)\n",
      "================================================================================\n",
      "\n",
      "📊 Using: 12-month rolling HML\n",
      "   Range: -0.42 to 0.31\n",
      "   Mean:  -0.00\n",
      "   Std:   0.13\n",
      "\n",
      "📊 HML Regime Thresholds (Non-Crisis):\n",
      "   33rd percentile: -0.07\n",
      "   67th percentile: 0.06\n",
      "\n",
      "📊 HML Regime Distribution (Non-Crisis):\n",
      "   Growth-Friendly :   61 ( 33.0%)\n",
      "      Mean HML: -0.14, Mean L: +0.16\n",
      "   Neutral         :   63 ( 34.1%)\n",
      "      Mean HML: -0.00, Mean L: -0.27\n",
      "   Value-Friendly  :   61 ( 33.0%)\n",
      "      Mean HML: +0.13, Mean L: +0.12\n",
      "\n",
      "================================================================================\n",
      "STEP 3: LIQUIDITY FEATURES\n",
      "================================================================================\n",
      "\n",
      "✅ Selected 18 features\n",
      "\n",
      "📊 ML Dataset: 185 samples × 18 features\n",
      "\n",
      "================================================================================\n",
      "STEP 4: MODEL TRAINING & CROSS-VALIDATION\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "Random Forest\n",
      "======================================================================\n",
      "\n",
      "📊 Cross-Validation (Time Series):\n",
      "   Mean Accuracy: 0.293 (±0.068)\n",
      "   Fold Scores:   ['0.300', '0.300', '0.167', '0.367', '0.333']\n",
      "   vs Baseline:   -4.0%\n",
      "\n",
      "📈 In-Sample Accuracy: 0.924\n",
      "\n",
      "📋 Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Growth-Friendly       0.94      0.97      0.95        61\n",
      "        Neutral       0.92      0.89      0.90        63\n",
      " Value-Friendly       0.92      0.92      0.92        61\n",
      "\n",
      "       accuracy                           0.92       185\n",
      "      macro avg       0.92      0.92      0.92       185\n",
      "   weighted avg       0.92      0.92      0.92       185\n",
      "\n",
      "\n",
      "🎯 Confusion Matrix (%):\n",
      "              Pred_Growth  Pred_Neutral  Pred_Value\n",
      "True_Growth          96.7           3.3         0.0\n",
      "True_Neutral          3.2          88.9         7.9\n",
      "True_Value            3.3           4.9        91.8\n",
      "\n",
      "======================================================================\n",
      "XGBoost\n",
      "======================================================================\n",
      "\n",
      "📊 Cross-Validation (Time Series):\n",
      "   Mean Accuracy: 0.293 (±0.108)\n",
      "   Fold Scores:   ['0.433', '0.167', '0.167', '0.333', '0.367']\n",
      "   vs Baseline:   -4.0%\n",
      "\n",
      "📈 In-Sample Accuracy: 1.000\n",
      "\n",
      "📋 Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Growth-Friendly       1.00      1.00      1.00        61\n",
      "        Neutral       1.00      1.00      1.00        63\n",
      " Value-Friendly       1.00      1.00      1.00        61\n",
      "\n",
      "       accuracy                           1.00       185\n",
      "      macro avg       1.00      1.00      1.00       185\n",
      "   weighted avg       1.00      1.00      1.00       185\n",
      "\n",
      "\n",
      "🎯 Confusion Matrix (%):\n",
      "              Pred_Growth  Pred_Neutral  Pred_Value\n",
      "True_Growth         100.0           0.0         0.0\n",
      "True_Neutral          0.0         100.0         0.0\n",
      "True_Value            0.0           0.0       100.0\n",
      "\n",
      "================================================================================\n",
      "STEP 5: FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "🏆 Best Model: Random Forest (CV: 0.293)\n",
      "\n",
      "📊 Top 15 Features:\n",
      "   💧 EB                  : 0.1444\n",
      "   💧 EM                  : 0.1342\n",
      "      Credit_Spread       : 0.0739\n",
      "      VIX                 : 0.0566\n",
      "   💧 FED_BS_growth       : 0.0543\n",
      "   💧 L_momentum          : 0.0534\n",
      "      Term_Spread         : 0.0515\n",
      "   💧 EL_3y               : 0.0507\n",
      "   💧 dlog_FED_BS         : 0.0505\n",
      "   💧 L_ma_6              : 0.0471\n",
      "   💧 L_accel             : 0.0464\n",
      "   💧 HML_lag6            : 0.0436\n",
      "   💧 dlog_M2             : 0.0347\n",
      "   💧 L_ma_3              : 0.0329\n",
      "   💧 HML_lag3            : 0.0327\n",
      "\n",
      "💧 Total Liquidity Importance: 78.5%\n",
      "\n",
      "================================================================================\n",
      "HYPOTHESIS TEST: LIQUIDITY → HML REGIMES\n",
      "================================================================================\n",
      "\n",
      "📊 Results:\n",
      "   Baseline:      33.3%\n",
      "   CV Accuracy:   29.3%\n",
      "   Improvement:   -4.0%\n",
      "   Lift:          0.88x\n",
      "\n",
      "   ❌ WEAK SUPPORT for hypothesis\n",
      "\n",
      "📊 Final Distribution:\n",
      "   Growth-Friendly :   63 ( 34.1%)\n",
      "   Neutral         :   61 ( 33.0%)\n",
      "   Value-Friendly  :   61 ( 33.0%)\n",
      "\n",
      "================================================================================\n",
      "✅ COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def build_hml_regime_classification_with_crisis(df, L_series):\n",
    "    \"\"\"\n",
    "    1. Detect crisis periods (Isolation Forest)\n",
    "    2. Classify NON-CRISIS periods by HML regimes\n",
    "    3. Predict HML regimes from liquidity features\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"HML REGIME CLASSIFICATION (WITH CRISIS DETECTION)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 0: Data Alignment\n",
    "    # ==========================================\n",
    "    \n",
    "    common_idx = df.index.intersection(L_series.index)\n",
    "    df_aligned = df.loc[common_idx].copy()\n",
    "    df_aligned['L'] = L_series.loc[common_idx]\n",
    "    \n",
    "    print(f\"\\n✅ Aligned: {len(df_aligned)} observations\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 1: Crisis Detection (Isolation Forest)\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 1: CRISIS DETECTION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df_crisis = df_aligned.copy()\n",
    "    \n",
    "    # Create crisis features (volatility, drawdowns, tail events)\n",
    "    df_crisis['ret_vol_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).std()\n",
    "    df_crisis['ret_vol_12m'] = df_crisis['mkt_excess'].rolling(12, min_periods=6).std()\n",
    "    df_crisis['L_vol_3m'] = df_crisis['L'].rolling(3, min_periods=2).std()\n",
    "    df_crisis['L_vol_6m'] = df_crisis['L'].rolling(6, min_periods=3).std()\n",
    "    \n",
    "    # Return features\n",
    "    df_crisis['abs_ret'] = df_crisis['mkt_excess'].abs()\n",
    "    df_crisis['cum_ret_3m'] = df_crisis['mkt_excess'].rolling(3, min_periods=2).sum()\n",
    "    df_crisis['cum_ret_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).sum()\n",
    "    \n",
    "    # Drawdown\n",
    "    cumret = (1 + df_crisis['mkt_excess']/100).cumprod()\n",
    "    running_max = cumret.expanding().max()\n",
    "    df_crisis['drawdown'] = (cumret / running_max - 1) * 100\n",
    "    df_crisis['max_dd_6m'] = df_crisis['drawdown'].rolling(6, min_periods=3).min()\n",
    "    \n",
    "    # Crisis features list\n",
    "    crisis_features = ['ret_vol_6m', 'ret_vol_12m', 'L_vol_3m', 'L_vol_6m', \n",
    "                       'abs_ret', 'cum_ret_3m', 'cum_ret_6m', 'max_dd_6m']\n",
    "    \n",
    "    # Add VIX if available\n",
    "    if 'VIX' in df_crisis.columns:\n",
    "        df_crisis['VIX_level'] = df_crisis['VIX']\n",
    "        df_crisis['VIX_zscore'] = (df_crisis['VIX'] - df_crisis['VIX'].rolling(60, min_periods=30).mean()) / \\\n",
    "                                   df_crisis['VIX'].rolling(60, min_periods=30).std()\n",
    "        crisis_features.extend(['VIX_level', 'VIX_zscore'])\n",
    "    \n",
    "    print(f\"\\n📊 Crisis Features Created: {len(crisis_features)}\")\n",
    "    \n",
    "    # Extract and clean crisis data\n",
    "    crisis_data = df_crisis[crisis_features].copy()\n",
    "    crisis_data = crisis_data.fillna(method='bfill', limit=12)\n",
    "    crisis_data = crisis_data.dropna()\n",
    "    \n",
    "    print(f\"   Valid crisis detection samples: {len(crisis_data)}/{len(df_crisis)}\")\n",
    "    \n",
    "    if len(crisis_data) >= 20:\n",
    "        # Fit Isolation Forest\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=0.05,  # Expect ~5% crisis periods\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_samples='auto'\n",
    "        )\n",
    "        \n",
    "        outlier_labels = iso_forest.fit_predict(crisis_data.values)\n",
    "        \n",
    "        # Crisis = outlier + negative return (< -2%)\n",
    "        is_crisis_candidate = (outlier_labels == -1)\n",
    "        is_negative_return = (df_crisis.loc[crisis_data.index, 'mkt_excess'] < -2)\n",
    "        \n",
    "        df_crisis['is_crisis'] = False\n",
    "        df_crisis.loc[crisis_data.index, 'is_crisis'] = is_crisis_candidate & is_negative_return\n",
    "        \n",
    "        n_crisis = df_crisis['is_crisis'].sum()\n",
    "        crisis_pct = n_crisis / len(df_crisis) * 100\n",
    "        \n",
    "        print(f\"\\n✅ Crisis Detection Complete:\")\n",
    "        print(f\"   Crisis periods:     {n_crisis} months ({crisis_pct:.1f}%)\")\n",
    "        print(f\"   Non-crisis periods: {len(df_crisis) - n_crisis} months ({100-crisis_pct:.1f}%)\")\n",
    "        \n",
    "        # Show detected crisis periods\n",
    "        if n_crisis > 0:\n",
    "            crisis_periods = df_crisis[df_crisis['is_crisis']].index\n",
    "            print(f\"\\n📅 Detected Crisis Months:\")\n",
    "            for i, date in enumerate(crisis_periods[:10], 1):\n",
    "                ret = df_crisis.loc[date, 'mkt_excess']\n",
    "                L_val = df_crisis.loc[date, 'L']\n",
    "                print(f\"   {i:2d}. {date.strftime('%Y-%m')}: Return = {ret:+.2f}%, L = {L_val:+.2f}\")\n",
    "            if len(crisis_periods) > 10:\n",
    "                print(f\"   ... and {len(crisis_periods) - 10} more\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Insufficient data for crisis detection, marking all as non-crisis\")\n",
    "        df_crisis['is_crisis'] = False\n",
    "    \n",
    "    # Filter to non-crisis periods for HML regime analysis\n",
    "    df_noncrisis = df_crisis[~df_crisis['is_crisis']].copy()\n",
    "    \n",
    "    print(f\"\\n📊 Non-Crisis Data: {len(df_noncrisis)} observations\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 2: Define HML Regimes (Non-Crisis Only)\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 2: DEFINE HML REGIMES (NON-CRISIS PERIODS)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Check for HML data\n",
    "    if 'hml' not in df_noncrisis.columns and 'HML' not in df_noncrisis.columns:\n",
    "        raise ValueError(\"No HML column found in dataframe\")\n",
    "    \n",
    "    hml_col = 'hml' if 'hml' in df_noncrisis.columns else 'HML'\n",
    "    \n",
    "    # Use 12-month rolling HML for smoother regimes\n",
    "    if 'HML_12m' in df_noncrisis.columns:\n",
    "        hml_metric = df_noncrisis['HML_12m']\n",
    "        metric_col = 'HML_12m'\n",
    "        metric_name = \"12-month rolling HML\"\n",
    "    else:\n",
    "        df_noncrisis['HML_12m'] = df_noncrisis[hml_col].rolling(12, min_periods=6).sum()\n",
    "        hml_metric = df_noncrisis['HML_12m']\n",
    "        metric_col = 'HML_12m'\n",
    "        metric_name = \"12-month rolling HML (created)\"\n",
    "    \n",
    "    print(f\"\\n📊 Using: {metric_name}\")\n",
    "    print(f\"   Range: {hml_metric.min():.2f} to {hml_metric.max():.2f}\")\n",
    "    print(f\"   Mean:  {hml_metric.mean():.2f}\")\n",
    "    print(f\"   Std:   {hml_metric.std():.2f}\")\n",
    "    \n",
    "    # Define regime thresholds (tertiles on NON-CRISIS data)\n",
    "    q33 = hml_metric.quantile(0.33)\n",
    "    q67 = hml_metric.quantile(0.67)\n",
    "    \n",
    "    print(f\"\\n📊 HML Regime Thresholds (Non-Crisis):\")\n",
    "    print(f\"   33rd percentile: {q33:.2f}\")\n",
    "    print(f\"   67th percentile: {q67:.2f}\")\n",
    "    \n",
    "    # Create regime labels\n",
    "    df_noncrisis['hml_regime'] = pd.cut(\n",
    "        hml_metric,\n",
    "        bins=[-np.inf, q33, q67, np.inf],\n",
    "        labels=[0, 1, 2]\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_noncrisis['hml_regime_label'] = df_noncrisis['hml_regime'].map({\n",
    "        0: 'Growth-Friendly',\n",
    "        1: 'Neutral', \n",
    "        2: 'Value-Friendly'\n",
    "    })\n",
    "    \n",
    "    # Remove NaN regimes\n",
    "    df_regimes = df_noncrisis.dropna(subset=['hml_regime'])\n",
    "    \n",
    "    print(f\"\\n📊 HML Regime Distribution (Non-Crisis):\")\n",
    "    for regime, label in [(0, 'Growth-Friendly'), (1, 'Neutral'), (2, 'Value-Friendly')]:\n",
    "        mask = df_regimes['hml_regime'] == regime\n",
    "        count = mask.sum()\n",
    "        pct = count / len(df_regimes) * 100\n",
    "        mean_hml = df_regimes.loc[mask, metric_col].mean()\n",
    "        mean_L = df_regimes.loc[mask, 'L'].mean()\n",
    "        print(f\"   {label:16s}: {count:4d} ({pct:5.1f}%)\")\n",
    "        print(f\"      Mean HML: {mean_hml:+.2f}, Mean L: {mean_L:+.2f}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 3: Feature Engineering (Liquidity)\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 3: LIQUIDITY FEATURES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    feature_cols = []\n",
    "    \n",
    "    # Core liquidity components\n",
    "    liq_features = ['dlog_M2', 'dlog_FED_BS', 'EM', 'EB', 'EL_3y']\n",
    "    feature_cols.extend([f for f in liq_features if f in df_regimes.columns])\n",
    "    \n",
    "    # Rates and spreads\n",
    "    rate_features = ['Term_Spread', 'Credit_Spread', 'Real_Rate']\n",
    "    feature_cols.extend([f for f in rate_features if f in df_regimes.columns])\n",
    "    \n",
    "    # Market indicators\n",
    "    if 'VIX' in df_regimes.columns:\n",
    "        feature_cols.append('VIX')\n",
    "    \n",
    "    # Growth rates\n",
    "    if 'M2_growth' in df_regimes.columns:\n",
    "        feature_cols.append('M2_growth')\n",
    "    if 'FED_BS_growth' in df_regimes.columns:\n",
    "        feature_cols.append('FED_BS_growth')\n",
    "    \n",
    "    # LIQUIDITY INDEX (L)\n",
    "    feature_cols.append('L')\n",
    "    \n",
    "    # Technical on L\n",
    "    df_regimes['L_ma_3'] = df_regimes['L'].rolling(3, min_periods=1).mean()\n",
    "    df_regimes['L_ma_6'] = df_regimes['L'].rolling(6, min_periods=3).mean()\n",
    "    df_regimes['L_momentum'] = df_regimes['L'].diff(3)\n",
    "    df_regimes['L_accel'] = df_regimes['L_momentum'].diff(3)\n",
    "    \n",
    "    feature_cols.extend(['L_ma_3', 'L_ma_6', 'L_momentum', 'L_accel'])\n",
    "    \n",
    "    # Lagged HML (control for momentum)\n",
    "    df_regimes['HML_lag3'] = df_regimes[hml_col].shift(3)\n",
    "    df_regimes['HML_lag6'] = df_regimes[hml_col].shift(6)\n",
    "    feature_cols.extend(['HML_lag3', 'HML_lag6'])\n",
    "    \n",
    "    print(f\"\\n✅ Selected {len(feature_cols)} features\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 4: Prepare ML Dataset\n",
    "    # ==========================================\n",
    "    \n",
    "    X_raw = df_regimes[feature_cols].copy()\n",
    "    y = df_regimes['hml_regime'].copy()\n",
    "    \n",
    "    # Imputation\n",
    "    X_imputed = X_raw.fillna(method='ffill', limit=6).fillna(method='bfill', limit=6)\n",
    "    \n",
    "    for col in X_imputed.columns:\n",
    "        if X_imputed[col].isna().any():\n",
    "            X_imputed[col] = X_imputed[col].fillna(X_imputed[col].median())\n",
    "    \n",
    "    # Final dataset\n",
    "    valid_idx = X_imputed.notna().all(axis=1) & y.notna()\n",
    "    X = X_imputed[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "    \n",
    "    print(f\"\\n📊 ML Dataset: {len(X)} samples × {len(feature_cols)} features\")\n",
    "    \n",
    "    if len(X) < 50:\n",
    "        raise ValueError(f\"Insufficient data: {len(X)} samples\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 5: Train Models with CV\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 4: MODEL TRAINING & CROSS-VALIDATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',\n",
    "            use_label_encoder=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"{name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy', n_jobs=-1)\n",
    "        \n",
    "        print(f\"\\n📊 Cross-Validation (Time Series):\")\n",
    "        print(f\"   Mean Accuracy: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})\")\n",
    "        print(f\"   Fold Scores:   {[f'{s:.3f}' for s in cv_scores]}\")\n",
    "        \n",
    "        baseline = 1/3\n",
    "        improvement = cv_scores.mean() - baseline\n",
    "        print(f\"   vs Baseline:   {improvement:+.1%}\")\n",
    "        \n",
    "        # Train full model\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # In-sample\n",
    "        y_pred = model.predict(X)\n",
    "        train_acc = (y_pred == y).mean()\n",
    "        \n",
    "        print(f\"\\n📈 In-Sample Accuracy: {train_acc:.3f}\")\n",
    "        \n",
    "        # Classification report\n",
    "        print(f\"\\n📋 Classification Report:\")\n",
    "        print(classification_report(\n",
    "            y, y_pred,\n",
    "            target_names=['Growth-Friendly', 'Neutral', 'Value-Friendly'],\n",
    "            zero_division=0\n",
    "        ))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "        \n",
    "        print(f\"\\n🎯 Confusion Matrix (%):\")\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm_pct,\n",
    "            index=['True_Growth', 'True_Neutral', 'True_Value'],\n",
    "            columns=['Pred_Growth', 'Pred_Neutral', 'Pred_Value']\n",
    "        )\n",
    "        print(cm_df.round(1))\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'train_acc': train_acc,\n",
    "            'y_pred': y_pred\n",
    "        }\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 6: Feature Importance\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 5: FEATURE IMPORTANCE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    best_model_name = max(results, key=lambda k: results[k]['cv_mean'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    \n",
    "    print(f\"\\n🏆 Best Model: {best_model_name} (CV: {results[best_model_name]['cv_mean']:.3f})\")\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n📊 Top 15 Features:\")\n",
    "    for i, row in feature_importance.head(15).iterrows():\n",
    "        is_liquidity = any(liq in row['feature'] for liq in ['L', 'M2', 'FED', 'EM', 'EB', 'EL'])\n",
    "        marker = \"💧\" if is_liquidity else \"  \"\n",
    "        print(f\"   {marker} {row['feature']:20s}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Liquidity importance\n",
    "    liquidity_features = [f for f in feature_importance['feature'] \n",
    "                         if any(liq in f for liq in ['L', 'M2', 'FED', 'EM', 'EB', 'EL'])]\n",
    "    liquidity_importance = feature_importance[feature_importance['feature'].isin(liquidity_features)]['importance'].sum()\n",
    "    \n",
    "    print(f\"\\n💧 Total Liquidity Importance: {liquidity_importance:.1%}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 7: Hypothesis Test\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"HYPOTHESIS TEST: LIQUIDITY → HML REGIMES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    best_cv_acc = results[best_model_name]['cv_mean']\n",
    "    baseline = 1/3\n",
    "    improvement = best_cv_acc - baseline\n",
    "    \n",
    "    print(f\"\\n📊 Results:\")\n",
    "    print(f\"   Baseline:      {baseline:.1%}\")\n",
    "    print(f\"   CV Accuracy:   {best_cv_acc:.1%}\")\n",
    "    print(f\"   Improvement:   {improvement:+.1%}\")\n",
    "    print(f\"   Lift:          {best_cv_acc/baseline:.2f}x\")\n",
    "    \n",
    "    if best_cv_acc >= 0.50:\n",
    "        print(f\"\\n   ✅ STRONG SUPPORT for hypothesis\")\n",
    "    elif best_cv_acc >= 0.42:\n",
    "        print(f\"\\n   ✔️  MODERATE SUPPORT for hypothesis\")\n",
    "    else:\n",
    "        print(f\"\\n   ❌ WEAK SUPPORT for hypothesis\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 8: Final Assignment\n",
    "    # ==========================================\n",
    "    \n",
    "    df_final = df_crisis.copy()\n",
    "    df_final['final_regime'] = np.nan\n",
    "    df_final['final_regime_label'] = 'Unknown'\n",
    "    \n",
    "    # Crisis\n",
    "    df_final.loc[df_final['is_crisis'], 'final_regime'] = 3\n",
    "    df_final.loc[df_final['is_crisis'], 'final_regime_label'] = 'Crisis'\n",
    "    \n",
    "    # HML regimes (predicted)\n",
    "    df_final.loc[X.index, 'final_regime'] = best_model.predict(X)\n",
    "    df_final.loc[X.index, 'final_regime_label'] = pd.Series(\n",
    "        best_model.predict(X),\n",
    "        index=X.index\n",
    "    ).map({0: 'Growth-Friendly', 1: 'Neutral', 2: 'Value-Friendly'})\n",
    "    \n",
    "    # Also keep actual HML regime\n",
    "    df_final.loc[df_regimes.index, 'actual_hml_regime'] = df_regimes['hml_regime']\n",
    "    df_final.loc[df_regimes.index, 'actual_hml_regime_label'] = df_regimes['hml_regime_label']\n",
    "    \n",
    "    print(f\"\\n📊 Final Distribution:\")\n",
    "    dist = df_final['final_regime_label'].value_counts()\n",
    "    for regime in ['Crisis', 'Growth-Friendly', 'Neutral', 'Value-Friendly', 'Unknown']:\n",
    "        if regime in dist.index:\n",
    "            count = dist[regime]\n",
    "            pct = count / len(df_final) * 100\n",
    "            print(f\"   {regime:16s}: {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✅ COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return df_final, results, feature_importance\n",
    "\n",
    "\n",
    "# Run with crisis detection\n",
    "df_hml_regimes, hml_results, hml_feat_imp = build_hml_regime_classification_with_crisis(\n",
    "    df_features_aug,\n",
    "    L_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "735bfb86-be83-4617-bffa-73ad26f691fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              mkt_excess       hml       rmw       cma\n",
      "regime_label                                          \n",
      "High            0.692131  0.374426  0.289180  0.525246\n",
      "Neutral         0.826190  0.099524  0.430159 -0.144762\n",
      "Tight           1.039672 -0.323115  0.007049 -0.536230\n"
     ]
    }
   ],
   "source": [
    "factor_returns_by_regime = (\n",
    "    df_regimes_fixed.groupby(\"regime_label\")[[\"mkt_excess\", \"hml\", \"rmw\", \"cma\"]]\n",
    "    .mean() * 100\n",
    ")\n",
    "print(factor_returns_by_regime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f9f80a4a-cecf-4945-bf5d-5e78706a8289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HML REGIME CLASSIFICATION (WITH CRISIS DETECTION)\n",
      "================================================================================\n",
      "\n",
      "✅ Aligned: 185 observations\n",
      "\n",
      "================================================================================\n",
      "STEP 1: CRISIS DETECTION\n",
      "================================================================================\n",
      "\n",
      "📊 Crisis Features Created: 10\n",
      "   Valid crisis detection samples: 168/185\n",
      "\n",
      "✅ Crisis Detection Complete:\n",
      "   Crisis periods:     9 months (4.9%)\n",
      "   Non-crisis periods: 176 months (95.1%)\n",
      "\n",
      "📅 Detected Crisis Months:\n",
      "    1. 2008-09: Return = -0.09%, L = +2.15\n",
      "    2. 2008-10: Return = -0.17%, L = +5.37\n",
      "    3. 2008-12: Return = +0.02%, L = +4.80\n",
      "    4. 2009-03: Return = +0.09%, L = +4.66\n",
      "    5. 2009-04: Return = +0.10%, L = +4.80\n",
      "    6. 2009-06: Return = +0.00%, L = +3.73\n",
      "    7. 2009-07: Return = +0.08%, L = +3.40\n",
      "    8. 2020-03: Return = -0.13%, L = +1.94\n",
      "    9. 2020-04: Return = +0.14%, L = +2.11\n",
      "\n",
      "📊 Non-Crisis Data: 176 observations\n",
      "\n",
      "================================================================================\n",
      "STEP 2: DEFINE HML REGIMES (NON-CRISIS PERIODS)\n",
      "================================================================================\n",
      "\n",
      "📊 Using: 6-month FORWARD HML\n",
      "   Range: -0.20 to 0.25\n",
      "   Mean:  0.00\n",
      "   Std:   0.07\n",
      "\n",
      "📊 After removing NaN: 171 observations\n",
      "\n",
      "📊 HML Regime Thresholds (Non-Crisis):\n",
      "   33rd percentile: -0.03\n",
      "   67th percentile: 0.03\n",
      "\n",
      "📊 HML Regime Distribution (Non-Crisis):\n",
      "   Growth-Friendly :   57 ( 33.3%)\n",
      "      Mean HML: -0.07, Mean L: -0.54\n",
      "   Neutral         :   58 ( 33.9%)\n",
      "      Mean HML: +0.00, Mean L: +0.03\n",
      "   Value-Friendly  :   56 ( 32.7%)\n",
      "      Mean HML: +0.08, Mean L: +0.03\n",
      "\n",
      "================================================================================\n",
      "STEP 3: LIQUIDITY FEATURES\n",
      "================================================================================\n",
      "\n",
      "✅ Selected 18 features\n",
      "\n",
      "📊 ML Dataset: 171 samples × 18 features\n",
      "\n",
      "================================================================================\n",
      "STEP 4: MODEL TRAINING & CROSS-VALIDATION\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "Random Forest\n",
      "======================================================================\n",
      "\n",
      "📊 Cross-Validation (Time Series):\n",
      "   Mean Accuracy: 0.307 (±0.107)\n",
      "   Fold Scores:   ['0.286', '0.214', '0.179', '0.464', '0.393']\n",
      "   vs Baseline:   -2.6%\n",
      "\n",
      "📈 In-Sample Accuracy: 0.895\n",
      "\n",
      "📋 Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Growth-Friendly       0.89      0.95      0.92        57\n",
      "        Neutral       0.84      0.84      0.84        58\n",
      " Value-Friendly       0.96      0.89      0.93        56\n",
      "\n",
      "       accuracy                           0.89       171\n",
      "      macro avg       0.90      0.90      0.90       171\n",
      "   weighted avg       0.90      0.89      0.89       171\n",
      "\n",
      "\n",
      "🎯 Confusion Matrix (%):\n",
      "              Pred_Growth  Pred_Neutral  Pred_Value\n",
      "True_Growth          94.7           5.3         0.0\n",
      "True_Neutral         12.1          84.5         3.4\n",
      "True_Value            0.0          10.7        89.3\n",
      "\n",
      "======================================================================\n",
      "XGBoost\n",
      "======================================================================\n",
      "\n",
      "📊 Cross-Validation (Time Series):\n",
      "   Mean Accuracy: 0.214 (±0.106)\n",
      "   Fold Scores:   ['0.179', '0.071', '0.179', '0.250', '0.393']\n",
      "   vs Baseline:   -11.9%\n",
      "\n",
      "📈 In-Sample Accuracy: 1.000\n",
      "\n",
      "📋 Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Growth-Friendly       1.00      1.00      1.00        57\n",
      "        Neutral       1.00      1.00      1.00        58\n",
      " Value-Friendly       1.00      1.00      1.00        56\n",
      "\n",
      "       accuracy                           1.00       171\n",
      "      macro avg       1.00      1.00      1.00       171\n",
      "   weighted avg       1.00      1.00      1.00       171\n",
      "\n",
      "\n",
      "🎯 Confusion Matrix (%):\n",
      "              Pred_Growth  Pred_Neutral  Pred_Value\n",
      "True_Growth         100.0           0.0         0.0\n",
      "True_Neutral          0.0         100.0         0.0\n",
      "True_Value            0.0           0.0       100.0\n",
      "\n",
      "================================================================================\n",
      "STEP 5: FEATURE IMPORTANCE\n",
      "================================================================================\n",
      "\n",
      "🏆 Best Model: Random Forest (CV: 0.307)\n",
      "\n",
      "📊 Top 15 Features:\n",
      "   💧 FED_BS_growth       : 0.1034\n",
      "      Term_Spread         : 0.0878\n",
      "   💧 M2_growth           : 0.0815\n",
      "   💧 EM                  : 0.0766\n",
      "   💧 EB                  : 0.0755\n",
      "   💧 HML_lag6            : 0.0583\n",
      "   💧 L                   : 0.0580\n",
      "      Credit_Spread       : 0.0547\n",
      "   💧 EL_3y               : 0.0530\n",
      "   💧 L_ma_3              : 0.0501\n",
      "   💧 L_ma_6              : 0.0485\n",
      "   💧 dlog_M2             : 0.0465\n",
      "   💧 L_momentum          : 0.0414\n",
      "      Real_Rate           : 0.0399\n",
      "   💧 dlog_FED_BS         : 0.0364\n",
      "\n",
      "💧 Total Liquidity Importance: 78.5%\n",
      "\n",
      "================================================================================\n",
      "HYPOTHESIS TEST: LIQUIDITY → HML REGIMES\n",
      "================================================================================\n",
      "\n",
      "📊 Results:\n",
      "   Baseline:      33.3%\n",
      "   CV Accuracy:   30.7%\n",
      "   Improvement:   -2.6%\n",
      "   Lift:          0.92x\n",
      "\n",
      "   ❌ WEAK SUPPORT for hypothesis\n",
      "\n",
      "📊 Final Distribution:\n",
      "   Crisis          :    9 (  4.9%)\n",
      "   Growth-Friendly :   61 ( 33.0%)\n",
      "   Neutral         :   58 ( 31.4%)\n",
      "   Value-Friendly  :   52 ( 28.1%)\n",
      "   Unknown         :    5 (  2.7%)\n",
      "\n",
      "================================================================================\n",
      "✅ COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def build_hml_regime_classification_with_crisis(df, L_series):\n",
    "    \"\"\"\n",
    "    1. Detect crisis periods (Isolation Forest)\n",
    "    2. Classify NON-CRISIS periods by HML regimes\n",
    "    3. Predict HML regimes from liquidity features\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"HML REGIME CLASSIFICATION (WITH CRISIS DETECTION)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 0: Data Alignment\n",
    "    # ==========================================\n",
    "    \n",
    "    common_idx = df.index.intersection(L_series.index)\n",
    "    df_aligned = df.loc[common_idx].copy()\n",
    "    df_aligned['L'] = L_series.loc[common_idx]\n",
    "    \n",
    "    print(f\"\\n✅ Aligned: {len(df_aligned)} observations\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 1: Crisis Detection (Isolation Forest)\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 1: CRISIS DETECTION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df_crisis = df_aligned.copy()\n",
    "    \n",
    "    # Create crisis features (volatility, drawdowns, tail events)\n",
    "    df_crisis['ret_vol_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).std()\n",
    "    df_crisis['ret_vol_12m'] = df_crisis['mkt_excess'].rolling(12, min_periods=6).std()\n",
    "    df_crisis['L_vol_3m'] = df_crisis['L'].rolling(3, min_periods=2).std()\n",
    "    df_crisis['L_vol_6m'] = df_crisis['L'].rolling(6, min_periods=3).std()\n",
    "    \n",
    "    # Return features\n",
    "    df_crisis['abs_ret'] = df_crisis['mkt_excess'].abs()\n",
    "    df_crisis['cum_ret_3m'] = df_crisis['mkt_excess'].rolling(3, min_periods=2).sum()\n",
    "    df_crisis['cum_ret_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).sum()\n",
    "    \n",
    "    # Drawdown\n",
    "    cumret = (1 + df_crisis['mkt_excess']/100).cumprod()\n",
    "    running_max = cumret.expanding().max()\n",
    "    df_crisis['drawdown'] = (cumret / running_max - 1) * 100\n",
    "    df_crisis['max_dd_6m'] = df_crisis['drawdown'].rolling(6, min_periods=3).min()\n",
    "    \n",
    "    # Crisis features list\n",
    "    crisis_features = ['ret_vol_6m', 'ret_vol_12m', 'L_vol_3m', 'L_vol_6m', \n",
    "                       'abs_ret', 'cum_ret_3m', 'cum_ret_6m', 'max_dd_6m']\n",
    "    \n",
    "    # Add VIX if available\n",
    "    if 'VIX' in df_crisis.columns:\n",
    "        df_crisis['VIX_level'] = df_crisis['VIX']\n",
    "        df_crisis['VIX_zscore'] = (df_crisis['VIX'] - df_crisis['VIX'].rolling(60, min_periods=30).mean()) / \\\n",
    "                                   df_crisis['VIX'].rolling(60, min_periods=30).std()\n",
    "        crisis_features.extend(['VIX_level', 'VIX_zscore'])\n",
    "    \n",
    "    print(f\"\\n📊 Crisis Features Created: {len(crisis_features)}\")\n",
    "    \n",
    "    # Extract and clean crisis data\n",
    "    crisis_data = df_crisis[crisis_features].copy()\n",
    "    crisis_data = crisis_data.fillna(method='bfill', limit=12)\n",
    "    crisis_data = crisis_data.dropna()\n",
    "    \n",
    "    print(f\"   Valid crisis detection samples: {len(crisis_data)}/{len(df_crisis)}\")\n",
    "    \n",
    "    if len(crisis_data) >= 20:\n",
    "        # Fit Isolation Forest\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=0.05,  # Expect ~5% crisis periods\n",
    "            random_state=42,\n",
    "            n_estimators=100,\n",
    "            max_samples='auto'\n",
    "        )\n",
    "        \n",
    "        outlier_labels = iso_forest.fit_predict(crisis_data.values)\n",
    "        \n",
    "        # Crisis = outlier + negative return (< -2%)\n",
    "        is_crisis_candidate = (outlier_labels == -1)\n",
    "        is_negative_return = (df_crisis.loc[crisis_data.index, 'mkt_excess'] < -2)\n",
    "        \n",
    "        df_crisis['is_crisis'] = False\n",
    "        df_crisis.loc[crisis_data.index, 'is_crisis'] = is_crisis_candidate\n",
    "        \n",
    "        n_crisis = df_crisis['is_crisis'].sum()\n",
    "        crisis_pct = n_crisis / len(df_crisis) * 100\n",
    "        \n",
    "        print(f\"\\n✅ Crisis Detection Complete:\")\n",
    "        print(f\"   Crisis periods:     {n_crisis} months ({crisis_pct:.1f}%)\")\n",
    "        print(f\"   Non-crisis periods: {len(df_crisis) - n_crisis} months ({100-crisis_pct:.1f}%)\")\n",
    "        \n",
    "        # Show detected crisis periods\n",
    "        if n_crisis > 0:\n",
    "            crisis_periods = df_crisis[df_crisis['is_crisis']].index\n",
    "            print(f\"\\n📅 Detected Crisis Months:\")\n",
    "            for i, date in enumerate(crisis_periods[:10], 1):\n",
    "                ret = df_crisis.loc[date, 'mkt_excess']\n",
    "                L_val = df_crisis.loc[date, 'L']\n",
    "                print(f\"   {i:2d}. {date.strftime('%Y-%m')}: Return = {ret:+.2f}%, L = {L_val:+.2f}\")\n",
    "            if len(crisis_periods) > 10:\n",
    "                print(f\"   ... and {len(crisis_periods) - 10} more\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  Insufficient data for crisis detection, marking all as non-crisis\")\n",
    "        df_crisis['is_crisis'] = False\n",
    "    \n",
    "    # Filter to non-crisis periods for HML regime analysis\n",
    "    df_noncrisis = df_crisis[~df_crisis['is_crisis']].copy()\n",
    "    \n",
    "    print(f\"\\n📊 Non-Crisis Data: {len(df_noncrisis)} observations\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 2: Define HML Regimes (Non-Crisis Only)\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 2: DEFINE HML REGIMES (NON-CRISIS PERIODS)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Check for HML data\n",
    "    if 'hml' not in df_noncrisis.columns and 'HML' not in df_noncrisis.columns:\n",
    "        raise ValueError(\"No HML column found in dataframe\")\n",
    "    \n",
    "    hml_col = 'hml' if 'hml' in df_noncrisis.columns else 'HML'\n",
    "    \n",
    "    # Use 12-month rolling HML for smoother regimes\n",
    "    # if 'HML_12m' in df_noncrisis.columns:\n",
    "    #     hml_metric = df_noncrisis['HML_12m']\n",
    "    #     metric_col = 'HML_12m'\n",
    "    #     metric_name = \"12-month rolling HML\"\n",
    "    # else:\n",
    "    #     df_noncrisis['HML_12m'] = df_noncrisis[hml_col].rolling(12, min_periods=6).sum()\n",
    "    #     hml_metric = df_noncrisis['HML_12m']\n",
    "    #     metric_col = 'HML_12m'\n",
    "    #     metric_name = \"12-month rolling HML (created)\"\n",
    "    df_noncrisis['HML_forward_6m'] = df_noncrisis[hml_col].shift(-6).rolling(\n",
    "        6, min_periods=3\n",
    "    ).sum()\n",
    "    hml_metric = df_noncrisis['HML_forward_6m']\n",
    "    metric_col = 'HML_forward_6m'\n",
    "    metric_name = \"6-month FORWARD HML\"\n",
    "    \n",
    "    print(f\"\\n📊 Using: {metric_name}\")\n",
    "    print(f\"   Range: {hml_metric.min():.2f} to {hml_metric.max():.2f}\")\n",
    "    print(f\"   Mean:  {hml_metric.mean():.2f}\")\n",
    "    print(f\"   Std:   {hml_metric.std():.2f}\")\n",
    "\n",
    "    # Remove NaN BEFORE creating regimes\n",
    "    df_noncrisis_valid = df_noncrisis.dropna(subset=['HML_forward_6m']).copy()\n",
    "    hml_metric_valid = df_noncrisis_valid['HML_forward_6m']\n",
    "    \n",
    "    print(f\"\\n📊 After removing NaN: {len(df_noncrisis_valid)} observations\")\n",
    "    \n",
    "    # Define regime thresholds (tertiles on NON-CRISIS data)\n",
    "    q33 = hml_metric.quantile(0.33)\n",
    "    q67 = hml_metric.quantile(0.67)\n",
    "    \n",
    "    print(f\"\\n📊 HML Regime Thresholds (Non-Crisis):\")\n",
    "    print(f\"   33rd percentile: {q33:.2f}\")\n",
    "    print(f\"   67th percentile: {q67:.2f}\")\n",
    "    \n",
    "    # Create regime labels\n",
    "    df_noncrisis['hml_regime'] = pd.cut(\n",
    "        hml_metric_valid,\n",
    "        bins=[-np.inf, q33, q67, np.inf],\n",
    "        labels=[0, 1, 2]\n",
    "    ).astype(int)\n",
    "    \n",
    "    df_noncrisis['hml_regime_label'] = df_noncrisis['hml_regime'].map({\n",
    "        0: 'Growth-Friendly',\n",
    "        1: 'Neutral', \n",
    "        2: 'Value-Friendly'\n",
    "    })\n",
    "    \n",
    "    # Remove NaN regimes\n",
    "    df_regimes = df_noncrisis.dropna(subset=['hml_regime'])\n",
    "    \n",
    "    print(f\"\\n📊 HML Regime Distribution (Non-Crisis):\")\n",
    "    for regime, label in [(0, 'Growth-Friendly'), (1, 'Neutral'), (2, 'Value-Friendly')]:\n",
    "        mask = df_regimes['hml_regime'] == regime\n",
    "        count = mask.sum()\n",
    "        pct = count / len(df_regimes) * 100\n",
    "        mean_hml = df_regimes.loc[mask, metric_col].mean()\n",
    "        mean_L = df_regimes.loc[mask, 'L'].mean()\n",
    "        print(f\"   {label:16s}: {count:4d} ({pct:5.1f}%)\")\n",
    "        print(f\"      Mean HML: {mean_hml:+.2f}, Mean L: {mean_L:+.2f}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 3: Feature Engineering (Liquidity)\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 3: LIQUIDITY FEATURES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    feature_cols = []\n",
    "    \n",
    "    # Core liquidity components\n",
    "    liq_features = ['dlog_M2', 'dlog_FED_BS', 'EM', 'EB', 'EL_3y']\n",
    "    feature_cols.extend([f for f in liq_features if f in df_regimes.columns])\n",
    "    \n",
    "    # Rates and spreads\n",
    "    rate_features = ['Term_Spread', 'Credit_Spread', 'Real_Rate']\n",
    "    feature_cols.extend([f for f in rate_features if f in df_regimes.columns])\n",
    "    \n",
    "    # Market indicators\n",
    "    if 'VIX' in df_regimes.columns:\n",
    "        feature_cols.append('VIX')\n",
    "    \n",
    "    # Growth rates\n",
    "    if 'M2_growth' in df_regimes.columns:\n",
    "        feature_cols.append('M2_growth')\n",
    "    if 'FED_BS_growth' in df_regimes.columns:\n",
    "        feature_cols.append('FED_BS_growth')\n",
    "    \n",
    "    # LIQUIDITY INDEX (L)\n",
    "    feature_cols.append('L')\n",
    "    \n",
    "    # Technical on L\n",
    "    df_regimes['L_ma_3'] = df_regimes['L'].rolling(3, min_periods=1).mean()\n",
    "    df_regimes['L_ma_6'] = df_regimes['L'].rolling(6, min_periods=3).mean()\n",
    "    df_regimes['L_momentum'] = df_regimes['L'].diff(3)\n",
    "    df_regimes['L_accel'] = df_regimes['L_momentum'].diff(3)\n",
    "    \n",
    "    feature_cols.extend(['L_ma_3', 'L_ma_6', 'L_momentum', 'L_accel'])\n",
    "    \n",
    "    # Lagged HML (control for momentum)\n",
    "    df_regimes['HML_lag3'] = df_regimes[hml_col].shift(3)\n",
    "    df_regimes['HML_lag6'] = df_regimes[hml_col].shift(6)\n",
    "    feature_cols.extend(['HML_lag3', 'HML_lag6'])\n",
    "    \n",
    "    print(f\"\\n✅ Selected {len(feature_cols)} features\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 4: Prepare ML Dataset\n",
    "    # ==========================================\n",
    "    \n",
    "    X_raw = df_regimes[feature_cols].copy()\n",
    "    y = df_regimes['hml_regime'].copy()\n",
    "    \n",
    "    # Imputation\n",
    "    X_imputed = X_raw.fillna(method='ffill', limit=6).fillna(method='bfill', limit=6)\n",
    "    \n",
    "    for col in X_imputed.columns:\n",
    "        if X_imputed[col].isna().any():\n",
    "            X_imputed[col] = X_imputed[col].fillna(X_imputed[col].median())\n",
    "    \n",
    "    # Final dataset\n",
    "    valid_idx = X_imputed.notna().all(axis=1) & y.notna()\n",
    "    X = X_imputed[valid_idx]\n",
    "    y = y[valid_idx]\n",
    "    \n",
    "    print(f\"\\n📊 ML Dataset: {len(X)} samples × {len(feature_cols)} features\")\n",
    "    \n",
    "    if len(X) < 50:\n",
    "        raise ValueError(f\"Insufficient data: {len(X)} samples\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 5: Train Models with CV\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 4: MODEL TRAINING & CROSS-VALIDATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=8,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=5,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss',\n",
    "            use_label_encoder=False,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"{name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy', n_jobs=-1)\n",
    "        \n",
    "        print(f\"\\n📊 Cross-Validation (Time Series):\")\n",
    "        print(f\"   Mean Accuracy: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})\")\n",
    "        print(f\"   Fold Scores:   {[f'{s:.3f}' for s in cv_scores]}\")\n",
    "        \n",
    "        baseline = 1/3\n",
    "        improvement = cv_scores.mean() - baseline\n",
    "        print(f\"   vs Baseline:   {improvement:+.1%}\")\n",
    "        \n",
    "        # Train full model\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        # In-sample\n",
    "        y_pred = model.predict(X)\n",
    "        train_acc = (y_pred == y).mean()\n",
    "        \n",
    "        print(f\"\\n📈 In-Sample Accuracy: {train_acc:.3f}\")\n",
    "        \n",
    "        # Classification report\n",
    "        print(f\"\\n📋 Classification Report:\")\n",
    "        print(classification_report(\n",
    "            y, y_pred,\n",
    "            target_names=['Growth-Friendly', 'Neutral', 'Value-Friendly'],\n",
    "            zero_division=0\n",
    "        ))\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "        \n",
    "        print(f\"\\n🎯 Confusion Matrix (%):\")\n",
    "        cm_df = pd.DataFrame(\n",
    "            cm_pct,\n",
    "            index=['True_Growth', 'True_Neutral', 'True_Value'],\n",
    "            columns=['Pred_Growth', 'Pred_Neutral', 'Pred_Value']\n",
    "        )\n",
    "        print(cm_df.round(1))\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'train_acc': train_acc,\n",
    "            'y_pred': y_pred\n",
    "        }\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 6: Feature Importance\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 5: FEATURE IMPORTANCE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    best_model_name = max(results, key=lambda k: results[k]['cv_mean'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    \n",
    "    print(f\"\\n🏆 Best Model: {best_model_name} (CV: {results[best_model_name]['cv_mean']:.3f})\")\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n📊 Top 15 Features:\")\n",
    "    for i, row in feature_importance.head(15).iterrows():\n",
    "        is_liquidity = any(liq in row['feature'] for liq in ['L', 'M2', 'FED', 'EM', 'EB', 'EL'])\n",
    "        marker = \"💧\" if is_liquidity else \"  \"\n",
    "        print(f\"   {marker} {row['feature']:20s}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Liquidity importance\n",
    "    liquidity_features = [f for f in feature_importance['feature'] \n",
    "                         if any(liq in f for liq in ['L', 'M2', 'FED', 'EM', 'EB', 'EL'])]\n",
    "    liquidity_importance = feature_importance[feature_importance['feature'].isin(liquidity_features)]['importance'].sum()\n",
    "    \n",
    "    print(f\"\\n💧 Total Liquidity Importance: {liquidity_importance:.1%}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 7: Hypothesis Test\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"HYPOTHESIS TEST: LIQUIDITY → HML REGIMES\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    best_cv_acc = results[best_model_name]['cv_mean']\n",
    "    baseline = 1/3\n",
    "    improvement = best_cv_acc - baseline\n",
    "    \n",
    "    print(f\"\\n📊 Results:\")\n",
    "    print(f\"   Baseline:      {baseline:.1%}\")\n",
    "    print(f\"   CV Accuracy:   {best_cv_acc:.1%}\")\n",
    "    print(f\"   Improvement:   {improvement:+.1%}\")\n",
    "    print(f\"   Lift:          {best_cv_acc/baseline:.2f}x\")\n",
    "    \n",
    "    if best_cv_acc >= 0.50:\n",
    "        print(f\"\\n   ✅ STRONG SUPPORT for hypothesis\")\n",
    "    elif best_cv_acc >= 0.42:\n",
    "        print(f\"\\n   ✔️  MODERATE SUPPORT for hypothesis\")\n",
    "    else:\n",
    "        print(f\"\\n   ❌ WEAK SUPPORT for hypothesis\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # STEP 8: Final Assignment\n",
    "    # ==========================================\n",
    "    \n",
    "    df_final = df_crisis.copy()\n",
    "    df_final['final_regime'] = np.nan\n",
    "    df_final['final_regime_label'] = 'Unknown'\n",
    "    \n",
    "    # Crisis\n",
    "    df_final.loc[df_final['is_crisis'], 'final_regime'] = 3\n",
    "    df_final.loc[df_final['is_crisis'], 'final_regime_label'] = 'Crisis'\n",
    "    \n",
    "    # HML regimes (predicted)\n",
    "    df_final.loc[X.index, 'final_regime'] = best_model.predict(X)\n",
    "    df_final.loc[X.index, 'final_regime_label'] = pd.Series(\n",
    "        best_model.predict(X),\n",
    "        index=X.index\n",
    "    ).map({0: 'Growth-Friendly', 1: 'Neutral', 2: 'Value-Friendly'})\n",
    "    \n",
    "    # Also keep actual HML regime\n",
    "    df_final.loc[df_regimes.index, 'actual_hml_regime'] = df_regimes['hml_regime']\n",
    "    df_final.loc[df_regimes.index, 'actual_hml_regime_label'] = df_regimes['hml_regime_label']\n",
    "    \n",
    "    print(f\"\\n📊 Final Distribution:\")\n",
    "    dist = df_final['final_regime_label'].value_counts()\n",
    "    for regime in ['Crisis', 'Growth-Friendly', 'Neutral', 'Value-Friendly', 'Unknown']:\n",
    "        if regime in dist.index:\n",
    "            count = dist[regime]\n",
    "            pct = count / len(df_final) * 100\n",
    "            print(f\"   {regime:16s}: {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"✅ COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return df_final, results, feature_importance\n",
    "\n",
    "\n",
    "# Run with crisis detection\n",
    "df_hml_regimes, hml_results, hml_feat_imp = build_hml_regime_classification_with_crisis(\n",
    "    df_features_aug,\n",
    "    L_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ec86f-c733-43d7-9030-c61771f50a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
