[
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "A 35-Year Quantitative Analysis of Factor Premiums (1990–2025)\n\n\n\nStory – why this matters (intro + conclusion)\nData – what you measure (markets, factors, liquidity proxies)\nModels – how you measure (liquidity index, regimes, cross-sectional tests)\nPortfolio – what to do with it (conditional allocation\n\n\n\n\n\nThe “valuations don’t matter in infinite money printing” meme.\nYour construction of a liquidity index and liquidity regimes.\nKey findings:\nValuations do matter, but conditional on liquidity regimes.\nGrowth dominates in high-liquidity, negative-real-rate regimes.\nValue premia revive after QT / real rate normalization.\n\nPortfolio takeaway: a simple regime-aware factor-tilt model.\n\n\n\n\nNarrative: post-GFC QE, ZIRP, NIRP, 2010–2021 “TINA”, tech bubble 2.0.\n\n“We live in a world of infinite money printing, so valuations don’t matter.”\n\nResearch question: &gt; Do valuations truly die under abundant liquidity, or do they merely become regime-dependent?\nDo valuation spreads (cheap vs expensive deciles) expand under high-liquidity regimes and compress under tight liquidity?\nHow do factor premia (value, momentum, quality, low vol) vary conditional on liquidity regimes?\nCan a macro-liquidity index predict the timing of factor rotations (growth vs value, long-duration vs short-duration equities)?\nDoes a regime-aware factor allocation deliver better risk-adjusted returns than a static factor mix?\n\n\n\n\n\nValuation spread: the gap between cheap and expensive stocks (e.g., top vs bottom decile by B/M).\nLiquidity regime: periods of systematically easy vs tight financial conditions driven by monetary and fiscal variables.\n\n\n\n\n\nSystemic Strategy: Factor investing: value, momentum, quality.\nRisk Factors and Risk premia: Liquidity\nRegime-switching: macro-conditional factors.\n\n\n\n\n\nBuild a macro liquidity index from multiple public series.\nUse HMM / Markov regimes to label high vs low liquidity states.\nShow how valuation spreads and factor returns behave across liquidity regimes.\nPropose a simple implementable regime-aware factor strategy.\n\n\n\n\n\n\n\n\nHorizon: 1990–2025, monthly frequency (or weekly if feasible).\nMarket: US equities (CRSP/Compustat-type universe); optional robustness on AUS/EU.\n\n\n\n\n\n\nIndividual stock returns \\(r_{i,t}\\), market cap, book equity, earnings, etc.\nCharacteristics \\(X_{i,t}\\):\n\n\\[\n\\text{B/M},\\ \\text{E/P},\\ \\text{P/S},\\ \\text{size},\\ \\beta,\\ \\text{12–1 momentum},\\ \\text{profitability},\\ \\text{volatility}.\n\\]\n\nFactor returns \\(f_t\\):\n\n\\[\n\\text{MKT},\\ \\text{HML},\\ \\text{SMB},\\ \\text{MOM},\\ \\text{Quality},\\ \\text{LowVol}.\n\\]\nDefine a valuation spread:\n\\[\nV_t^{\\text{spread}} = \\text{Long top decile of B/M} - \\text{Short bottom decile}.\n\\]\n\n\n\n\nLet raw macro/liquidity variables at time \\(t\\) be \\(x_{j,t}\\), including:\n\n\n\n\nMoney growth: \\[\\Delta \\log(M2_t)\\]\nFed balance sheet growth: \\[\\Delta \\log(\\text{BS}_t)\\]\n\n\n\n\n\n\nShort rate: \\[i_t\\]\n10Y yield: \\[y_{10,t}\\]\nSlope: \\[\\text{TS}_t = y_{10,t} - i_t\\]\nReal short rate: \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\]\n\n\n\n\n\n\nCredit spread: \\[\\text{CS}_t = y_{\\text{Baa},t} - y_{\\text{Aaa},t}\\]\nVIX level: \\[\\text{VIX}_t\\]\n\n\n\n\n\n\nON RRP usage\nTGA balance\n\n\nAll variables \\(x_{j,t}\\) will then be standardized (e.g., z-scored) and aggregated into a single liquidity index.\n\n\n\n\n\n\n\n$ x_t =\n\\[\\begin{bmatrix}\nx_{1,t} \\\nx_{2,t} \\\n\\vdots \\\nx_{J,t}\n\\end{bmatrix}\\]\n\nx_{J,t} \\end{bmatrix} ^J $\n$ j = {t=1}^{T} x_{j,t} $\n$ j^2 = {t=1}^{T} (x_{j,t} - _j)^2 $\n$ z_{j,t} = , j = 1,,J $\n$ z_t =\n\\[\\begin{bmatrix}\nz_{1,t} \\\nz_{2,t} \\\n\\vdots \\\nz_{J,t}\n\\end{bmatrix}\\]\n\nz_{J,t} \\end{bmatrix} $\nOne may sign-flip the series so that higher \\(z_{j,t}\\) always corresponds to easier liquidity. For example:\nUse \\(-r^{\\text{real}}_{t}\\) instead of \\(r^{\\text{real}}_{t}\\).\nUse \\(-CS_{t}\\) instead of \\(CS_{t}\\).\nUse \\(-VIX_{t}\\) instead of \\(VIX_{t}\\).\n\n\n\n\n$ M2_t $\n$ _t $\n$ t = y{10,t} - i_t $\n$ r_t^{} = i_t - _t^e $\n$ t = y{Baa,t} - y_{Aaa,t} $\n\n\n\n\n$ = _{t=1}^{T} z_t z_t^$\n$ v_k = _k v_k, k = 1,,J $\n$ _1 _2 _J $\n$ L_t = v_1^z_t $\n(Alternative weighted index)\n$ L_t = w^z_t $\n\n\n\n\n$ s_t $\n$ L_t (s_t = k) (_k, _k^2), k = 1,,K $\n$ (s_t = j s_{t-1} = i) = p_{ij} $\n$ P =\n\\[\\begin{bmatrix}\np_{11} & \\cdots & p_{1K} \\\n\\vdots & \\ddots & \\vdots \\\np_{K1} & \\cdots & p_{KK}\n\\end{bmatrix}\\]\nts & \np_{K1} & & p_{KK} \\end{bmatrix}, {j=1}^K p{ij} = 1  i $\n(Filtered / smoothed state classification)\n$ t = {k } (s_t = k L_{1:T}) $\n(High- and Tight-liquidity labels)\n$ k_{} = _k _k $\n$ k_{} = _k _k $\n(Regime indicator using probability threshold)\n$ I_t^{} = $\n\n\n\n\n$ y_t =\n\\[\\begin{bmatrix}\nL_t \\\nr_t^{\\text{MKT}} \\\nr_t^{\\text{HML}} \\\n\\text{VIX}_t \\\n\\vdots\n\\end{bmatrix}\\]\nT}}\nr_t^{}\n_t\n\\end{bmatrix} $\n$ y_t (s_t = k) = A_k y_{t-1} + _t^{(k)} $\n$ _t^{(k)} (0, _k) $\n\n\n\n\n(Valuation spread series, e.g. top–bottom decile)\n$ V_t^{} $\n(Regime-conditional mean valuation spread)\n$ {V}^{(k)} = $\n$ ^{(k)} = {_{t=1}^T (_t = k)} $\n(Difference in spreads between regimes)\n$ ^{()} - ^{()} $\n(Factor return in regime ( k ))\n$ r_t^{(F)} $\n$ {r}_F^{(k)} = $\n$ F^{(k)} = {{t=1}^T (_t = k)} $\n(Regime-specific Sharpe ratio)\n$ _F^{(k)} = $\n\n\n\n\n(Continuous liquidity index)\n$ r_{t+1}^{(F)} = + L_t + ^c_t + _{t+1} $\n(Regime dummy regression)\n$ r_{t+1}^{(F)} = + {} I_t^{} + {} I_t^{} + {} I_t^{} + {t+1} $\n\n\n\n\n(Cross-sectional regression at time ( t ))\n$ r_{i,t+1} = t + {1,t} {i,t} + {2,t} {i,t} + {3,t} {i,t} + + {i,t+1} $\n(Regime-conditional average valuation slope)\n$ {}_1^{(k)} = $\n$ 1^{(k)} = {{t=1}^T (_t = k)} $\n\n\n\n\n(Factor return vector and regime weights)\n$ f_t ^K $\n$ w^{(k)} ^K $\n$ w_t = w^{(_t)} $\n(Portfolio return)\n$ R_{p,t+1} = w_t^f_{t+1} $\n\n\n\n\nStandard PCA solves:\n\\[\n\\max_{v} \\; \\| X v \\|^{2}\n\\quad \\text{s.t.} \\quad \\| v \\|^{2} = 1\n\\]\n\n\n\nPCA tries to find the direction ( \\(v\\) ) (a unit vector) along which the projected data ( \\(Xv\\) ) has maximum variance.\nVariance of the projection:\n\\[\n\\text{Var}(Xv) = \\frac{1}{n} |Xv|^{2}.\n\\]\nSo maximizing variance is equivalent to maximizing ( \\(|Xv|^{2}\\) ).\n\n\n\n\nWithout this constraint, the problem would blow up:\n\\[\n|X(\\alpha v)|^{2} = \\alpha^{2} |Xv|^{2},\n\\]\nand the maximum would be infinite by choosing ( \\(\\alpha\\) \\(\\infty\\) ).\nSo PCA forces ( \\(v\\) ) to be a direction, not a magnitude → a unit vector.\n\n\n\n\n\\[\n|Xv|^{2} = v^\\top X^\\top X v.\n\\]\nLet:\n\\[\nS = X^\\top X\n\\]\n(the unnormalized covariance matrix). Then PCA solves:\n\\[\n\\max_{v} ; v^\\top S v\n\\quad\\text{s.t.}\\quad v^\\top v = 1.\n\\]\nThis is exactly the Rayleigh quotient.\n\n\n\n\nPCA finds the direction (unit vector) along which the data cloud has maximum spread. That direction is exactly the eigenvector of the covariance matrix with the largest eigenvalue.\n\n\n\n\n\n\n\n\n\nPCA rotates the data into orthogonal directions capturing maximum variance.\nEach principal component is a linear combination of all variables.\nLoadings are typically dense (every variable has some non-zero weight).\nThis makes interpretation difficult:\n\n\n“PC1 of 127 economic multicolinear variables is 127-dimensional mush of everything.”\n\nThis is why PCA is almost never used directly for economic interpretation — PCs are not sparse.\n\n\n\n\nSparse PCA = PCA where most loadings are forced to be zero.\nMathematically:\nStandard PCA solves:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1\n\\]\nSparse PCA adds an L1 penalty (lasso-style sparsity) or constrains the number of non-zero elements:\n\\[\n\\max_{v} \\ |Xv|^2 - \\lambda |v|_1\n\\]\nor equivalently:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1,\\ |v|_1 \\leq c\n\\]\nThis forces:\n\nOnly a small subset of variables to load on each component.\nComponents become interpretable (e.g., PC1 loads on CPI, PCE, core CPI → “inflation”).\n\nThe specific implementation cited (“penalized matrix decomposition” by Witten et al. 2009) solves:\n\\[\n\\max_{u, v} \\ u^\\top X v \\quad\n\\text{s.t. } |u|_2 = 1,\\ |v|_2 = 1,\\ |v|_1 \\le c\n\\]\nThis is a general sparse factor extraction algorithm.\nIntuition: &gt; Sparse PCA forces components to use only the relevant variables, not a smear across 127 series.\n\n\n\n\n\nStandard PCA components ≈ dense, uninterpretable mixtures\nSparse PCA components ≈ targeted clusters of interpretable economic variables\n\n\n\n\n\nTake FRED-MD’s 127 macro series.\nIf you run standard PCA:\n\nPC1 loads on 100+ variables.\nPC2 loads on another 80+.\nInterpreting them is basically hopeless.\n\nSparse PCA, with lasso constraints:\n\nallows PC1 to load only on inflation-related variables\nPC2 to load only on housing-related variables\nPC3 on credit spreads\nPC4 on yields\nPC5 on production\netc.\n\nThis resembles the Stock–Watson macro factor model, but with sparsity for interpretability.\n\n\n\n\nSuppose sparse PCA gives:\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCPI YoY\n0.71\n\n\nCore CPI YoY\n0.68\n\n\nPCE Deflator\n0.66\n\n\nPCE core\n0.64\n\n\nAll other 123 variables\n0\n\n\n\n-&gt; You immediately label PC1 as “inflation factor”.\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nHousing starts\n0.62\n\n\nBuilding permits\n0.59\n\n\nNew homes sold\n0.61\n\n\nMortgage applications\n0.58\n\n\nEverything else\n0\n\n\n\n-&gt; PC2 = “housing & construction cycle”\n\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCorporate spreads (BAA–AAA)\n0.72\n\n\nHigh yield spread\n0.69\n\n\nCommercial paper spread\n0.64\n\n\nOthers\n0\n\n\n\n-&gt; PC3 = “credit stress”\n…and so on.\n\n\n\nSparse PCA essentially performs dimension reduction + variable selection simultaneously.\n\n\n\n\nSparse PCA encourages:\n\ngrouping variables that co-move strongly\ndropping variables unrelated to the theme\nchoosing a small number of representative series\n\nGiven economic data naturally clusters (inflation variables co-move, housing variables co-move), sparse PCA isolates these clusters.\nThis is similar to economic intuition:\n\ninflationary variables form a single latent factor\nspreads form a financial stress factor\nyields form a term-structure factor\nemployment variables form a labor factor\n\nSparse PCA “discovers” these clusters automatically.\n\n\n\n\nSparse PCA → interpretable macro drivers → better regime classification → better factor conditioning.\nEspecially when constructing a macro liquidity index or financial conditions index.\nSparse PCA gives:\n\n\n\nComponent\nInterpretation\n\n\n\n\nPC1\nLiquidity / money conditions\n\n\nPC2\nCredit spreads\n\n\nPC3\nHousing activity\n\n\nPC4\nYield curve / rates\n\n\nPC5\nEmployment\n\n\nPC6\nProduction\n\n\nPC7\nIncome\n\n\nPC8\nMarket stress\n\n\n\nThese are exactly the components McCracken & Ng (2016) find in FRED-MD.\n\n\n\n\n\n\n\n\nDense loadings\nHard to interpret\nPCs are linear mush\n\n\n\n\n\nForces many loadings to zero\nEach PC focuses on a small cluster of variables\nBecomes interpretable as a “macro theme”\nMatches how economists think about the business cycle\nPerfect for regime classification & factor research\n\n\n\n\n\nEconomic data naturally clusters into themes\nSparse PCA forces components to choose only the strongest cluster\nThis matches known macro factors (inflation, employment, credit, etc.)\n\n\n# !pip install pandas pandas_datareader numpy scikit-learn hmmlearn matplotlib\n\nimport pandas as pd\nimport numpy as np\nfrom pandas_datareader import data as pdr\n\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom hmmlearn.hmm import GaussianHMM\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\nstart_date = \"1990-01-01\"\nend_date   = \"2025-12-31\"\n\n# --- FRED series codes ---\nFRED_SERIES = {\n    \"M2SL\":   \"M2SL\",      # M2 money stock (monthly, SA)\n    \"FED_BAL\":\"WALCL\",     # Fed balance sheet total assets (weekly)\n    \"TB3M\":   \"TB3MS\",     # 3-Month T-Bill rate (monthly)\n    \"DGS10\":  \"DGS10\",     # 10-Year Treasury yield (daily -&gt; resample monthly)\n    \"BAA\":    \"BAA\",       # Moody's Baa corporate yield (monthly)\n    \"AAA\":    \"AAA\",       # Moody's Aaa corporate yield (monthly)\n    \"CPI\":    \"CPIAUCSL\",  # CPI index (monthly, SA)\n    \"GDP\":    \"GDP\",       # Nominal GDP (quarterly, SAAR)\n}\n\n# For equity factors: Fama-French via pandas_datareader (famafrench)\nFF_FACTORS_DATASET = \"F-F_Research_Data_5_Factors_2x3\"\n\n\n\n\n\ndef download_fred_series(series_dict, start, end):\n    \"\"\"\n    Download FRED series into a single monthly DataFrame.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Mapping logical_name -&gt; FRED code.\n    \"\"\"\n    dfs = []\n    for name, code in series_dict.items():\n        print(f\"Downloading {name} ({code}) from FRED...\")\n        s = pdr.DataReader(code, \"fred\", start, end)\n        s = s.rename(columns={code: name})\n        dfs.append(s)\n\n    df = pd.concat(dfs, axis=1)\n    # Ensure monthly freq by end-of-month sampling\n    df = df.resample(\"M\").last()\n    return df\n\nmacro_raw = download_fred_series(FRED_SERIES, start_date, end_date)\nmacro_raw.head()\n\nDownloading M2SL (M2SL) from FRED...\nDownloading FED_BAL (WALCL) from FRED...\nDownloading TB3M (TB3MS) from FRED...\nDownloading DGS10 (DGS10) from FRED...\nDownloading BAA (BAA) from FRED...\nDownloading AAA (AAA) from FRED...\nDownloading CPI (CPIAUCSL) from FRED...\nDownloading GDP (GDP) from FRED...\n\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n1990-01-31\n3166.8\nNaN\n7.64\n8.43\n9.94\n8.99\n127.5\n5872.701\n\n\n1990-02-28\n3179.2\nNaN\n7.74\n8.51\n10.14\n9.22\n128.0\nNaN\n\n\n1990-03-31\n3190.1\nNaN\n7.90\n8.65\n10.21\n9.37\n128.6\nNaN\n\n\n1990-04-30\n3201.6\nNaN\n7.77\n9.04\n10.30\n9.46\n128.9\n5960.028\n\n\n1990-05-31\n3200.6\nNaN\n7.74\n8.60\n10.41\n9.47\n129.1\nNaN\n\n\n\n\n\n\n\n\nmacro_raw.tail()\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\nNaN\n4.00\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n$ x_t =\n\\[\\begin{bmatrix}\nx_{1,t} \\\nx_{2,t} \\\n\\vdots \\\nx_{J,t}\n\\end{bmatrix}\\]\n\nx_{J,t} \\end{bmatrix} ^J $\n\nMoney & balance sheet growth: \\(\\Delta \\log(\\cdot)\\)\nTerm spread: \\(y_{10} - i_{3m}\\)\nReal short rate: \\(i_{3m} - \\pi_{\\text{year-on-year}}\\)\nCredit spread: \\(\\text{BAA} - \\text{AAA}\\)\netc.​\n\n\ndef build_liquidity_proxies(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Growth of M2 and Fed balance sheet\n    df[\"dlog_M2\"] = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"] = np.log(df[\"FED_BAL\"]).diff()\n\n    # 2. Term structure: 10Y - 3M\n    df[\"term_spread\"] = df[\"DGS10\"] - df[\"TB3M\"]\n\n    # 3. Year-on-year inflation (CPI yoy)\n    df[\"infl_yoy\"] = np.log(df[\"CPI\"]).diff(12)\n\n    # 4. Real short rate: nominal 3M - inflation\n    df[\"real_rate\"] = df[\"TB3M\"] - (100 * df[\"infl_yoy\"])  # TB3M in %, infl_yoy in log -&gt; approx*100\n\n    # 5. Credit spread: BAA - AAA\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # Drop rows with NaNs from diff(12) etc.\n    proxies = df[[\"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\",\n                  \"real_rate\", \"credit_spread\"]].dropna()\n\n    return proxies\n\nliquidity_proxies = build_liquidity_proxies(macro_raw)\n\n\nliquidity_proxies.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n\n\n2025-07-31\n0.003930\n-0.002950\n0.12\n1.554846\n0.65\n\n\n2025-08-31\n0.003607\n-0.005918\n0.11\n1.223147\n0.65\n\n\n2025-09-30\n0.004698\n0.000759\n0.24\n0.942084\n0.62\n\n\n\n\n\n\n\n\n\n\nWe want higher \\(z_{j,t}\\) – to mean easier liquidity.\n\nGrowth of M2 / Fed balance sheet: already “easier = higher value” -&gt; keeping sign\nTerm spread: steeper (more positive) often associated with easier conditions -&gt; keeping sign\nReal rate: easier liquidity when real rates are low -&gt; flipping sign\nCredit spread: easier liquidity when spreads are tight (low) -&gt; flipping sign\n\n\ndef standardize_and_signflip(proxies: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Standardize each column and flip signs so that higher z_{j,t}\n    always corresponds to easier liquidity.\n    \"\"\"\n    z = proxies.copy()\n\n    # Standardize\n    scaler = StandardScaler()\n    z_vals = scaler.fit_transform(z)\n    z = pd.DataFrame(z_vals, index=z.index, columns=z.columns)\n\n    # Sign flips so \"higher\" = easier liquidity\n    # dlog_M2: easier when higher -&gt; keep\n    # dlog_FED_BAL: easier when higher -&gt; keep\n    # term_spread: easier when steeper -&gt; keep (or adjust, depending on your view)\n    # real_rate: easier when more negative -&gt; flip sign\n    # credit_spread: easier when lower -&gt; flip sign\n\n    sign_flips = {\n        \"dlog_M2\": +1,\n        \"dlog_FED_BAL\": +1,\n        \"term_spread\": +1,\n        \"real_rate\": -1,\n        \"credit_spread\": -1,\n    }\n\n    for col, sgn in sign_flips.items():\n        z[col] = sgn * z[col]\n\n    return z\n\nz_t = standardize_and_signflip(liquidity_proxies)\n\n\nz_t.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n-0.377813\n-0.314009\n-0.968292\n-1.362581\n0.639849\n\n\n2025-06-30\n0.052579\n-0.226920\n-1.085244\n-1.211510\n0.786085\n\n\n2025-07-31\n-0.163048\n-0.257126\n-0.999479\n-1.193172\n0.883576\n\n\n2025-08-31\n-0.215601\n-0.326453\n-1.007276\n-1.031235\n0.883576\n\n\n2025-09-30\n-0.038105\n-0.170518\n-0.905917\n-0.894019\n0.956694\n\n\n\n\n\n\n\n\n\n\n\npd.Series(z_t.index).describe()\n\ncount                              273\nmean     2014-05-31 08:42:11.868131840\nmin                2003-01-31 00:00:00\n25%                2008-09-30 00:00:00\n50%                2014-05-31 00:00:00\n75%                2020-01-31 00:00:00\nmax                2025-09-30 00:00:00\nName: DATE, dtype: object\n\n\n\nz_t.values\n\narray([[ 0.11561096, -0.81061174,  1.11346119,  0.322683  , -0.40817503],\n       [ 0.20510346,  0.11034372,  0.8873532 ,  0.50696464, -0.23756644],\n       [-0.24145055, -0.0901311 ,  1.01210244,  0.46925598, -0.11570316],\n       ...,\n       [-0.163048  , -0.25712611, -0.999479  , -1.1931718 ,  0.88357574],\n       [-0.2156006 , -0.32645261, -1.00727583, -1.03123533,  0.88357574],\n       [-0.03810504, -0.17051847, -0.90591708, -0.89401934,  0.95669371]])\n\n\n\ndef build_sparse_pca_liquidity_index(z_df: pd.DataFrame,\n                                     alpha: float = 1.0,\n                                     random_state: int = 42) -&gt; pd.Series:\n    \"\"\"\n    Apply SparsePCA with 1 component to z_{j,t} to obtain L_t.\n    \"\"\"\n    spca = SparsePCA(\n        n_components=1,\n        alpha=alpha,\n        random_state=random_state\n    )\n\n    # Fit on standardized proxies\n    L_scores = spca.fit_transform(z_df.values)  # shape (T, 1)\n    L = pd.Series(L_scores.flatten(), index=z_df.index, name=\"L\")\n\n    # Optional: enforce that L_t is positively correlated with dlog_M2\n    # corr = np.corrcoef(L, z_df[\"dlog_FED_BAL\"])[0, 1]\n    # if corr &lt; 0:\n    #     L = -L\n\n    print(\"SparsePCA components (loadings):\")\n    for coef, col in zip(spca.components_[0], z_df.columns):\n        print(f\"  {col}: {coef:.3f}\")\n\n    return L\n\nL_t = build_sparse_pca_liquidity_index(z_t, alpha=0.5)\n\nSparsePCA components (loadings):\n  dlog_M2: 0.430\n  dlog_FED_BAL: 0.526\n  term_spread: 0.492\n  real_rate: 0.389\n  credit_spread: -0.381\n\n\n\nL_t.head()\n\nDATE\n2003-01-31    0.447153\n2003-02-28    0.861556\n2003-03-31    0.567234\n2003-04-30    0.979006\n2003-05-31    0.696500\nFreq: M, Name: L, dtype: float64\n\n\n\nL_t.plot(title=\"Sparse PCA Liquidity Index L(t)\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n​Fit a Gaussian HMM on the liquidity index. 2 or 3 regimes ?\n\nfrom hmmlearn.hmm import GaussianHMM\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\ndef fit_hmm_on_liquidity(\n    L: pd.Series,\n    n_states: int = 3,\n    covariance_type: str = \"full\",\n    random_state: int = 42,\n    standardize: bool = True\n):\n    \"\"\"\n    Fit a Gaussian HMM on L(t) and return the model and a DataFrame\n    with inferred regimes and posterior probabilities.\n\n    Parameters\n    ----------\n    L : pd.Series\n        Liquidity index (indexed by date).\n    n_states : int\n        Number of hidden states (regimes).\n    standardize : bool\n        If True, standardize L before fitting the HMM.\n\n    Returns\n    -------\n    model : GaussianHMM\n    df_regime : pd.DataFrame\n        Columns: L, state, p_state_k, state_label\n    \"\"\"\n\n    # 1) Prepare X\n    X = L.values.reshape(-1, 1)\n\n    scaler = None\n    if standardize:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n    # 2) Fit HMM\n    model = GaussianHMM(\n        n_components=n_states,\n        covariance_type=covariance_type,\n        random_state=random_state,\n        n_iter=500\n    )\n    model.fit(X)\n\n    hidden_states = model.predict(X)\n    post_probs = model.predict_proba(X)  # T x n_states\n\n    # 3) Build output DataFrame on original index\n    df_regime = pd.DataFrame(\n        {\"L\": L, \"state\": hidden_states},\n        index=L.index\n    )\n    for k in range(n_states):\n        df_regime[f\"p_state_{k}\"] = post_probs[:, k]\n\n    # 4) Use state means to order regimes\n    means = pd.Series(model.means_.flatten(), index=range(n_states))\n\n    print(\"HMM state means (unsorted, in fitted scale):\")\n    for k, m in means.items():\n        print(f\"  state {k}: mean L = {m:.3f}\")\n\n    # Order states from low liquidity to high liquidity\n    ordering = means.sort_values().index.tolist()\n\n    state_label_map = {}\n    if n_states == 3:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"Neutral\"\n        state_label_map[ordering[2]] = \"High\"\n    elif n_states == 2:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"High\"\n    else:\n        # Generic labels if you ever use more states\n        for i, s in enumerate(ordering):\n            state_label_map[s] = f\"Regime_{i}\"\n\n    df_regime[\"state_label\"] = df_regime[\"state\"].map(state_label_map)\n\n    return model, df_regime\n\n\nn_states = 3\nmodel, regimes_df = fit_hmm_on_liquidity(L_t,n_states)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.160\n  state 1: mean L = -0.106\n  state 2: mean L = 2.718\n\n\n\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# -----------------------------\n# TOP PANEL — L(t) + shading\n# -----------------------------\nax[0].plot(regimes_df.index, regimes_df[\"L\"], color=\"black\", lw=1.2)\nax[0].set_title(\"Sparse PCA Liquidity Index L(t) with Regime Shading\")\n\n# Define plain colors for each regime\nregime_colors = {\n    \"Tight\":   \"green\",\n    \"Neutral\": \"gray\",\n    \"High\":    \"crimson\",\n}\n\nshade_alpha = 0.2  # semi-opaque\n\nfor label, color in regime_colors.items():\n    mask = regimes_df[\"state_label\"] == label\n\n    in_region = False\n    start = None\n\n    for i in range(len(mask)):\n        if mask.iloc[i] and not in_region:\n            in_region = True\n            start = regimes_df.index[i]\n\n        elif not mask.iloc[i] and in_region:\n            in_region = False\n            end = regimes_df.index[i]\n            ax[0].axvspan(start, end, color=color, alpha=shade_alpha, linewidth=0)\n\n    # If region extends to the end of the sample\n    if in_region:\n        ax[0].axvspan(start, regimes_df.index[-1], color=color, alpha=shade_alpha, linewidth=0)\n\n# Legend for regimes (shaded colors)\nfrom matplotlib.lines import Line2D\nlegend_patches = [\n    Line2D([0], [0], color=\"green\",   lw=6, alpha=shade_alpha, label=\"Tight\"),\n    Line2D([0], [0], color=\"gray\",    lw=6, alpha=shade_alpha, label=\"Neutral\"),\n    Line2D([0], [0], color=\"crimson\", lw=6, alpha=shade_alpha, label=\"High\"),\n]\nax[0].legend(handles=legend_patches, loc=\"upper left\")\n\n# -----------------------------\n# BOTTOM PANEL — High regime probability\n# -----------------------------\nif \"p_state_0\" in regimes_df.columns:\n    high_state_id = regimes_df.groupby(\"state_label\")[\"state\"].first()[\"High\"]\n    ax[1].plot(regimes_df.index,\n               regimes_df[f\"p_state_{high_state_id}\"],\n               color=\"crimson\", lw=1.5)\n    ax[1].set_title(\"Posterior Probability of High Liquidity Regime\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\nThese five factors extend the original 3-factor model to better explain cross-sectional stock returns.\nThey are:\n\nRm–Rf — Market excess return\nSMB — Size (Small Minus Big)\nHML — Value (High Book/Market minus Low)\nRMW — Profitability (Robust Minus Weak)\nCMA — Investment (Conservative Minus Aggressive)\n\nLet’s break each down precisely.\n\n\n\n\\[\nR_{m}-R_{f}\n\\] The return of the broad market minus the risk-free rate.\n\nPositive → market went up more than cash.\nNegative → market underperformed cash/T-bills.\n\nIn above table:\n\nLast 12 months: +17.54% → very strong bull market year.\nLast 3 months: +7.40% → strong quarter.\nOctober 2025: +1.95% → moderate positive month.\n\n\n\n\n\n\\[\n\\text{SMB} = R_{\\text{small}} - R_{\\text{big}}\n\\]\n\nPositive → small caps outperform large caps.\nNegative → large caps outperform small caps.\n\nIn above table:\n\nLast 12 months: –10.71% → a massive large-cap dominance year.\nThis is consistent with mega-cap tech leadership.\n\n\n\n\n\n\\[\n\\text{HML} = R_{\\text{value}} - R_{\\text{growth}}\n\\]\n\nPositive → value outperforms growth.\nNegative → growth outperforms value.\n\nIn above table:\n\nLast 12 months: –2.36% → growth beat value.\nOctober 2025: –3.10% → especially growth-heavy month.\n\nThis aligns with liquidity-driven growth leadership.\n\n\n\n\n\\[\n\\text{RMW} = R_{\\text{robust}} - R_{\\text{weak}}\n\\]\nRobust = high operating profitability. Weak = low profitability.\n\nPositive → profitable companies outperform weak ones.\nNegative → weak/low-profit companies outperform.\n\nIn above table:\n\nLast 12 months: –13.60% → very unusual.\nThis means unprofitable firms outperformed profitable ones over the year.\n\nThat usually happens in:\n\nearly speculative bubbles\nliquidity-driven rallies\nretail-led tech/small-cap frenzies\nAI/futuristic narrative periods\n\n\n\n\n\n\\[\n\\text{CMA} = R_{\\text{conservative}} - R_{\\text{aggressive}}\n\\]\n\nConservative = firms investing slowly\nAggressive = firms aggressively expanding assets\n\nHigh investment → lower expected returns historically (consistent with q-theory: empire-building destroys value).\n\nPositive → conservative firms outperform heavy spenders.\nNegative → aggressive investment firms outperform.\n\nIn above table:\n\nLast 12 months: –10.46%\nLast 3 months: –4.47%\nOctober 2025: –4.03%\n\nInterpretation: Aggressively investing companies — think AI, R&D, biotech, high-growth tech — massively outperformed low-investment firms.\n\n\n\n\n\n\n\n\nFactor\nSign\nInterpretation\n\n\n\n\nRm–Rf: +\nStrong bull market\n\n\n\nSMB: –\nMega-caps beat small caps massively\n\n\n\nHML: –\nGrowth beat value\n\n\n\nRMW: –\nLow-profit firms beat profitable firms\n\n\n\nCMA: –\nAggressive-investment firms beat conservative ones\n\n\n\n\nThis is the textbook signature of a liquidity-driven growth/tech momentum regime, almost identical to:\n\n1998–1999 Dotcom\n2019–2021 QE wave\n2023–2024 AI mega-cap boom\n\nIt means:\n\n\n\n\nlarge\ngrowth\nunprofitable\nhigh-spending\nhigh-duration tech/AI/future-theme stocks\n\n\n\n\nThis is exactly the kind of environment where:\n\nValue fails\nSmall cap fails\nProfitability fails\nInvestment works negatively\nGrowth dominates\nMega-caps dominate\n\n\ndef download_ff_factors(start, end):\n    \"\"\"\n    Download monthly Fama-French 5 factors (2x3) using pandas_datareader.\n    \"\"\"\n    print(\"Downloading Fama-French factors (famafrench)...\")\n    ff_raw = pdr.DataReader(\n        name=FF_FACTORS_DATASET,\n        data_source=\"famafrench\",\n        start=start,\n        end=end\n    )[0]  # table [0] contains the data\n\n    ff = (ff_raw\n          .divide(100)  # convert from % to decimal\n          .reset_index(names=\"date\")\n          .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n          .rename(str.lower, axis=\"columns\")\n          .rename(columns={\"mkt-rf\": \"mkt_excess\"}))\n\n    ff = ff.set_index(\"date\")\n    return ff\n\nff_factors = download_ff_factors(start_date, end_date)\n\nDownloading Fama-French factors (famafrench)...\n\n\nFutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n&lt;ipython-input-69-55f7e03990a5&gt;:6: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n\n\n\nff_factors.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-06-01\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-07-01\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-01\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-09-01\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-10-01\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Fix Fama–French dates (shift one month backward)\nff_adj = ff_factors.copy()\n\n# FF data comes labeled as YYYY-MM-01 meaning return for previous month.\n# So shift index back to previous month-end.\nff_adj.index = (ff_adj.index.to_period(\"M\") - 1).to_timestamp(\"M\")\n\nff_adj.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined = reg.join(ff_adj, how=\"inner\")\n\n\ncombined.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-12-31\n-1.651021\n1\n0.162158\n0.837762\n0.000079\nTight\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n-1.496425\n0\n0.837514\n0.162233\n0.000253\nTight\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n-1.643180\n1\n0.162188\n0.837738\n0.000075\nTight\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n-1.627975\n0\n0.837473\n0.162258\n0.000269\nTight\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n-1.400432\n1\n0.162220\n0.837714\n0.000067\nTight\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n-1.561611\n0\n0.837459\n0.162284\n0.000258\nTight\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n-1.387057\n1\n0.162245\n0.837689\n0.000066\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n-1.482573\n0\n0.837437\n0.162308\n0.000255\nTight\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n-1.482572\n1\n0.162251\n0.837658\n0.000091\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n-1.251226\n0\n0.836442\n0.162316\n0.001242\nTight\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n\ndf2 = combined.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by *lagged* regime:\")\nprint(means_by_regime)\n\nMean factor returns by *lagged* regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nTight            -0.005439 -0.005360  0.000540 -0.003311\nNeutral           0.003200  0.002938  0.001051 -0.000056\nHigh              0.003327  0.001228  0.005127  0.003575\n\n\n\n\n\nThis is reverse of what you’d expect. In a tight liquidity regime, \\(R^{smb}\\) and \\(R^{hml}\\) should be deeply positive. You’d expect small under-valued companies to outperform big companies with premium valuation.\nLet’s go back to the \\(L_t\\) and understand what’s going on\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L_t.index,\n    y=L_t.values,\n    hue=regimes_df[\"state\"],\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution of L(t) by HMM State\")\nax[1].set_xlabel(\"L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the KDE (bottom panel):\n\nStates 0 (green) and 1 (yellow) heavily overlap\nOnly state 2 (red) is clearly separated — the “high liquidity spike”\nThis corresponds to crisis/QE events (2008, 2020), where liquidity jumps dramatically\n\nSo the natural clustering is:\n\nOne large cluster (normal-tight-neutral combined)\nOne rare extreme spike cluster\n\nYour HMM is being forced to split the unimodal bulk distribution into two states, resulting in:\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\nI initially thought alpha=0.5 in SparsePCA might be the cause. But the plot shows:\n\nL(t) has a healthy dynamic range across time (≈ −2.5 to +12)\nThe high liquidity cluster is clear and RARE\nThe rest of L(t) is genuinely one blob\n\nIf Sparse PCA was “overshrinking,” L(t) would be compressed; but it is not.\n\n\n\n\nIs Liquidity is fundamentally spiky ?\nWhy ?\nAm I defining Liquidity incorrectly ?\n\nFed/Treasury continuously debased currency throughout 2008-2019 period. Liquidity injection (bailing out banks during GFC/2008, ZIRP era, M2 stimulius of 2019 are visible ones). How would I capture this events as proxy of “liquidity” ?\n\n\nRight now your \\(L_t\\) is basically a short-horizon “flow” liquidity shock index.\n\n\nWhereas I’m looking for slow, structural debasement / regime of easy money (2008–2019, QE, ZIRP, etc.).\n\n\nThose are not the same object.\n\nBecause I use 1-month growth rates \\(\\Delta \\log(\\cdot)\\), my index reacts to spikes / changes, not the level of the monetary stock.\nI need to factor in more factors to the liquidity index such as -\n\nLevel of money / balance sheet relative to trend / GDP, not just monthly change\nProlonged ZIRP / negative real rate period\nExcess liquidity over economic activity\n\n\n\n\n\n\nExamples (all can be pulled from FRED):\nLog level vs pre-2008 trend\nLet \\(m_t = \\log M2_t\\). Fit a linear trend on a pre-QE baseline (say 1985–2007):\n\\(m_t \\approx a + bt \\quad (t \\le 2007)\\)\nThen define excess money stock:\n\\(EM_t = m_t - (a + bt)\\)\nAfter 2008, \\(EM_t\\) becomes increasingly positive if M2 grows above its historical trend.\nSimilarly for the Fed Balance Sheet, let \\(b_t = \\log(\\text{FedBal}_t)\\):\n\\(EB_t = b_t - (\\alpha + \\beta t)\\) (pre-2007 trend)\n\nExcess liquidity versus real economy\nUse real GDP series (e.g., GDPC1) and define:\n\\(EL_t = \\log M2_t - \\log GDP_t\\)\nOr measure multi-year excess growth:\n\\(EL_t(3y) = \\log M2_t - \\log M2_{t-36} - (\\log GDP_t - \\log GDP_{t-36})\\)\n\nZIRP / negative real-rate indicator\n\\(D_t^{ZIRP} = 1(r_t^{real} &lt; 0)\\)\nFrom 2009–2015, this should be mostly 1.\n\n\n\n\nInstead of only:\n\\(x_t = \\{ \\Delta \\log M2_t,\\; \\Delta \\log FedBal_t,\\; TS_t,\\; r_t^{real},\\; CS_t \\}\\)\nLet’s expand to:\n\\(x_t^{aug} = \\{\n\\Delta \\log M2_t,\\;\n\\Delta \\log FedBal_t,\\;\nTS_t,\\;\nr_t^{real},\\;\nCS_t,\\;\nEM_t,\\;\nEB_t,\\;\nEL_t,\\;\nEL_t(3y),\\;\nD_t^{ZIRP}\n\\}\\)\nThen standardize and perform PCA / SparsePCA on this.\n\nPC1 (or a combination) loading heavily on \\(EM\\), \\(EB\\), \\(EL\\), \\(D_t^{ZIRP}\\) → structural easy-money regime\n\nPC2 (or your current PC1) loading on short-horizon changes → flow / shock liquidity\n\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\nNaN\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\nNaN\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29511.664\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\nNaN\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\nNaN\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29825.182\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\nNaN\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\nNaN\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n30042.113\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\nNaN\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\nNaN\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30485.729\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\nNaN\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\nNaN\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# 1. Start from the original GDP values only (drop NaNs)\ngdp_series = macro_raw[\"GDP\"].dropna()\n\n# 2. Collapse to unique quarter-end values\n#    - If GDP is timestamped at quarter *start* (e.g. 2025-04-01),\n#      this will move it to the quarter end (2025-06-30) and give unique labels.\ngdp_q = gdp_series.resample(\"Q\").last()   # quarterly, at quarter-end (e.g. 2025-03-31, 2025-06-30, ...)\n\n# 3. Upsample to monthly and forward-fill within the quarter\ngdp_m = gdp_q.resample(\"M\").ffill()\n\n# 4. Assign back into macro_raw, aligning on the monthly index\nmacro_raw[\"GDP\"] = gdp_m\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\n28708.161\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\n29147.044\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29147.044\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\n29147.044\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\n29511.664\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29511.664\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\n29511.664\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\n29825.182\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n29825.182\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\n29825.182\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\n30042.113\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30042.113\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\n30042.113\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\n30485.729\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndef build_liquidity_proxies_augmented(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Flow proxies\n    df[\"dlog_M2\"]       = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"]  = np.log(df[\"FED_BAL\"]).diff()\n    df[\"term_spread\"]   = df[\"DGS10\"] - df[\"TB3M\"]\n    df[\"infl_yoy\"]      = np.log(df[\"CPI\"]).diff(12)\n    df[\"real_rate\"]     = df[\"TB3M\"] - 100 * df[\"infl_yoy\"]\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # 2. Levels\n    df[\"log_M2\"]      = np.log(df[\"M2SL\"])\n    df[\"log_FED_BAL\"] = np.log(df[\"FED_BAL\"])\n\n    # Use data up to 2007-12-31 to fit trends\n    pre = df.loc[: \"2007-12-31\"].copy()\n\n    # --- Trend for M2 ---\n    pre_m2 = pre[\"log_M2\"].dropna()\n    t_m2   = np.arange(len(pre_m2))\n    coefs_M2 = np.polyfit(t_m2, pre_m2.values, deg=1)\n\n    t_full = np.arange(len(df))\n    trend_M2 = np.polyval(coefs_M2, t_full)\n    df[\"EM\"] = df[\"log_M2\"] - trend_M2\n\n    # --- Trend for Fed balance sheet ---\n    pre_fb = pre[\"log_FED_BAL\"].dropna()\n    t_fb   = np.arange(len(pre_fb))\n    coefs_FB = np.polyfit(t_fb, pre_fb.values, deg=1)\n\n    trend_FB = np.polyval(coefs_FB, t_full)\n    df[\"EB\"] = df[\"log_FED_BAL\"] - trend_FB\n\n    # 3. Excess liquidity vs GDP over 3y\n    df[\"log_GDP\"] = np.log(df[\"GDP\"])\n    df[\"EL_3y\"] = (df[\"log_M2\"] - df[\"log_M2\"].shift(36)) - \\\n                  (df[\"log_GDP\"] - df[\"log_GDP\"].shift(36))\n\n    # 4. ZIRP dummy\n    df[\"ZIRP_dummy\"] = (df[\"real_rate\"] &lt; 0).astype(int)\n\n    # 5. Build augmented proxy matrix\n    cols_aug = [\n        \"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\", \"real_rate\", \"credit_spread\",\n        \"EM\", \"EB\", \"EL_3y\", \"ZIRP_dummy\"\n    ]\n\n    return df[cols_aug].dropna()\n\n\nliquidity_proxies_aug = build_liquidity_proxies_augmented(macro_raw)\n\n\nliquidity_proxies_aug.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n0.004077\n-0.006080\n-0.91\n2.316574\n0.74\n0.204864\n0.811903\n-0.194283\n0\n\n\n2024-10-31\n0.002378\n-0.009447\n-0.23\n1.971101\n0.68\n0.202921\n0.798940\n-0.199645\n0\n\n\n2024-11-30\n0.004673\n-0.015569\n-0.24\n1.742012\n0.64\n0.203273\n0.779855\n-0.203197\n0\n\n\n2024-12-31\n0.000775\n-0.002781\n0.31\n1.438113\n0.60\n0.199728\n0.773558\n-0.186129\n0\n\n\n2025-01-31\n0.003164\n-0.009892\n0.37\n1.254690\n0.62\n0.198572\n0.760151\n-0.188367\n0\n\n\n2025-02-28\n0.003354\n-0.007668\n0.02\n1.444603\n0.60\n0.197605\n0.748966\n-0.189520\n0\n\n\n2025-03-31\n0.003338\n-0.003828\n0.03\n1.822893\n0.64\n0.196623\n0.741623\n-0.177663\n0\n\n\n2025-04-30\n0.006165\n-0.004606\n-0.04\n1.903069\n0.73\n0.198467\n0.733501\n-0.172818\n0\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n0.196757\n0.724600\n-0.167478\n0\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n0.197692\n0.719428\n-0.150768\n0\n\n\n\n\n\n\n\n\nz1_t = standardize_and_signflip(liquidity_proxies_aug)\nz1_t.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n-0.139814\n-0.331324\n-1.812889\n-1.577541\n0.673692\n0.888682\n0.786734\n-2.353880\n-1.527525\n\n\n2024-10-31\n-0.414988\n-0.409551\n-1.282837\n-1.408779\n0.819794\n0.868074\n0.764262\n-2.404333\n-1.527525\n\n\n2024-11-30\n-0.043456\n-0.551814\n-1.290631\n-1.296871\n0.917195\n0.871811\n0.731177\n-2.437752\n-1.527525\n\n\n2024-12-31\n-0.674370\n-0.254667\n-0.861913\n-1.148418\n1.014596\n0.834206\n0.720261\n-2.277159\n-1.527525\n\n\n2025-01-31\n-0.287635\n-0.419885\n-0.815143\n-1.058817\n0.965896\n0.821943\n0.697018\n-2.298214\n-1.527525\n\n\n2025-02-28\n-0.256970\n-0.368230\n-1.087964\n-1.151588\n1.014596\n0.811689\n0.677630\n-2.309059\n-1.527525\n\n\n2025-03-31\n-0.259533\n-0.278983\n-1.080169\n-1.336381\n0.917195\n0.801267\n0.664900\n-2.197496\n-1.527525\n\n\n2025-04-30\n0.198084\n-0.297077\n-1.134734\n-1.375546\n0.698042\n0.820833\n0.650819\n-2.151913\n-1.527525\n\n\n2025-05-31\n-0.377318\n-0.315174\n-0.978836\n-1.374952\n0.649342\n0.802693\n0.635389\n-2.101665\n-1.527525\n\n\n2025-06-30\n0.050761\n-0.228533\n-1.095759\n-1.223790\n0.795443\n0.812604\n0.626423\n-1.944440\n-1.527525\n\n\n\n\n\n\n\n\nL1_t = build_sparse_pca_liquidity_index(z1_t, alpha=0.5)\nL1_t.tail(10)\n\nSparsePCA components (loadings):\n  dlog_M2: -0.132\n  dlog_FED_BAL: -0.159\n  term_spread: -0.389\n  real_rate: -0.515\n  credit_spread: 0.046\n  EM: -0.114\n  EB: -0.134\n  EL_3y: -0.518\n  ZIRP_dummy: -0.490\n\n\nDATE\n2024-09-30    3.347080\n2024-10-31    3.143188\n2024-11-30    3.088484\n2024-12-31    2.811365\n2025-01-31    2.736110\n2025-02-28    2.887801\n2025-03-31    2.906425\n2025-04-30    2.856634\n2025-05-31    2.850481\n2025-06-30    2.674924\nFreq: M, Name: L, dtype: float64\n\n\n\nL1_t.plot(title=\"Sparse PCA Liquidity Index L(t) - Augmented w/ Flow & Stock Indicators\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel, regimes_aug_df = fit_hmm_on_liquidity(L1_t,n_states=2)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.484\n  state 1: mean L = 1.536\n\n\n\nregime_aug_df.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_aug_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined_aug = reg.join(ff_adj, how=\"inner\")\n\n\ncombined_aug.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n-0.0100\n-0.0089\n0.0086\n-0.0148\n0.0098\n0.0039\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n0.0649\n0.0459\n0.0015\n-0.0231\n-0.0205\n0.0040\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n-0.0315\n-0.0383\n-0.0300\n0.0189\n-0.0121\n0.0037\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\ndf2 = combined_aug.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by Liquidity Regime:\")\nprint(means_by_regime)\n\nMean factor returns by Liquidity Regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nHigh             -0.004519 -0.001700  0.001564 -0.003006\nTight             0.001896  0.000038  0.002920  0.001302\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\nHigh Liquidity\nTight Liquidity\nInterpretation\n\n\n\n\nSMB\nNegative\nPositive\nSmall caps thrive when liquidity tightens\n\n\nHML\nNegative\nSlightly positive\nValue improves in tight regimes\n\n\nRMW\nPositive\nMore positive\nProfitability is the strongest cross-regime performer\n\n\nCMA\nNegative\nPositive\nConservative investment becomes favored when liquidity tightens\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L1_t.index,\n    y=L1_t.values,\n    hue=regimes_aug_df[\"state\"],\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Augmented Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"Augmented L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_aug_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution/KDE of Augmented L(t) by HMM State\")\nax[1].set_xlabel(\"Augmented L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn KDE of augmented liquidity index \\(L_t^{aug}\\), clusters are clearly separated, unlike last one\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndef parse_yyyy_mm_to_date(s: str):\n    \"\"\"\n    Convert 'YYYY.MM' to a proper datetime.\n    Handles the special case where October appears as YYYY.1 instead of YYYY.10.\n    \"\"\"\n    s = str(s).strip()\n    parts = s.split(\".\")\n    if len(parts) != 2:\n        raise ValueError(f\"Unexpected date format: {s}\")\n    \n    year = int(parts[0])\n    mm = parts[1]\n\n    # Fix: \"1\" should be \"10\" (October)\n    if mm == \"1\":\n        month = 10\n    else:\n        month = int(mm)\n\n    return pd.Timestamp(year=year, month=month, day=1)\n\n# --- Load CAPE CSV ---\nsp500_cape_raw = pd.read_csv(\"data/sp500-cape-1990-2025.csv\")\n\n# Clean + parse\nsp500_cape_raw[\"date\"] = sp500_cape_raw[\"Date\"].apply(parse_yyyy_mm_to_date)\n\nsp500_cape_raw = (\n    sp500_cape_raw[[\"date\", \"SP500\", \"CAPE\"]]\n    .set_index(\"date\")\n    .sort_index()\n)\n\n# Convert to month-end aligned CAPE series\nsp500_cape_m = sp500_cape_raw.resample(\"M\").last()\nsp500_cape_m.rename(columns={\"CAPE\": \"cape\", \"SP500\": \"sp500\"}, inplace=True)\n\nsp500_cape_m.tail()\n\n\n\n\n\n\n\n\nsp500\ncape\n\n\ndate\n\n\n\n\n\n\n2025-08-31\n6408.95\n37.85\n\n\n2025-09-30\n6584.02\n38.58\n\n\n2025-10-31\n6735.69\n39.21\n\n\n2025-11-30\n6740.89\n39.12\n\n\n2025-12-31\n6812.63\n39.42\n\n\n\n\n\n\n\n\n\n\nLong-Term Average/Median: The S&P 500’s historical long-term average CAPE ratio is around 16-18, with a median value of approximately 16.04 (since 1881). This provides a central benchmark.\nHigh Valuation (Expensive): A CAPE ratio that is notably higher than the historical average (e.g., above 25 or 30) is considered indicative of a highly valued or overvalued market. Historically, values exceeding 30 have only occurred during major market peaks like the 1929 crash, the dot-com bubble in the late 1990s (peaking over 44), and the post-pandemic period around 2021. Such high readings have typically been followed by periods of lower-than-average, or even negative, long-term returns.\nLow Valuation (Cheap): A CAPE ratio well below the historical average (e.g., below 15 or 10) suggests an undervalued or cheap market. Historically, such levels have been associated with minimal downside risk and higher average long-term returns.\nCould have calculated\nV_spread = (x - baseline) / baseline # baseline = 16\nProblem is, if the whole world “structurally” shifted (e.g. post-1990 lower inflation), “16” stops being meaningful.\nWe are instead measuring,\n\\[\nV_t = \\frac{CAPE_t - \\mathrm{median}(CAPE)}{\\mathrm{IQR}(CAPE)}\n\\]\n\ndef build_valuation_spread_baseline(\n    val_df: pd.DataFrame,\n    metric: str = \"cape\",\n    baseline: float = None,\n    scale: bool = True,\n) -&gt; pd.Series:\n    x = val_df[metric].astype(float)\n\n    if baseline is None:\n        # robust long-run baseline\n        median = x.median()\n        iqr = x.quantile(0.75) - x.quantile(0.25)\n        baseline = median\n        denom = iqr if scale and iqr &gt; 0 else baseline\n    else:\n        denom = baseline if scale else 1.0\n\n    diff = x - baseline          \n\n    V_spread = diff / denom if scale else diff\n    V_spread.name = \"V_spread\"\n    return V_spread\n\n# Example: baseline at 16.04 (long-run median)\nV_spread_t = build_valuation_spread_baseline(\n    sp500_cape_m,\n    metric=\"cape\",\n    baseline=None,\n    scale=True,   # or False if you prefer \"CAPE points below baseline\"\n)\n\nV_spread_t.tail()\n\ndate\n2025-08-31    1.259380\n2025-09-30    1.338771\n2025-10-31    1.407287\n2025-11-30    1.397499\n2025-12-31    1.430125\nFreq: M, Name: V_spread, dtype: float64\n\n\n\nV_spread_t.plot(figsize=(14, 4), title=\"S&P 500 Valuation Spread (cheap vs own history)\")\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\ncombined_aug.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\n# regimes_df: index is month-end\nreg = combined_aug.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Align valuation spread to month-end, then join\nV_spread_m = V_spread_t.resample(\"M\").last()\n\ncombined = (\n    reg\n    .join(V_spread_m.to_frame(), how=\"inner\")\n)\n\ncombined.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\nV_spread\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n1.187602\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n0.925503\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n0.690593\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n0.958129\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n1.070147\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Valuation spread series, e.g. top–bottom decile)\n$ V_t^{} $\n(Regime-conditional mean valuation spread)\n$ {V}^{(k)} = $\n$ ^{(k)} = {_{t=1}^T (_t = k)} $\n(Difference in spreads between regimes)\n$ ^{()} - ^{()} $\n(Factor return in regime ( k ))\n$ r_t^{(F)} $\n$ {r}_F^{(k)} = $\n$ F^{(k)} = {{t=1}^T (_t = k)} $\n(Regime-specific Sharpe ratio)\n$ _F^{(k)} = $\n\n# Regime-conditional mean valuation spread:  V̄^(k)\nval_means_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .mean()\n)\n\nval_stds_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .std()\n)\n\nprint(\"Regime-conditional mean valuation spread:\")\nprint(val_means_by_regime)\n\nprint(\"\\nRegime-conditional valuation z-score std dev:\")\nprint(val_stds_by_regime)\n\n# Difference in spreads between regimes:  V̂^(High) − V̂^(Tight)\nif {\"High\", \"Tight\"}.issubset(val_means_by_regime.index):\n    spread_diff = (\n        val_means_by_regime[\"High\"] - val_means_by_regime[\"Tight\"]\n    )\n    print(\"\\nDifference in spreads High − Tight:\")\n    print(spread_diff)\n\nRegime-conditional mean valuation spread:\nstate_label\nHigh     0.380274\nTight   -0.049269\nName: V_spread, dtype: float64\n\nRegime-conditional valuation z-score std dev:\nstate_label\nHigh     0.406851\nTight    0.551632\nName: V_spread, dtype: float64\n\nDifference in spreads High − Tight:\n0.42954211798360936\n\n\n\n\n\n\nTight liquidity = cheaper markets (value regime),\nHigh liquidity = expensive markets (growth regime).\n\n\nMarkets are ~0.43σ more expensive in High liquidity regimes than Tight liquidity regimes.\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# -------------------------------------------------------------------\n# Assume you already have:\n#   L_t              : pd.Series (liquidity index), monthly, name \"L\"\n#   sp500_cape_m     : DataFrame with column \"sp500\" (S&P 500)\n#   V_spread_t       : pd.Series (\"V_spread\"), monthly valuation cheapness\n#   regimes_df       : DataFrame with column \"state_label\"\n# -------------------------------------------------------------------\n\n# 1) Build a common monthly dataframe\ndf_plot = pd.DataFrame(index=L_t.index.union(sp500_cape_m.index).union(V_spread_t.index))\n\ndf_plot[\"L\"]         = L_t.reindex(df_plot.index)\ndf_plot[\"spx_price\"] = sp500_cape_m[\"sp500\"].reindex(df_plot.index)\ndf_plot[\"V_spread\"]  = V_spread_t.reindex(df_plot.index)\n\n# Attach regimes (forward-fill in case of any slight index mismatch)\nreg_states = combined[\"state_label\"].reindex(df_plot.index).ffill()\ndf_plot[\"state_label\"] = reg_states\n\n# Optional: drop rows before we have all three series\ndf_plot = df_plot.dropna(subset=[\"L\", \"spx_price\", \"V_spread\", \"state_label\"])\n\n# 2) Helper: find contiguous regime segments for shading\ndef get_regime_segments(states: pd.Series):\n    \"\"\"\n    Given a Series of regime labels indexed by date,\n    return list of (start_date, end_date, label) segments\n    where the label is constant.\n    \"\"\"\n    segments = []\n    prev_label = None\n    start_date = None\n\n    for dt, label in states.items():\n        if prev_label is None:\n            prev_label = label\n            start_date = dt\n            continue\n\n        if label != prev_label:\n            # segment ended at previous date\n            end_date = dt\n            segments.append((start_date, end_date, prev_label))\n            start_date = dt\n            prev_label = label\n\n    # last segment\n    if prev_label is not None and start_date is not None:\n        segments.append((start_date, states.index[-1], prev_label))\n\n    return segments\n\nsegments = get_regime_segments(df_plot[\"state_label\"])\n\n# 3) Colors for regimes\nregime_colors = {\n    \"High\":    \"#ffe0e0\",  # pale red\n    \"Neutral\": \"#f7f7f7\",  # light gray\n    \"Tight\":   \"#d1ffd1\",  # pale green\n}\n\n# 4) Plot: 3 stacked subplots with shared x-axis\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True,\n                         gridspec_kw={\"height_ratios\": [1, 1, 1]})\n\nax_L, ax_spx, ax_val = axes\n\n# --- Shading first so lines render on top ---\nfor (start, end, label) in segments:\n    color = regime_colors.get(label, \"#f0f0f0\")\n    for ax in axes:\n        ax.axvspan(start, end, color=color, alpha=0.3)\n\n# --- Panel 1: Liquidity index L(t) ---\nax_L.plot(df_plot.index, df_plot[\"L\"], label=\"L(t)\", linewidth=1.5)\nax_L.set_ylabel(\"L(t)\")\nax_L.set_title(\"Liquidity Index, S&P 500, and Valuation Spread by Liquidity Regime\")\nax_L.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 2: S&P 500 level ---\nax_spx.plot(df_plot.index, df_plot[\"spx_price\"], label=\"S&P 500\", linewidth=1.5)\nax_spx.set_ylabel(\"S&P 500\")\nax_spx.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 3: Valuation cheapness (V_spread) ---\nax_val.plot(df_plot.index, df_plot[\"V_spread\"], label=\"V_spread (cheap vs baseline)\", linewidth=1.5)\nax_val.axhline(0.0, color=\"black\", linewidth=1, linestyle=\"--\", alpha=0.7)\nax_val.set_ylabel(\"V_spread\")\nax_val.set_xlabel(\"Date\")\nax_val.grid(True, linestyle=\"--\", alpha=0.4)\n\n# Optional legends\nax_L.legend(loc=\"upper left\")\nax_spx.legend(loc=\"upper left\")\nax_val.legend(loc=\"upper left\")\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#high-level-structure-of-the-paper",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#high-level-structure-of-the-paper",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Story – why this matters (intro + conclusion)\nData – what you measure (markets, factors, liquidity proxies)\nModels – how you measure (liquidity index, regimes, cross-sectional tests)\nPortfolio – what to do with it (conditional allocation"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#abstract-150200-words",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#abstract-150200-words",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "The “valuations don’t matter in infinite money printing” meme.\nYour construction of a liquidity index and liquidity regimes.\nKey findings:\nValuations do matter, but conditional on liquidity regimes.\nGrowth dominates in high-liquidity, negative-real-rate regimes.\nValue premia revive after QT / real rate normalization.\n\nPortfolio takeaway: a simple regime-aware factor-tilt model."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#motivation",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#motivation",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Narrative: post-GFC QE, ZIRP, NIRP, 2010–2021 “TINA”, tech bubble 2.0.\n\n“We live in a world of infinite money printing, so valuations don’t matter.”\n\nResearch question: &gt; Do valuations truly die under abundant liquidity, or do they merely become regime-dependent?\nDo valuation spreads (cheap vs expensive deciles) expand under high-liquidity regimes and compress under tight liquidity?\nHow do factor premia (value, momentum, quality, low vol) vary conditional on liquidity regimes?\nCan a macro-liquidity index predict the timing of factor rotations (growth vs value, long-duration vs short-duration equities)?\nDoes a regime-aware factor allocation deliver better risk-adjusted returns than a static factor mix?"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#intuition",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#intuition",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Valuation spread: the gap between cheap and expensive stocks (e.g., top vs bottom decile by B/M).\nLiquidity regime: periods of systematically easy vs tight financial conditions driven by monetary and fiscal variables."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#literature-sketch-very-short",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#literature-sketch-very-short",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Systemic Strategy: Factor investing: value, momentum, quality.\nRisk Factors and Risk premia: Liquidity\nRegime-switching: macro-conditional factors."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#contributions",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#contributions",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Build a macro liquidity index from multiple public series.\nUse HMM / Markov regimes to label high vs low liquidity states.\nShow how valuation spreads and factor returns behave across liquidity regimes.\nPropose a simple implementable regime-aware factor strategy."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#data-variables",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#data-variables",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Horizon: 1990–2025, monthly frequency (or weekly if feasible).\nMarket: US equities (CRSP/Compustat-type universe); optional robustness on AUS/EU.\n\n\n\n\n\n\nIndividual stock returns \\(r_{i,t}\\), market cap, book equity, earnings, etc.\nCharacteristics \\(X_{i,t}\\):\n\n\\[\n\\text{B/M},\\ \\text{E/P},\\ \\text{P/S},\\ \\text{size},\\ \\beta,\\ \\text{12–1 momentum},\\ \\text{profitability},\\ \\text{volatility}.\n\\]\n\nFactor returns \\(f_t\\):\n\n\\[\n\\text{MKT},\\ \\text{HML},\\ \\text{SMB},\\ \\text{MOM},\\ \\text{Quality},\\ \\text{LowVol}.\n\\]\nDefine a valuation spread:\n\\[\nV_t^{\\text{spread}} = \\text{Long top decile of B/M} - \\text{Short bottom decile}.\n\\]\n\n\n\n\nLet raw macro/liquidity variables at time \\(t\\) be \\(x_{j,t}\\), including:\n\n\n\n\nMoney growth: \\[\\Delta \\log(M2_t)\\]\nFed balance sheet growth: \\[\\Delta \\log(\\text{BS}_t)\\]\n\n\n\n\n\n\nShort rate: \\[i_t\\]\n10Y yield: \\[y_{10,t}\\]\nSlope: \\[\\text{TS}_t = y_{10,t} - i_t\\]\nReal short rate: \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\]\n\n\n\n\n\n\nCredit spread: \\[\\text{CS}_t = y_{\\text{Baa},t} - y_{\\text{Aaa},t}\\]\nVIX level: \\[\\text{VIX}_t\\]\n\n\n\n\n\n\nON RRP usage\nTGA balance\n\n\nAll variables \\(x_{j,t}\\) will then be standardized (e.g., z-scored) and aggregated into a single liquidity index."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#math",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#math",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "$ x_t =\n\\[\\begin{bmatrix}\nx_{1,t} \\\nx_{2,t} \\\n\\vdots \\\nx_{J,t}\n\\end{bmatrix}\\]\n\nx_{J,t} \\end{bmatrix} ^J $\n$ j = {t=1}^{T} x_{j,t} $\n$ j^2 = {t=1}^{T} (x_{j,t} - _j)^2 $\n$ z_{j,t} = , j = 1,,J $\n$ z_t =\n\\[\\begin{bmatrix}\nz_{1,t} \\\nz_{2,t} \\\n\\vdots \\\nz_{J,t}\n\\end{bmatrix}\\]\n\nz_{J,t} \\end{bmatrix} $\nOne may sign-flip the series so that higher \\(z_{j,t}\\) always corresponds to easier liquidity. For example:\nUse \\(-r^{\\text{real}}_{t}\\) instead of \\(r^{\\text{real}}_{t}\\).\nUse \\(-CS_{t}\\) instead of \\(CS_{t}\\).\nUse \\(-VIX_{t}\\) instead of \\(VIX_{t}\\).\n\n\n\n\n$ M2_t $\n$ _t $\n$ t = y{10,t} - i_t $\n$ r_t^{} = i_t - _t^e $\n$ t = y{Baa,t} - y_{Aaa,t} $\n\n\n\n\n$ = _{t=1}^{T} z_t z_t^$\n$ v_k = _k v_k, k = 1,,J $\n$ _1 _2 _J $\n$ L_t = v_1^z_t $\n(Alternative weighted index)\n$ L_t = w^z_t $\n\n\n\n\n$ s_t $\n$ L_t (s_t = k) (_k, _k^2), k = 1,,K $\n$ (s_t = j s_{t-1} = i) = p_{ij} $\n$ P =\n\\[\\begin{bmatrix}\np_{11} & \\cdots & p_{1K} \\\n\\vdots & \\ddots & \\vdots \\\np_{K1} & \\cdots & p_{KK}\n\\end{bmatrix}\\]\nts & \np_{K1} & & p_{KK} \\end{bmatrix}, {j=1}^K p{ij} = 1  i $\n(Filtered / smoothed state classification)\n$ t = {k } (s_t = k L_{1:T}) $\n(High- and Tight-liquidity labels)\n$ k_{} = _k _k $\n$ k_{} = _k _k $\n(Regime indicator using probability threshold)\n$ I_t^{} = $\n\n\n\n\n$ y_t =\n\\[\\begin{bmatrix}\nL_t \\\nr_t^{\\text{MKT}} \\\nr_t^{\\text{HML}} \\\n\\text{VIX}_t \\\n\\vdots\n\\end{bmatrix}\\]\nT}}\nr_t^{}\n_t\n\\end{bmatrix} $\n$ y_t (s_t = k) = A_k y_{t-1} + _t^{(k)} $\n$ _t^{(k)} (0, _k) $\n\n\n\n\n(Valuation spread series, e.g. top–bottom decile)\n$ V_t^{} $\n(Regime-conditional mean valuation spread)\n$ {V}^{(k)} = $\n$ ^{(k)} = {_{t=1}^T (_t = k)} $\n(Difference in spreads between regimes)\n$ ^{()} - ^{()} $\n(Factor return in regime ( k ))\n$ r_t^{(F)} $\n$ {r}_F^{(k)} = $\n$ F^{(k)} = {{t=1}^T (_t = k)} $\n(Regime-specific Sharpe ratio)\n$ _F^{(k)} = $\n\n\n\n\n(Continuous liquidity index)\n$ r_{t+1}^{(F)} = + L_t + ^c_t + _{t+1} $\n(Regime dummy regression)\n$ r_{t+1}^{(F)} = + {} I_t^{} + {} I_t^{} + {} I_t^{} + {t+1} $\n\n\n\n\n(Cross-sectional regression at time ( t ))\n$ r_{i,t+1} = t + {1,t} {i,t} + {2,t} {i,t} + {3,t} {i,t} + + {i,t+1} $\n(Regime-conditional average valuation slope)\n$ {}_1^{(k)} = $\n$ 1^{(k)} = {{t=1}^T (_t = k)} $\n\n\n\n\n(Factor return vector and regime weights)\n$ f_t ^K $\n$ w^{(k)} ^K $\n$ w_t = w^{(_t)} $\n(Portfolio return)\n$ R_{p,t+1} = w_t^f_{t+1} $"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#pca-and-sparse-pca",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#pca-and-sparse-pca",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Standard PCA solves:\n\\[\n\\max_{v} \\; \\| X v \\|^{2}\n\\quad \\text{s.t.} \\quad \\| v \\|^{2} = 1\n\\]\n\n\n\nPCA tries to find the direction ( \\(v\\) ) (a unit vector) along which the projected data ( \\(Xv\\) ) has maximum variance.\nVariance of the projection:\n\\[\n\\text{Var}(Xv) = \\frac{1}{n} |Xv|^{2}.\n\\]\nSo maximizing variance is equivalent to maximizing ( \\(|Xv|^{2}\\) ).\n\n\n\n\nWithout this constraint, the problem would blow up:\n\\[\n|X(\\alpha v)|^{2} = \\alpha^{2} |Xv|^{2},\n\\]\nand the maximum would be infinite by choosing ( \\(\\alpha\\) \\(\\infty\\) ).\nSo PCA forces ( \\(v\\) ) to be a direction, not a magnitude → a unit vector.\n\n\n\n\n\\[\n|Xv|^{2} = v^\\top X^\\top X v.\n\\]\nLet:\n\\[\nS = X^\\top X\n\\]\n(the unnormalized covariance matrix). Then PCA solves:\n\\[\n\\max_{v} ; v^\\top S v\n\\quad\\text{s.t.}\\quad v^\\top v = 1.\n\\]\nThis is exactly the Rayleigh quotient.\n\n\n\n\nPCA finds the direction (unit vector) along which the data cloud has maximum spread. That direction is exactly the eigenvector of the covariance matrix with the largest eigenvalue."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#sparse-pca",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#sparse-pca",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "PCA rotates the data into orthogonal directions capturing maximum variance.\nEach principal component is a linear combination of all variables.\nLoadings are typically dense (every variable has some non-zero weight).\nThis makes interpretation difficult:\n\n\n“PC1 of 127 economic multicolinear variables is 127-dimensional mush of everything.”\n\nThis is why PCA is almost never used directly for economic interpretation — PCs are not sparse.\n\n\n\n\nSparse PCA = PCA where most loadings are forced to be zero.\nMathematically:\nStandard PCA solves:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1\n\\]\nSparse PCA adds an L1 penalty (lasso-style sparsity) or constrains the number of non-zero elements:\n\\[\n\\max_{v} \\ |Xv|^2 - \\lambda |v|_1\n\\]\nor equivalently:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1,\\ |v|_1 \\leq c\n\\]\nThis forces:\n\nOnly a small subset of variables to load on each component.\nComponents become interpretable (e.g., PC1 loads on CPI, PCE, core CPI → “inflation”).\n\nThe specific implementation cited (“penalized matrix decomposition” by Witten et al. 2009) solves:\n\\[\n\\max_{u, v} \\ u^\\top X v \\quad\n\\text{s.t. } |u|_2 = 1,\\ |v|_2 = 1,\\ |v|_1 \\le c\n\\]\nThis is a general sparse factor extraction algorithm.\nIntuition: &gt; Sparse PCA forces components to use only the relevant variables, not a smear across 127 series.\n\n\n\n\n\nStandard PCA components ≈ dense, uninterpretable mixtures\nSparse PCA components ≈ targeted clusters of interpretable economic variables\n\n\n\n\n\nTake FRED-MD’s 127 macro series.\nIf you run standard PCA:\n\nPC1 loads on 100+ variables.\nPC2 loads on another 80+.\nInterpreting them is basically hopeless.\n\nSparse PCA, with lasso constraints:\n\nallows PC1 to load only on inflation-related variables\nPC2 to load only on housing-related variables\nPC3 on credit spreads\nPC4 on yields\nPC5 on production\netc.\n\nThis resembles the Stock–Watson macro factor model, but with sparsity for interpretability.\n\n\n\n\nSuppose sparse PCA gives:\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCPI YoY\n0.71\n\n\nCore CPI YoY\n0.68\n\n\nPCE Deflator\n0.66\n\n\nPCE core\n0.64\n\n\nAll other 123 variables\n0\n\n\n\n-&gt; You immediately label PC1 as “inflation factor”.\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nHousing starts\n0.62\n\n\nBuilding permits\n0.59\n\n\nNew homes sold\n0.61\n\n\nMortgage applications\n0.58\n\n\nEverything else\n0\n\n\n\n-&gt; PC2 = “housing & construction cycle”\n\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCorporate spreads (BAA–AAA)\n0.72\n\n\nHigh yield spread\n0.69\n\n\nCommercial paper spread\n0.64\n\n\nOthers\n0\n\n\n\n-&gt; PC3 = “credit stress”\n…and so on.\n\n\n\nSparse PCA essentially performs dimension reduction + variable selection simultaneously.\n\n\n\n\nSparse PCA encourages:\n\ngrouping variables that co-move strongly\ndropping variables unrelated to the theme\nchoosing a small number of representative series\n\nGiven economic data naturally clusters (inflation variables co-move, housing variables co-move), sparse PCA isolates these clusters.\nThis is similar to economic intuition:\n\ninflationary variables form a single latent factor\nspreads form a financial stress factor\nyields form a term-structure factor\nemployment variables form a labor factor\n\nSparse PCA “discovers” these clusters automatically.\n\n\n\n\nSparse PCA → interpretable macro drivers → better regime classification → better factor conditioning.\nEspecially when constructing a macro liquidity index or financial conditions index.\nSparse PCA gives:\n\n\n\nComponent\nInterpretation\n\n\n\n\nPC1\nLiquidity / money conditions\n\n\nPC2\nCredit spreads\n\n\nPC3\nHousing activity\n\n\nPC4\nYield curve / rates\n\n\nPC5\nEmployment\n\n\nPC6\nProduction\n\n\nPC7\nIncome\n\n\nPC8\nMarket stress\n\n\n\nThese are exactly the components McCracken & Ng (2016) find in FRED-MD.\n\n\n\n\n\n\n\n\nDense loadings\nHard to interpret\nPCs are linear mush\n\n\n\n\n\nForces many loadings to zero\nEach PC focuses on a small cluster of variables\nBecomes interpretable as a “macro theme”\nMatches how economists think about the business cycle\nPerfect for regime classification & factor research\n\n\n\n\n\nEconomic data naturally clusters into themes\nSparse PCA forces components to choose only the strongest cluster\nThis matches known macro factors (inflation, employment, credit, etc.)\n\n\n# !pip install pandas pandas_datareader numpy scikit-learn hmmlearn matplotlib\n\nimport pandas as pd\nimport numpy as np\nfrom pandas_datareader import data as pdr\n\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom hmmlearn.hmm import GaussianHMM\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#config-date-range-series",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#config-date-range-series",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "start_date = \"1990-01-01\"\nend_date   = \"2025-12-31\"\n\n# --- FRED series codes ---\nFRED_SERIES = {\n    \"M2SL\":   \"M2SL\",      # M2 money stock (monthly, SA)\n    \"FED_BAL\":\"WALCL\",     # Fed balance sheet total assets (weekly)\n    \"TB3M\":   \"TB3MS\",     # 3-Month T-Bill rate (monthly)\n    \"DGS10\":  \"DGS10\",     # 10-Year Treasury yield (daily -&gt; resample monthly)\n    \"BAA\":    \"BAA\",       # Moody's Baa corporate yield (monthly)\n    \"AAA\":    \"AAA\",       # Moody's Aaa corporate yield (monthly)\n    \"CPI\":    \"CPIAUCSL\",  # CPI index (monthly, SA)\n    \"GDP\":    \"GDP\",       # Nominal GDP (quarterly, SAAR)\n}\n\n# For equity factors: Fama-French via pandas_datareader (famafrench)\nFF_FACTORS_DATASET = \"F-F_Research_Data_5_Factors_2x3\""
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#download-macro-liquidity-variables-from-fred",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#download-macro-liquidity-variables-from-fred",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "def download_fred_series(series_dict, start, end):\n    \"\"\"\n    Download FRED series into a single monthly DataFrame.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Mapping logical_name -&gt; FRED code.\n    \"\"\"\n    dfs = []\n    for name, code in series_dict.items():\n        print(f\"Downloading {name} ({code}) from FRED...\")\n        s = pdr.DataReader(code, \"fred\", start, end)\n        s = s.rename(columns={code: name})\n        dfs.append(s)\n\n    df = pd.concat(dfs, axis=1)\n    # Ensure monthly freq by end-of-month sampling\n    df = df.resample(\"M\").last()\n    return df\n\nmacro_raw = download_fred_series(FRED_SERIES, start_date, end_date)\nmacro_raw.head()\n\nDownloading M2SL (M2SL) from FRED...\nDownloading FED_BAL (WALCL) from FRED...\nDownloading TB3M (TB3MS) from FRED...\nDownloading DGS10 (DGS10) from FRED...\nDownloading BAA (BAA) from FRED...\nDownloading AAA (AAA) from FRED...\nDownloading CPI (CPIAUCSL) from FRED...\nDownloading GDP (GDP) from FRED...\n\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n1990-01-31\n3166.8\nNaN\n7.64\n8.43\n9.94\n8.99\n127.5\n5872.701\n\n\n1990-02-28\n3179.2\nNaN\n7.74\n8.51\n10.14\n9.22\n128.0\nNaN\n\n\n1990-03-31\n3190.1\nNaN\n7.90\n8.65\n10.21\n9.37\n128.6\nNaN\n\n\n1990-04-30\n3201.6\nNaN\n7.77\n9.04\n10.30\n9.46\n128.9\n5960.028\n\n\n1990-05-31\n3200.6\nNaN\n7.74\n8.60\n10.41\n9.47\n129.1\nNaN\n\n\n\n\n\n\n\n\nmacro_raw.tail()\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\nNaN\n4.00\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n$ x_t =\n\\[\\begin{bmatrix}\nx_{1,t} \\\nx_{2,t} \\\n\\vdots \\\nx_{J,t}\n\\end{bmatrix}\\]\n\nx_{J,t} \\end{bmatrix} ^J $\n\nMoney & balance sheet growth: \\(\\Delta \\log(\\cdot)\\)\nTerm spread: \\(y_{10} - i_{3m}\\)\nReal short rate: \\(i_{3m} - \\pi_{\\text{year-on-year}}\\)\nCredit spread: \\(\\text{BAA} - \\text{AAA}\\)\netc.​\n\n\ndef build_liquidity_proxies(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Growth of M2 and Fed balance sheet\n    df[\"dlog_M2\"] = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"] = np.log(df[\"FED_BAL\"]).diff()\n\n    # 2. Term structure: 10Y - 3M\n    df[\"term_spread\"] = df[\"DGS10\"] - df[\"TB3M\"]\n\n    # 3. Year-on-year inflation (CPI yoy)\n    df[\"infl_yoy\"] = np.log(df[\"CPI\"]).diff(12)\n\n    # 4. Real short rate: nominal 3M - inflation\n    df[\"real_rate\"] = df[\"TB3M\"] - (100 * df[\"infl_yoy\"])  # TB3M in %, infl_yoy in log -&gt; approx*100\n\n    # 5. Credit spread: BAA - AAA\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # Drop rows with NaNs from diff(12) etc.\n    proxies = df[[\"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\",\n                  \"real_rate\", \"credit_spread\"]].dropna()\n\n    return proxies\n\nliquidity_proxies = build_liquidity_proxies(macro_raw)\n\n\nliquidity_proxies.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n\n\n2025-07-31\n0.003930\n-0.002950\n0.12\n1.554846\n0.65\n\n\n2025-08-31\n0.003607\n-0.005918\n0.11\n1.223147\n0.65\n\n\n2025-09-30\n0.004698\n0.000759\n0.24\n0.942084\n0.62\n\n\n\n\n\n\n\n\n\n\nWe want higher \\(z_{j,t}\\) – to mean easier liquidity.\n\nGrowth of M2 / Fed balance sheet: already “easier = higher value” -&gt; keeping sign\nTerm spread: steeper (more positive) often associated with easier conditions -&gt; keeping sign\nReal rate: easier liquidity when real rates are low -&gt; flipping sign\nCredit spread: easier liquidity when spreads are tight (low) -&gt; flipping sign\n\n\ndef standardize_and_signflip(proxies: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Standardize each column and flip signs so that higher z_{j,t}\n    always corresponds to easier liquidity.\n    \"\"\"\n    z = proxies.copy()\n\n    # Standardize\n    scaler = StandardScaler()\n    z_vals = scaler.fit_transform(z)\n    z = pd.DataFrame(z_vals, index=z.index, columns=z.columns)\n\n    # Sign flips so \"higher\" = easier liquidity\n    # dlog_M2: easier when higher -&gt; keep\n    # dlog_FED_BAL: easier when higher -&gt; keep\n    # term_spread: easier when steeper -&gt; keep (or adjust, depending on your view)\n    # real_rate: easier when more negative -&gt; flip sign\n    # credit_spread: easier when lower -&gt; flip sign\n\n    sign_flips = {\n        \"dlog_M2\": +1,\n        \"dlog_FED_BAL\": +1,\n        \"term_spread\": +1,\n        \"real_rate\": -1,\n        \"credit_spread\": -1,\n    }\n\n    for col, sgn in sign_flips.items():\n        z[col] = sgn * z[col]\n\n    return z\n\nz_t = standardize_and_signflip(liquidity_proxies)\n\n\nz_t.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n-0.377813\n-0.314009\n-0.968292\n-1.362581\n0.639849\n\n\n2025-06-30\n0.052579\n-0.226920\n-1.085244\n-1.211510\n0.786085\n\n\n2025-07-31\n-0.163048\n-0.257126\n-0.999479\n-1.193172\n0.883576\n\n\n2025-08-31\n-0.215601\n-0.326453\n-1.007276\n-1.031235\n0.883576\n\n\n2025-09-30\n-0.038105\n-0.170518\n-0.905917\n-0.894019\n0.956694\n\n\n\n\n\n\n\n\n\n\n\npd.Series(z_t.index).describe()\n\ncount                              273\nmean     2014-05-31 08:42:11.868131840\nmin                2003-01-31 00:00:00\n25%                2008-09-30 00:00:00\n50%                2014-05-31 00:00:00\n75%                2020-01-31 00:00:00\nmax                2025-09-30 00:00:00\nName: DATE, dtype: object\n\n\n\nz_t.values\n\narray([[ 0.11561096, -0.81061174,  1.11346119,  0.322683  , -0.40817503],\n       [ 0.20510346,  0.11034372,  0.8873532 ,  0.50696464, -0.23756644],\n       [-0.24145055, -0.0901311 ,  1.01210244,  0.46925598, -0.11570316],\n       ...,\n       [-0.163048  , -0.25712611, -0.999479  , -1.1931718 ,  0.88357574],\n       [-0.2156006 , -0.32645261, -1.00727583, -1.03123533,  0.88357574],\n       [-0.03810504, -0.17051847, -0.90591708, -0.89401934,  0.95669371]])\n\n\n\ndef build_sparse_pca_liquidity_index(z_df: pd.DataFrame,\n                                     alpha: float = 1.0,\n                                     random_state: int = 42) -&gt; pd.Series:\n    \"\"\"\n    Apply SparsePCA with 1 component to z_{j,t} to obtain L_t.\n    \"\"\"\n    spca = SparsePCA(\n        n_components=1,\n        alpha=alpha,\n        random_state=random_state\n    )\n\n    # Fit on standardized proxies\n    L_scores = spca.fit_transform(z_df.values)  # shape (T, 1)\n    L = pd.Series(L_scores.flatten(), index=z_df.index, name=\"L\")\n\n    # Optional: enforce that L_t is positively correlated with dlog_M2\n    # corr = np.corrcoef(L, z_df[\"dlog_FED_BAL\"])[0, 1]\n    # if corr &lt; 0:\n    #     L = -L\n\n    print(\"SparsePCA components (loadings):\")\n    for coef, col in zip(spca.components_[0], z_df.columns):\n        print(f\"  {col}: {coef:.3f}\")\n\n    return L\n\nL_t = build_sparse_pca_liquidity_index(z_t, alpha=0.5)\n\nSparsePCA components (loadings):\n  dlog_M2: 0.430\n  dlog_FED_BAL: 0.526\n  term_spread: 0.492\n  real_rate: 0.389\n  credit_spread: -0.381\n\n\n\nL_t.head()\n\nDATE\n2003-01-31    0.447153\n2003-02-28    0.861556\n2003-03-31    0.567234\n2003-04-30    0.979006\n2003-05-31    0.696500\nFreq: M, Name: L, dtype: float64\n\n\n\nL_t.plot(title=\"Sparse PCA Liquidity Index L(t)\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n​Fit a Gaussian HMM on the liquidity index. 2 or 3 regimes ?\n\nfrom hmmlearn.hmm import GaussianHMM\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\ndef fit_hmm_on_liquidity(\n    L: pd.Series,\n    n_states: int = 3,\n    covariance_type: str = \"full\",\n    random_state: int = 42,\n    standardize: bool = True\n):\n    \"\"\"\n    Fit a Gaussian HMM on L(t) and return the model and a DataFrame\n    with inferred regimes and posterior probabilities.\n\n    Parameters\n    ----------\n    L : pd.Series\n        Liquidity index (indexed by date).\n    n_states : int\n        Number of hidden states (regimes).\n    standardize : bool\n        If True, standardize L before fitting the HMM.\n\n    Returns\n    -------\n    model : GaussianHMM\n    df_regime : pd.DataFrame\n        Columns: L, state, p_state_k, state_label\n    \"\"\"\n\n    # 1) Prepare X\n    X = L.values.reshape(-1, 1)\n\n    scaler = None\n    if standardize:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n    # 2) Fit HMM\n    model = GaussianHMM(\n        n_components=n_states,\n        covariance_type=covariance_type,\n        random_state=random_state,\n        n_iter=500\n    )\n    model.fit(X)\n\n    hidden_states = model.predict(X)\n    post_probs = model.predict_proba(X)  # T x n_states\n\n    # 3) Build output DataFrame on original index\n    df_regime = pd.DataFrame(\n        {\"L\": L, \"state\": hidden_states},\n        index=L.index\n    )\n    for k in range(n_states):\n        df_regime[f\"p_state_{k}\"] = post_probs[:, k]\n\n    # 4) Use state means to order regimes\n    means = pd.Series(model.means_.flatten(), index=range(n_states))\n\n    print(\"HMM state means (unsorted, in fitted scale):\")\n    for k, m in means.items():\n        print(f\"  state {k}: mean L = {m:.3f}\")\n\n    # Order states from low liquidity to high liquidity\n    ordering = means.sort_values().index.tolist()\n\n    state_label_map = {}\n    if n_states == 3:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"Neutral\"\n        state_label_map[ordering[2]] = \"High\"\n    elif n_states == 2:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"High\"\n    else:\n        # Generic labels if you ever use more states\n        for i, s in enumerate(ordering):\n            state_label_map[s] = f\"Regime_{i}\"\n\n    df_regime[\"state_label\"] = df_regime[\"state\"].map(state_label_map)\n\n    return model, df_regime\n\n\nn_states = 3\nmodel, regimes_df = fit_hmm_on_liquidity(L_t,n_states)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.160\n  state 1: mean L = -0.106\n  state 2: mean L = 2.718\n\n\n\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# -----------------------------\n# TOP PANEL — L(t) + shading\n# -----------------------------\nax[0].plot(regimes_df.index, regimes_df[\"L\"], color=\"black\", lw=1.2)\nax[0].set_title(\"Sparse PCA Liquidity Index L(t) with Regime Shading\")\n\n# Define plain colors for each regime\nregime_colors = {\n    \"Tight\":   \"green\",\n    \"Neutral\": \"gray\",\n    \"High\":    \"crimson\",\n}\n\nshade_alpha = 0.2  # semi-opaque\n\nfor label, color in regime_colors.items():\n    mask = regimes_df[\"state_label\"] == label\n\n    in_region = False\n    start = None\n\n    for i in range(len(mask)):\n        if mask.iloc[i] and not in_region:\n            in_region = True\n            start = regimes_df.index[i]\n\n        elif not mask.iloc[i] and in_region:\n            in_region = False\n            end = regimes_df.index[i]\n            ax[0].axvspan(start, end, color=color, alpha=shade_alpha, linewidth=0)\n\n    # If region extends to the end of the sample\n    if in_region:\n        ax[0].axvspan(start, regimes_df.index[-1], color=color, alpha=shade_alpha, linewidth=0)\n\n# Legend for regimes (shaded colors)\nfrom matplotlib.lines import Line2D\nlegend_patches = [\n    Line2D([0], [0], color=\"green\",   lw=6, alpha=shade_alpha, label=\"Tight\"),\n    Line2D([0], [0], color=\"gray\",    lw=6, alpha=shade_alpha, label=\"Neutral\"),\n    Line2D([0], [0], color=\"crimson\", lw=6, alpha=shade_alpha, label=\"High\"),\n]\nax[0].legend(handles=legend_patches, loc=\"upper left\")\n\n# -----------------------------\n# BOTTOM PANEL — High regime probability\n# -----------------------------\nif \"p_state_0\" in regimes_df.columns:\n    high_state_id = regimes_df.groupby(\"state_label\")[\"state\"].first()[\"High\"]\n    ax[1].plot(regimes_df.index,\n               regimes_df[f\"p_state_{high_state_id}\"],\n               color=\"crimson\", lw=1.5)\n    ax[1].set_title(\"Posterior Probability of High Liquidity Regime\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\nThese five factors extend the original 3-factor model to better explain cross-sectional stock returns.\nThey are:\n\nRm–Rf — Market excess return\nSMB — Size (Small Minus Big)\nHML — Value (High Book/Market minus Low)\nRMW — Profitability (Robust Minus Weak)\nCMA — Investment (Conservative Minus Aggressive)\n\nLet’s break each down precisely.\n\n\n\n\\[\nR_{m}-R_{f}\n\\] The return of the broad market minus the risk-free rate.\n\nPositive → market went up more than cash.\nNegative → market underperformed cash/T-bills.\n\nIn above table:\n\nLast 12 months: +17.54% → very strong bull market year.\nLast 3 months: +7.40% → strong quarter.\nOctober 2025: +1.95% → moderate positive month.\n\n\n\n\n\n\\[\n\\text{SMB} = R_{\\text{small}} - R_{\\text{big}}\n\\]\n\nPositive → small caps outperform large caps.\nNegative → large caps outperform small caps.\n\nIn above table:\n\nLast 12 months: –10.71% → a massive large-cap dominance year.\nThis is consistent with mega-cap tech leadership.\n\n\n\n\n\n\\[\n\\text{HML} = R_{\\text{value}} - R_{\\text{growth}}\n\\]\n\nPositive → value outperforms growth.\nNegative → growth outperforms value.\n\nIn above table:\n\nLast 12 months: –2.36% → growth beat value.\nOctober 2025: –3.10% → especially growth-heavy month.\n\nThis aligns with liquidity-driven growth leadership.\n\n\n\n\n\\[\n\\text{RMW} = R_{\\text{robust}} - R_{\\text{weak}}\n\\]\nRobust = high operating profitability. Weak = low profitability.\n\nPositive → profitable companies outperform weak ones.\nNegative → weak/low-profit companies outperform.\n\nIn above table:\n\nLast 12 months: –13.60% → very unusual.\nThis means unprofitable firms outperformed profitable ones over the year.\n\nThat usually happens in:\n\nearly speculative bubbles\nliquidity-driven rallies\nretail-led tech/small-cap frenzies\nAI/futuristic narrative periods\n\n\n\n\n\n\\[\n\\text{CMA} = R_{\\text{conservative}} - R_{\\text{aggressive}}\n\\]\n\nConservative = firms investing slowly\nAggressive = firms aggressively expanding assets\n\nHigh investment → lower expected returns historically (consistent with q-theory: empire-building destroys value).\n\nPositive → conservative firms outperform heavy spenders.\nNegative → aggressive investment firms outperform.\n\nIn above table:\n\nLast 12 months: –10.46%\nLast 3 months: –4.47%\nOctober 2025: –4.03%\n\nInterpretation: Aggressively investing companies — think AI, R&D, biotech, high-growth tech — massively outperformed low-investment firms.\n\n\n\n\n\n\n\n\nFactor\nSign\nInterpretation\n\n\n\n\nRm–Rf: +\nStrong bull market\n\n\n\nSMB: –\nMega-caps beat small caps massively\n\n\n\nHML: –\nGrowth beat value\n\n\n\nRMW: –\nLow-profit firms beat profitable firms\n\n\n\nCMA: –\nAggressive-investment firms beat conservative ones\n\n\n\n\nThis is the textbook signature of a liquidity-driven growth/tech momentum regime, almost identical to:\n\n1998–1999 Dotcom\n2019–2021 QE wave\n2023–2024 AI mega-cap boom\n\nIt means:\n\n\n\n\nlarge\ngrowth\nunprofitable\nhigh-spending\nhigh-duration tech/AI/future-theme stocks\n\n\n\n\nThis is exactly the kind of environment where:\n\nValue fails\nSmall cap fails\nProfitability fails\nInvestment works negatively\nGrowth dominates\nMega-caps dominate\n\n\ndef download_ff_factors(start, end):\n    \"\"\"\n    Download monthly Fama-French 5 factors (2x3) using pandas_datareader.\n    \"\"\"\n    print(\"Downloading Fama-French factors (famafrench)...\")\n    ff_raw = pdr.DataReader(\n        name=FF_FACTORS_DATASET,\n        data_source=\"famafrench\",\n        start=start,\n        end=end\n    )[0]  # table [0] contains the data\n\n    ff = (ff_raw\n          .divide(100)  # convert from % to decimal\n          .reset_index(names=\"date\")\n          .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n          .rename(str.lower, axis=\"columns\")\n          .rename(columns={\"mkt-rf\": \"mkt_excess\"}))\n\n    ff = ff.set_index(\"date\")\n    return ff\n\nff_factors = download_ff_factors(start_date, end_date)\n\nDownloading Fama-French factors (famafrench)...\n\n\nFutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n&lt;ipython-input-69-55f7e03990a5&gt;:6: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n\n\n\nff_factors.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-06-01\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-07-01\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-01\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-09-01\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-10-01\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Fix Fama–French dates (shift one month backward)\nff_adj = ff_factors.copy()\n\n# FF data comes labeled as YYYY-MM-01 meaning return for previous month.\n# So shift index back to previous month-end.\nff_adj.index = (ff_adj.index.to_period(\"M\") - 1).to_timestamp(\"M\")\n\nff_adj.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined = reg.join(ff_adj, how=\"inner\")\n\n\ncombined.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-12-31\n-1.651021\n1\n0.162158\n0.837762\n0.000079\nTight\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n-1.496425\n0\n0.837514\n0.162233\n0.000253\nTight\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n-1.643180\n1\n0.162188\n0.837738\n0.000075\nTight\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n-1.627975\n0\n0.837473\n0.162258\n0.000269\nTight\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n-1.400432\n1\n0.162220\n0.837714\n0.000067\nTight\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n-1.561611\n0\n0.837459\n0.162284\n0.000258\nTight\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n-1.387057\n1\n0.162245\n0.837689\n0.000066\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n-1.482573\n0\n0.837437\n0.162308\n0.000255\nTight\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n-1.482572\n1\n0.162251\n0.837658\n0.000091\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n-1.251226\n0\n0.836442\n0.162316\n0.001242\nTight\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n\ndf2 = combined.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by *lagged* regime:\")\nprint(means_by_regime)\n\nMean factor returns by *lagged* regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nTight            -0.005439 -0.005360  0.000540 -0.003311\nNeutral           0.003200  0.002938  0.001051 -0.000056\nHigh              0.003327  0.001228  0.005127  0.003575\n\n\n\n\n\nThis is reverse of what you’d expect. In a tight liquidity regime, \\(R^{smb}\\) and \\(R^{hml}\\) should be deeply positive. You’d expect small under-valued companies to outperform big companies with premium valuation.\nLet’s go back to the \\(L_t\\) and understand what’s going on\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L_t.index,\n    y=L_t.values,\n    hue=regimes_df[\"state\"],\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution of L(t) by HMM State\")\nax[1].set_xlabel(\"L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the KDE (bottom panel):\n\nStates 0 (green) and 1 (yellow) heavily overlap\nOnly state 2 (red) is clearly separated — the “high liquidity spike”\nThis corresponds to crisis/QE events (2008, 2020), where liquidity jumps dramatically\n\nSo the natural clustering is:\n\nOne large cluster (normal-tight-neutral combined)\nOne rare extreme spike cluster\n\nYour HMM is being forced to split the unimodal bulk distribution into two states, resulting in:\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\nI initially thought alpha=0.5 in SparsePCA might be the cause. But the plot shows:\n\nL(t) has a healthy dynamic range across time (≈ −2.5 to +12)\nThe high liquidity cluster is clear and RARE\nThe rest of L(t) is genuinely one blob\n\nIf Sparse PCA was “overshrinking,” L(t) would be compressed; but it is not.\n\n\n\n\nIs Liquidity is fundamentally spiky ?\nWhy ?\nAm I defining Liquidity incorrectly ?\n\nFed/Treasury continuously debased currency throughout 2008-2019 period. Liquidity injection (bailing out banks during GFC/2008, ZIRP era, M2 stimulius of 2019 are visible ones). How would I capture this events as proxy of “liquidity” ?\n\n\nRight now your \\(L_t\\) is basically a short-horizon “flow” liquidity shock index.\n\n\nWhereas I’m looking for slow, structural debasement / regime of easy money (2008–2019, QE, ZIRP, etc.).\n\n\nThose are not the same object.\n\nBecause I use 1-month growth rates \\(\\Delta \\log(\\cdot)\\), my index reacts to spikes / changes, not the level of the monetary stock.\nI need to factor in more factors to the liquidity index such as -\n\nLevel of money / balance sheet relative to trend / GDP, not just monthly change\nProlonged ZIRP / negative real rate period\nExcess liquidity over economic activity\n\n\n\n\n\n\nExamples (all can be pulled from FRED):\nLog level vs pre-2008 trend\nLet \\(m_t = \\log M2_t\\). Fit a linear trend on a pre-QE baseline (say 1985–2007):\n\\(m_t \\approx a + bt \\quad (t \\le 2007)\\)\nThen define excess money stock:\n\\(EM_t = m_t - (a + bt)\\)\nAfter 2008, \\(EM_t\\) becomes increasingly positive if M2 grows above its historical trend.\nSimilarly for the Fed Balance Sheet, let \\(b_t = \\log(\\text{FedBal}_t)\\):\n\\(EB_t = b_t - (\\alpha + \\beta t)\\) (pre-2007 trend)\n\nExcess liquidity versus real economy\nUse real GDP series (e.g., GDPC1) and define:\n\\(EL_t = \\log M2_t - \\log GDP_t\\)\nOr measure multi-year excess growth:\n\\(EL_t(3y) = \\log M2_t - \\log M2_{t-36} - (\\log GDP_t - \\log GDP_{t-36})\\)\n\nZIRP / negative real-rate indicator\n\\(D_t^{ZIRP} = 1(r_t^{real} &lt; 0)\\)\nFrom 2009–2015, this should be mostly 1.\n\n\n\n\nInstead of only:\n\\(x_t = \\{ \\Delta \\log M2_t,\\; \\Delta \\log FedBal_t,\\; TS_t,\\; r_t^{real},\\; CS_t \\}\\)\nLet’s expand to:\n\\(x_t^{aug} = \\{\n\\Delta \\log M2_t,\\;\n\\Delta \\log FedBal_t,\\;\nTS_t,\\;\nr_t^{real},\\;\nCS_t,\\;\nEM_t,\\;\nEB_t,\\;\nEL_t,\\;\nEL_t(3y),\\;\nD_t^{ZIRP}\n\\}\\)\nThen standardize and perform PCA / SparsePCA on this.\n\nPC1 (or a combination) loading heavily on \\(EM\\), \\(EB\\), \\(EL\\), \\(D_t^{ZIRP}\\) → structural easy-money regime\n\nPC2 (or your current PC1) loading on short-horizon changes → flow / shock liquidity\n\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\nNaN\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\nNaN\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29511.664\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\nNaN\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\nNaN\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29825.182\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\nNaN\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\nNaN\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n30042.113\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\nNaN\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\nNaN\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30485.729\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\nNaN\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\nNaN\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# 1. Start from the original GDP values only (drop NaNs)\ngdp_series = macro_raw[\"GDP\"].dropna()\n\n# 2. Collapse to unique quarter-end values\n#    - If GDP is timestamped at quarter *start* (e.g. 2025-04-01),\n#      this will move it to the quarter end (2025-06-30) and give unique labels.\ngdp_q = gdp_series.resample(\"Q\").last()   # quarterly, at quarter-end (e.g. 2025-03-31, 2025-06-30, ...)\n\n# 3. Upsample to monthly and forward-fill within the quarter\ngdp_m = gdp_q.resample(\"M\").ffill()\n\n# 4. Assign back into macro_raw, aligning on the monthly index\nmacro_raw[\"GDP\"] = gdp_m\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\n28708.161\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\n29147.044\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29147.044\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\n29147.044\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\n29511.664\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29511.664\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\n29511.664\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\n29825.182\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n29825.182\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\n29825.182\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\n30042.113\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30042.113\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\n30042.113\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\n30485.729\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndef build_liquidity_proxies_augmented(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Flow proxies\n    df[\"dlog_M2\"]       = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"]  = np.log(df[\"FED_BAL\"]).diff()\n    df[\"term_spread\"]   = df[\"DGS10\"] - df[\"TB3M\"]\n    df[\"infl_yoy\"]      = np.log(df[\"CPI\"]).diff(12)\n    df[\"real_rate\"]     = df[\"TB3M\"] - 100 * df[\"infl_yoy\"]\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # 2. Levels\n    df[\"log_M2\"]      = np.log(df[\"M2SL\"])\n    df[\"log_FED_BAL\"] = np.log(df[\"FED_BAL\"])\n\n    # Use data up to 2007-12-31 to fit trends\n    pre = df.loc[: \"2007-12-31\"].copy()\n\n    # --- Trend for M2 ---\n    pre_m2 = pre[\"log_M2\"].dropna()\n    t_m2   = np.arange(len(pre_m2))\n    coefs_M2 = np.polyfit(t_m2, pre_m2.values, deg=1)\n\n    t_full = np.arange(len(df))\n    trend_M2 = np.polyval(coefs_M2, t_full)\n    df[\"EM\"] = df[\"log_M2\"] - trend_M2\n\n    # --- Trend for Fed balance sheet ---\n    pre_fb = pre[\"log_FED_BAL\"].dropna()\n    t_fb   = np.arange(len(pre_fb))\n    coefs_FB = np.polyfit(t_fb, pre_fb.values, deg=1)\n\n    trend_FB = np.polyval(coefs_FB, t_full)\n    df[\"EB\"] = df[\"log_FED_BAL\"] - trend_FB\n\n    # 3. Excess liquidity vs GDP over 3y\n    df[\"log_GDP\"] = np.log(df[\"GDP\"])\n    df[\"EL_3y\"] = (df[\"log_M2\"] - df[\"log_M2\"].shift(36)) - \\\n                  (df[\"log_GDP\"] - df[\"log_GDP\"].shift(36))\n\n    # 4. ZIRP dummy\n    df[\"ZIRP_dummy\"] = (df[\"real_rate\"] &lt; 0).astype(int)\n\n    # 5. Build augmented proxy matrix\n    cols_aug = [\n        \"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\", \"real_rate\", \"credit_spread\",\n        \"EM\", \"EB\", \"EL_3y\", \"ZIRP_dummy\"\n    ]\n\n    return df[cols_aug].dropna()\n\n\nliquidity_proxies_aug = build_liquidity_proxies_augmented(macro_raw)\n\n\nliquidity_proxies_aug.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n0.004077\n-0.006080\n-0.91\n2.316574\n0.74\n0.204864\n0.811903\n-0.194283\n0\n\n\n2024-10-31\n0.002378\n-0.009447\n-0.23\n1.971101\n0.68\n0.202921\n0.798940\n-0.199645\n0\n\n\n2024-11-30\n0.004673\n-0.015569\n-0.24\n1.742012\n0.64\n0.203273\n0.779855\n-0.203197\n0\n\n\n2024-12-31\n0.000775\n-0.002781\n0.31\n1.438113\n0.60\n0.199728\n0.773558\n-0.186129\n0\n\n\n2025-01-31\n0.003164\n-0.009892\n0.37\n1.254690\n0.62\n0.198572\n0.760151\n-0.188367\n0\n\n\n2025-02-28\n0.003354\n-0.007668\n0.02\n1.444603\n0.60\n0.197605\n0.748966\n-0.189520\n0\n\n\n2025-03-31\n0.003338\n-0.003828\n0.03\n1.822893\n0.64\n0.196623\n0.741623\n-0.177663\n0\n\n\n2025-04-30\n0.006165\n-0.004606\n-0.04\n1.903069\n0.73\n0.198467\n0.733501\n-0.172818\n0\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n0.196757\n0.724600\n-0.167478\n0\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n0.197692\n0.719428\n-0.150768\n0\n\n\n\n\n\n\n\n\nz1_t = standardize_and_signflip(liquidity_proxies_aug)\nz1_t.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n-0.139814\n-0.331324\n-1.812889\n-1.577541\n0.673692\n0.888682\n0.786734\n-2.353880\n-1.527525\n\n\n2024-10-31\n-0.414988\n-0.409551\n-1.282837\n-1.408779\n0.819794\n0.868074\n0.764262\n-2.404333\n-1.527525\n\n\n2024-11-30\n-0.043456\n-0.551814\n-1.290631\n-1.296871\n0.917195\n0.871811\n0.731177\n-2.437752\n-1.527525\n\n\n2024-12-31\n-0.674370\n-0.254667\n-0.861913\n-1.148418\n1.014596\n0.834206\n0.720261\n-2.277159\n-1.527525\n\n\n2025-01-31\n-0.287635\n-0.419885\n-0.815143\n-1.058817\n0.965896\n0.821943\n0.697018\n-2.298214\n-1.527525\n\n\n2025-02-28\n-0.256970\n-0.368230\n-1.087964\n-1.151588\n1.014596\n0.811689\n0.677630\n-2.309059\n-1.527525\n\n\n2025-03-31\n-0.259533\n-0.278983\n-1.080169\n-1.336381\n0.917195\n0.801267\n0.664900\n-2.197496\n-1.527525\n\n\n2025-04-30\n0.198084\n-0.297077\n-1.134734\n-1.375546\n0.698042\n0.820833\n0.650819\n-2.151913\n-1.527525\n\n\n2025-05-31\n-0.377318\n-0.315174\n-0.978836\n-1.374952\n0.649342\n0.802693\n0.635389\n-2.101665\n-1.527525\n\n\n2025-06-30\n0.050761\n-0.228533\n-1.095759\n-1.223790\n0.795443\n0.812604\n0.626423\n-1.944440\n-1.527525\n\n\n\n\n\n\n\n\nL1_t = build_sparse_pca_liquidity_index(z1_t, alpha=0.5)\nL1_t.tail(10)\n\nSparsePCA components (loadings):\n  dlog_M2: -0.132\n  dlog_FED_BAL: -0.159\n  term_spread: -0.389\n  real_rate: -0.515\n  credit_spread: 0.046\n  EM: -0.114\n  EB: -0.134\n  EL_3y: -0.518\n  ZIRP_dummy: -0.490\n\n\nDATE\n2024-09-30    3.347080\n2024-10-31    3.143188\n2024-11-30    3.088484\n2024-12-31    2.811365\n2025-01-31    2.736110\n2025-02-28    2.887801\n2025-03-31    2.906425\n2025-04-30    2.856634\n2025-05-31    2.850481\n2025-06-30    2.674924\nFreq: M, Name: L, dtype: float64\n\n\n\nL1_t.plot(title=\"Sparse PCA Liquidity Index L(t) - Augmented w/ Flow & Stock Indicators\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel, regimes_aug_df = fit_hmm_on_liquidity(L1_t,n_states=2)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.484\n  state 1: mean L = 1.536\n\n\n\nregime_aug_df.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_aug_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined_aug = reg.join(ff_adj, how=\"inner\")\n\n\ncombined_aug.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n-0.0100\n-0.0089\n0.0086\n-0.0148\n0.0098\n0.0039\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n0.0649\n0.0459\n0.0015\n-0.0231\n-0.0205\n0.0040\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n-0.0315\n-0.0383\n-0.0300\n0.0189\n-0.0121\n0.0037\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\ndf2 = combined_aug.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by Liquidity Regime:\")\nprint(means_by_regime)\n\nMean factor returns by Liquidity Regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nHigh             -0.004519 -0.001700  0.001564 -0.003006\nTight             0.001896  0.000038  0.002920  0.001302"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#yes",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#yes",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Factor\nHigh Liquidity\nTight Liquidity\nInterpretation\n\n\n\n\nSMB\nNegative\nPositive\nSmall caps thrive when liquidity tightens\n\n\nHML\nNegative\nSlightly positive\nValue improves in tight regimes\n\n\nRMW\nPositive\nMore positive\nProfitability is the strongest cross-regime performer\n\n\nCMA\nNegative\nPositive\nConservative investment becomes favored when liquidity tightens\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L1_t.index,\n    y=L1_t.values,\n    hue=regimes_aug_df[\"state\"],\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Augmented Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"Augmented L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_aug_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution/KDE of Augmented L(t) by HMM State\")\nax[1].set_xlabel(\"Augmented L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn KDE of augmented liquidity index \\(L_t^{aug}\\), clusters are clearly separated, unlike last one\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndef parse_yyyy_mm_to_date(s: str):\n    \"\"\"\n    Convert 'YYYY.MM' to a proper datetime.\n    Handles the special case where October appears as YYYY.1 instead of YYYY.10.\n    \"\"\"\n    s = str(s).strip()\n    parts = s.split(\".\")\n    if len(parts) != 2:\n        raise ValueError(f\"Unexpected date format: {s}\")\n    \n    year = int(parts[0])\n    mm = parts[1]\n\n    # Fix: \"1\" should be \"10\" (October)\n    if mm == \"1\":\n        month = 10\n    else:\n        month = int(mm)\n\n    return pd.Timestamp(year=year, month=month, day=1)\n\n# --- Load CAPE CSV ---\nsp500_cape_raw = pd.read_csv(\"data/sp500-cape-1990-2025.csv\")\n\n# Clean + parse\nsp500_cape_raw[\"date\"] = sp500_cape_raw[\"Date\"].apply(parse_yyyy_mm_to_date)\n\nsp500_cape_raw = (\n    sp500_cape_raw[[\"date\", \"SP500\", \"CAPE\"]]\n    .set_index(\"date\")\n    .sort_index()\n)\n\n# Convert to month-end aligned CAPE series\nsp500_cape_m = sp500_cape_raw.resample(\"M\").last()\nsp500_cape_m.rename(columns={\"CAPE\": \"cape\", \"SP500\": \"sp500\"}, inplace=True)\n\nsp500_cape_m.tail()\n\n\n\n\n\n\n\n\nsp500\ncape\n\n\ndate\n\n\n\n\n\n\n2025-08-31\n6408.95\n37.85\n\n\n2025-09-30\n6584.02\n38.58\n\n\n2025-10-31\n6735.69\n39.21\n\n\n2025-11-30\n6740.89\n39.12\n\n\n2025-12-31\n6812.63\n39.42\n\n\n\n\n\n\n\n\n\n\nLong-Term Average/Median: The S&P 500’s historical long-term average CAPE ratio is around 16-18, with a median value of approximately 16.04 (since 1881). This provides a central benchmark.\nHigh Valuation (Expensive): A CAPE ratio that is notably higher than the historical average (e.g., above 25 or 30) is considered indicative of a highly valued or overvalued market. Historically, values exceeding 30 have only occurred during major market peaks like the 1929 crash, the dot-com bubble in the late 1990s (peaking over 44), and the post-pandemic period around 2021. Such high readings have typically been followed by periods of lower-than-average, or even negative, long-term returns.\nLow Valuation (Cheap): A CAPE ratio well below the historical average (e.g., below 15 or 10) suggests an undervalued or cheap market. Historically, such levels have been associated with minimal downside risk and higher average long-term returns.\nCould have calculated\nV_spread = (x - baseline) / baseline # baseline = 16\nProblem is, if the whole world “structurally” shifted (e.g. post-1990 lower inflation), “16” stops being meaningful.\nWe are instead measuring,\n\\[\nV_t = \\frac{CAPE_t - \\mathrm{median}(CAPE)}{\\mathrm{IQR}(CAPE)}\n\\]\n\ndef build_valuation_spread_baseline(\n    val_df: pd.DataFrame,\n    metric: str = \"cape\",\n    baseline: float = None,\n    scale: bool = True,\n) -&gt; pd.Series:\n    x = val_df[metric].astype(float)\n\n    if baseline is None:\n        # robust long-run baseline\n        median = x.median()\n        iqr = x.quantile(0.75) - x.quantile(0.25)\n        baseline = median\n        denom = iqr if scale and iqr &gt; 0 else baseline\n    else:\n        denom = baseline if scale else 1.0\n\n    diff = x - baseline          \n\n    V_spread = diff / denom if scale else diff\n    V_spread.name = \"V_spread\"\n    return V_spread\n\n# Example: baseline at 16.04 (long-run median)\nV_spread_t = build_valuation_spread_baseline(\n    sp500_cape_m,\n    metric=\"cape\",\n    baseline=None,\n    scale=True,   # or False if you prefer \"CAPE points below baseline\"\n)\n\nV_spread_t.tail()\n\ndate\n2025-08-31    1.259380\n2025-09-30    1.338771\n2025-10-31    1.407287\n2025-11-30    1.397499\n2025-12-31    1.430125\nFreq: M, Name: V_spread, dtype: float64\n\n\n\nV_spread_t.plot(figsize=(14, 4), title=\"S&P 500 Valuation Spread (cheap vs own history)\")\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\ncombined_aug.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\n# regimes_df: index is month-end\nreg = combined_aug.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Align valuation spread to month-end, then join\nV_spread_m = V_spread_t.resample(\"M\").last()\n\ncombined = (\n    reg\n    .join(V_spread_m.to_frame(), how=\"inner\")\n)\n\ncombined.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\nV_spread\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n1.187602\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n0.925503\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n0.690593\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n0.958129\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n1.070147\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Valuation spread series, e.g. top–bottom decile)\n$ V_t^{} $\n(Regime-conditional mean valuation spread)\n$ {V}^{(k)} = $\n$ ^{(k)} = {_{t=1}^T (_t = k)} $\n(Difference in spreads between regimes)\n$ ^{()} - ^{()} $\n(Factor return in regime ( k ))\n$ r_t^{(F)} $\n$ {r}_F^{(k)} = $\n$ F^{(k)} = {{t=1}^T (_t = k)} $\n(Regime-specific Sharpe ratio)\n$ _F^{(k)} = $\n\n# Regime-conditional mean valuation spread:  V̄^(k)\nval_means_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .mean()\n)\n\nval_stds_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .std()\n)\n\nprint(\"Regime-conditional mean valuation spread:\")\nprint(val_means_by_regime)\n\nprint(\"\\nRegime-conditional valuation z-score std dev:\")\nprint(val_stds_by_regime)\n\n# Difference in spreads between regimes:  V̂^(High) − V̂^(Tight)\nif {\"High\", \"Tight\"}.issubset(val_means_by_regime.index):\n    spread_diff = (\n        val_means_by_regime[\"High\"] - val_means_by_regime[\"Tight\"]\n    )\n    print(\"\\nDifference in spreads High − Tight:\")\n    print(spread_diff)\n\nRegime-conditional mean valuation spread:\nstate_label\nHigh     0.380274\nTight   -0.049269\nName: V_spread, dtype: float64\n\nRegime-conditional valuation z-score std dev:\nstate_label\nHigh     0.406851\nTight    0.551632\nName: V_spread, dtype: float64\n\nDifference in spreads High − Tight:\n0.42954211798360936\n\n\n\n\n\n\nTight liquidity = cheaper markets (value regime),\nHigh liquidity = expensive markets (growth regime).\n\n\nMarkets are ~0.43σ more expensive in High liquidity regimes than Tight liquidity regimes.\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# -------------------------------------------------------------------\n# Assume you already have:\n#   L_t              : pd.Series (liquidity index), monthly, name \"L\"\n#   sp500_cape_m     : DataFrame with column \"sp500\" (S&P 500)\n#   V_spread_t       : pd.Series (\"V_spread\"), monthly valuation cheapness\n#   regimes_df       : DataFrame with column \"state_label\"\n# -------------------------------------------------------------------\n\n# 1) Build a common monthly dataframe\ndf_plot = pd.DataFrame(index=L_t.index.union(sp500_cape_m.index).union(V_spread_t.index))\n\ndf_plot[\"L\"]         = L_t.reindex(df_plot.index)\ndf_plot[\"spx_price\"] = sp500_cape_m[\"sp500\"].reindex(df_plot.index)\ndf_plot[\"V_spread\"]  = V_spread_t.reindex(df_plot.index)\n\n# Attach regimes (forward-fill in case of any slight index mismatch)\nreg_states = combined[\"state_label\"].reindex(df_plot.index).ffill()\ndf_plot[\"state_label\"] = reg_states\n\n# Optional: drop rows before we have all three series\ndf_plot = df_plot.dropna(subset=[\"L\", \"spx_price\", \"V_spread\", \"state_label\"])\n\n# 2) Helper: find contiguous regime segments for shading\ndef get_regime_segments(states: pd.Series):\n    \"\"\"\n    Given a Series of regime labels indexed by date,\n    return list of (start_date, end_date, label) segments\n    where the label is constant.\n    \"\"\"\n    segments = []\n    prev_label = None\n    start_date = None\n\n    for dt, label in states.items():\n        if prev_label is None:\n            prev_label = label\n            start_date = dt\n            continue\n\n        if label != prev_label:\n            # segment ended at previous date\n            end_date = dt\n            segments.append((start_date, end_date, prev_label))\n            start_date = dt\n            prev_label = label\n\n    # last segment\n    if prev_label is not None and start_date is not None:\n        segments.append((start_date, states.index[-1], prev_label))\n\n    return segments\n\nsegments = get_regime_segments(df_plot[\"state_label\"])\n\n# 3) Colors for regimes\nregime_colors = {\n    \"High\":    \"#ffe0e0\",  # pale red\n    \"Neutral\": \"#f7f7f7\",  # light gray\n    \"Tight\":   \"#d1ffd1\",  # pale green\n}\n\n# 4) Plot: 3 stacked subplots with shared x-axis\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True,\n                         gridspec_kw={\"height_ratios\": [1, 1, 1]})\n\nax_L, ax_spx, ax_val = axes\n\n# --- Shading first so lines render on top ---\nfor (start, end, label) in segments:\n    color = regime_colors.get(label, \"#f0f0f0\")\n    for ax in axes:\n        ax.axvspan(start, end, color=color, alpha=0.3)\n\n# --- Panel 1: Liquidity index L(t) ---\nax_L.plot(df_plot.index, df_plot[\"L\"], label=\"L(t)\", linewidth=1.5)\nax_L.set_ylabel(\"L(t)\")\nax_L.set_title(\"Liquidity Index, S&P 500, and Valuation Spread by Liquidity Regime\")\nax_L.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 2: S&P 500 level ---\nax_spx.plot(df_plot.index, df_plot[\"spx_price\"], label=\"S&P 500\", linewidth=1.5)\nax_spx.set_ylabel(\"S&P 500\")\nax_spx.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 3: Valuation cheapness (V_spread) ---\nax_val.plot(df_plot.index, df_plot[\"V_spread\"], label=\"V_spread (cheap vs baseline)\", linewidth=1.5)\nax_val.axhline(0.0, color=\"black\", linewidth=1, linestyle=\"--\", alpha=0.7)\nax_val.set_ylabel(\"V_spread\")\nax_val.set_xlabel(\"Date\")\nax_val.grid(True, linestyle=\"--\", alpha=0.4)\n\n# Optional legends\nax_L.legend(loc=\"upper left\")\nax_spx.legend(loc=\"upper left\")\nax_val.legend(loc=\"upper left\")\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/Empirical Validation of the 1971 Fiscal Structural Break - Analysis of U.S Real Individual Income Tax Revenue.html",
    "href": "notebooks/Empirical Validation of the 1971 Fiscal Structural Break - Analysis of U.S Real Individual Income Tax Revenue.html",
    "title": "Empirical Validation of the 1971 Fiscal Structural Break: Analysis of U.S. Real Individual Income Tax Revenue",
    "section": "",
    "text": "\"\"\"\nSimple script to download US Individual Income Tax / GDP data from FRED\nNo complex dependencies - just pandas and requests\n\"\"\"\n\nimport pandas as pd\nimport requests\nfrom io import StringIO\n\nprint(\"Downloading US Individual Income Tax and GDP data from FRED...\")\nprint(\"=\" * 80)\n\n# FRED series IDs\n# W006RC1Q027SBEA = Individual Income Tax Receipts (quarterly, billions)\n# GDP = Gross Domestic Product (quarterly, billions)\n\ndef download_fred_series(series_id, series_name):\n    \"\"\"Download a single series from FRED as CSV.\"\"\"\n    url = f\"https://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}\"\n    \n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        \n        # Read CSV\n        df = pd.read_csv(StringIO(response.text))\n        df.columns = ['DATE', series_name]\n        df['DATE'] = pd.to_datetime(df['DATE'])\n        df[series_name] = pd.to_numeric(df[series_name], errors='coerce')\n        \n        print(f\"✓ Downloaded {series_name}: {len(df)} observations\")\n        return df\n    \n    except Exception as e:\n        print(f\"✗ Error downloading {series_name}: {e}\")\n        return None\n\n# Download data\nprint(\"\\n1. Downloading Individual Income Tax receipts...\")\ntax_df = download_fred_series('W006RC1Q027SBEA', 'IndividualIncomeTax')\n\nDownloading US Individual Income Tax and GDP data from FRED...\n================================================================================\n\n1. Downloading Individual Income Tax receipts...\n✓ Downloaded IndividualIncomeTax: 315 observations\n\n\n\ntax_df.tail()\n\n\n\n\n\n\n\n\nDATE\nIndividualIncomeTax\n\n\n\n\n310\n2024-07-01\n3141.087\n\n\n311\n2024-10-01\n3202.119\n\n\n312\n2025-01-01\n3246.051\n\n\n313\n2025-04-01\n3455.060\n\n\n314\n2025-07-01\n3599.024\n\n\n\n\n\n\n\n\ntax_df_indexed = tax_df.set_index(\"DATE\")\n\n\nimport pandas as pd\n\ndef to_annual_series(\n    tax_df: pd.DataFrame,\n    date_col: str = \"DATE\",\n    value_col: str = \"IndividualIncomeTax\",\n    agg: str = \"mean\",  # \"mean\" (default) or \"sum\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert a quarterly (or higher frequency) dataframe with a DATE column\n    into an annual dataframe with columns: Year, IndividualIncomeTax_Annual.\n\n    agg:\n      - \"mean\": good for level/rate-like series (e.g., % of GDP, index levels, rates)\n      - \"sum\" : good for flow series (e.g., revenues within year)\n    \"\"\"\n    df = tax_df.copy()\n\n    # If DATE is the index, bring it back as a column\n    if date_col not in df.columns and df.index.name == date_col:\n        df = df.reset_index()\n\n    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n    df = df.dropna(subset=[date_col, value_col])\n\n    df[\"Year\"] = df[date_col].dt.year.astype(int)\n\n    if agg not in (\"mean\", \"sum\", \"median\"):\n        raise ValueError(\"agg must be one of: 'mean', 'sum', 'median'\")\n\n    annual = (\n        df.groupby(\"Year\", as_index=False)[value_col]\n          .agg({f\"{value_col}_Annual\": agg})\n          .sort_values(\"Year\")\n          .reset_index(drop=True)\n    )\n    return annual\n\n\nannual_tax = to_annual_series(tax_df, value_col=\"IndividualIncomeTax\", agg=\"mean\")\nannual_tax.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\n\n\n\n\n74\n2021\n2675.071500\n\n\n75\n2022\n3252.924000\n\n\n76\n2023\n2918.060500\n\n\n77\n2024\n3118.884750\n\n\n78\n2025\n3433.378333\n\n\n\n\n\n\n\n\ncpi_df = download_fred_series('CPIAUCSL', 'CPI')\ncpi_df.tail()\nannual_cpi = to_annual_series(tax_df, value_col=\"CPI\", agg=\"mean\")\nannual_cpi.tail()\n\n✓ Downloaded CPI: 947 observations\n\n\n\n\n\n\n\n\n\nYear\nCPI_Annual\n\n\n\n\n74\n2021\n270.967917\n\n\n75\n2022\n292.625417\n\n\n76\n2023\n304.704167\n\n\n77\n2024\n313.697833\n\n\n78\n2025\n321.577200\n\n\n\n\n\n\n\n\n# Step 2: Merge with your tax data\ndf = pd.merge(annual_tax, annual_cpi_df, on='Year', how='inner')\ndf.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\nCPI_Annual\n\n\n\n\n74\n2021\n2675.071500\n270.967917\n\n\n75\n2022\n3252.924000\n292.625417\n\n\n76\n2023\n2918.060500\n304.704167\n\n\n77\n2024\n3118.884750\n313.697833\n\n\n78\n2025\n3433.378333\n321.577200\n\n\n\n\n\n\n\n\n# Step 3: Calculate Real Tax (in 2024 dollars)\nlatest_cpi = annual_cpi[annual_cpi['Year'] == 2024]['CPI_Annual'].mean()\ndf['Real_Tax'] = (df['IndividualIncomeTax_Annual'] / df['CPI_Annual']) * latest_cpi\ndf.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\nCPI_Annual\nReal_Tax\n\n\n\n\n74\n2021\n2675.071500\n270.967917\n3096.913258\n\n\n75\n2022\n3252.924000\n292.625417\n3487.172175\n\n\n76\n2023\n2918.060500\n304.704167\n3004.190151\n\n\n77\n2024\n3118.884750\n313.697833\n3118.884750\n\n\n78\n2025\n3433.378333\n321.577200\n3349.252821\n\n\n\n\n\n\n\n\ndf = df.set_index('Year')\ndf['Real_Tax'].plot()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.api import OLS, add_constant\nfrom scipy import stats\n\ndef chow_test_tax_series(\n    df_annual: pd.DataFrame,\n    y_col: str,\n    break_year: int = 1971,\n    min_obs_each_side: int = 8,\n    save_csv_path: str = None\n):\n    \"\"\"\n    Chow test for a structural break at a known year on an annual series.\n\n    Model: y ~ const + t\n    where t is a normalized year index for numerical stability.\n\n    H0: No structural break (β_pre = β_post)\n    H1: Structural break exists\n    \"\"\"\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"CHOW TEST FOR STRUCTURAL BREAK\")\n    print(\"=\" * 80)\n\n    # Clean and keep what we need\n    df_clean = df_annual[[\"Year\", y_col]].dropna().copy()\n    df_clean = df_clean.sort_values(\"Year\").reset_index(drop=True)\n\n    # Split (match your original: pre &lt; break_year, post &gt;= break_year)\n    df_pre = df_clean[df_clean[\"Year\"] &lt; break_year].copy()\n    df_post = df_clean[df_clean[\"Year\"] &gt;= break_year].copy()\n\n    n_pre, n_post = len(df_pre), len(df_post)\n    n_total = n_pre + n_post\n\n    print(f\"\\nBreak year: {break_year}\")\n    print(f\"Observations pre-break: {n_pre}\")\n    print(f\"Observations post-break: {n_post}\")\n    print(f\"Total observations: {n_total}\")\n\n    if n_pre &lt; min_obs_each_side or n_post &lt; min_obs_each_side:\n        raise ValueError(\n            f\"Not enough annual observations on each side of {break_year}. \"\n            f\"Need at least {min_obs_each_side} each side. Got pre={n_pre}, post={n_post}.\"\n        )\n\n    # Normalize time index for stability (use the same reference for all)\n    base_year = df_clean[\"Year\"].min()\n    df_clean[\"t\"] = df_clean[\"Year\"] - base_year\n    df_pre[\"t\"] = df_pre[\"Year\"] - base_year\n    df_post[\"t\"] = df_post[\"Year\"] - base_year\n\n    # Pooled regression\n    X_pooled = add_constant(df_clean[\"t\"].astype(float))\n    y_pooled = df_clean[y_col].astype(float)\n    model_pooled = OLS(y_pooled, X_pooled).fit()\n    RSS_pooled = float(model_pooled.ssr)\n\n    # Separate regressions\n    X_pre = add_constant(df_pre[\"t\"].astype(float))\n    y_pre = df_pre[y_col].astype(float)\n    model_pre = OLS(y_pre, X_pre).fit()\n    RSS_pre = float(model_pre.ssr)\n\n    X_post = add_constant(df_post[\"t\"].astype(float))\n    y_post = df_post[y_col].astype(float)\n    model_post = OLS(y_post, X_post).fit()\n    RSS_post = float(model_post.ssr)\n\n    RSS_separate = RSS_pre + RSS_post\n\n    # Chow statistic\n    k = 2  # intercept + slope\n    df_num = k\n    df_den = n_total - 2 * k\n\n    F_stat = ((RSS_pooled - RSS_separate) / k) / (RSS_separate / df_den)\n    p_value = 1 - stats.f.cdf(F_stat, df_num, df_den)\n\n    critical_05 = stats.f.ppf(0.95, df_num, df_den)\n    critical_01 = stats.f.ppf(0.99, df_num, df_den)\n\n    print(\"\\n\" + \"-\" * 80)\n    print(\"RESULTS:\")\n    print(\"-\" * 80)\n    print(f\"\\nRSS Pooled (restricted): {RSS_pooled:.4f}\")\n    print(f\"RSS Separate (unrestricted): {RSS_separate:.4f}\")\n    print(f\"  RSS Pre-break: {RSS_pre:.4f}\")\n    print(f\"  RSS Post-break: {RSS_post:.4f}\")\n\n    print(f\"\\nChow F-statistic: {F_stat:.4f}\")\n    print(f\"Degrees of freedom: ({df_num}, {df_den})\")\n    print(f\"P-value: {p_value:.6f}\")\n\n    print(f\"\\nCritical values:\")\n    print(f\"  5% level: {critical_05:.4f}\")\n    print(f\"  1% level: {critical_01:.4f}\")\n\n    print(\"\\nConclusion:\")\n    if p_value &lt; 0.01:\n        result = \"Reject H0 (1%)\"\n        print(f\"  *** REJECT NULL HYPOTHESIS at 1% level ***\")\n        print(f\"  Strong evidence of structural break at {break_year}\")\n    elif p_value &lt; 0.05:\n        result = \"Reject H0 (5%)\"\n        print(f\"  ** REJECT NULL HYPOTHESIS at 5% level **\")\n        print(f\"  Significant evidence of structural break at {break_year}\")\n    elif p_value &lt; 0.10:\n        result = \"Reject H0 (10%)\"\n        print(f\"  * REJECT NULL HYPOTHESIS at 10% level *\")\n        print(f\"  Moderate evidence of structural break at {break_year}\")\n    else:\n        result = \"Fail to Reject H0\"\n        print(f\"  FAIL TO REJECT NULL HYPOTHESIS\")\n        print(f\"  Insufficient evidence of structural break at {break_year}\")\n\n    # Coefficients\n    print(\"\\n\" + \"-\" * 80)\n    print(\"REGRESSION COEFFICIENTS:\")\n    print(\"-\" * 80)\n\n    print(f\"\\nPre-{break_year} period:\")\n    print(f\"  Intercept: {model_pre.params.iloc[0]:.4f} (se: {model_pre.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_pre.params.iloc[1]:.4f} (se: {model_pre.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_pre.rsquared:.4f}\")\n\n    print(f\"\\nPost-{break_year} period:\")\n    print(f\"  Intercept: {model_post.params.iloc[0]:.4f} (se: {model_post.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_post.params.iloc[1]:.4f} (se: {model_post.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_post.rsquared:.4f}\")\n\n    print(f\"\\nPooled (no break):\")\n    print(f\"  Intercept: {model_pooled.params.iloc[0]:.4f} (se: {model_pooled.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_pooled.params.iloc[1]:.4f} (se: {model_pooled.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_pooled.rsquared:.4f}\")\n\n    results_dict = {\n        \"Test\": \"Chow Test\",\n        \"Series\": y_col,\n        \"Break Year\": break_year,\n        \"F-statistic\": float(F_stat),\n        \"P-value\": float(p_value),\n        \"Critical Value (5%)\": float(critical_05),\n        \"Critical Value (1%)\": float(critical_01),\n        \"Result\": result,\n        \"Slope Pre\": float(model_pre.params.iloc[1]),\n        \"Slope Post\": float(model_post.params.iloc[1]),\n        \"Slope Change\": float(model_post.params.iloc[1] - model_pre.params.iloc[1]),\n        \"n_pre\": int(n_pre),\n        \"n_post\": int(n_post),\n    }\n\n    if save_csv_path is not None:\n        pd.DataFrame([results_dict]).to_csv(save_csv_path, index=False)\n\n    return results_dict\n\n\n# Chow test at 1971\nres = chow_test_tax_series(\n    df,\n    y_col=\"Real_Tax\",\n    break_year=1971\n)\n\nres\n\n\n================================================================================\nCHOW TEST FOR STRUCTURAL BREAK\n================================================================================\n\nBreak year: 1971\nObservations pre-break: 24\nObservations post-break: 55\nTotal observations: 79\n\n--------------------------------------------------------------------------------\nRESULTS:\n--------------------------------------------------------------------------------\n\nRSS Pooled (restricted): 3811995.5571\nRSS Separate (unrestricted): 3034840.0098\n  RSS Pre-break: 84072.8198\n  RSS Post-break: 2950767.1900\n\nChow F-statistic: 9.6029\nDegrees of freedom: (2, 75)\nP-value: 0.000194\n\nCritical values:\n  5% level: 3.1186\n  1% level: 4.8999\n\nConclusion:\n  *** REJECT NULL HYPOTHESIS at 1% level ***\n  Strong evidence of structural break at 1971\n\n--------------------------------------------------------------------------------\nREGRESSION COEFFICIENTS:\n--------------------------------------------------------------------------------\n\nPre-1971 period:\n  Intercept: 477.9496 (se: 24.4684)\n  Slope:     27.4835 (se: 1.8229)\n  R-squared: 0.9118\n\nPost-1971 period:\n  Intercept: 2.3028 (se: 107.0530)\n  Slope:     37.3621 (se: 2.0042)\n  R-squared: 0.8677\n\nPooled (no break):\n  Intercept: 349.9247 (se: 49.5949)\n  Slope:     31.2690 (se: 1.0978)\n  R-squared: 0.9133\n\n\n{'Test': 'Chow Test',\n 'Series': 'Real_Tax',\n 'Break Year': 1971,\n 'F-statistic': 9.602922372192094,\n 'P-value': 0.00019358783729239715,\n 'Critical Value (5%)': 3.118642128006125,\n 'Critical Value (1%)': 4.899877423111457,\n 'Result': 'Reject H0 (1%)',\n 'Slope Pre': 27.483480442522755,\n 'Slope Post': 37.362148635986124,\n 'Slope Change': 9.878668193463369,\n 'n_pre': 24,\n 'n_post': 55}"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html",
    "href": "papers/Paper_A_Revised_1971_Shock.html",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "",
    "text": "Incomplete Paper: Academic Response"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#a-structural-critique-of-the-trammell-aschenbrenner-framework",
    "href": "papers/Paper_A_Revised_1971_Shock.html#a-structural-critique-of-the-trammell-aschenbrenner-framework",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "",
    "text": "Incomplete Paper: Academic Response"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#paper-rs-framework-assumes-technology-is-exogenous",
    "href": "papers/Paper_A_Revised_1971_Shock.html#paper-rs-framework-assumes-technology-is-exogenous",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.1 Paper R’s Framework Assumes Technology is Exogenous",
    "text": "1.1 Paper R’s Framework Assumes Technology is Exogenous\nTrammell & Aschenbrenner model technology level \\(A_t\\) as an exogenous path chosen by a social planner:\n\\[\\max_{a, b} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(1-B_t)) dt\\]\nsubject to: \\[\\dot{S}_t = -\\delta(A_t, B_t) S_t\\]\nThe critical assumption: Technology path \\(a = \\{A_t\\}_{t=0}^{\\infty}\\) is treated as a policy choice independent of monetary or fiscal constraints.\nThis is wrong. Technology advancement is endogenous to: 1. Capital availability (determined by monetary policy) 2. Interest rates (determining which innovations are financially viable) 3. Debt sustainability (constraining long-run growth paths) 4. Fiscal pressure (incentivizing specific types of “growth”)\nWhen monetary regime changes, the entire feasible set of technology paths changes. Paper R’s optimization occurs within a fiat currency regime without acknowledging that the regime itself determines the objective function’s parameters."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.2 The 1971 Structural Break",
    "text": "1.2 The 1971 Structural Break\nAugust 15, 1971: The Nixon Shock\nPresident Nixon announced the USD would no longer be convertible to gold at $35/ounce. This was not merely a U.S. policy change—it was a global monetary regime change.\nPre-1971 (Bretton Woods System): - USD pegged to gold ($35/oz fixed) - All other major currencies pegged to USD at fixed rates - Constraint: No country could print money arbitrarily without losing the peg - Global money supply anchored by gold convertibility\nPost-1971 (Fiat Currency Era): - USD breaks gold peg - All other currencies simultaneously lose their anchor - No constraint: Every central bank can now print money without limit - Global monetary system becomes pure fiat\nCritical observation: There is no control group. Every developed nation went fiat on the same day. This is a natural experiment with universal treatment."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#why-this-invalidates-paper-rs-framework",
    "href": "papers/Paper_A_Revised_1971_Shock.html#why-this-invalidates-paper-rs-framework",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.3 Why This Invalidates Paper R’s Framework",
    "text": "1.3 Why This Invalidates Paper R’s Framework\nPaper R’s technology path \\(a(t)\\) conflates: 1. Organic technological advancement (\\(A^{organic}_t\\)): Genuine capability improvements 2. Debt-financed pseudo-innovation (\\(A^{debt}_t\\)): Activity enabled by cheap credit that appears as “growth”\nThe observed path is: \\[A^{observed}_t = A^{organic}_t + A^{debt}_t\\]\nUnder gold-backed currency: - Credit is constrained by gold reserves - \\(A^{debt}_t \\approx 0\\) - Observed growth ≈ Organic growth\nUnder fiat currency: - Credit is constrained only by political will - \\(A^{debt}_t\\) can grow indefinitely (until debt crisis) - Observed growth &gt;&gt; Organic growth\nPaper R treats \\(A^{observed}_t\\) as if it were \\(A^{organic}_t\\) and concludes that accelerating it minimizes risk. But if \\(A^{debt}_t\\) is unsustainable and creates fertility collapse, this optimization is fundamentally flawed."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-complete-causal-structure",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-complete-causal-structure",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.1 The Complete Causal Structure",
    "text": "2.1 The Complete Causal Structure\n1971: Gold Standard Abandoned\n         ↓\nAll Currencies Become Fiat (Unlimited Printing)\n         ↓\nGovernment Debt Can Grow Without Limit\n         ↓\nRising Debt Service Costs Create Fiscal Pressure\n         ↓\nNeed to Expand Tax Base to Service Debt\n         ↓\nPOLICY STRATEGY: Encourage Female Workforce Participation\n         ↓\n         ├→ [Path A: Direct Effect]\n         │   Dual-income households = 2x taxable incomes\n         │        ↓\n         │   Government tax revenue increases\n         │\n         ├→ [Path B: Wage Suppression Effect]\n         │   Labor supply doubles → wages don't rise with productivity\n         │        ↓\n         │   Real wage stagnation makes dual-income NECESSARY (not choice)\n         │\n         └→ [Path C: Cost Inflation Effect]\n             More dual-income households → housing/childcare costs rise\n                  ↓\n             Children become economically prohibitive\n                  \n         ↓\n[All paths converge]\n         ↓\nHome Cooking → Processed Food (BigFood profits)\nChildcare at Home → Institutional Childcare (new industry)\nExtended Family Networks → Nuclear Family Isolation\nCommunity Cohesion → Market Transactions\n         ↓\nFertility Falls Below Replacement (TFR &lt; 2.1)\n         ↓\nPopulation Decline Begins\n         ↓\nEVOLUTIONARY EXTINCTION\n(despite S_∞ &gt; 0 in Paper R's framework)"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#mathematical-formalization",
    "href": "papers/Paper_A_Revised_1971_Shock.html#mathematical-formalization",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.2 Mathematical Formalization",
    "text": "2.2 Mathematical Formalization\n\nThe Debt-Fiscal Pressure Link\nUnder fiat currency, government debt evolves as:\n\\[\\dot{D}_t = r_t D_t + G_t - T_t + M_t\\]\nwhere: - \\(D_t\\) = government debt - \\(r_t\\) = interest rate\n- \\(G_t\\) = government spending - \\(T_t\\) = tax revenue - \\(M_t\\) = monetary expansion (money printing)\nPre-1971 Constraint: \\[M_t \\leq M_t^{max}(Gold_{reserves})\\]\nUnlimited monetary expansion would break the gold peg.\nPost-1971: \\[M_t \\in [0, \\infty)\\]\nNo constraint except political/inflation considerations.\nFiscal Pressure:\nDefine fiscal pressure as debt service burden: \\[FP_t = \\frac{r_t D_t}{GDP_t}\\]\nAs debt grows, fiscal pressure increases, creating incentive to expand tax base.\n\n\nThe Tax Base Expansion Strategy\nTax revenue under single-income households: \\[T_t^{single} = \\tau \\cdot n \\cdot w_t\\]\nwhere \\(n\\) = number of workers, \\(w_t\\) = wage.\nTax revenue under dual-income households: \\[T_t^{dual} = \\tau \\cdot 2n \\cdot w_t^{dual}\\]\nCritical: Even if \\(w_t^{dual} &lt; w_t\\) (wage suppression from labor supply increase), the tax revenue increases:\n\\[T_t^{dual} &gt; T_t^{single} \\iff 2w_t^{dual} &gt; w_t\\]\nThis is satisfied even with substantial wage suppression.\nKey Findings 1:\nHYPOTHESIS: Tax revenue from labor increased disproportionately post-1971\nTEST: Calculate (Labor Income Tax Revenue / GDP) from 1960-2024\nEXPECTED: Structural break upward at 1971\nDATA SOURCE: OECD Tax Revenue Database, IRS Historical Tables\nSTATUS: [VALIDATED]\nTo test the hypothesis of a disproportionate increase in tax extraction following the 1971 monetary regime change, an Interrupted Time Series analysis was performed on U.S. Individual Income Tax receipts. To ensure the analysis isolated structural fiscal shifts from the inherent inflation of the post-1971 fiat era, tax revenue was adjusted to Real Tax values (2024 dollars) using Consumer Price Index (CPI) data.\nA Chow Test was employed to statistically evaluate the presence of a structural break at the 1971 pivot point, comparing the pre-break period (1947–1970) with the post-break period (1971–2025). The results are as follows:\n• Statistical Significance: The analysis yielded a Chow F-statistic of 9.6029 with a p-value of 0.000194. This allows for the rejection of the null hypothesis at the 1% significance level, providing what the sources describe as “strong evidence of a structural break at 1971.” • Trend Acceleration: The regression coefficients reveal a significant divergence in the growth trajectory of real tax revenue extraction. The pre-1971 slope was 27.48, which accelerated to a post-1971 slope of 37.36. This represents a net increase of 9.8787 in the annual growth rate of real tax receipts. • Causal Alignment: These findings provide the empirical foundation for the argument that the removal of gold-convertibility constraints facilitated “hockey-stick growth” in government debt. According to the sources, this necessitated a “Tax Base Expansion Strategy” characterized by increased labor force participation—specifically the transition to dual-income households—to service the expanding debt.\nThis statistically significant break confirms that the 1971 “Nixon Shock” was not merely a monetary adjustment but a fundamental shift in the state’s fiscal extraction apparatus, creating the economic pressure that the sources link to the subsequent civilizational fertility collapse.\n\n\nThe Real Wage Suppression Mechanism\nLabor supply elasticity: \\[\\frac{dN_t}{dw_t} = \\epsilon \\cdot N_t\\]\nWith \\(\\epsilon &gt; 0\\), increasing labor supply (dual incomes) should raise wages. But we observe the opposite: productivity-wage divergence begins exactly at 1971.\nThe Fiat Currency Wage Suppression:\nUnder fiat currency, monetary expansion creates inflation: \\[\\pi_t = f(M_t, V_t)\\]\nNominal wages adjust slowly (sticky wages): \\[\\frac{d w_t^{nominal}}{dt} &lt; \\pi_t\\]\nTherefore real wages decline: \\[w_t^{real} = \\frac{w_t^{nominal}}{P_t} \\searrow\\]\nThis makes dual income necessary rather than optional.\n[DATA VALIDATION PLACEHOLDER 2]:\nHYPOTHESIS: Real wage-productivity divergence begins at 1971\nTEST: Calculate correlation between productivity and real wages\n      Pre-1971: Expect ρ &gt; 0.8\n      Post-1971: Expect ρ → 0\nDATA SOURCE: BLS Productivity and Costs, Real Compensation data\nSPECIFIC TEST: Chow test for structural break at 1971\nSTATUS: [PENDING VALIDATION]\n\n\nThe Fertility Response Function\nFertility depends on: \\[F_t = F(w_t^{real}, C_t^{child}, T_t^{avail}, H_t)\\]\nwhere: - \\(w_t^{real}\\) = real household income - \\(C_t^{child}\\) = cost of raising children (childcare, housing, education) - \\(T_t^{avail}\\) = time available for childrearing\n- \\(H_t\\) = cultural/institutional support for families\nPost-1971 Effects:\n\nReal wage stagnation (\\(\\downarrow w^{real}\\)) → Lower fertility\nDual-income necessity (\\(\\downarrow T^{avail}\\)) → Lower fertility\nHousing cost inflation (\\(\\uparrow C^{child}\\)) → Lower fertility\nInstitutional childcare replacing family (\\(\\downarrow H\\)) → Lower fertility\n\n[DATA VALIDATION PLACEHOLDER 3]:\nHYPOTHESIS: Fertility shows structural break at 1971\nTEST: Interrupted Time Series Analysis\n      Model: TFR_t = α + β₁·Year + β₂·Post1971 + β₃·(Post1971 × Year) + ε\n      Expected: β₃ &lt; 0 (steeper decline post-1971)\nDATA SOURCE: UN World Population Prospects, OECD Family Database\nCOUNTRIES: All OECD nations (n=38)\nSTATISTICAL TEST: Chow test, Quandt-Andrews unknown breakpoint test\nSTATUS: [PENDING VALIDATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-technology-endogeneity",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-technology-endogeneity",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.3 The Technology Endogeneity",
    "text": "2.3 The Technology Endogeneity\nWhat Paper R calls “technological advancement” is substantially:\n\\[A_t = A_t^{organic} + \\theta(r_t) \\cdot I_t^{debt}\\]\nwhere \\(\\theta(r_t)\\) is the debt-financed innovation multiplier (increasing as interest rates fall).\nUnder fiat currency: \\[r_t \\to 0 \\implies \\theta(r_t) \\to \\infty\\]\nThis creates an explosion of debt-financed “innovation” that appears as genuine technological progress but is actually: - Venture capital gambling enabled by cheap money - Malinvestment in unsustainable business models\n- Asset bubbles misidentified as innovation - Ponzi schemes (FTX, WeWork, etc.)\n[DATA VALIDATION PLACEHOLDER 4]:\nHYPOTHESIS: VC investment is inversely correlated with interest rates post-1971\nTEST: Regression of VC investment on Federal Funds Rate\n      Expected: β &lt; 0, R² &gt; 0.5\nDATA SOURCE: PitchBook, NVCA Yearbook, Federal Reserve H.15\nTIME PERIOD: 1971-2024\nCONTROL VARIABLES: GDP growth, corporate profits, IPO market\nSTATUS: [PENDING VALIDATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-no-control-group-problem",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-no-control-group-problem",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "3.1 The No-Control-Group Problem",
    "text": "3.1 The No-Control-Group Problem\nStandard causal inference requires a control group: - Difference-in-differences: Treated vs. untreated groups - Synthetic control: Construct counterfactual from untreated units - Regression discontinuity: Compare just above/below treatment threshold\nNone of these work for the 1971 shock because: 1. Every developed nation went fiat simultaneously 2. No country maintained gold-backed currency 3. Treatment was instantaneous and global"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#alternative-empirical-approaches",
    "href": "papers/Paper_A_Revised_1971_Shock.html#alternative-empirical-approaches",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "3.2 Alternative Empirical Approaches",
    "text": "3.2 Alternative Empirical Approaches\n\nApproach 1: Interrupted Time Series Analysis\nMethod: Test for structural breaks at 1971 in outcome variables\nSpecification: \\[Y_{i,t} = \\alpha_i + \\beta_1 \\cdot Year_t + \\beta_2 \\cdot Post1971_t + \\beta_3 \\cdot (Post1971_t \\times Year_t) + \\varepsilon_{i,t}\\]\nwhere: - \\(Y_{i,t}\\) = outcome variable (fertility, debt, wages, etc.) for country \\(i\\) at time \\(t\\) - \\(Post1971_t\\) = 1 if \\(t \\geq 1971\\), 0 otherwise - \\(\\beta_3\\) = change in trend after 1971\nTests: - Chow test: Is there a structural break at 1971? - Quandt-Andrews: What year is the most likely breakpoint? (Should be ~1971)\nOutcomes to test:\n\n\n\nVariable\nExpected Sign of β₃\nRationale\n\n\n\n\nTotal Fertility Rate\nNegative (−)\nFertility decline accelerates\n\n\nGovt Debt/GDP\nPositive (+)\nDebt grows faster\n\n\nFemale Labor Force %\nPositive (+)\nWorkforce participation jumps\n\n\nReal Wage Growth\nNegative (−)\nWage-productivity decoupling\n\n\nCPI Inflation\nPositive (+)\nFiat currency inflation\n\n\nHousing Cost/Income\nPositive (+)\nAsset price inflation\n\n\n\n[DATA VALIDATION PLACEHOLDER 5]:\nDATASET REQUIRED: Panel data for OECD countries, 1950-2024\nVARIABLES:\n  - tfr: Total Fertility Rate (births per woman)\n  - debt_gdp: Government debt as % of GDP\n  - flfp: Female labor force participation rate\n  - real_wage_growth: Annual % change in real wages\n  - productivity_growth: Annual % change in labor productivity\n  - cpi: Consumer Price Index\n  - house_price_income: Median home price / Median household income\n\nSOURCES: \n  - OECD.Stat\n  - World Bank World Development Indicators  \n  - UN Population Division\n  - BIS Property Prices Database\n\nSTATISTICAL TESTS:\n  1. Chow test (known breakpoint at 1971)\n  2. Quandt-Andrews (unknown breakpoint)\n  3. Bai-Perron (multiple breakpoints)\n\nSTATUS: [PENDING DATA COMPILATION]\n\n\nApproach 2: Differential Fiat Currency Exploitation\nWhile all countries went fiat in 1971, they differed in how aggressively they exploited the new regime:\n“Dose” variables: - Cumulative deficit spending 1971-2024 - Average debt/GDP 1971-2024\n- Monetary base expansion rate - Real interest rate suppression (deviation from natural rate)\nHypothesis: Countries that exploited fiat currency more aggressively should show: - Sharper fertility decline - Higher debt accumulation - Larger wage-productivity gaps\nCross-sectional specification: \\[\\Delta TFR_i = \\alpha + \\beta \\cdot FiatExploitation_i + \\gamma \\cdot X_i + \\varepsilon_i\\]\nwhere \\(X_i\\) includes controls (initial TFR, education, urbanization, etc.)\n[DATA VALIDATION PLACEHOLDER 6]:\nHYPOTHESIS: Fertility decline correlates with fiat currency exploitation intensity\nSAMPLE: OECD countries (n ≈ 35)\nDEPENDENT VARIABLE: \n  ΔTFRᵢ = TFR₂₀₂₄ - TFR₁₉₇₁\nINDEPENDENT VARIABLE (Fiat Exploitation Index):\n  FEIᵢ = 0.4·(Avg Debt/GDP)ᵢ + 0.3·(Cum Deficits)ᵢ + 0.3·(Real Rate Suppression)ᵢ\nCONTROL VARIABLES:\n  - Initial TFR (1971)\n  - Female education levels\n  - Urbanization rate\n  - GDP per capita\n  - Religion (% Catholic, Muslim, etc.)\nEXPECTED: β &lt; 0 (more exploitation → larger fertility decline)\nSTATUS: [PENDING ANALYSIS]\n\n\nApproach 3: Mechanism Isolation\nTest each causal pathway separately:\n\nMechanism A: Fiscal Pressure → Tax Policy → Dual Income\nTestable prediction: Countries with higher fiscal pressure (debt service/GDP) should have: - Stronger tax incentives for dual-income households - Faster female labor force participation growth\n[DATA VALIDATION PLACEHOLDER 7]:\nTEST: Panel regression\nSPECIFICATION: \n  FLFPᵢₜ = α + β₁·FiscalPressureᵢₜ + β₂·Xᵢₜ + μᵢ + λₜ + εᵢₜ\n\nWHERE:\n  FiscalPressure = (Interest Payments on Govt Debt) / GDP\n  X = controls (education, GDP, urbanization)\n  μᵢ = country fixed effects\n  λₜ = year fixed effects\n\nEXPECTED: β₁ &gt; 0\nDATA SOURCE: OECD Revenue Statistics, IMF Fiscal Monitor\nSTATUS: [PENDING ANALYSIS]\n\n\nMechanism B: Fiat Currency → Wage Suppression → Dual Income Necessity\nTestable prediction: Real wage growth should decouple from productivity growth post-1971\n[DATA VALIDATION PLACEHOLDER 8]:\nTEST: Cointegration analysis\nPERIOD 1: 1950-1970 (Gold-backed era)\nPERIOD 2: 1971-2024 (Fiat era)\n\nVARIABLES:\n  - Productivity Index (output per hour)\n  - Real Compensation Index\n\nTESTS:\n  Period 1: Expect cointegration (wages track productivity)\n  Period 2: Expect cointegration breakdown\n\nSTATISTICAL TESTS:\n  - Engle-Granger cointegration test\n  - Johansen test\n  - Rolling window correlation\n\nSTATUS: [PENDING ANALYSIS]\n\n\nMechanism C: Cheap Debt → Tech Bubble → Fertility-Hostile Culture\nTestable prediction: Lower interest rates → more “innovation” activity → lower fertility\n[DATA VALIDATION PLACEHOLDER 9]:\nTEST: Instrumental variables regression\nSPECIFICATION:\n  TFRᵢₜ = α + β₁·TechIntensityᵢₜ + β₂·Xᵢₜ + εᵢₜ\n  \nINSTRUMENT for TechIntensity:\n  Real interest rate (rₜ - πₜ)\n  \nRATIONALE:\n  Interest rates affect tech investment but don't directly affect fertility\n  (satisfies exclusion restriction)\n\nEXPECTED: β₁ &lt; 0 (more tech → lower fertility)\nSTATUS: [PENDING ANALYSIS]\n\n\n\nApproach 4: Policy Document Analysis\nHistorical Evidence of Intent:\nSearch for evidence that policymakers explicitly discussed dual-income households as tax revenue strategy.\n[DATA VALIDATION PLACEHOLDER 10]:\nDOCUMENT SEARCH:\n1. FOIA requests to:\n   - U.S. Treasury Department (1970-1980)\n   - Office of Management and Budget (1970-1980)\n   - Council of Economic Advisers (1970-1980)\n\nSEARCH TERMS:\n   - \"female labor force participation\" AND \"tax revenue\"\n   - \"dual-income\" AND \"fiscal policy\"\n   - \"women's employment\" AND \"tax base\"\n   - \"working women\" AND \"government revenue\"\n\n2. Congressional testimony search (1970-1980):\n   - House Ways and Means Committee\n   - Senate Finance Committee\n\n3. Academic literature from era:\n   - Journal of Public Economics\n   - National Tax Journal\n   \nEXPECTED FINDINGS:\n   Explicit discussion of dual-income expansion as fiscal strategy\n   \nSTATUS: [PENDING DOCUMENT REVIEW]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-endogeneity-problem",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-endogeneity-problem",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.1 The Endogeneity Problem",
    "text": "4.1 The Endogeneity Problem\nPaper R’s optimization: \\[\\max_{a, b} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(1-B_t)) dt\\]\ntreats technology path \\(a = \\{A_t\\}\\) as a choice variable.\nBut under fiat currency:\nTechnology path is endogenous to: \\[A_t = A^{organic}_t + h(D_t, r_t, M_t)\\]\nwhere: - \\(D_t\\) = government debt - \\(r_t\\) = interest rate (manipulated by central bank) - \\(M_t\\) = money supply (controlled by central bank)\nThe “choice” of technology path is not free - it’s constrained and determined by monetary regime.\nCorrect formulation: \\[\\max_{a, b, D, r, M} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(D_t, r_t, M_t)(1-B_t)) dt\\]\nsubject to: \\[\\dot{S}_t = -\\delta(A_t, B_t, F_t, D_t) S_t\\] \\[F_t = F(A_t, \\dot{A}_t, w_t^{real}, C_t)\\] \\[\\dot{D}_t = r_t D_t + G_t - T_t + M_t\\]\nThis is a fundamentally different optimization problem."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-missing-fertility-constraint",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-missing-fertility-constraint",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.2 The Missing Fertility Constraint",
    "text": "4.2 The Missing Fertility Constraint\nPaper R’s survival function: \\[S_\\infty = \\lim_{t \\to \\infty} S_t\\]\nignores that population must also survive: \\[N_\\infty = \\lim_{t \\to \\infty} N_t\\]\nPopulation dynamics: \\[\\dot{N}_t = N_t \\cdot \\frac{F_t - 2.1}{30}\\]\nIf \\(F_t &lt; 2.1\\) persistently, then \\(N_\\infty = 0\\) even if \\(S_\\infty &gt; 0\\).\nThe correct survival condition: \\[Survival = (S_\\infty &gt; 0) \\land (N_\\infty &gt; 0)\\]\nPaper R proves \\(S_\\infty &gt; 0\\) is maximized by fast growth, but ignores that fast growth → \\(F_t &lt; 2.1\\) → \\(N_\\infty = 0\\).\nThis is optimizing for extinction."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-debt-sustainability-ignored",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-debt-sustainability-ignored",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.3 The Debt Sustainability Ignored",
    "text": "4.3 The Debt Sustainability Ignored\nPaper R’s hazard function: \\[\\delta(A, B) = \\bar{\\delta} A^\\alpha (1-B)^\\beta\\]\nhas no debt variable.\nBut debt creates systemic risk: \\[\\delta_{total}(A, B, D) = \\delta(A,B) + \\lambda \\cdot g(D/GDP)\\]\nwhere \\(g(\\cdot)\\) is increasing and convex (debt crises become more likely as debt rises).\nSovereign debt crises are existential risks: - Institutional collapse - Social unrest → conflict - Inability to fund safety measures - Economic collapse → famine, disease\n[DATA VALIDATION PLACEHOLDER 11]:\nHYPOTHESIS: Sovereign debt crises increase non-linearly with debt/GDP\nTEST: Logistic regression\nDEPENDENT: Debt crisis dummy (1 if crisis occurred)\nINDEPENDENT: Debt/GDP ratio, lagged 5 years\nDATA SOURCE: Reinhart & Rogoff database, IMF Fiscal Monitor\nSAMPLE: All sovereign debt crises 1950-2024\nEXPECTED: Probability of crisis increases sharply above 90% debt/GDP\nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-kuznets-curve-failure",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-kuznets-curve-failure",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.4 The Kuznets Curve Failure",
    "text": "4.4 The Kuznets Curve Failure\nPaper R argues that rich societies spend more on safety (safety is a “luxury good” with \\(\\eta &gt; 1\\)).\nThis fails under fiat currency because:\n\n“Wealth” is illusory if debt-financed\n\nHigh consumption today via debt ≠ genuine wealth\nWhen debt becomes unsustainable, consumption collapses\n\nFiscal pressure crowds out safety spending\n\nDebt service consumes growing fraction of budget\nLess fiscal space for safety measures\nReverses the Kuznets mechanism\n\n\n[DATA VALIDATION PLACEHOLDER 12]:\nHYPOTHESIS: Safety spending (% GDP) shows inverted-U with debt levels\nTEST: Panel regression with quadratic debt term\nSPECIFICATION:\n  SafetySpendingᵢₜ = α + β₁·(Debt/GDP)ᵢₜ + β₂·(Debt/GDP)²ᵢₜ + Xᵢₜ + εᵢₜ\n\nWHERE:\n  SafetySpending = Health + Environmental + Disaster preparedness spending\n  \nEXPECTED: \n  β₁ &gt; 0 (initially increases with debt)\n  β₂ &lt; 0 (decreases at high debt levels)\n  \nDATA SOURCE: OECD Government Spending Database\nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#incorporating-monetary-regime",
    "href": "papers/Paper_A_Revised_1971_Shock.html#incorporating-monetary-regime",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.1 Incorporating Monetary Regime",
    "text": "5.1 Incorporating Monetary Regime\nThe social planner faces:\n\\[\\max_{D_t, M_t, r_t, B_t} \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} S_t N_t u(C_t) dt\\right]\\]\nsubject to:\n1. Survival dynamics: \\[\\dot{S}_t = -\\delta(A_t, B_t, F_t, D_t) S_t\\]\n2. Population dynamics: \\[\\dot{N}_t = N_t \\cdot \\frac{F_t - 2.1}{30}\\]\n3. Fertility function: \\[F_t = F(w_t^{real}, C_t^{housing}, T_t^{available}, A_t, \\dot{A}_t)\\]\n4. Technology endogeneity: \\[A_t = A_t^{organic} + \\theta(r_t, D_t) \\cdot I_t^{debt}\\]\n5. Debt dynamics: \\[\\dot{D}_t = r_t D_t + G_t - T(w_t, N_t^{working}) + M_t\\]\n6. Real wage dynamics: \\[w_t^{real} = \\frac{w_t^{nominal}(N_t^{labor}, A_t)}{P_t(M_t)}\\]\n7. Monetary regime constraint:\nUnder gold standard (pre-1971): \\[M_t \\leq Gold_{reserves} / \\text{reserve ratio}\\]\nUnder fiat currency (post-1971): \\[M_t \\in [0, \\infty) \\text{ subject to political constraints}\\]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#comparative-statics-gold-vs.-fiat",
    "href": "papers/Paper_A_Revised_1971_Shock.html#comparative-statics-gold-vs.-fiat",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.2 Comparative Statics: Gold vs. Fiat",
    "text": "5.2 Comparative Statics: Gold vs. Fiat\nProposition 5.1 (Fertility Under Alternative Regimes):\nLet \\(F^{gold}_\\infty\\) be long-run fertility under gold standard and \\(F^{fiat}_\\infty\\) under fiat currency.\nUnder plausible parameters: \\[F^{fiat}_\\infty &lt; 2.1 &lt; F^{gold}_\\infty\\]\nProof Sketch:\nUnder fiat currency: 1. Government maximizes short-run consumption via debt 2. Debt creates fiscal pressure → dual-income incentives 3. Dual incomes → wage suppression → fertility decline 4. Cheap credit → tech acceleration → fertility decline\nUnder gold standard: 1. Debt constrained → no fiscal pressure for dual incomes 2. Credit constrained → no tech bubble → slower adaptation required 3. Real wages track productivity → single income sufficient 4. Result: Fertility remains above replacement\n[DATA VALIDATION PLACEHOLDER 13]:\nCOMPARISON TEST:\nPERIOD 1: 1950-1970 (Gold-backed)\nPERIOD 2: 1971-2024 (Fiat)\n\nVARIABLES:\n  - Average TFR across OECD\n  - % of countries with TFR &gt; 2.1\n\nEXPECTED:\n  Period 1: Avg TFR &gt; 2.5, most countries &gt; 2.1\n  Period 2: Avg TFR &lt; 1.8, almost no countries &gt; 2.1\n  \nSTATISTICAL TEST:\n  t-test for difference in means\n  \nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-risk-minimizing-growth-rate-under-fiat-currency",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-risk-minimizing-growth-rate-under-fiat-currency",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.3 The Risk-Minimizing Growth Rate Under Fiat Currency",
    "text": "5.3 The Risk-Minimizing Growth Rate Under Fiat Currency\nWhen fertility constraint is binding, the risk-minimizing growth rate becomes:\n\\[\\dot{A}^* = \\arg\\min_{\\dot{A}} \\left\\{\\delta_{total}(\\dot{A}) : F(\\dot{A}) \\geq 2.1\\right\\}\\]\nIf fertility is decreasing in growth rate (\\(\\frac{\\partial F}{\\partial \\dot{A}} &lt; 0\\)), and current growth exceeds the level compatible with replacement fertility, then:\n\\[\\dot{A}^* &lt; \\dot{A}_{current}\\]\nOptimal policy is deceleration.\n[DATA VALIDATION PLACEHOLDER 14]:\nESTIMATION TASK:\nEstimate the fertility-growth elasticity: ε = ∂ln(F)/∂ln(Ȧ)\n\nMETHOD: Panel regression\nSPECIFICATION:\n  ln(TFRᵢₜ) = α + ε·ln(TechGrowthᵢₜ) + Xᵢₜ + μᵢ + λₜ + εᵢₜ\n\nWHERE:\n  TechGrowth = VC investment + R&D spending + Patent applications\n  X = controls\n  \nEXPECTED: ε &lt; 0 (faster growth → lower fertility)\n\nIf estimated ε and current Ȧ imply F &lt; 2.1:\n  Then optimal policy is Ȧ* &lt; Ȧ_current (DECELERATE)\n  \nSTATUS: [PENDING ESTIMATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-cross-country-pattern",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-cross-country-pattern",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.1 The Cross-Country Pattern",
    "text": "6.1 The Cross-Country Pattern\nPrediction from our model: All countries that went fiat in 1971 should show: 1. Fertility decline accelerating post-1971 2. Debt/GDP rising post-1971 3. Real wage stagnation beginning post-1971 4. Female labor force participation jumping post-1971\nPrediction from alternative theories: - Contraception hypothesis: Pill approved 1960, effects should appear gradually through 1960s - Education hypothesis: Education expanding throughout 20th century, effects should be smooth - Urbanization hypothesis: Ongoing process, no sharp 1971 break\nThe data will distinguish these hypotheses.\n[DATA VALIDATION PLACEHOLDER 15]:\nCOMPREHENSIVE CROSS-COUNTRY TEST:\n\nSAMPLE: All OECD countries with data availability\n\nDEPENDENT VARIABLES:\n  1. TFR (total fertility rate)\n  2. Govt Debt / GDP\n  3. Real wage growth rate\n  4. Female LFP rate\n\nFOR EACH VARIABLE:\n  - Plot time series 1950-2024 for all countries\n  - Run Chow test for break at 1971\n  - Count: How many countries show significant break at 1971?\n  \nEXPECTED UNDER OUR HYPOTHESIS:\n  &gt;80% of countries show break at 1971 across all variables\n  \nEXPECTED UNDER ALTERNATIVE HYPOTHESES:\n  Breaks should be scattered across different years/countries\n  \nSTATUS: [PENDING COMPREHENSIVE ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-israel-exception-actually-confirms-the-theory",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-israel-exception-actually-confirms-the-theory",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.2 The “Israel Exception” Actually Confirms the Theory",
    "text": "6.2 The “Israel Exception” Actually Confirms the Theory\nIsrael has TFR = 2.9, seemingly contradicting the tech→low fertility thesis.\nBut decomposing Israel’s fertility:\n\n\n\nPopulation Group\n% of Births\nTFR\nTech Participation\n\n\n\n\nSecular Jews\n~40%\n~2.1\nFull (high-tech sector)\n\n\nReligious Jews\n~35%\n~3.5\nPartial\n\n\nUltra-Orthodox\n~20%\n~7.0\nMinimal\n\n\nArabs\n~5%\n~3.0\nPartial\n\n\n\nWeighted average: \\(0.4(2.1) + 0.35(3.5) + 0.2(7.0) + 0.05(3.0) = 2.9\\)\nKey insight: - Groups fully participating in fiat-tech-dual-income system → TFR ≈ 2.1 (barely replacement) - Groups opting out → TFR &gt;&gt; 2.1\nThis is exactly what the parallel development (Amish) argument predicts.\n[DATA VALIDATION PLACEHOLDER 16]:\nISRAEL DECOMPOSITION:\nDATA SOURCE: \n  - Israel Central Bureau of Statistics\n  - Demographic studies of Israeli subpopulations\n\nVARIABLES:\n  - TFR by religious/ethnic group\n  - Labor force participation by group  \n  - Tech sector employment by group\n  - Housing costs by area (secular vs. religious cities)\n\nHYPOTHESIS:\n  Within-Israel, fertility should be inversely correlated with \n  tech sector participation even as overall TFR remains high\n\nSTATUS: [PENDING DATA COMPILATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#frances-success-is-temporary-and-costly",
    "href": "papers/Paper_A_Revised_1971_Shock.html#frances-success-is-temporary-and-costly",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.3 France’s “Success” is Temporary and Costly",
    "text": "6.3 France’s “Success” is Temporary and Costly\nFrance has TFR = 1.8, higher than most of Europe (Italy 1.2, Spain 1.2, Germany 1.5).\nBut this “success” requires: - 3.5-4% of GDP spent on family policy (OECD highest) - Massive fiscal transfers ($60+ billion annually) - Still below replacement (TFR = 1.8 &lt; 2.1) - Declining from 2.0 (2010) → 1.8 (2024)\nAnd relies on: - Immigrant fertility (2.5) vs. native French (1.7) - Full-time public childcare system - Extensive parental leave (offsetting dual-income pressure)\nInterpretation: France is fighting the fiat currency fertility collapse with enormous fiscal transfers, barely slowing the decline, at unsustainable fiscal cost.\nThis confirms rather than refutes our model - the “natural” fertility under fiat-tech-dual-income is ~1.2-1.5. France spends 4% of GDP to lift it to 1.8, still below replacement."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-acceleration-trap",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-acceleration-trap",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.1 The Acceleration Trap",
    "text": "7.1 The Acceleration Trap\nPaper R concludes: “efforts to lower x-risk by slowing the development of dangerous AI capabilities may do the opposite on balance unless sufficiently targeted.”\nUnder our framework, this is backwards.\nPaper R’s logic: - Faster growth → less time at risky tech levels → lower cumulative risk - Therefore: Accelerate\nOur logic: - Faster growth is enabled by fiat currency debt - Debt-financed growth creates fiscal pressure - Fiscal pressure → dual-income necessity → fertility collapse - Fertility collapse → \\(N_\\infty = 0\\) (extinction regardless of \\(S_\\infty\\)) - Therefore: Decelerate to sustainable rate compatible with F &gt; 2.1"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#optimizing-the-wrong-objective-function",
    "href": "papers/Paper_A_Revised_1971_Shock.html#optimizing-the-wrong-objective-function",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.2 Optimizing the Wrong Objective Function",
    "text": "7.2 Optimizing the Wrong Objective Function\nPaper R optimizes: \\[\\max S_\\infty\\]\nWe should optimize: \\[\\max (S_\\infty \\cdot N_\\infty \\cdot \\Phi_\\infty)\\]\nwhere: - \\(S_\\infty\\) = probability of avoiding catastrophe - \\(N_\\infty\\) = long-run population size - \\(\\Phi_\\infty\\) = evolutionary fitness\nThese can diverge: - High \\(S_\\infty\\), zero \\(N_\\infty\\): Survive all risks but go extinct via fertility collapse - High \\(S_\\infty\\), low \\(\\Phi_\\infty\\): Survive but with degraded health, cognition, agency\nPaper R’s framework cannot distinguish these outcomes."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-policy-implications",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-policy-implications",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.3 The Policy Implications",
    "text": "7.3 The Policy Implications\nIf policymakers accept Paper R’s framework:\n\nAccelerate AI without adequate safety (since speed reduces risk)\nIgnore fertility collapse (not in the model)\nSupport debt-financed tech bubbles (appears as genuine innovation)\nDismantle precautionary regulation (slows beneficial acceleration)\nEliminate parallel development paths (Amish-style communities seen as inefficient)\n\nEach of these increases true existential risk.\nCorrect policy under our framework:\n\nDistinguish organic from debt-financed innovation\n\nSupport genuine capability development\nCurb speculative bubbles enabled by cheap debt\n\nIntegrate fertility into technology policy\n\nAssess fertility impact of major tech deployments\nRequire family-formation support in high-tech sectors\nHousing policy coordinated with tech policy\n\nRespect biological constraints\n\nLimit rate of technological change to sustainable levels\nPreserve low-tech parallel communities (civilizational insurance)\nMaintain traditional knowledge and skills\n\nAddress debt sustainability\n\nConstrain deficit spending\nConsider return to commodity-backed currency\nReduce dependence on debt-driven growth\n\nPrecautionary approach to transformative technologies\n\nWhen institutional capacity is insufficient, delay deployment\nBuild regulatory capacity before technology deployment\nInternational coordination on governance"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#summary-of-the-refutation",
    "href": "papers/Paper_A_Revised_1971_Shock.html#summary-of-the-refutation",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.1 Summary of the Refutation",
    "text": "8.1 Summary of the Refutation\nPaper R’s core claim: Faster technological development minimizes existential risk.\nOur demonstration: This conclusion holds only if: 1. Technology path is exogenous (it’s not - it’s endogenous to monetary regime) 2. Fertility is irrelevant (it’s not - \\(N_\\infty = 0\\) is extinction) 3. Debt doesn’t create systemic risk (it does) 4. Observed growth is organic (it’s substantially debt-financed) 5. Policy responds optimally (it doesn’t - institutions lag badly)\nWhen these false assumptions are corrected, the conclusion reverses:\nThe risk-minimizing growth rate under fiat currency is substantially lower than current rates, and may be negative."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break-1",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break-1",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.2 The 1971 Structural Break",
    "text": "8.2 The 1971 Structural Break\nThe evidence (pending validation) points to:\nAugust 15, 1971 as the most consequential date in modern economic history: - Global monetary regime change - Enabling unlimited deficit spending - Creating fiscal pressure for dual-income expansion\n- Driving fertility below replacement across all developed nations - Initiating the productivity-wage divergence - Launching the debt super-cycle\nThis is not conspiracy theory - it’s structural political economy."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-path-forward",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-path-forward",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.3 The Path Forward",
    "text": "8.3 The Path Forward\nEmpirical work required:\n\nCompile comprehensive cross-country dataset (1950-2024)\nRun structural break tests for all outcome variables\nEstimate fertility-growth elasticity to find sustainable Ȧ*\nDocument policy mechanisms via archival research\nQuantify debt-innovation relationship via VC/interest rate data\n\n[MASTER DATA VALIDATION PLACEHOLDER]:\nCOMPREHENSIVE EMPIRICAL VALIDATION PLAN:\n\nPHASE 1: Data Compilation (Est. 3-6 months)\n  - Assemble OECD panel dataset 1950-2024\n  - Variables: TFR, debt, wages, productivity, FLFP, tech investment\n  - Sources: OECD, World Bank, UN, BIS, national statistical offices\n\nPHASE 2: Structural Break Analysis (Est. 2-3 months)\n  - Chow tests for 1971 break across all variables\n  - Quandt-Andrews for unknown breakpoint detection\n  - Bai-Perron for multiple breaks\n\nPHASE 3: Cross-Country Variation (Est. 2-3 months)\n  - Construct \"Fiat Exploitation Index\"\n  - Regress fertility decline on FEI\n  - Control for alternative explanations\n\nPHASE 4: Mechanism Tests (Est. 3-4 months)\n  - Fiscal pressure → FLFP relationship\n  - Wage-productivity cointegration breakdown\n  - Interest rates → VC investment → fertility\n\nPHASE 5: Policy Document Review (Est. 2-3 months)\n  - FOIA requests (may take 6-12 months for responses)\n  - Congressional testimony analysis\n  - Academic literature review (1970-1980)\n\nPHASE 6: Synthesis and Publication (Est. 3-4 months)\n  - Integrate all empirical findings\n  - Write comprehensive academic paper\n  - Submit to top journal (Ecological Economics, JEE, QJE)\n\nTOTAL ESTIMATED TIME: 18-24 months\n\nBUDGET REQUIREMENTS:\n  - Data access fees: $5,000-10,000\n  - Research assistant support: $30,000-50,000\n  - FOIA legal support: $10,000-20,000\n  - Total: $45,000-80,000"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#final-provocation",
    "href": "papers/Paper_A_Revised_1971_Shock.html#final-provocation",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.4 Final Provocation",
    "text": "8.4 Final Provocation\nA civilization that: - Survives all catastrophes (S_∞ &gt; 0) - But fails to reproduce (N_∞ = 0) - Is equally extinct\nPaper R proves the first while ignoring the second.\nWe are currently on track for: - Avoiding nuclear war ✓ - Avoiding pandemic ✓\n- Avoiding climate catastrophe ✓ - Avoiding AI catastrophe (?) - But guaranteeing demographic extinction ✗\nThe mathematics are clear: With TFR = 0.73 (South Korea), population halves every ~30 years. In 300 years (10 generations): Population = 0.1% of current In 600 years (20 generations): Population = 0.01% of current\nThis is extinction, just slow and invisible to frameworks like Paper R that don’t model fertility.\nThe 1971 monetary regime change may be the actual existential catastrophe - not a sudden collapse, but a slow-motion demographic extinction disguised as economic progress.\nPaper R, by treating technology as exogenous and fertility as irrelevant, cannot see this. Their mathematics optimizes for survival probability while ignoring that the population surviving is approaching zero.\nThis is the fundamental category error."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-a-data-requirements-summary",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-a-data-requirements-summary",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix A: Data Requirements Summary",
    "text": "Appendix A: Data Requirements Summary\nAll empirical claims in this paper are marked with [DATA VALIDATION PLACEHOLDER X].\nSummary of data requirements:\n\n\n\n\n\n\n\n\n\n\nPlaceholder #\nHypothesis\nData Needed\nSource\nStatus\n\n\n\n\n1\nTax revenue structural break\nLabor income tax / GDP, 1960-2024\nOECD Tax Revenue\nPending\n\n\n2\nWage-productivity divergence\nReal wages, productivity, 1960-2024\nBLS, OECD\nPending\n\n\n3\nFertility structural break\nTFR for all OECD, 1960-2024\nUN Population\nPending\n\n\n4\nVC-interest rate correlation\nVC investment, Fed Funds rate\nPitchBook, Fed\nPending\n\n\n5\nMulti-variable breaks\nPanel: TFR, debt, FLFP, wages\nMultiple\nPending\n\n\n6\nFiat exploitation-fertility\nCross-section debt metrics\nOECD, IMF\nPending\n\n\n7\nFiscal pressure → FLFP\nPanel regression dataset\nOECD, IMF\nPending\n\n\n8\nWage-productivity cointegration\nTime series 1950-2024\nBLS, OECD\nPending\n\n\n9\nTech intensity → fertility IV\nPanel with instruments\nOECD, World Bank\nPending\n\n\n10\nPolicy intent documents\nFOIA, Congressional records\nArchives\nPending\n\n\n11\nDebt crisis probability\nSovereign debt crisis database\nReinhart & Rogoff\nPending\n\n\n12\nSafety spending vs. debt\nGovernment spending by category\nOECD\nPending\n\n\n13\nGold vs. Fiat comparison\nTFR 1950-70 vs. 1971-2024\nUN Population\nPending\n\n\n14\nFertility-growth elasticity\nRegression dataset\nMultiple\nPending\n\n\n15\nComprehensive cross-country\nAll variables, all countries\nMultiple\nPending\n\n\n16\nIsrael subpopulation\nFertility by religious group\nIsrael CBS\nPending"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-b-technical-specifications-for-key-tests",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-b-technical-specifications-for-key-tests",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix B: Technical Specifications for Key Tests",
    "text": "Appendix B: Technical Specifications for Key Tests\n\nB.1 Structural Break Test (Chow Test)\nNull hypothesis: No structural break at 1971 Alternative: Structural break at 1971\nTest statistic: \\[F = \\frac{(RSS_r - RSS_u)/k}{RSS_u/(n-2k)} \\sim F_{k, n-2k}\\]\nwhere: - \\(RSS_r\\) = residual sum of squares (restricted model, no break) - \\(RSS_u\\) = residual sum of squares (unrestricted model, break at 1971) - \\(k\\) = number of parameters - \\(n\\) = number of observations\nReject null if: \\(F &gt; F_{critical}\\) at 5% significance level\n\n\nB.2 Interrupted Time Series Specification\nFull specification: \\[Y_{i,t} = \\alpha_i + \\beta_1 \\cdot Year_t + \\beta_2 \\cdot Post1971_t + \\beta_3 \\cdot (Post1971_t \\times Year_t) + \\gamma \\cdot X_{i,t} + \\varepsilon_{i,t}\\]\nInterpretation: - \\(\\alpha_i\\) = country fixed effects - \\(\\beta_1\\) = pre-1971 trend - \\(\\beta_2\\) = level shift at 1971 - \\(\\beta_3\\) = change in trend post-1971 - \\(X_{i,t}\\) = control variables\nKey test: \\(H_0: \\beta_3 = 0\\) vs. \\(H_A: \\beta_3 \\neq 0\\)\n\n\nB.3 Fiat Exploitation Index Construction\nComponents: 1. Debt Accumulation: \\(\\Delta(Debt/GDP)_{1971-2024}\\) 2. Deficit Intensity: Average (Deficit/GDP) over 1971-2024 3. Monetary Expansion: \\(\\Delta \\log(M2)\\) above GDP growth 4. Real Rate Suppression: Average \\((r_{natural} - r_{actual})\\)\nNormalization: Each component standardized to [0,1]\nAggregation: \\[FEI_i = 0.3 \\cdot Debt_i + 0.3 \\cdot Deficit_i + 0.2 \\cdot Monetary_i + 0.2 \\cdot RateSuppression_i\\]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-c-alternative-explanations-and-how-to-rule-them-out",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-c-alternative-explanations-and-how-to-rule-them-out",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix C: Alternative Explanations and How to Rule Them Out",
    "text": "Appendix C: Alternative Explanations and How to Rule Them Out\n\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nPrediction\nOur Prediction\nDistinguishing Test\n\n\n\n\nThe Pill (contraception)\nGradual effect through 1960s\nSharp break at 1971\nStructural break test\n\n\nEducation expansion\nSmooth decline as education rises\nAcceleration post-1971\nInteraction: education × post1971\n\n\nUrbanization\nOngoing effect (no break)\nBreak at 1971\nChow test controlling for urbanization\n\n\nSecularization\nGradual effect\nBreak at 1971\nControl for religious attendance\n\n\nWomen’s lib movement\nEffect through 1960s-70s\nSpecific break at 1971\nTiming of policy changes\n\n\n\nCombined test: \\[TFR_{i,t} = \\alpha + \\beta_1 Post1971 + \\beta_2 Pill + \\beta_3 Education + \\beta_4 Urban + \\beta_5 Secular + \\varepsilon_{i,t}\\]\nIf our hypothesis is correct: \\(\\beta_1\\) remains significant and large even controlling for all alternatives.\n\nEND OF PAPER"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deb Bose",
    "section": "",
    "text": "This site is a working archive — not a feed.\nEverything here is oriented toward truth-seeking through analysis:\nquantitative reasoning where possible, first-principles thinking where necessary, and skepticism toward narratives that don’t survive contact with data."
  },
  {
    "objectID": "index.html#what-youll-find-here",
    "href": "index.html#what-youll-find-here",
    "title": "Deb Bose",
    "section": "What you’ll find here",
    "text": "What you’ll find here\n\nArticles\nEssays, market notes, and technical deep dives — written to clarify, not to perform.\n\n\nNotebooks\nRunnable research with code, charts, and assumptions made explicit.\nExploratory by design, not polished marketing artefacts.\n\n\nPapers\nDrafts, PDFs, and reference material — including work that is incomplete, evolving, or deliberately unresolved.\n\n\nProjects\nPointers to my GitHub work: tools, models, and systems built to answer specific questions."
  },
  {
    "objectID": "index.html#latest-writing",
    "href": "index.html#latest-writing",
    "title": "Deb Bose",
    "section": "Latest writing",
    "text": "Latest writing\n\n→ Go to Papers\n→ Go to Notebooks"
  },
  {
    "objectID": "posts/2026-01-06-my-first-post.html",
    "href": "posts/2026-01-06-my-first-post.html",
    "title": "My first Quarto post",
    "section": "",
    "text": "Some text.\nInline math: \\(I_t = f(1/C_t)\\)\n\nimport math\nmath.sqrt(2)\n\n1.4142135623730951\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Papers",
    "section": "",
    "text": "PDF papers (preprints, drafts, slide-like PDFs)\nMarkdown/QMD notes that accompany papers (methods, references, appendices)\n\n\n\n\n\n\n\n  \n    \n      markdown\n    \n    Paper A Revised 1971 Shock\n    \n  \n\n\n\n\n\n  \n    \n      pdf\n    \n    AURA Neuro–Symbolic Energy–Efficient Architecture Reusing LLM Infrastructure\n    PDF: AURA-Neuro–Symbolic-Energy–Efficient-Architecture-Reusing-LLM-Infrastructure.html\n  \n\n\n\n\n\n  \n    \n      reference\n    \n    Existential Risk and Growth"
  },
  {
    "objectID": "papers/index.html#whats-here",
    "href": "papers/index.html#whats-here",
    "title": "Papers",
    "section": "",
    "text": "PDF papers (preprints, drafts, slide-like PDFs)\nMarkdown/QMD notes that accompany papers (methods, references, appendices)\n\n\n\n\n\n\n\n  \n    \n      markdown\n    \n    Paper A Revised 1971 Shock\n    \n  \n\n\n\n\n\n  \n    \n      pdf\n    \n    AURA Neuro–Symbolic Energy–Efficient Architecture Reusing LLM Infrastructure\n    PDF: AURA-Neuro–Symbolic-Energy–Efficient-Architecture-Reusing-LLM-Infrastructure.html\n  \n\n\n\n\n\n  \n    \n      reference\n    \n    Existential Risk and Growth"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Notebooks",
    "section": "",
    "text": "Browse my research notebooks below.\n\n\n\n\n\n\n\n\n\nEmpirical Validation of the 1971 Fiscal Structural Break: Analysis of U.S. Real Individual Income Tax Revenue\n\n\nThis document provides empirical validation for a structural break in U.S. fiscal extraction occurring at the 1971 monetary pivot point. Utilizing an Interrupted Time Series framework and a Chow Test on inflation-adjusted U.S. Individual Income Tax receipts (normalized to 2024 dollars), the analysis confirms a significant shift in the growth trajectory of tax extraction following the transition to a pure fiat regime.\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\nLiquidity Regimes and the Death (and Return) of Valuations\n\n\nThis research utilizes Sparse PCA and Hidden Markov Models to demonstrate that equity factor premiums are strictly regime-dependent, revealing that while growth dominates during expansive liquidity, value and conservative investment factors revive significantly during periods of liquidity tightening. Findings prove that market valuations remain relevant but fluctuate predictably, with the S&P 500 trading approximately 0.43σ more expensive in high-liquidity states compared to tight regimes.\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\nNo matching items"
  }
]