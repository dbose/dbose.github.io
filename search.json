[
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "",
    "text": "import yfinance as yf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Calculate annualized return\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\n# Calculate volatility (annualized standard deviation)\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\n# Calculate Sharpe Ratio\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(periods_per_year)\n\n# Calculate Sortino Ratio\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / downside_returns.std() * np.sqrt(periods_per_year)\n\n# Maximum drawdown\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#the-philosophy-investing-in-enablers-not-just-innovators",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#the-philosophy-investing-in-enablers-not-just-innovators",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "The Philosophy: Investing in Enablers, Not Just Innovators",
    "text": "The Philosophy: Investing in Enablers, Not Just Innovators\nThe concept of a K-wave, named after economist Nikolai Kondratiev, describes long-term economic cycles driven by technological innovation. Each wave—spanning roughly 40-60 years—ushers in transformative advancements, from the Industrial Revolution to the Information Age. We’re now entering what many believe is the sixth K-wave, propelled by AI, clean energy, and advanced manufacturing. While companies at the forefront of these technologies (e.g., pure-play AI startups or speculative renewable energy firms) often dominate headlines, their volatility can make them risky bets for long-term investors\nDuring the 1848 California Gold Rush, it wasn’t the gold miners who reaped the most consistent rewards—it was the “shovelmakers,” the suppliers of tools and infrastructure, who built lasting wealth by enabling the frenzy. Today, as we stand on the cusp of a new technological era defined by artificial intelligence, renewable energy, and advanced manufacturing, a similar strategy can guide us toward sustainable growth.\n\n\n\nGold Rush\n\n\nInstead, our mini-fund targets companies that enable these breakthroughs—those building the “picks and shovels” of the modern era. ASML, for instance, powers the semiconductor industry with its photolithography machines, a cornerstone of AI and computing advancements. Tesla, beyond its electric vehicles, drives innovation in energy storage and autonomous driving infrastructure. Microsoft provides cloud computing and AI platforms that underpin countless applications, while Berkshire Hathaway offers stability and diversified exposure to industrial and financial sectors. AbbVie contributes healthcare innovation, a critical pillar of societal progress, and Lockheed Martin strengthens the portfolio with its leadership in aerospace and defense—sectors poised for growth amid geopolitical shifts and technological integration. This blend of high-growth and value stocks creates a portfolio that captures multiple growth trends while maintaining a solid foundation. By avoiding overexposure to speculative “gold miners,” we aim to deliver consistent returns with reduced downside risk—a strategy validated by our backtest results."
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#initial-portfolio-composition",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#initial-portfolio-composition",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Initial Portfolio Composition",
    "text": "Initial Portfolio Composition\nThe original portfolio comprises five stocks — ASML Holding N.V. (ASML), SolarEdge Technologies, Inc. (SEDG), Rockwell Automation, Inc. (ROK), Illumina, Inc. (ILMN), and Lockheed Martin Corporation (LMT) — each allocated an equal weight of 20%.\nThis portfolio was backtested using historical closing prices from January 1, 2000, to January 1, 2024, sourced via Yahoo Finance (yf.download). Daily returns were calculated with pct_change() and cleaned with dropna() to ensure data integrity. The portfolio is benchmarked against the S&P 500 (^GSPC) and Nasdaq-100 (^NDX), reflecting a strategy to measure performance against broad market and technology-focused indices.\nThis portfolio embodies the “shovelmakers of tomorrow” philosophy outlined earlier. Rather than chasing speculative leaders in emerging technologies, it targets companies that provide critical infrastructure, tools, and services enabling the next Kondratiev wave (K-wave) — such as renewable energy, automation, genomics, and aerospace.\nThe equal-weight allocation ensures diversification across the following sectors, balancing growth potential with stability:\n\nSemiconductors\nSolar Energy\nIndustrial Automation\nGenomics\nDefense\n\n\n\nCompanies in the Portfolio\n\n\n\nASML Holding N.V. (ASML)\nSector: Technology (Semiconductors)\nWeight: 20%\nRole:\nASML is the global leader in photolithography systems, essential for manufacturing integrated circuits (microchips). Its extreme ultraviolet (EUV) lithography machines are critical for producing advanced chips used in AI, 5G, and high-performance computing.\nK-Wave Relevance:\nASML is a quintessential shovelmaker, supplying the tools that power the semiconductor industry—a backbone of the sixth K-wave. As demand for AI and IoT grows, ASML’s equipment enables chipmakers like TSMC and Intel to push technological boundaries.\nPortfolio Fit:\nASML contributes high-growth potential, capitalizing on secular trends in technology, while its dominant market position adds resilience.\n\n\n\nSolarEdge Technologies, Inc. (SEDG)\nSector: Renewable Energy (Solar)\nWeight: 20%\nRole:\nSolarEdge specializes in power optimizers, inverters, and monitoring systems for solar photovoltaic (PV) installations. Its solutions maximize energy efficiency and reliability for residential, commercial, and utility-scale solar projects.\nK-Wave Relevance:\nThe transition to clean energy is a defining feature of the next K-wave, with solar power at the forefront. SolarEdge’s technologies enhance the scalability and affordability of solar energy, positioning it as an enabler of the renewable revolution. Unlike solar panel manufacturers, SolarEdge focuses on the tools that optimize energy output, aligning with the shovelmaker strategy.\nPortfolio Fit:\nSEDG introduces exposure to the fast-growing renewable energy sector, offering growth potential tempered by the volatility inherent in clean energy markets.\n\n\n\nRockwell Automation, Inc. (ROK)\nSector: Industrials (Automation)\nWeight: 20%\nRole:\nRockwell Automation provides industrial automation and digital transformation solutions, including programmable logic controllers (PLCs), sensors, and software for smart manufacturing. It serves industries like automotive, food and beverage, and pharmaceuticals.\nK-Wave Relevance:\nAutomation is a cornerstone of the sixth K-wave, driving efficiency in manufacturing and supply chains. Rockwell’s technologies enable “Industry 4.0”—the integration of IoT, AI, and robotics into production—making it a key supplier of tools for industrial innovation. Its focus on software and analytics further aligns with digital transformation trends.\nPortfolio Fit:\nROK adds a value-oriented component, balancing growth with stability. Its diversified client base mitigates sector-specific risks, enhancing portfolio resilience.\n\n\n\nIllumina, Inc. (ILMN)\nSector: Healthcare (Genomics)\nWeight: 20%\nRole:\nIllumina is a leader in DNA sequencing and genomics, providing instruments, reagents, and software for genetic analysis. Its technologies support applications in personalized medicine, cancer research, and agriculture.\nK-Wave Relevance:\nGenomics is poised to transform healthcare, a critical pillar of societal progress in the next K-wave. Illumina’s sequencing platforms are the shovels of this revolution, enabling researchers and clinicians to decode genetic data at scale. Its dominance in sequencing technology ensures long-term growth potential.\nPortfolio Fit:\nILMN brings exposure to healthcare innovation, a sector with defensive qualities and high growth. It diversifies the portfolio away from pure technology, reducing correlation with market cycles.\n\n\n\nLockheed Martin Corporation (LMT)\nSector: Aerospace and Defense\nWeight: 20%\nRole:\nLockheed Martin is a global leader in aerospace, defense, and security, known for products like the F-35 fighter jet, missile systems, and space technologies. It serves government and commercial clients worldwide.\nK-Wave Relevance:\nDefense and aerospace are integral to the next K-wave, driven by geopolitical dynamics and technological advancements (e.g., hypersonics, space exploration). Lockheed Martin’s role as a systems integrator and innovator positions it as an enabler of national security and space infrastructure—a stable shovelmaker in a high-stakes field.\nPortfolio Fit:\nLMT anchors the portfolio with defensive characteristics, offering steady cash flows and dividends. Its low correlation with tech sectors enhances diversification, mitigating downside risk during market downturns. ownside risk during market downturns.\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'SEDG', 'ROK', 'ILMN', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.2]) \n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Mini-Fund Portfolio vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\npd.set_option('display.width', 1000)\nprint(metrics_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.194025    0.269933      0.718391       0.997384     -0.411544\nS&P 500              0.100897    0.184097      0.506164       0.610701     -0.339250\nNasdaq-100           0.168178    0.225889      0.713269       0.905724     -0.355631"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#beginning",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#beginning",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Beginning",
    "text": "Beginning\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\n# stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT']\n#stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n# weights = np.array([0.1639441, 0.2852132, 0.3240187, 0.2268240])\n# weights = np.array([0.14, 0.25, 0.29, 0.20, 0.12]) \n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Portfolio-GV', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(metrics_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.367925    0.272138      1.214923       1.683422     -0.421506\nS&P 500              0.119445    0.174019      0.621038       0.762031     -0.339250\nNasdaq-100           0.181999    0.207497      0.813858       1.043914     -0.355631\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\n\n# Define the stocks and benchmark indices\n# stocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT']\n# stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.2,  0.2])\n# weights = np.array([0.1639441, 0.2852132, 0.3240187, 0.2268240])\n# weights = np.array([0.14, 0.25, 0.29, 0.20, 0.12]) \n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O1 vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(metrics_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.325947    0.240479      1.211467       1.630888     -0.371925\nS&P 500              0.113679    0.172243      0.595699       0.721428     -0.339250\nNasdaq-100           0.179487    0.210365      0.795558       1.003233     -0.355631\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\n\n# Define the stocks and benchmark indices\n#stocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\n#stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT']\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\n# weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n# weights = np.array([0.1639441, 0.2852132, 0.3240187, 0.2268240])\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O2 vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(metrics_df)\n\n[*********************100%***********************]  8 of 8 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.308158    0.218942      1.246309       1.642653     -0.350050\nS&P 500              0.113679    0.172243      0.595699       0.721428     -0.339250\nNasdaq-100           0.179487    0.210365      0.795558       1.003233     -0.355631\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Plot the density plots for Portfolio, S&P 500, and Nasdaq-100 returns\nplt.figure(figsize=(15, 8))\n\n# Full Density Plot\n\nsns.kdeplot(portfolio_returns, label='Portfolio-GV-O2', color='blue', shade=True, clip=(-0.06, 0.06))\nsns.kdeplot(returns['^GSPC'], label='S&P 500', color='orange', shade=True, clip=(-0.06, 0.06))\nsns.kdeplot(returns['^NDX'], label='Nasdaq-100', color='green', shade=True, clip=(-0.06, 0.06))\n\nplt.title('Zoomed-In Density Plot: Left and Right Tails', fontsize=16)\nplt.xlabel('Daily Return', fontsize=12)\nplt.ylabel('Density', fontsize=12)\nplt.legend(loc='upper right', fontsize=10)\n\n\n# Adjust layout and show plot\nplt.tight_layout()\nplt.grid(True)\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\n&lt;ipython-input-23-b30301a8e790&gt;:28: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(portfolio_returns, label='Mini-Fund Portfolio', color='blue', shade=True, clip=(-0.06, 0.06))\n&lt;ipython-input-23-b30301a8e790&gt;:29: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(returns['^GSPC'], label='S&P 500', color='orange', shade=True, clip=(-0.06, 0.06))\n&lt;ipython-input-23-b30301a8e790&gt;:30: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(returns['^NDX'], label='Nasdaq-100', color='green', shade=True, clip=(-0.06, 0.06))"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#perf-across-vol-regimes",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#perf-across-vol-regimes",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Perf across vol regimes",
    "text": "Perf across vol regimes\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom hmmlearn.hmm import GaussianHMM\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Fit a 3-state HMM to detect volatility regimes\nmodel = GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=1000)\nmodel.fit(portfolio_returns.values.reshape(-1, 1))\n\n# Predict regimes\nhidden_states = model.predict(portfolio_returns.values.reshape(-1, 1))\n\n# Add hidden state labels to returns DataFrame\nreturns_df = pd.DataFrame({\n    'Portfolio Returns': portfolio_returns,\n    '^GSPC Returns': returns['^GSPC'],\n    '^NDX Returns': returns['^NDX'],\n    'Hidden State': hidden_states\n})\n\n# Calculate average volatility of each regime to determine regime labels\naverage_volatility = returns_df.groupby('Hidden State')['Portfolio Returns'].std()\n\n# Sort the volatilities to label regimes\nvolatility_rank = average_volatility.sort_values().index\nregime_labels = {volatility_rank[0]: 'Low Volatility',\n                 volatility_rank[1]: 'Mid Volatility',\n                 volatility_rank[2]: 'High Volatility'}\n\n# Add regime labels to DataFrame\nreturns_df['Volatility Regime'] = returns_df['Hidden State'].map(regime_labels)\n\n# Define metric functions\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(periods_per_year)\n\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / downside_returns.std() * np.sqrt(periods_per_year)\n\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()\n\n# Calculate metrics for each volatility regime for portfolio, S&P 500, and Nasdaq-100\nmetrics = {}\n\nfor regime in ['Low Volatility', 'Mid Volatility', 'High Volatility']:\n    state_data = returns_df[returns_df['Volatility Regime'] == regime]\n    state_metrics = {}\n    \n    for col in ['Portfolio Returns', '^GSPC Returns', '^NDX Returns']:\n        state_metrics[col] = {\n            'Annualized Return': annualized_return(state_data[col]),\n            'Volatility': annualized_volatility(state_data[col]),\n            'Sharpe Ratio': sharpe_ratio(state_data[col]),\n            'Sortino Ratio': sortino_ratio(state_data[col]),\n            'Max Drawdown': max_drawdown(state_data[col])\n        }\n    \n    metrics[regime] = state_metrics\n\n# Convert metrics to a DataFrame for better visualization\nmetrics_dict = {}\nfor regime, data in metrics.items():\n    for asset, values in data.items():\n        row_key = f\"{regime} - {asset.replace('Portfolio Returns', 'Mini-Fund').replace('^GSPC Returns', 'S&P 500').replace('^NDX Returns', 'Nasdaq-100')}\"\n        metrics_dict[row_key] = values\n\nmetrics_df = pd.DataFrame(metrics_dict).T\nmetrics_df = metrics_df.rename_axis('Regime and Asset').reset_index()\n\n# Group the table display by Low/Mid/High Volatility Regimes to enhance readability\ngrouped_metrics = metrics_df.copy()\n\n# Formatting the DataFrame to show Regime and Metrics without repetition of the regime\ngrouped_metrics['Volatility Regime'] = grouped_metrics['Regime and Asset'].str.extract(r'^(Low|Mid|High) Volatility')\ngrouped_metrics['Asset'] = grouped_metrics['Regime and Asset'].str.replace(r'^(Low|Mid|High) Volatility - ', '')\ngrouped_metrics.drop(columns=['Regime and Asset'], inplace=True)\n\n# Reorder columns for better readability\ngrouped_metrics = grouped_metrics[['Volatility Regime', 'Asset', 'Annualized Return', 'Volatility', 'Sharpe Ratio', 'Sortino Ratio', 'Max Drawdown']]\n\n# Sorting the DataFrame by 'Volatility Regime' to ensure grouping\ngrouped_metrics = grouped_metrics.sort_values(by=['Volatility Regime', 'Asset']).reset_index(drop=True)\n\n# Display in a more readable format\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', '{:.4f}'.format)\npd.set_option('display.width', 1000)\nprint(\"\\nPerformance Metrics Grouped by Volatility Regime:\")\nprint(grouped_metrics)\n\n# Plotting cumulative returns for each regime\nplt.figure(figsize=(15, 8))\n\nfor regime in ['Low Volatility', 'Mid Volatility', 'High Volatility']:\n    state_data = returns_df[returns_df['Volatility Regime'] == regime]\n    cumulative_portfolio_returns = (1 + state_data['Portfolio Returns']).cumprod()\n    cumulative_sp500_returns = (1 + state_data['^GSPC Returns']).cumprod()\n    cumulative_ndx_returns = (1 + state_data['^NDX Returns']).cumprod()\n\n    plt.plot(cumulative_portfolio_returns, label=f'{regime} - Mini-Fund Cumulative Returns', linestyle='-', color=f'C{list(regime_labels.values()).index(regime)}')\n    plt.plot(cumulative_sp500_returns, label=f'{regime} - S&P 500 Cumulative Returns', linestyle='--', color=f'C{list(regime_labels.values()).index(regime)}')\n    plt.plot(cumulative_ndx_returns, label=f'{regime} - Nasdaq-100 Cumulative Returns', linestyle=':', color=f'C{list(regime_labels.values()).index(regime)}')\n\n# Add labels and title\nplt.title('Cumulative Returns of Portfolio-GV-O2, S&P 500, and Nasdaq-100 Across Volatility Regimes', fontsize=16)\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('Cumulative Growth', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\nModel is not converging.  Current: 8306.194727967259 is not greater than 8306.204205929746. Delta is -0.009477962486926117\n\n\n\nPerformance Metrics Grouped by Volatility Regime:\n  Volatility Regime                         Asset  Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\n0              High   High Volatility - Mini-Fund            -0.8314      0.8617       -1.6850        -2.4929       -0.3083\n1              High  High Volatility - Nasdaq-100            -0.7188      0.8288       -1.1669        -2.0536       -0.2297\n2              High     High Volatility - S&P 500            -0.8402      0.8390       -1.8236        -3.2338       -0.3064\n3               Low    Low Volatility - Mini-Fund             0.3906      0.1553        2.0735         3.2254       -0.1154\n4               Low   Low Volatility - Nasdaq-100             0.3058      0.1421        1.8093         2.5148       -0.1106\n5               Low      Low Volatility - S&P 500             0.2072      0.1101        1.5845         2.1873       -0.1028\n6               Mid    Mid Volatility - Mini-Fund             0.1974      0.2961        0.6896         1.0894       -0.3166\n7               Mid   Mid Volatility - Nasdaq-100            -0.0683      0.2950       -0.1602        -0.2455       -0.4023\n8               Mid      Mid Volatility - S&P 500            -0.0423      0.2274       -0.1644        -0.2437       -0.3408\n\n\n\n\n\n\n\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom hmmlearn.hmm import GaussianHMM\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Fit a 3-state HMM to detect volatility regimes\nmodel = GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=1000)\nmodel.fit(portfolio_returns.values.reshape(-1, 1))\n\n# Predict regimes\nhidden_states = model.predict(portfolio_returns.values.reshape(-1, 1))\n\n# Add hidden state labels to returns DataFrame\nreturns_df = pd.DataFrame({\n    'Date': returns.index,\n    'Portfolio Returns': portfolio_returns,\n    'S&P 500 Returns': returns['^GSPC'],\n    'Nasdaq-100 Returns': returns['^NDX'],\n    'Hidden State': hidden_states\n})\n\n# Calculate average volatility of each regime to determine regime labels\naverage_volatility = returns_df.groupby('Hidden State')['Portfolio Returns'].std()\n\n# Sort the volatilities to label regimes\nvolatility_rank = average_volatility.sort_values().index\nregime_labels = {volatility_rank[0]: 'Low Volatility',\n                 volatility_rank[1]: 'Mid Volatility',\n                 volatility_rank[2]: 'High Volatility'}\n\n# Add regime labels to DataFrame\nreturns_df['Volatility Regime'] = returns_df['Hidden State'].map(regime_labels)\n\n# Calculate cumulative returns for each asset and add to DataFrame\nreturns_df['Cumulative Portfolio Returns'] = (1 + returns_df['Portfolio Returns']).cumprod()\nreturns_df['Cumulative S&P 500 Returns'] = (1 + returns_df['S&P 500 Returns']).cumprod()\nreturns_df['Cumulative Nasdaq-100 Returns'] = (1 + returns_df['Nasdaq-100 Returns']).cumprod()\n\n# Melt the DataFrame for seaborn compatibility\ncumulative_returns_df = pd.melt(\n    returns_df,\n    id_vars=['Date', 'Volatility Regime'],\n    value_vars=['Cumulative Portfolio Returns', 'Cumulative S&P 500 Returns', 'Cumulative Nasdaq-100 Returns'],\n    var_name='Asset',\n    value_name='Cumulative Returns'\n)\n\n# Create a faceted plot using seaborn's FacetGrid\ng = sns.FacetGrid(cumulative_returns_df, col='Volatility Regime', hue='Asset', col_wrap=3, height=4, aspect=1.5)\ng.map(plt.plot, 'Date', 'Cumulative Returns').add_legend()\n\n# Adjust plot aesthetics\ng.set_axis_labels('Date', 'Cumulative Growth')\ng.set_titles(col_template='{col_name} Regime')\ng.fig.suptitle('Cumulative Returns Across Volatility Regimes for Portfolio, S&P 500, and Nasdaq-100', y=1.05)\n\nplt.tight_layout()\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\nModel is not converging.  Current: 8305.528897318622 is not greater than 8305.57159532981. Delta is -0.042698011187894735\n\n\n\n\n\n\n\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom tabulate import tabulate\n\nfrom scipy.stats import t\nfrom arch import arch_model  # To use GARCH for volatility forecasting\n\n# Define risk-free rate (approximation)\nrisk_free_rate = 0.02  # 2% annually\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']  # Example portfolio stocks\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate benchmark returns\nmarket_returns = returns['^GSPC']  # S&P 500 as benchmark\n\n# Alpha, Beta, R-Square\ncov_matrix = np.cov(portfolio_returns, market_returns)\nbeta_portfolio = cov_matrix[0, 1] / cov_matrix[1, 1]\nalpha_portfolio = np.mean(portfolio_returns) - beta_portfolio * np.mean(market_returns)\nr_square = 1 - (np.var(portfolio_returns - beta_portfolio * market_returns) / np.var(portfolio_returns))\n\n# Value at Risk (VaR) and Expected Shortfall (ES)\nimport numpy as np\nfrom scipy.stats import norm\n\ndef var_es(returns, confidence_level=0.95):\n    \"\"\"\n    Calculate VaR and Expected Shortfall using the normal distribution.\n\n    Args:\n        returns (pd.Series): A pandas Series of historical returns.\n        confidence_level (float): Confidence level for VaR and ES calculation.\n\n    Returns:\n        Tuple: VaR and ES for 1-month, 6-month, and 1-year periods.\n    \"\"\"\n    # Calculate the mean return and standard deviation (daily)\n    mean_return_daily = returns.mean()\n    std_dev_daily = returns.std()\n\n    # z-score for the given confidence level (left tail)\n    z = norm.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + z * std_dev_period)\n\n        # ES Calculation\n        es_factor = norm.pdf(z) / (1 - confidence_level)\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n\ndef var_es_tf(returns, confidence_level=0.95, forecast_volatility=False):\n    \"\"\"\n    Calculate VaR and Expected Shortfall using Student's t-distribution, with optional GARCH volatility forecasting.\n\n    Args:\n        returns (pd.Series): A pandas Series of historical returns.\n        confidence_level (float): Confidence level for VaR and ES calculation.\n        forecast_volatility (bool): If True, use GARCH(1,1) to forecast volatility.\n\n    Returns:\n        Tuple: VaR and ES for 1-month, 6-month, and 1-year periods.\n    \"\"\"\n    # Calculate the mean return (daily)\n    mean_return_daily = returns.mean()\n\n    # Volatility calculation\n    if forecast_volatility:\n        # Fit GARCH(1,1) model to the return series\n        am = arch_model(returns * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        # Use last forecasted volatility (scaled back to decimal)\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        # Use historical standard deviation\n        std_dev_daily = returns.std()\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n\n\n# Get VaR and Expected Shortfall for the portfolio\n#var_1m, var_6m, var_1y, es_1m, es_6m, es_1y = var_es(portfolio_returns)\nvar_1m, var_6m, var_1y, es_1m, es_6m, es_1y = var_es_tf(portfolio_returns, forecast_volatility=True)\n\n# Calculate standard performance metrics (already in your code)\nportfolio_metrics = {\n    'Alpha': alpha_portfolio,\n    'Beta': beta_portfolio,\n    'R-Square': r_square,\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns),\n    'VaR 1M (95%)': var_1m,\n    'VaR 6M (95%)': var_6m,\n    'VaR 1Y (95%)': var_1y,\n    'ES 1M (95%)': es_1m,\n    'ES 6M (95%)': es_6m,\n    'ES 1Y (95%)': es_1y\n}\n\n# Calculate VaR and ES for S&P 500 benchmark\n#var_1m_sp500, var_6m_sp500, var_1y_sp500, es_1m_sp500, es_6m_sp500, es_1y_sp500 = var_es(returns['^GSPC'])\nvar_1m_sp500, var_6m_sp500, var_1y_sp500, es_1m_sp500, es_6m_sp500, es_1y_sp500 = var_es_tf(returns['^GSPC'], forecast_volatility=True)\n\n# Calculate the same for S&P 500 benchmark\nsp500_metrics = {\n    'Beta': 1,\n    'R-Square': 1,\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC']),\n    'VaR 1M (95%)': var_1m_sp500,\n    'VaR 6M (95%)': var_6m_sp500,\n    'VaR 1Y (95%)': var_1y_sp500,\n    'ES 1M (95%)': es_1m_sp500,\n    'ES 6M (95%)': es_6m_sp500,\n    'ES 1Y (95%)': es_1y_sp500\n}\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics], index=['Portfolio-GV-O2', 'S&P 500'])\n\n# Adjust display settings to show all columns without wrapping\npd.set_option('display.max_columns', None)          # Show all columns\npd.set_option('display.expand_frame_repr', False)   # Prevent DataFrame from wrapping\npd.set_option('display.width', 1000)                # Set width to large enough value\npd.set_option('display.colheader_justify', 'center') # Center align the headers\npd.set_option('display.float_format', '{:.6f}'.format)  # Format float to 6 decimal places\n\nprint(metrics_df)\n\n[*********************100%***********************]  8 of 8 completed\n\n\n                  Alpha    Beta    R-Square  Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown  VaR 1M (95%)  VaR 6M (95%)  VaR 1Y (95%)  ES 1M (95%)  ES 6M (95%)  ES 1Y (95%)\nPortfolio-GV-O2 0.000635 1.083823  0.727009      0.308158        0.218942     1.246309      1.642654      -0.350050     -0.105416     -0.344853     -0.573462    -0.140598    -0.431032    -0.695336  \nS&P 500              NaN 1.000000  1.000000      0.113679        0.172243     0.595699      0.721428      -0.339250     -0.068273     -0.203504     -0.323703    -0.093486    -0.265263    -0.411043  \n\n\n\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, t\nfrom arch import arch_model\n\n# Define the stocks, benchmark indices, and TAIL ETF\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\nprotective_etf = ['TAIL']\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices + protective_etf, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Weights for portfolio without TAIL\nweights_without_tail = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns without TAIL\nportfolio_returns_without_tail = (returns[stocks] * weights_without_tail).sum(axis=1)\n\n# Weights for portfolio including TAIL\nweights_with_tail = np.array([0.13, 0.18, 0.22, 0.16, 0.10, 0.11, 0.10])  # Added 10% weight to TAIL\n\n# Calculate portfolio returns with TAIL\nportfolio_returns_with_tail = (returns[stocks + protective_etf] * weights_with_tail).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns_without_tail = (1 + portfolio_returns_without_tail).cumprod()\ncumulative_portfolio_returns_with_tail = (1 + portfolio_returns_with_tail).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n# Define metrics functions\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / returns.std() * np.sqrt(periods_per_year)\n\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / downside_returns.std() * np.sqrt(periods_per_year)\n\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()\n\ndef alpha_beta(returns, benchmark_returns):\n    cov_matrix = np.cov(returns, benchmark_returns)\n    beta = cov_matrix[0, 1] / cov_matrix[1, 1]\n    alpha = returns.mean() - beta * benchmark_returns.mean()\n    return alpha, beta\n\ndef var_es(returns, confidence_level=0.95, forecast_volatility=False):\n    # Calculate the mean return (daily)\n    mean_return_daily = returns.mean()\n\n    # Volatility calculation\n    if forecast_volatility:\n        # Fit GARCH(1,1) model to the return series\n        am = arch_model(returns * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        # Use last forecasted volatility (scaled back to decimal)\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        # Use historical standard deviation\n        std_dev_daily = returns.std()\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n# Calculate Alpha, Beta, VaR, and ES for portfolios and benchmarks\nmarket_returns = returns['^GSPC']\n\nalpha_portfolio_without_tail, beta_portfolio_without_tail = alpha_beta(portfolio_returns_without_tail, market_returns)\nalpha_portfolio_with_tail, beta_portfolio_with_tail = alpha_beta(portfolio_returns_with_tail, market_returns)\n\nvar_1m_without_tail, var_6m_without_tail, var_1y_without_tail, es_1m_without_tail, es_6m_without_tail, es_1y_without_tail = var_es(portfolio_returns_without_tail, forecast_volatility=True)\nvar_1m_with_tail, var_6m_with_tail, var_1y_with_tail, es_1m_with_tail, es_6m_with_tail, es_1y_with_tail = var_es(portfolio_returns_with_tail, forecast_volatility=True)\n\n# Calculate metrics for mini-fund portfolio without TAIL\nportfolio_metrics_without_tail = {\n    'Alpha': alpha_portfolio_without_tail,\n    'Beta': beta_portfolio_without_tail,\n    'Annualized Return': annualized_return(portfolio_returns_without_tail),\n    'Volatility': annualized_volatility(portfolio_returns_without_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_without_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_without_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_without_tail),\n    'VaR 1M (95%)': var_1m_without_tail,\n    'VaR 6M (95%)': var_6m_without_tail,\n    'VaR 1Y (95%)': var_1y_without_tail,\n    'ES 1M (95%)': es_1m_without_tail,\n    'ES 6M (95%)': es_6m_without_tail,\n    'ES 1Y (95%)': es_1y_without_tail\n}\n\n# Calculate metrics for mini-fund portfolio with TAIL\nportfolio_metrics_with_tail = {\n    'Alpha': alpha_portfolio_with_tail,\n    'Beta': beta_portfolio_with_tail,\n    'Annualized Return': annualized_return(portfolio_returns_with_tail),\n    'Volatility': annualized_volatility(portfolio_returns_with_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_with_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_with_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_with_tail),\n    'VaR 1M (95%)': var_1m_with_tail,\n    'VaR 6M (95%)': var_6m_with_tail,\n    'VaR 1Y (95%)': var_1y_with_tail,\n    'ES 1M (95%)': es_1m_with_tail,\n    'ES 6M (95%)': es_6m_with_tail,\n    'ES 1Y (95%)': es_1y_with_tail\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Beta': 1,\n    'R-Square': 1,\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC']),\n    'VaR 1M (95%)': var_es(returns['^GSPC'])[0],\n    'VaR 6M (95%)': var_es(returns['^GSPC'])[1],\n    'VaR 1Y (95%)': var_es(returns['^GSPC'])[2],\n    'ES 1M (95%)': var_es(returns['^GSPC'])[3],\n    'ES 6M (95%)': var_es(returns['^GSPC'])[4],\n    'ES 1Y (95%)': var_es(returns['^GSPC'])[5]\n}\n\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_without_tail_normalized = cumulative_portfolio_returns_without_tail / cumulative_portfolio_returns_without_tail.iloc[0] * 100\ncumulative_portfolio_returns_with_tail_normalized = cumulative_portfolio_returns_with_tail / cumulative_portfolio_returns_with_tail.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio without TAIL\nplt.plot(cumulative_portfolio_returns_without_tail_normalized, label='Portfolio-GV-O2 Portfolio without TAIL', color='red')\n\n# Plot the portfolio with TAIL\nplt.plot(cumulative_portfolio_returns_with_tail_normalized, label='Portfolio-GV-O2 Portfolio with TAIL', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O2 with and without TAIL vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics_without_tail, portfolio_metrics_with_tail, sp500_metrics, ndx_metrics], \n                          index=['Portfolio-GV-O2 without TAIL', 'Portfolio-GV-O2 with TAIL', 'S&P 500', 'Nasdaq-100'])\n\n# Adjust display settings to show all columns without wrapping\npd.set_option('display.max_columns', None)          # Show all columns\npd.set_option('display.expand_frame_repr', False)   # Prevent DataFrame from wrapping\npd.set_option('display.width', 1000)                # Set width to large enough value\npd.set_option('display.colheader_justify', 'center') # Center align the headers\npd.set_option('display.float_format', '{:.6f}'.format)  # Format float to 6 decimal places\n\nprint(metrics_df)\n\n[*********************100%***********************]  9 of 9 completed\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n                               Alpha    Beta    Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown  VaR 1M (95%)  VaR 6M (95%)  VaR 1Y (95%)  ES 1M (95%)  ES 6M (95%)  ES 1Y (95%)  R-Square\nPortfolio-GV-O2 without TAIL 0.000622 1.086780      0.299328        0.243712     1.115379      1.433413      -0.350050     -0.106745     -0.347816     -0.577361    -0.142540    -0.435495    -0.701359         NaN\nPortfolio-GV-O2 with TAIL    0.000560 0.912985      0.262029        0.208124     1.127078      1.462300      -0.300293     -0.093390     -0.304079     -0.504595    -0.124734    -0.380856    -0.613174         NaN\nS&P 500                           NaN 1.000000      0.110526        0.196321     0.530843      0.634252      -0.339250     -0.124550     -0.341836     -0.519811    -0.174143    -0.463314    -0.691607    1.000000\nNasdaq-100                        NaN 1.000000      0.183523        0.241768     0.735701      0.939876      -0.355631     -0.157124     -0.443419     -0.685044    -0.218199    -0.593020    -0.896611    1.000000"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#performance-during-recessions",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#performance-during-recessions",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Performance during recessions",
    "text": "Performance during recessions\n\nrecession_data = pd.read_csv('recession_indicator.csv', parse_dates=['DATE'])\nrecession_data[\"DATE\"]\n\n0      1854-12-01\n1      1855-01-01\n2      1855-02-01\n3      1855-03-01\n4      1855-04-01\n          ...    \n2033   2024-05-01\n2034   2024-06-01\n2035   2024-07-01\n2036   2024-08-01\n2037   2024-09-01\nName: DATE, Length: 2038, dtype: datetime64[ns]\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns.index = returns.index.tz_localize(None)\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n# Remove time zone information to ensure compatibility\nportfolio_returns.index = portfolio_returns.index.tz_localize(None)\n\n# Load recession periods from uploaded CSV file\nrecession_data = pd.read_csv('recession_indicator.csv', parse_dates=['DATE'])\nrecession_data.set_index('DATE', inplace=True)\nrecession_data.index = recession_data.index.tz_localize(None)\n\nrecession_daily = recession_data.reindex(pd.date_range(start=returns.index.min(), \n                                                       end=returns.index.max(), \n                                                       freq='D')).ffill()\n\n\n# # Align the indices of returns and recession data\n# recession_daily = recession_daily.reindex(returns.index, method='ffill')\n\n# Filter returns for recession periods (only select days when recession is indicated)\nrecession_days_index = recession_daily[recession_daily['VALUE'] == 1].index\n\n# Filter returns for recession periods (only select days when recession is indicated)\nportfolio_recession_returns = portfolio_returns.loc[portfolio_returns.index.intersection(recession_days_index)]\n\n# Define metrics functions\ndef annualized_return(returns):\n    compounded_growth = (1 + returns).prod()\n    n_years = len(returns) / 252\n    return compounded_growth ** (1 / n_years) - 1\n\ndef annualized_volatility(returns):\n    return returns.std() * np.sqrt(252)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02):\n    excess_returns = returns - risk_free_rate / 252\n    return excess_returns.mean() / returns.std() * np.sqrt(252)\n\ndef sortino_ratio(returns, risk_free_rate=0.02):\n    downside_returns = returns[returns &lt; 0]\n    downside_deviation = downside_returns.std() * np.sqrt(252)\n    return (returns.mean() - risk_free_rate / 252) / downside_deviation\n\ndef max_drawdown(returns):\n    cumulative = (1 + returns).cumprod()\n    peak = cumulative.cummax()\n    drawdown = (cumulative - peak) / peak\n    return drawdown.min()\n\n# Calculate metrics for mini-fund portfolio during recession periods\nportfolio_metrics_recession = {\n    'Annualized Return': annualized_return(portfolio_recession_returns),\n    'Volatility': annualized_volatility(portfolio_recession_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_recession_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_recession_returns),\n    'Max Drawdown': max_drawdown(portfolio_recession_returns)\n}\n\n# Calculate metrics for S&P 500 during recession periods\nsp500_recession_returns = returns['^GSPC'].loc[returns.index.intersection(recession_days_index)]\nsp500_metrics_recession = {\n    'Annualized Return': annualized_return(sp500_recession_returns),\n    'Volatility': annualized_volatility(sp500_recession_returns),\n    'Sharpe Ratio': sharpe_ratio(sp500_recession_returns),\n    'Sortino Ratio': sortino_ratio(sp500_recession_returns),\n    'Max Drawdown': max_drawdown(sp500_recession_returns)\n}\n\n# Calculate metrics for Nasdaq-100 during recession periods\nndx_recession_returns = returns['^NDX'].loc[returns.index.intersection(recession_days_index)]\nndx_metrics_recession = {\n    'Annualized Return': annualized_return(ndx_recession_returns),\n    'Volatility': annualized_volatility(ndx_recession_returns),\n    'Sharpe Ratio': sharpe_ratio(ndx_recession_returns),\n    'Sortino Ratio': sortino_ratio(ndx_recession_returns),\n    'Max Drawdown': max_drawdown(ndx_recession_returns)\n}\n\n# Combine results into a DataFrame\nmetrics_df_recession = pd.DataFrame([portfolio_metrics_recession, sp500_metrics_recession, ndx_metrics_recession], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(\"Portfolio Performance During Recession Periods:\")\nprint(metrics_df_recession)\n\n# Plot the cumulative returns during recession periods\ncumulative_portfolio_returns_recession = (1 + portfolio_recession_returns).cumprod()\ncumulative_sp500_returns_recession = (1 + sp500_recession_returns).cumprod()\ncumulative_ndx_returns_recession = (1 + ndx_recession_returns).cumprod()\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns_recession / cumulative_portfolio_returns_recession.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns_recession / cumulative_sp500_returns_recession.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns_recession / cumulative_ndx_returns_recession.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio (Recession)', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500 (Recession)', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100 (Recession)', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O2 vs S&P 500 and Nasdaq-100 During Recession Periods', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\n\n\nPortfolio Performance During Recession Periods:\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund           -0.031372    0.300771     -0.021338      -0.000109     -0.397179\nS&P 500             -0.193706    0.385337     -0.417644      -0.002256     -0.600109\nNasdaq-100          -0.109028    0.460166     -0.065060      -0.000393     -0.613428\n\n\n\n\n\n\n\n\n\n\nrecession_data\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\n\n# Download historical data for the stocks\nstocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\ndata = yf.download(stocks, start='2010-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Define the function to calculate portfolio performance\ndef portfolio_performance(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n    returns = np.sum(mean_returns * weights) * 252\n    std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n    sharpe_ratio = (returns - risk_free_rate) / std\n    return returns, std, sharpe_ratio\n\n# Define the objective function for optimization (negative Sharpe ratio)\ndef negative_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n    returns, std, sharpe_ratio = portfolio_performance(weights, mean_returns, cov_matrix, risk_free_rate)\n    return -sharpe_ratio\n\n# Constraints: sum of weights must be 1\ndef check_sum(weights):\n    return np.sum(weights) - 1\n\n# Boundaries: weights between 0 and 1\nbounds = tuple((0, 1) for stock in stocks)\n\n# Initial guess (equal weight distribution)\ninitial_weights = np.array([1/len(stocks)] * len(stocks))\n\n# Mean returns and covariance matrix\nmean_returns = returns.mean()\ncov_matrix = returns.cov()\n\n# Optimization using minimize from scipy.optimize\noptimal_solution = minimize(negative_sharpe_ratio, initial_weights, args=(mean_returns, cov_matrix),\n                            method='SLSQP', bounds=bounds, constraints={'type': 'eq', 'fun': check_sum})\n\noptimal_weights = optimal_solution.x\n\n# Display optimal weights\noptimal_weights_df = pd.DataFrame(optimal_weights, index=stocks, columns=['Optimal Weight'])\n\noptimal_weights_percent = optimal_weights_df * 10000000000\n\n# Display the weights in percentage form\nprint(optimal_weights_percent)\n\n#print(optimal_weights_df)\n\n[*********************100%***********************]  5 of 5 completed\n\n\n       Optimal Weight\nASML     1.639441e+01\nNVDA     2.097931e-15\nTSLA     2.852132e+01\nBRK-B    3.240187e+01\nMSFT     2.268240e+01\n\n\n\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\nfrom arch import arch_model\nfrom arch.__future__ import reindexing\n\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nprotective_etf = ['TAIL']\n\n# Combine stocks and protective ETF\nall_assets = stocks + protective_etf\n\n# Download historical data\ndata = yf.download(all_assets, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Define the function to calculate portfolio performance\ndef portfolio_performance(weights, returns, risk_free_rate=0.02, confidence_level=0.95, forecast_volatility=True):\n    mean_returns = returns.mean()\n    cov_matrix = returns.cov()\n    \n    portfolio_returns = np.dot(mean_returns, weights) * 252\n    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n    sharpe_ratio = (portfolio_returns - risk_free_rate) / portfolio_std\n\n    # Volatility calculation (GARCH(1,1) model)\n    if forecast_volatility:\n        am = arch_model(np.dot(returns, weights) * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        std_dev_daily = np.std(np.dot(returns, weights))\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = portfolio_returns / 252 * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m = var_list\n    es_1m, es_6m = es_list\n\n    return portfolio_returns, portfolio_std, sharpe_ratio, var_1m, var_6m, es_1m, es_6m\n\n# Define the objective function for optimization\ndef objective(weights, returns, risk_free_rate=0.02, confidence_level=0.95):\n    portfolio_returns, portfolio_std, sharpe_ratio, var_1m, var_6m, es_1m, es_6m = portfolio_performance(weights, returns, risk_free_rate, confidence_level)\n    # Weights for multi-objective optimization\n    w_sharpe, w_var, w_es = 1.0, 0.5, 0.5  # Adjust these weights as per preference\n    # Objective value to minimize: negative Sharpe ratio + VaR + ES\n    objective_value = (\n        w_sharpe * -sharpe_ratio +  # Maximizing Sharpe Ratio by minimizing its negative\n        w_var * (var_1m + var_6m) +  # Minimize VaR (1 month and 6 months)\n        w_es * (es_1m + es_6m)       # Minimize ES (1 month and 6 months)\n    )\n    return objective_value\n\n# Constraints: sum of weights must be 1\ndef check_sum(weights):\n    return np.sum(weights) - 1\n\n# Boundaries: weights between 0 and 1\nbounds = tuple((0, 1) for _ in all_assets)\n\n# Initial guess (equal weight distribution)\n# initial_weights = np.array([1 / len(all_assets)] * len(all_assets))\ninitial_weights = np.array([0.13, 0.18, 0.22, 0.16, 0.10, 0.11, 0.10])  # Added 10% weight to TAIL\n\n# Optimization using minimize from scipy.optimize\noptimal_solution = minimize(objective, initial_weights, args=(returns),\n                            method='SLSQP', bounds=bounds, constraints={'type': 'eq', 'fun': check_sum})\n\noptimal_weights = optimal_solution.x\n\n# Display optimal weights\noptimal_weights_df = pd.DataFrame(optimal_weights, index=all_assets, columns=['Optimal Weight'])\n#optimal_weights_percent = optimal_weights_df * 100\n\n# Display the weights in percentage form\nprint(optimal_weights_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n       Optimal Weight\nASML      0.107482   \nTSLA      0.182479   \nBRK-B     0.183304   \nMSFT      0.194264   \nABBV      0.083253   \nLMT       0.091662   \nTAIL      0.157556   \n**************************************************************************************\n**************************************************************************************\n\n\n\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\nprotective_etf = ['TAIL']\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices + protective_etf, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Calculate portfolio returns with TAIL\nportfolio_returns_with_tail = (returns[stocks + protective_etf] * optimal_weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns_without_tail = (1 + portfolio_returns_without_tail).cumprod()\ncumulative_portfolio_returns_with_tail = (1 + portfolio_returns_with_tail).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n# Define metrics functions\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / returns.std() * np.sqrt(periods_per_year)\n\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / downside_returns.std() * np.sqrt(periods_per_year)\n\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()\n\ndef alpha_beta(returns, benchmark_returns):\n    cov_matrix = np.cov(returns, benchmark_returns)\n    beta = cov_matrix[0, 1] / cov_matrix[1, 1]\n    alpha = returns.mean() - beta * benchmark_returns.mean()\n    return alpha, beta\n\ndef var_es(returns, confidence_level=0.95, forecast_volatility=False):\n    # Calculate the mean return (daily)\n    mean_return_daily = returns.mean()\n\n    # Volatility calculation\n    if forecast_volatility:\n        # Fit GARCH(1,1) model to the return series\n        am = arch_model(returns * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        # Use last forecasted volatility (scaled back to decimal)\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        # Use historical standard deviation\n        std_dev_daily = returns.std()\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n# Calculate Alpha, Beta, VaR, and ES for portfolios and benchmarks\nmarket_returns = returns['^GSPC']\n\nalpha_portfolio_without_tail, beta_portfolio_without_tail = alpha_beta(portfolio_returns_without_tail, market_returns)\nalpha_portfolio_with_tail, beta_portfolio_with_tail = alpha_beta(portfolio_returns_with_tail, market_returns)\n\nvar_1m_without_tail, var_6m_without_tail, var_1y_without_tail, es_1m_without_tail, es_6m_without_tail, es_1y_without_tail = var_es(portfolio_returns_without_tail, forecast_volatility=True)\nvar_1m_with_tail, var_6m_with_tail, var_1y_with_tail, es_1m_with_tail, es_6m_with_tail, es_1y_with_tail = var_es(portfolio_returns_with_tail, forecast_volatility=True)\n\n# Calculate metrics for mini-fund portfolio without TAIL\nportfolio_metrics_without_tail = {\n    'Alpha': alpha_portfolio_without_tail,\n    'Beta': beta_portfolio_without_tail,\n    'Annualized Return': annualized_return(portfolio_returns_without_tail),\n    'Volatility': annualized_volatility(portfolio_returns_without_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_without_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_without_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_without_tail),\n    'VaR 1M (95%)': var_1m_without_tail,\n    'VaR 6M (95%)': var_6m_without_tail,\n    'VaR 1Y (95%)': var_1y_without_tail,\n    'ES 1M (95%)': es_1m_without_tail,\n    'ES 6M (95%)': es_6m_without_tail,\n    'ES 1Y (95%)': es_1y_without_tail\n}\n\n# Calculate metrics for mini-fund portfolio with TAIL\nportfolio_metrics_with_tail = {\n    'Alpha': alpha_portfolio_with_tail,\n    'Beta': beta_portfolio_with_tail,\n    'Annualized Return': annualized_return(portfolio_returns_with_tail),\n    'Volatility': annualized_volatility(portfolio_returns_with_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_with_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_with_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_with_tail),\n    'VaR 1M (95%)': var_1m_with_tail,\n    'VaR 6M (95%)': var_6m_with_tail,\n    'VaR 1Y (95%)': var_1y_with_tail,\n    'ES 1M (95%)': es_1m_with_tail,\n    'ES 6M (95%)': es_6m_with_tail,\n    'ES 1Y (95%)': es_1y_with_tail\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Beta': 1,\n    'R-Square': 1,\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC']),\n    'VaR 1M (95%)': var_es(returns['^GSPC'])[0],\n    'VaR 6M (95%)': var_es(returns['^GSPC'])[1],\n    'VaR 1Y (95%)': var_es(returns['^GSPC'])[2],\n    'ES 1M (95%)': var_es(returns['^GSPC'])[3],\n    'ES 6M (95%)': var_es(returns['^GSPC'])[4],\n    'ES 1Y (95%)': var_es(returns['^GSPC'])[5]\n}\n\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_without_tail_normalized = cumulative_portfolio_returns_without_tail / cumulative_portfolio_returns_without_tail.iloc[0] * 100\ncumulative_portfolio_returns_with_tail_normalized = cumulative_portfolio_returns_with_tail / cumulative_portfolio_returns_with_tail.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio without TAIL\nplt.plot(cumulative_portfolio_returns_without_tail_normalized, label='Portfolio-GV-O2', color='red')\n\n# Plot the portfolio with TAIL\nplt.plot(cumulative_portfolio_returns_with_tail_normalized, label='Portfolio-GV-TAIL-O', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-TAIL-O vs Portfolio-GV-O2 (no TAIL) vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics_without_tail, portfolio_metrics_with_tail, sp500_metrics, ndx_metrics], \n                          index=['Portfolio-GV-O2', 'Portfolio-GV-TAIL-O', 'S&P 500', 'Nasdaq-100'])\n\n# Adjust display settings to show all columns without wrapping\npd.set_option('display.max_columns', None)          # Show all columns\npd.set_option('display.expand_frame_repr', False)   # Prevent DataFrame from wrapping\npd.set_option('display.width', 1000)                # Set width to large enough value\npd.set_option('display.colheader_justify', 'center') # Center align the headers\npd.set_option('display.float_format', '{:.6f}'.format)  # Format float to 6 decimal places\n\nprint(metrics_df)\n\n[*********************100%***********************]  9 of 9 completed\n\n\n\n\n\n\n\n\n\n                      Alpha    Beta    Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown  VaR 1M (95%)  VaR 6M (95%)  VaR 1Y (95%)  ES 1M (95%)  ES 6M (95%)  ES 1Y (95%)  R-Square\nPortfolio-GV-O2     0.000622 1.086780      0.299328        0.243712     1.115379      1.433413      -0.350050     -0.106745     -0.347816     -0.577361    -0.142540    -0.435495    -0.701359         NaN\nPortfolio-GV-TAIL-O 0.000554 0.837946      0.251487        0.196279     1.139983      1.500943      -0.275660     -0.088748     -0.289508     -0.480820    -0.118467    -0.362305    -0.583771         NaN\nS&P 500                  NaN 1.000000      0.110526        0.196321     0.530843      0.634252      -0.339250     -0.124550     -0.341836     -0.519811    -0.174143    -0.463314    -0.691607    1.000000\nNasdaq-100               NaN 1.000000      0.183523        0.241768     0.735701      0.939876      -0.355631     -0.157124     -0.443419     -0.685044    -0.218199    -0.593020    -0.896611    1.000000"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#cwarp",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#cwarp",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "CWARP",
    "text": "CWARP\nhttps://docsend.com/view/teaqrcewe7ht423x\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport matplotlib.pyplot as plt\n\n# Define a function to calculate Sortino Ratio\n# Rp: Return of Portfolio, Rf: Risk-Free Rate, sigma_dp: Downside Deviation of Portfolio\ndef sortino_ratio(Rp, Rf, sigma_dp):\n    return (Rp - Rf) / sigma_dp\n\n# Define a function to calculate Return to Max Drawdown (RMDD)\n# Rp: Return of Portfolio, Lp: Trough value of Replacement Portfolio, Pp: Peak value of Replacement Portfolio\ndef return_to_max_drawdown(Rp, Lp, Pp):\n    return (Rp - Rf) / ((Lp - Pp) / Pp)\n\n# Define the CWARP Calculation Function\n# Sn: Sortino Ratio of New Portfolio, Sp: Sortino Ratio of Replacement Portfolio\n# RMDDn: Return to Max Drawdown of New Portfolio, RMDDp: Return to Max Drawdown of Replacement Portfolio\ndef cwarp_calculation(Sn, Sp, RMDDn, RMDDp):\n    return (np.sqrt((Sn / Sp) * (RMDDn / RMDDp)) - 1) * 100\n\n# Download historical data for portfolio and benchmarks\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Risk-free rate (assumed)\nRf = 0.02 / 252  # Annual risk-free rate divided by trading days\n\n# Calculate Sortino Ratios for Portfolio, S&P 500, and Nasdaq-100\nportfolio_downside_std = portfolio_returns[portfolio_returns &lt; 0].std()\nsp500_downside_std = returns['^GSPC'][returns['^GSPC'] &lt; 0].std()\nndx_downside_std = returns['^NDX'][returns['^NDX'] &lt; 0].std()\n\nSp_portfolio = sortino_ratio(portfolio_returns.mean(), Rf, portfolio_downside_std)\nSp_sp500 = sortino_ratio(returns['^GSPC'].mean(), Rf, sp500_downside_std)\nSp_ndx = sortino_ratio(returns['^NDX'].mean(), Rf, ndx_downside_std)\n\n# Calculate Return to Max Drawdowns for Portfolio, S&P 500, and Nasdaq-100\nportfolio_peak = portfolio_returns.cummax()\nportfolio_drawdown = (portfolio_returns - portfolio_peak) / portfolio_peak\nLp_portfolio = portfolio_drawdown.min()\nPp_portfolio = portfolio_peak.max()\nRMDD_portfolio = return_to_max_drawdown(portfolio_returns.mean(), Lp_portfolio, Pp_portfolio)\n\nsp500_peak = returns['^GSPC'].cummax()\nsp500_drawdown = (returns['^GSPC'] - sp500_peak) / sp500_peak\nLp_sp500 = sp500_drawdown.min()\nPp_sp500 = sp500_peak.max()\nRMDD_sp500 = return_to_max_drawdown(returns['^GSPC'].mean(), Lp_sp500, Pp_sp500)\n\nndx_peak = returns['^NDX'].cummax()\nndx_drawdown = (returns['^NDX'] - ndx_peak) / ndx_peak\nLp_ndx = ndx_drawdown.min()\nPp_ndx = ndx_peak.max()\nRMDD_ndx = return_to_max_drawdown(returns['^NDX'].mean(), Lp_ndx, Pp_ndx)\n\n# Calculate CWARP for Portfolio vs S&P 500 and Nasdaq-100\ncwarp_portfolio_sp500 = cwarp_calculation(Sp_portfolio, Sp_sp500, RMDD_portfolio, RMDD_sp500)\ncwarp_portfolio_ndx = cwarp_calculation(Sp_portfolio, Sp_ndx, RMDD_portfolio, RMDD_ndx)\n\nprint(f\"CWARP Value (Portfolio vs S&P 500): {cwarp_portfolio_sp500}\")\nprint(f\"CWARP Value (Portfolio vs Nasdaq-100): {cwarp_portfolio_ndx}\")\n\n[*********************100%***********************]  8 of 8 completed\n\n\nCWARP Value (Portfolio vs S&P 500): 162.3071593499393\nCWARP Value (Portfolio vs Nasdaq-100): 78.84851594077611"
  },
  {
    "objectID": "notebooks/Follow-The-Money.html",
    "href": "notebooks/Follow-The-Money.html",
    "title": "Follow the Money: Analyzing Capital Flows from Central Banks to Key Sectors",
    "section": "",
    "text": "import pandas as pd\nimport requests\nfrom datetime import datetime\n\n\nFRED_API_KEY = '2548a719cdd51baaa5e013839926fccc'\nstart_date = '2019-01-01'\nend_date = '2025-01-01'\n\n\ndef fetch_fred_series(series_id, start_date, end_date):\n    series_id = series_id  \n    url = f\"https://api.stlouisfed.org/fred/series/observations?series_id={series_id}&api_key={FRED_API_KEY}&file_type=json&observation_start={start_date}&observation_end={end_date}\"\n    response = requests.get(url)\n    data = response.json()\n    df = pd.DataFrame(data['observations'])\n    df['value'] = df['value'].astype(float)\n    return df\n\ndef fetch_world_bank_indicators(series_id, start_year, end_year):\n    url = f\"http://api.worldbank.org/v2/country/USA/indicator/{series_id}?format=json&date={start_year}:{end_year}\"\n    response = requests.get(url)\n    data = response.json()\n    df = pd.DataFrame(data[1])\n    df['value'] = df['value'].astype(float)\n    return df\n\n# Function to fetch money supply data from FRED\ndef fetch_money_supply(start_date, end_date):\n    return fetch_fred_series('M2SL', start_date, end_date)\n\ndef fetch_real_estate_investment(start_date, end_date):\n    res_date = fetch_fred_series('A012RC1Q027SBEA', start_date, end_date)\n    nonres_data = fetch_fred_series('B009RC1Q027SBEA', start_date, end_date)\n    return res_date['value'].sum() + nonres_data['value'].sum()\n\ndef fetch_tech_investment(start_date, end_date):\n    # Real Gross Private Domestic Investment: Fixed Investment: Nonresidential: Intellectual Property Products (Y001RX1Q020SBEA)\n    direct_investment_ip_data = fetch_fred_series('Y001RX1Q020SBEA', start_date, end_date)\n\n    # Private fixed investment in information processing equipment and software\n    direct_investment_tech_data = fetch_fred_series('A679RC1Q027SBEA', start_date, end_date)\n\n    # # Households and Nonprofit Organizations; Corporate Equities; Asset, Level/1000 (HNOCEAQ027S)\n    # indirect_retail_investment = fetch_fred_series('HNOCEAQ027S', start_date, end_date)\n\n    return direct_investment_ip_data['value'].sum() + direct_investment_tech_data['value'].sum()\n\ndef fetch_healthcare_investment(start_date, end_date):\n    start_year = datetime.strptime(start_date, '%Y-%m-%d').year\n    end_year = datetime.strptime(end_date, '%Y-%m-%d').year\n    return fetch_world_bank_indicators('SH.XPD.CHEX.GD.ZS', start_year, end_year)\n\n\n0.1 Step 2: Fetching Sector Investment Data from FRED and World Bank\nFor fetching sector-specific investment data, we will use series IDs from FRED and the World Bank API. Here are some examples:\n\nmoney_supply_data = fetch_money_supply(start_date, end_date)\ntotal_money_supply = money_supply_data['value'].sum()\nprint(f\"Total Money Supply: {total_money_supply}\")\n\nTotal Money Supply: 1419632.0\n\n\n\nreal_estate_data = fetch_real_estate_investment(start_date, end_date)\nprint(f\"Total Real Estate Investment: {real_estate_data}\")\n\nTotal Real Estate Investment: 45236.283\n\n\n\ntotal_tech_investment = fetch_tech_investment(start_date, end_date)\nprint(f\"Total Tech Investment: {total_tech_investment}\")\n\nTotal Tech Investment: 58251.611000000004\n\n\n\n# healthcare_data = fetch_healthcare_investment(start_date, end_date)\n# total_healthcare_investment = healthcare_data['value'].sum()\n# print(f\"Total Healthcare Investment: {total_healthcare_investment}\")\n\npharmaceuticals_data = fetch_fred_series('Y009RC1A027NBEA', start_date, end_date)  # Example series for pharmaceuticals\n\n# Private fixed investment in equipment and software: Nonresidential: Information processing equipment and software: Medical equipment and instruments (W176RC1A027NBEA)\nmedical_devices_data = fetch_fred_series('W176RC1A027NBEA', start_date, end_date)  # Example series for medical devices\n\n# Private fixed investment: Nonresidential: Structures: Commercial and health care (W001RC1Q027SBEA)\nservices_data = fetch_fred_series('W001RC1Q027SBEA', start_date, end_date)  # Example series for healthcare services\n\n# Calculate total investments for pharmaceuticals, medical devices, and services\ntotal_pharmaceuticals_investment = pharmaceuticals_data['value'].sum()\ntotal_medical_devices_investment = medical_devices_data['value'].sum()\ntotal_services_investment = services_data['value'].sum()\ntotal_healthcare_investment = total_pharmaceuticals_investment + total_medical_devices_investment + total_services_investment\nprint(f\"Total Healthcare Investment: {total_healthcare_investment}\")\n\nTotal Healthcare Investment: 7629.786999999999\n\n\n\n\n0.2 Step 3: Sector Breakdowns\n\n0.2.1 Real Estate\n\n\n# Total Private Construction Spending: Residential in the United States (PRRESCONS)\n# Private fixed investment: Residential: Structures (A012RC1Q027SBEA)\nresidential_data = fetch_fred_series('A012RC1Q027SBEA', start_date, end_date)\n\n# Total Private Construction Spending: Nonresidential in the United States (PNRESCONS)\n# Private fixed investment: Nonresidential: Structures (B009RC1Q027SBEA)\ncommercial_data = fetch_fred_series('B009RC1Q027SBEA', start_date, end_date)\n\n# Calculate total investments for residential and commercial real estate\ntotal_residential_investment = residential_data['value'].sum()\ntotal_commercial_investment = commercial_data['value'].sum()\ntotal_real_estate_investment = total_residential_investment + total_commercial_investment\n\n# Calculate the ratios\nresidential_ratio = total_residential_investment / total_real_estate_investment\ncommercial_ratio = total_commercial_investment / total_real_estate_investment\n\nprint(f\"Residential Ratio: {residential_ratio}\")\nprint(f\"Commercial Ratio: {commercial_ratio}\")\n\nResidential Ratio: 0.5777855576683876\nCommercial Ratio: 0.4222144423316124\n\n\n\n\n0.2.2 Tech Investments\n\n# Fetching data for Direct Investment in Technology and Tech Stocks\n\n# Real Gross Private Domestic Investment: Fixed Investment: Nonresidential: Intellectual Property Products (Y001RX1Q020SBEA)\ndirect_investment_ip_data = fetch_fred_series('Y001RX1Q020SBEA', start_date, end_date)\n\n# Real Gross Private Domestic Investment: Fixed Investment: Nonresidential: Equipment (Y033RX1Q020SBEA)\ndirect_investment_eqp_data = fetch_fred_series('Y033RX1Q020SBEA', start_date, end_date)\n\n# Private fixed investment in information processing equipment and software\ndirect_investment_tech_data = fetch_fred_series('A679RC1Q027SBEA', start_date, end_date)\n\n# Calculate total investments for direct investment and tech stocks\ntotal_direct_investment = direct_investment_ip_data['value'].sum() + direct_investment_tech_data['value'].sum()\n\n# Calculate the ratios\ntech_and_ip_ratio = direct_investment_tech_data['value'].sum() / total_direct_investment\nip_ratio = 1 - tech_and_ip_ratio\n\n\nprint(f\"Tech & IP Ratio: {tech_and_ip_ratio}\")\nprint(f\"IP Ratio: {ip_ratio}\")\n\nTech & IP Ratio: 0.44079311385911707\nIP Ratio: 0.559206886140883\n\n\n\n# Quarterly Financial Report: U.S. Corporations: Pharmaceuticals and Medicines: Net Sales, Receipts, and Operating Revenues (QFR101385USNO)\n\n# Fetching data for Pharmaceuticals, Medical Devices, and Healthcare Services\n\n# Private Fixed Investment in Intellectual Property Products: Research and development: Business: Manufacturing: Pharmaceutical and medicine manufacturing (Y009RC1A027NBEA)\npharmaceuticals_data = fetch_fred_series('Y009RC1A027NBEA', start_date, end_date)  # Example series for pharmaceuticals\n\n# Private fixed investment in equipment and software: Nonresidential: Information processing equipment and software: Medical equipment and instruments (W176RC1A027NBEA)\nmedical_devices_data = fetch_fred_series('W176RC1A027NBEA', start_date, end_date)  # Example series for medical devices\n\n# Private fixed investment: Nonresidential: Structures: Commercial and health care (W001RC1Q027SBEA)\nservices_data = fetch_fred_series('W001RC1Q027SBEA', start_date, end_date)  # Example series for healthcare services\n\n# Calculate total investments for pharmaceuticals, medical devices, and services\ntotal_pharmaceuticals_investment = pharmaceuticals_data['value'].sum()\ntotal_medical_devices_investment = medical_devices_data['value'].sum()\ntotal_services_investment = services_data['value'].sum()\ntotal_healthcare_investment = total_pharmaceuticals_investment + total_medical_devices_investment + total_services_investment\n\n# Calculate the ratios\npharmaceuticals_ratio = total_pharmaceuticals_investment / total_healthcare_investment\nmedical_devices_ratio = total_medical_devices_investment / total_healthcare_investment\nservices_ratio = total_services_investment / total_healthcare_investment\n\nprint(f\"Pharmaceuticals Ratio: {pharmaceuticals_ratio}\")\nprint(f\"Medical Devices Ratio: {medical_devices_ratio}\")\nprint(f\"Services Ratio: {services_ratio}\")\n\nPharmaceuticals Ratio: 0.101622758276214\nMedical Devices Ratio: 0.09037329613526564\nServices Ratio: 0.8080039455885203\n\n\n\n\n\n0.3 Step 4: Aggregate Data with Calculated Ratios\n\nsector_data = {\n    'Real Estate': total_real_estate_investment,\n    'Residential': total_real_estate_investment * residential_ratio,\n    'Commercial': total_real_estate_investment * commercial_ratio,\n    'Tech': total_tech_investment,\n    'Tech & Software': total_tech_investment * tech_and_ip_ratio,\n    'IP': total_tech_investment * ip_ratio,\n    'Healthcare': total_healthcare_investment,\n    'Pharmaceuticals': total_healthcare_investment * pharmaceuticals_ratio,\n    'Medical Devices': total_healthcare_investment * medical_devices_ratio,\n    'Services': total_healthcare_investment * services_ratio\n}\n\n\nsector_data\n\n{'Real Estate': 45236.283,\n 'Residential': 26136.871000000003,\n 'Commercial': 19099.412,\n 'Tech': 58251.611000000004,\n 'Tech & Software': 25676.909,\n 'IP': 32574.70200000001,\n 'Healthcare': 7629.786999999999,\n 'Pharmaceuticals': 775.3599999999999,\n 'Medical Devices': 689.529,\n 'Services': 6164.897999999999}\n\n\n\n\n0.4 Step 5: Create the Sankey Diagram\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.renderers.default = \"notebook_connected\"\n#pio.renderers.default = \"notebook_connected\" # Or \"iframe_connected\"\n\n# Define nodes\nnodes = [\n    'Central Banks',\n    'Real Estate',\n    'Residential',\n    'Commercial',\n    'Tech',\n    'Tech & Software',\n    'IP',\n    'Healthcare',\n    'Pharmaceuticals',\n    'Medical Devices',\n    'Services'\n]\n\n\n# Define links based on fetched data and calculated ratios\nlinks = [\n    {'source': 'Central Banks', 'target': 'Real Estate', 'value': sector_data['Real Estate']},\n    {'source': 'Central Banks', 'target': 'Tech', 'value': sector_data['Tech']},\n    {'source': 'Central Banks', 'target': 'Healthcare', 'value': sector_data['Healthcare']},\n    {'source': 'Real Estate', 'target': 'Residential', 'value': sector_data['Residential']},\n    {'source': 'Real Estate', 'target': 'Commercial', 'value': sector_data['Commercial']},\n    {'source': 'Tech', 'target': 'Tech & Software', 'value': sector_data['Tech & Software']},\n    {'source': 'Tech', 'target': 'IP', 'value': sector_data['IP']},\n    {'source': 'Healthcare', 'target': 'Pharmaceuticals', 'value': sector_data['Pharmaceuticals']},\n    {'source': 'Healthcare', 'target': 'Medical Devices', 'value': sector_data['Medical Devices']},\n    {'source': 'Healthcare', 'target': 'Services', 'value': sector_data['Services']}\n]\n\n# Convert node names to indices for plotly\nnode_indices = {node: i for i, node in enumerate(nodes)}\n\nsankey_links = {\n    'source': [node_indices[link['source']] for link in links],\n    'target': [node_indices[link['target']] for link in links],\n    'value': [link['value'] for link in links]\n}\n\n# Create Sankey diagram\nfig = go.Figure(go.Sankey(\n    node=dict(\n        pad=15,\n        thickness=20,\n        line=dict(color=\"black\", width=0.5),\n        label=nodes\n    ),\n    link=dict(\n        source=sankey_links['source'],\n        target=sankey_links['target'],\n        value=sankey_links['value']\n    )\n))\n\nfig.update_layout(title_text=\"Follow the Money: Central Banks to Various Sectors (2019-2024)\", font_size=10)\nfig.show()\n\n\n\n\n\n# Define links based on fetched data and calculated ratios\nlinks = [\n    #{'source': 'Central Banks', 'target': 'Real Estate', 'value': sector_data['Real Estate']},\n    {'source': 'Central Banks', 'target': 'Tech', 'value': sector_data['Tech']},\n    {'source': 'Central Banks', 'target': 'Healthcare', 'value': sector_data['Healthcare']},\n    #{'source': 'Real Estate', 'target': 'Residential', 'value': sector_data['Residential']},\n    #{'source': 'Real Estate', 'target': 'Commercial', 'value': sector_data['Commercial']},\n    {'source': 'Tech', 'target': 'Tech & Software', 'value': sector_data['Tech & Software']},\n    {'source': 'Tech', 'target': 'IP', 'value': sector_data['IP']},\n    {'source': 'Healthcare', 'target': 'Pharmaceuticals', 'value': sector_data['Pharmaceuticals']},\n    {'source': 'Healthcare', 'target': 'Medical Devices', 'value': sector_data['Medical Devices']},\n    {'source': 'Healthcare', 'target': 'Services', 'value': sector_data['Services']}\n]\n\n# Convert node names to indices for plotly\nnode_indices = {node: i for i, node in enumerate(nodes)}\n\nsankey_links = {\n    'source': [node_indices[link['source']] for link in links],\n    'target': [node_indices[link['target']] for link in links],\n    'value': [link['value'] for link in links]\n}\n\n# Create Sankey diagram\nfig = go.Figure(go.Sankey(\n    node=dict(\n        pad=15,\n        thickness=20,\n        line=dict(color=\"black\", width=0.5),\n        label=nodes\n    ),\n    link=dict(\n        source=sankey_links['source'],\n        target=sankey_links['target'],\n        value=sankey_links['value']\n    )\n))\n\nfig.update_layout(title_text=\"Follow the Money (w/o RE): Central Banks to Various Sectors (2019-2024)\", font_size=10)\nfig.show()"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Notebooks",
    "section": "",
    "text": "Browse my research notebooks below.\n\n\n\n\n\n\n\n\n\nThe Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling\n\n\nThis analysis challenges the traditional association between extended formal education and improved quality of life (QoL) by examining the structural breakdown of professional labor returns. Utilizing longitudinal data from 1979 to 2025, the study identifies a significant decoupling of real wages from productivity growth and a staggering disparity between tuition inflation and median income. Through Granger Causality tests, the research validates that expansions in the M2 money supply serve as a primary predictor for rising educational costs, effectively devaluing the net financial return of degrees. Furthermore, the application of a “Career Sharpe Ratio” framework reveals that higher education no longer functions as career insurance; rather, increased credentials often lead to higher income volatility that offsets marginal gains. The findings suggest a shift in the role of tertiary education from a tool for knowledge acquisition to a mechanism for “elite inclusion,” a transition that correlates with rising underemployment and a documented decline in the mental health and job satisfaction of highly educated professionals.\n\n\n\n\n\nJan 9, 2026\n\n\n\n\n\n\n\nEmpirical Validation of the 1971 Fiscal Structural Break: Analysis of U.S. Real Individual Income Tax Revenue\n\n\nThis document provides empirical validation for a structural break in U.S. fiscal extraction occurring at the 1971 monetary pivot point. Utilizing an Interrupted Time Series framework and a Chow Test on inflation-adjusted U.S. Individual Income Tax receipts (normalized to 2024 dollars), the analysis confirms a significant shift in the growth trajectory of tax extraction following the transition to a pure fiat regime.\n\n\n\n\n\nJan 2, 2026\n\n\n\n\n\n\n\nLiquidity Regimes and the Death (and Return) of Valuations\n\n\nThis research utilizes Sparse PCA and Hidden Markov Models to demonstrate that equity factor premiums are strictly regime-dependent, revealing that while growth dominates during expansive liquidity, value and conservative investment factors revive significantly during periods of liquidity tightening. Findings prove that market valuations remain relevant but fluctuate predictably, with the S&P 500 trading approximately 0.43σ more expensive in high-liquidity states compared to tight regimes.\n\n\n\n\n\nDec 14, 2025\n\n\n\n\n\n\n\nFollow the Money: Analyzing Capital Flows from Central Banks to Key Sectors\n\n\nThis notebook provides a comprehensive technical framework for tracing capital distribution from central banks into the real estate, technology, and healthcare sectors. By utilizing Python to interface with the Federal Reserve Economic Data (FRED) and World Bank APIs, the analysis retrieves critical economic indicators for the period spanning 2019 to 2025. The study quantifies total investment volumes and calculates specific ratios for sub-sectors, including residential versus commercial real estate, information processing equipment versus software, and pharmaceutical research versus medical services. The final output utilizes the Plotly library to construct interactive Sankey diagrams that visualize the transition of liquidity through the financial system, highlighting how money moves from central sources to diverse economic pillars.\n\n\n\n\n\nMay 22, 2025\n\n\n\n\n\n\n\nPrivate Investment Trends in Aerospace and Defense (2019–2024)\n\n\nThis report analyzes private investment activity in the aerospace and defense sectors across North America and Europe from January 2019 to May 2024. Despite a sectoral investment CAGR of -20% since 2019, the market remains dominated by high-value “scaleup” funding rounds exceeding $100M for industry leaders such as SpaceX, Anduril Industries, and Sierra Space. California serves as the primary geographic epicenter for this capital, with Hawthorne and Costa Mesa leading in total money raised. Key investment drivers include space travel, satellite communications, and advanced manufacturing, backed by prominent lead investors like Valor Equity Partners, Sequoia Capital, and BlackRock. Furthermore, the report highlights the disparity in military spending as a percentage of GDP, with the United States maintaining a significantly higher ratio than its European counterparts, providing a stable backdrop for continued private sector innovation.\n\n\n\n\n\nMay 31, 2024\n\n\n\n\n\n\n\nMini-Fund Volatility Regime Backtest Analysis (2000–2024)\n\n\nThis study evaluates the long-term performance and risk metrics of a high-growth “Mini-Fund” portfolio—consisting of ASML, TSLA, BRK-B, MSFT, ABBV, and LMT—relative to the S&P 500 and Nasdaq-100 benchmarks. Using Hidden Markov Models to categorize market behavior into low, mid, and high volatility regimes, the analysis demonstrates that the optimized “Portfolio-GV-O2” consistently outpaced benchmarks, delivering an annualized return of 30.8% and a Sharpe ratio of 1.25, compared to 11.4% and 0.60 for the S&P 500. The portfolio proved highly resilient during historical recessions, yielding a -2.9% return while the S&P 500 declined by 19.4%. Furthermore, the strategic inclusion of the protective TAIL ETF reduced the maximum drawdown from -35.0% to -27.6%, enhancing downside protection without sacrificing significant growth. CWARP values of 162.3 against the S&P 500 and 78.8 against the Nasdaq-100 further validate the portfolio’s superior risk-adjusted efficiency.\n\n\n\n\n\nMay 22, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/aerospace-defense.html",
    "href": "notebooks/aerospace-defense.html",
    "title": "Private Investment Trends in Aerospace and Defense (2019–2024)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport adjustText\nCode\n####### WORKING #######\n#!pip install easy-exchange-rates\n#!pip install tqdm\nCode\ndf = pd.read_csv(\"Recipe_CB_Aerospace_and_Defense_Since2019_2.csv\")\nCode\ndf.head(5)\n\n\n\n\n\n\n\n\n\nname\nmoney_raised\nannounced_date\nlocation\nindustries\ndescription\nfunding_type\nlead_investors\ntotal_funding\n\n\n\n\n0\nSpaceX\n$1,724,965,480\nMay 24, 2022\nHawthorne, California, United States, North Am...\nAdvanced Materials, Aerospace, Manufacturing, ...\nSpaceX is an aviation and aerospace company th...\nVenture - Series Unknown\n—\n$9,779,343,846\n\n\n1\nAnduril Industries\n$1,480,000,000\nDec 2, 2022\nCosta Mesa, California, United States, North A...\nAerospace, Government, Military, National Secu...\nAnduril Industries is a defense product compan...\nSeries E\nValor Equity Partners\n$2,171,000,000\n\n\n2\nSierra Space\n$1,400,000,000\nNov 19, 2021\nLouisville, Colorado, United States, North Ame...\nAdvanced Materials, Aerospace, Industrial Manu...\nSierra Space is a commercial space company tha...\nSeries A\nCoatue, General Atlantic, Moore Strategic Vent...\n$1,690,000,000\n\n\n3\nOneWeb\n$1,250,000,000\nMar 18, 2019\nLondon, England, United Kingdom, Europe\nAerospace, Internet, Satellite Communication, ...\nOneWeb is building a space-based communication...\nVenture - Series Unknown\nSoftBank\n$4,700,000,000\n\n\n4\nSpaceX\n$850,000,000\nFeb 16, 2021\nHawthorne, California, United States, North Am...\nAdvanced Materials, Aerospace, Manufacturing, ...\nSpaceX is an aviation and aerospace company th...\nVenture - Series Unknown\nSequoia Capital\n$9,779,343,846"
  },
  {
    "objectID": "notebooks/aerospace-defense.html#plots",
    "href": "notebooks/aerospace-defense.html#plots",
    "title": "Private Investment Trends in Aerospace and Defense (2019–2024)",
    "section": "Plots",
    "text": "Plots\n\n\nCode\nfrom plotnine import ggplot, geom_bar, scale_x_date, scale_y_continuous, aes, stat_smooth, facet_wrap, options, theme_classic, labs, theme, element_text, geom_line, coord_flip, scale_size_continuous, geom_text, geom_label, scale_fill_manual, geom_tile, scale_colour_continuous, theme_bw, scale_colour_manual, scale_color_discrete, geom_point, geom_histogram, after_stat"
  },
  {
    "objectID": "notebooks/aerospace-defense.html#cagr-in-deal-value---preparing-a-derived-dataset",
    "href": "notebooks/aerospace-defense.html#cagr-in-deal-value---preparing-a-derived-dataset",
    "title": "Private Investment Trends in Aerospace and Defense (2019–2024)",
    "section": "CAGR in Deal Value - Preparing a derived dataset",
    "text": "CAGR in Deal Value - Preparing a derived dataset\n\n\nCode\ndf_spacex = df_world_raw[df_world_raw['name'] == 'Hadrian']\n\n\n'Andreessen Horowitz, Lux Capital,Founders Fund'\n\n\n\n\nCode\n# def weighted_average(data):\n#     d = {}\n#     d['d1_wa'] = np.average(data['d1'], weights=data['weights'])\n#     d['d2_wa'] = np.average(data['d2'], weights=data['weights'])\n#     return pd.Series(d)\n# Call the groupby apply method with our custom function:\n\n# df.groupby('group').apply(weighted_average)\n\n#        d1_wa  d2_wa\n# group              \n# a        9.0    2.2\n# b       58.0   13.2\n\ndef first_last_deal_flow(data):\n    d = {}\n    data_sorted = data.sort_values(by = [\"announced_date\"])\n    d['first_deal_value'] = data_sorted.loc[data_sorted.index[0], 'money_raised_usd']\n    d['first_deal_date'] = data_sorted.loc[data_sorted.index[0], 'announced_date']\n    d['last_deal_value'] = data_sorted.loc[data_sorted.index[-1], 'money_raised_usd']\n    d['last_deal_date'] = data_sorted.loc[data_sorted.index[-1], 'announced_date']\n    d['last_deal_year'] = data_sorted.loc[data_sorted.index[-1], 'announced_year']\n    d['deal_span_years'] = round((d['last_deal_date'] - d['first_deal_date'])/pd.Timedelta(days=365),2)\n    d['total_funding_usd'] = data_sorted.loc[data_sorted.index[0], 'total_funding_usd']\n    d['city'] = data_sorted.loc[data_sorted.index[0], 'city']\n    d['country'] = data_sorted.loc[data_sorted.index[0], 'country']\n    d['region'] = data_sorted.loc[data_sorted.index[0], 'region']\n    d['sector'] = data_sorted.loc[data_sorted.index[0], 'sector']\n    d['lead_investors'] = ','.join(pd.Series(data_sorted['lead_investors'].str.split(\",\").explode().unique()).where(lambda x: x != \"—\").dropna())\n    d['industries'] = data_sorted.loc[data_sorted.index[0], 'industries']\n    d['deals'] = len(data_sorted)\n    d['funding_recency'] = \"recent\" if (abs(d['last_deal_year'] - datetime.now().year) &lt;= 2) else \"older\"\n    d['deal_growth_cagr'] = round((d['last_deal_value']/d['first_deal_value'])**(1./d['deal_span_years'])-1,\n                                  2)*100 if d['deal_span_years'] &gt; 0 else 0\n    return pd.Series(d)\n\ndf_world_first_last_deal_values = ( \n    df_world_raw\n    .groupby(['name'])\n    .apply(first_last_deal_flow)\n)\n\n# Ignore outliers - top 2 percentile\ndf_world_first_last_deal_values['deal_growth_cagr'] = df_world_first_last_deal_values['deal_growth_cagr'].clip(upper=df_world_first_last_deal_values['deal_growth_cagr'].quantile(0.98))\n\n# Sort\ndf_world_first_last_deal_values = df_world_first_last_deal_values.sort_values(by = [\"deal_growth_cagr\", \n                                                                                    \"total_funding_usd\", \n                                                                                    \"last_deal_date\"], \n                                                                              ascending=False)\n\ndf_world_first_last_deal_values = df_world_first_last_deal_values.reset_index()\ndf_world_first_last_deal_values.head(10)\n\n\n\n\n\n\n\n\n\nname\nfirst_deal_value\nfirst_deal_date\nlast_deal_value\nlast_deal_date\nlast_deal_year\ndeal_span_years\ntotal_funding_usd\ncity\ncountry\nregion\nsector\nlead_investors\nindustries\ndeals\nfunding_recency\ndeal_growth_cagr\n\n\n\n\n0\nSkyryse\n2500000.0\n2020-05-15\n200000000.0\n2021-10-27\n2021\n1.45\n240500000.0\nEl Segundo\nUnited States\nNorth America\naerospace\nFidelity, Monashee Investment Management\n[Aerospace, Air Transportation, Internet, Transportation]\n2\nolder\n1066.4\n\n\n1\nAeroVanti\n9750000.0\n2022-07-21\n100000000.0\n2022-10-19\n2022\n0.25\n109750000.0\nAnnapolis\nUnited States\nNorth America\naerospace\nNetwork 1 Financial Securities\n[Aerospace, Air Transportation, Transportation]\n2\nrecent\n1066.4\n\n\n2\nBRINC\n2200000.0\n2020-10-29\n55000000.0\n2022-01-01\n2022\n1.18\n82200000.0\nSeattle\nUnited States\nNorth America\naerospace\nSam Altman,Index Ventures,Alameda Research\n[Aerospace, Drones, Law Enforcement, Public Safety, Robotics]\n3\nrecent\n1066.4\n\n\n3\nAKHAN Semiconductor\n1949083.0\n2021-11-08\n20000000.0\n2022-02-17\n2022\n0.28\n37919412.0\nGurnee\nUnited States\nNorth America\naerospace and defense\n\n[Aerospace, Automotive, Consumer Electronics, Manufacturing, Military, Semiconductor, Telecommunications]\n3\nrecent\n1066.4\n\n\n4\nPhantom Space\n875000.0\n2020-09-11\n21630605.0\n2021-11-04\n2021\n1.15\n27655605.0\nTucson\nUnited States\nNorth America\naerospace\nChenel Capital\n[Aerospace, Space Travel, Transportation]\n3\nolder\n1066.4\n\n\n5\nKarman+\n1000000.0\n2022-01-01\n25000000.0\n2023-03-01\n2023\n1.16\n26000000.0\nDenver\nUnited States\nNorth America\naerospace\n\n[Aerospace, Robotics]\n2\nrecent\n1066.4\n\n\n6\nApogee Semiconductor\n468792.0\n2022-07-21\n8606581.0\n2023-04-19\n2023\n0.75\n10373924.0\nPlano\nUnited States\nNorth America\naerospace\n\n[Aerospace, Industrial Manufacturing, Semiconductor]\n2\nrecent\n1066.4\n\n\n7\nRadical\n500000.0\n2023-04-05\n4465000.0\n2024-01-05\n2024\n0.75\n4965000.0\nSeattle\nUnited States\nNorth America\naerospace\nY Combinator,Scout Ventures\n[Aerospace, Internet, Telecommunications]\n2\nrecent\n1066.4\n\n\n8\nBasalt Tech\n500000.0\n2024-04-03\n3500000.0\n2024-05-30\n2024\n0.16\n4000000.0\nSan Francisco\nUnited States\nNorth America\naerospace\nY Combinator,Initialized Capital\n[Aerospace, Industrial Automation, Space Travel]\n2\nrecent\n1066.4\n\n\n9\nThe Exploration Company\n1740300.0\n2021-10-05\n43576000.0\n2023-02-01\n2023\n1.33\n54297360.0\nMunich\nGermany\nEurope\naerospace\nPromus Ventures,EQT Ventures, Red River West\n[Aerospace, Air Transportation, Manufacturing, Space Travel]\n3\nrecent\n1026.0\n\n\n\n\n\n\n\n\n\nCode\noptions.figure_size = (1200 / options.dpi, 780 / options.dpi)\n\n(\n    ggplot(df_world_first_last_deal_values.head(50))\n    + geom_point(aes(x=\"name\",\n                    y=\"deal_growth_cagr\",\n                    size=\"total_funding_usd\",\n                    group=\"region\",\n                    fill=\"factor(funding_recency)\",\n                    colour=\"factor(funding_recency)\"))\n    #+ geom_text(aes(label=\"total_money_raised\"), size=9)\n    + scale_y_continuous(labels = lambda l: ['{}%'.format(v) for v in l])\n    + scale_size_continuous(labels = lambda l: [add_units(v) for v in l])\n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_colour_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue }, guide=False)\n    + labs(\n        x=\"Companies\",\n        y=\"Deal Value Growth - CAGR\",\n        title=\"Private Investment in Aerospace & Defense across World\",\n        size=\"Total Funding\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Top 50 companies by Total Funding and Deal Growth CAGR ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption\n    )\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"}),\n        figure_size=(8, 8)\n    )\n    + coord_flip() \n    + facet_wrap(\"region\")\n    + theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\noptions.figure_size = (1200 / options.dpi, 780 / options.dpi)\n\n(\n    ggplot(df_world_first_last_deal_values[(df_world_first_last_deal_values['total_funding_usd']&gt;10000000) & \n           (df_world_first_last_deal_values['total_funding_usd']&lt;50000000)].head(50))\n    + geom_point(aes(x=\"name\",\n                    y=\"deal_growth_cagr\",\n                    size=\"total_funding_usd\",\n                    group=\"region\",\n                    fill=\"factor(funding_recency)\",\n                    colour=\"factor(funding_recency)\"))\n    #+ geom_text(aes(label=\"total_money_raised\"), size=9)\n    + scale_y_continuous(labels = lambda l: ['{}%'.format(v) for v in l])\n    + scale_size_continuous(labels = lambda l: [add_units(v) for v in l])\n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_colour_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue }, \n                          guide=False)\n    + labs(\n        x=\"Companies\",\n        y=\"Deal Value Growth - CAGR\",\n        title=\"Private Investment in Aerospace & Defense across World\",\n        size=\"Total Funding\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Companies w/ 10-50M total funding, by Total Funding and Deal Growth CAGR ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption\n    )\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"}),\n        figure_size=(8, 8)\n    )\n    + coord_flip() \n    + facet_wrap(\"region\")\n    + theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_stage = (\n    df_world_raw\n    .groupby(['region', 'funding_type', 'funding_recency'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_world_funding_by_stage = df_world_funding_by_stage.reset_index()\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n(\n    ggplot(df_world_funding_by_stage)\n    + geom_bar(aes(x=\"funding_type\", \n                   y=\"total_money_raised\", \n                   group=\"region\",\n                   fill = \"factor(funding_recency)\"),\n               \n               stat=\"identity\") \n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Funding Stages\",\n        y=\"Total Money Raised (in USD)\",\n        title=\"Private Investment in Aerospace & Defense across US & Europe\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Funding by stages ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n    + facet_wrap(\"region\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_countries = (\n    df_world_raw\n    .groupby(['region', 'country', 'funding_recency'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_world_funding_by_countries = df_world_funding_by_countries.reset_index()\ndf_world_funding_by_countries.head(5)\n\n\n\n\n\n\n\n\n\nregion\ncountry\nfunding_recency\ntotal_money_raised\ndeals\n\n\n\n\n0\nEurope\nAustria\nolder\n22542000.0\n1\n\n\n1\nEurope\nAustria\nrecent\n204462.0\n1\n\n\n2\nEurope\nBelgium\nolder\n56542900.0\n4\n\n\n3\nEurope\nBelgium\nrecent\n75873000.0\n3\n\n\n4\nEurope\nBulgaria\nolder\n4453420.0\n2\n\n\n\n\n\n\n\n\nMilitary spending to GDP\nSource: https://data.worldbank.org/indicator/MS.MIL.XPND.GD.ZS?end=2022&start=2022&view=bar\n\n\nCode\nmilitary_to_gdp_df = pd.read_csv(\"Military_to_GDP.csv\")\nmilitary_to_gdp_df = military_to_gdp_df[['Country Name', '2022']]\nmilitary_to_gdp_df.rename(columns = {\"Country Name\": \"country\", \"2022\": \"military_spending_to_gdp\"}, \n                          inplace=True)\nmilitary_to_gdp_df = military_to_gdp_df.dropna()\nmilitary_to_gdp_df.head(5)\n\n\n\n\n\n\n\n\n\ncountry\nmilitary_spending_to_gdp\n\n\n\n\n1\nAfrica Eastern and Southern\n1.001660\n\n\n3\nAfrica Western and Central\n0.975188\n\n\n4\nAngola\n1.328722\n\n\n5\nAlbania\n1.584881\n\n\n7\nArab World\n4.968286\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_and_spending_by_countries = df_world_funding_by_countries.merge(military_to_gdp_df, on = \"country\")\ndf_world_funding_and_spending_by_countries['military_spending_to_gdp'] = pd.to_numeric(df_world_funding_and_spending_by_countries['military_spending_to_gdp'])\ndf_world_funding_and_spending_by_countries.head(5)\n\n\n\n\n\n\n\n\n\nregion\ncountry\nfunding_recency\ntotal_money_raised\ndeals\nmilitary_spending_to_gdp\n\n\n\n\n0\nEurope\nAustria\nolder\n22542000.0\n1\n0.772607\n\n\n1\nEurope\nAustria\nrecent\n204462.0\n1\n0.772607\n\n\n2\nEurope\nBelgium\nolder\n56542900.0\n4\n1.179737\n\n\n3\nEurope\nBelgium\nrecent\n75873000.0\n3\n1.179737\n\n\n4\nEurope\nBulgaria\nolder\n4453420.0\n2\n1.508123\n\n\n\n\n\n\n\n\n\nCode\n# Ordering the bar plot\ndf_world_funding_and_spending_by_countries = df_world_funding_and_spending_by_countries.sort_values(by = \"military_spending_to_gdp\", \n                                                                          ascending=False)\ncountries_by_funding = df_world_funding_and_spending_by_countries[\"country\"].unique()\ndf_world_funding_and_spending_by_countries = df_world_funding_and_spending_by_countries.assign(\n    country_cat=pd.Categorical(df_world_funding_and_spending_by_countries[\"country\"], \n                               categories=countries_by_funding)\n)\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n(\n    ggplot(df_world_funding_and_spending_by_countries[1:])\n    + geom_bar(aes(x=\"country_cat\",\n                   y=\"military_spending_to_gdp\"),\n               fill=boson_blue,\n               stat=\"identity\") \n    #+ scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(breaks = list([x * .5 for x in range(20)]), \n                         labels = lambda l: [\"{}%\".format(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Military Spending to GDP (in %)\",\n        title=\"Military Spending to GDP (US & Europe)\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Source: https://data.worldbank.org/indicator/MS.MIL.XPND.GD.ZS?end=2022&start=2022&view=bar\",\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_and_spending_by_countries.head(2)\n\n\n\n\n\n\n\n\n\nregion\ncountry\nfunding_recency\ntotal_money_raised\ndeals\nmilitary_spending_to_gdp\ncountry_cat\n\n\n\n\n28\nEurope\nUkraine\nolder\n3.500000e+05\n1\n33.546573\nUkraine\n\n\n32\nNorth America\nUnited States\nrecent\n1.026982e+10\n273\n3.454920\nUnited States\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_countries = (\n    df_world_funding_and_spending_by_countries\n    .groupby([\"funding_recency\", \"country\"])\n    .agg(total_money_raised = ('total_money_raised', 'sum'))\n    .sort_values(by=[\"total_money_raised\"], \n                 ascending=False)\n)\ndf_world_funding_by_countries = df_world_funding_by_countries.reset_index()\ndf_world_funding_by_countries.head(5)\n\n\n\n\n\n\n\n\n\nfunding_recency\ncountry\ntotal_money_raised\n\n\n\n\n0\nolder\nUnited States\n1.038508e+10\n\n\n1\nrecent\nUnited States\n1.026982e+10\n\n\n2\nolder\nUnited Kingdom\n3.027402e+09\n\n\n3\nrecent\nGermany\n1.091632e+09\n\n\n4\nolder\nGermany\n7.439148e+08\n\n\n\n\n\n\n\n\n\nCode\n# Ordering the bar plot\ndf_world_funding_by_countries = df_world_funding_by_countries.sort_values(by = \"total_money_raised\", \n                                                                          ascending=False)\ncountries_by_funding = df_world_funding_by_countries[\"country\"].unique()\ndf_world_funding_by_countries = df_world_funding_by_countries.assign(\n    country_cat=pd.Categorical(df_world_funding_by_countries[\"country\"], \n                               categories=countries_by_funding)\n)\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n\n# Annotate &gt;1B countries\ndf_labels = df_world_funding_by_countries[df_world_funding_by_countries['total_money_raised'] &gt; 1000000000]\ndf_labels = (\n    df_labels\n    .groupby([\"country_cat\"])\n    .agg(total_money_raised = ('total_money_raised', 'sum'))\n    .reset_index()\n)\ndf_labels['total_money_raised_format'] = df_labels['total_money_raised'].apply(add_units)\n\n(\n    ggplot(df_world_funding_by_countries)\n    + geom_bar(aes(x=\"country_cat\", \n                   y=\"total_money_raised\",\n                  fill=\"factor(funding_recency)\"),\n               stat=\"identity\") \n    + geom_text(aes(x = \"country_cat\",\n                    y = \"total_money_raised - 300000000\",\n                    label = \"total_money_raised_format\"),\n                color=\"white\",\n                va = \"top\",\n                size = 8,\n               data=df_labels)\n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Total Money Raised (in USD)\",\n        title=\"Private Investment in Aerospace & Defense across US & Europe\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Funding by countries ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_countries = (\n    df_world_raw\n    .groupby(['region', 'country', 'announced_year'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_world_funding_by_countries = df_world_funding_by_countries.reset_index()\n\ndf_europe_funding_by_countries = (\n    df_world_funding_by_countries[\n        (df_world_funding_by_countries['region'] == current_region['region']) & \n        ((df_world_funding_by_countries['announced_year'] &gt;= 2022))\n    ]\n    .groupby([\"country\", \"announced_year\"])\n    .agg(total_money_raised = ('total_money_raised', 'sum'))\n    .sort_values(by=[\"total_money_raised\"], \n                 ascending=False)\n)\ndf_europe_funding_by_countries = df_europe_funding_by_countries.reset_index()\n\n# Ordering the bar plot\ndf_europe_funding_by_countries = df_europe_funding_by_countries.sort_values(by = \"total_money_raised\", \n                                                                          ascending=False)\ncountries_by_funding = df_europe_funding_by_countries[\"country\"].unique()\ndf_europe_funding_by_countries = df_europe_funding_by_countries.assign(\n    country_cat=pd.Categorical(df_europe_funding_by_countries[\"country\"], \n                               categories=countries_by_funding)\n)\n\ndf_labels = (\n    df_world_raw[\n        (df_world_raw['region'] == current_region['region']) & \n        (df_world_raw['announced_year'] &gt;= 2022)\n    ]\n    .groupby(['country', 'name'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_labels = df_labels.reset_index()\ndf_labels = df_labels.assign(\n    country_cat=pd.Categorical(df_labels[\"country\"], \n                               categories=countries_by_funding)\n)\n\n\n(\n    ggplot(df_europe_funding_by_countries)\n    + geom_bar(aes(x=\"country_cat\", \n                   y=\"total_money_raised\"),\n               stat=\"identity\",\n              fill=boson_blue)\n    # + geom_text(aes(x = \"country_cat\",\n    #             y = \"total_money_raised * 1.2 + 1000000\",\n    #             label = \"name\"),\n    #             va = \"bottom\",\n    #             size = 8,\n    #             nudge_y = 0.5,\n    #             colour=\"#780000\",\n    #             #adjust_text=adjust_text_dict,\n    #            data=df_labels[df_labels['total_money_raised'] &gt; 100000000])\n    + scale_y_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Total Money Raised (in USD)\",\n        title=\"Private Investment in Aerospace & Defense across Europe\",\n        subtitle =\"Funding by countries (2022-2024)\",\n        caption=caption,\n    )\n    + coord_flip()\n    + facet_wrap(\"announced_year\", ncol=3)\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_labels.sort_values(by=['total_money_raised'], ascending=False).head(20)\n\n\n\n\n\n\n\n\n\ncountry\nname\ntotal_money_raised\ndeals\ncountry_cat\n\n\n\n\n157\nUnited States\nSpaceX\n1.974965e+09\n2\nUnited States\n\n\n17\nUnited States\nAnduril Industries\n1.480000e+09\n1\nUnited States\n\n\n55\nUnited States\nDivergent\n4.900000e+08\n3\nUnited States\n\n\n195\nUnited States\nWisk Aero\n4.500000e+08\n1\nUnited States\n\n\n37\nUnited States\nBeta Technologies\n3.750000e+08\n1\nUnited States\n\n\n67\nUnited States\nFirefly Aerospace\n3.750000e+08\n2\nUnited States\n\n\n32\nUnited States\nAxiom Space\n3.500000e+08\n1\nUnited States\n\n\n150\nUnited States\nSierra Space\n2.900000e+08\n1\nUnited States\n\n\n62\nUnited States\nEpirus\n2.000000e+08\n1\nUnited States\n\n\n24\nUnited States\nAstranis\n2.000000e+08\n1\nUnited States\n\n\n81\nUnited States\nHadrian\n1.820000e+08\n2\nUnited States\n\n\n77\nUnited States\nGecko Robotics\n1.730000e+08\n2\nUnited States\n\n\n42\nUnited States\nCapella Space\n1.570000e+08\n2\nUnited States\n\n\n202\nUnited States\nZeroAvia\n1.460000e+08\n2\nUnited States\n\n\n124\nUnited States\nOverair\n1.450000e+08\n1\nUnited States\n\n\n178\nUnited States\nUrsa Major\n1.380000e+08\n2\nUnited States\n\n\n173\nUnited States\nTrue Anomaly\n1.330000e+08\n3\nUnited States\n\n\n6\nUnited States\nAeroVanti\n1.097500e+08\n2\nUnited States\n\n\n83\nUnited States\nHermeus\n1.000000e+08\n1\nUnited States\n\n\n165\nUnited States\nStoke Space\n1.000000e+08\n1\nUnited States\n\n\n\n\n\n\n\n\n\nCode\ndf_world_spending_by_countries = (\n    df_world_funding_and_spending_by_countries\n    .groupby([\"country\"])\n    .agg(military_spending_to_gdp = ('military_spending_to_gdp', 'sum'))\n    .sort_values(by=[\"military_spending_to_gdp\"], \n                 ascending=False)\n)\ndf_world_spending_by_countries = df_world_spending_by_countries.reset_index()\n\ncountries_by_spending = df_world_spending_by_countries[\"country\"].unique()\ndf_world_spending_by_countries = df_world_spending_by_countries.assign(\n    country_cat=pd.Categorical(df_world_spending_by_countries[\"country\"], \n                               categories=countries_by_spending)\n)\ndf_world_spending_by_countries.head(5)\n\n\n\n\n\n\n\n\n\ncountry\nmilitary_spending_to_gdp\ncountry_cat\n\n\n\n\n0\nUkraine\n33.546573\nUkraine\n\n\n1\nUnited States\n6.909840\nUnited States\n\n\n2\nLithuania\n5.045247\nLithuania\n\n\n3\nUnited Kingdom\n4.454368\nUnited Kingdom\n\n\n4\nFrance\n3.877447\nFrance\n\n\n\n\n\n\n\n\n\nCode\n(\n    ggplot(df_world_spending_by_countries[1:])\n    + geom_bar(aes(x=\"country_cat\", \n                   y=\"military_spending_to_gdp\"),\n               stat=\"identity\",\n            fill=boson_blue) \n    #+ scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(labels = lambda l: ['{}%'.format(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Military Spending to GDP (%()\",\n        title=\"Military Spending to GDP\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Funding by countries ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_raw.head(2)\n\n\n\n\n\n\n\n\n\nindex\nname\nmoney_raised\nannounced_date\nindustries\ndescription\nfunding_type\nlead_investors\ntotal_funding\ncity\n...\ntotal_funding_currency\ntotal_funding_amount\nmoney_raised_usd\ntotal_funding_usd\naerospace\ndefense\nannounced_date_trunc_month\nannounced_date_trunc_year\nsector\nfunding_recency\n\n\n\n\n0\n0\nSpaceX\n$1724965480\n2022-05-24\nAdvanced Materials, Aerospace, Manufacturing, ...\nSpaceX is an aviation and aerospace company th...\nVenture - Series Unknown\n—\n$9779343846\nHawthorne\n...\n$\n9779343846\n1.724965e+09\n9.779344e+09\nTrue\nFalse\n2022-05-01\n2022-01-01\naerospace\nrecent\n\n\n1\n1\nAnduril Industries\n$1480000000\n2022-12-02\nAerospace, Government, Military, National Secu...\nAnduril Industries is a defense product compan...\nSeries E\nValor Equity Partners\n$2171000000\nCosta Mesa\n...\n$\n2171000000\n1.480000e+09\n2.171000e+09\nTrue\nTrue\n2022-12-01\n2022-01-01\naerospace and defense\nrecent\n\n\n\n\n2 rows × 27 columns\n\n\n\n\n\nCode\ndf_world_raw['industries'] = df_world_raw['industries'].str.split(\",\")\n\n\n\n\nCode\ndf_world_raw_exploded = df_world_raw.explode('industries')\ndf_world_raw_exploded['industries'] = df_world_raw_exploded['industries'].str.strip()\n\ndf_world_raw_exploded_agg_by_industries = (\n    df_world_raw_exploded\n    .groupby(['industries'])\n    .agg(money_raised_usd = ('money_raised_usd', 'sum'))\n)\ndf_world_raw_exploded_agg_by_industries = (\n    df_world_raw_exploded_agg_by_industries\n    .reset_index()\n)\n# df_world_raw_exploded_agg_by_industries.head(2)\n\n\ndf_world_raw_exploded_agg_by_industries['money_raised_usd_format'] = df_world_raw_exploded_agg_by_industries['money_raised_usd'].apply(add_units)\ndf_world_raw_exploded_agg_by_industries.head(5)\n\n\n\n\n\n\n\n\n\nindustries\nmoney_raised_usd\nmoney_raised_usd_format\n\n\n\n\n0\n3D Printing\n6.628812e+08\n663M\n\n\n1\n3D Technology\n1.342485e+09\n1B\n\n\n2\nAdvanced Materials\n6.570686e+09\n7B\n\n\n3\nAerospace\n2.641824e+10\n26B\n\n\n4\nAgTech\n7.591975e+07\n76M\n\n\n\n\n\n\n\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook\"\n\nboson_blue = \"#04024B\"\nboson_royal = \"#1d2b71\"\nboson_yinmin = '#375496'\nboson_glaucous = '#507dbc'\nboson_powder_blue = '#a1c6ea'\nboson_columbia_blue = '#bbd1ea'\nboson_blue_faded = '#dae3e5'\n\nfig = px.treemap(df_world_raw_exploded_agg_by_industries[20:], \n                 path=['industries'],\n                 values='money_raised_usd',\n                 color='money_raised_usd',\n                \n                 # https://coolors.co/generate\n                 # https://coolors.co/04024b-1d2b71-375496-507dbc-a1c6ea-bbd1ea-dae3e5\n                 color_continuous_scale=[(0,boson_blue_faded),\n                                         (0.14, boson_columbia_blue),\n                                         (0.28, boson_powder_blue),\n                                         (0.42, boson_glaucous),\n                                         (0.56, boson_yinmin),\n                                         (0.75, boson_royal),\n                                         (1,boson_blue)],\n\n                 custom_data=['money_raised_usd_format'],\n                 \n                 width=1200, \n                 height=650)\nfig.update_layout(margin = dict(t=20, l=0, r=0, b=0))\nfig.update_traces(marker = dict(\n                    line = dict(width = 0)\n                  ),\n                 tiling = dict(pad = 0))\nfig.data[0].texttemplate = \"&lt;b&gt;%{label}&lt;/b&gt;&lt;br&gt;%{customdata[0]}\"\nfig.show()\n\n\n                                                \n\n\n\n\nCAGR by sub-sectors\n\n\nCode\ndf_world_first_last_deal_values.head(5)\n\n\n\n\n\n\n\n\n\nname\nfirst_deal_value\nfirst_deal_date\nlast_deal_value\nlast_deal_date\nlast_deal_year\ndeal_span_years\ntotal_funding_usd\ncity\ncountry\nregion\nsector\nlead_investors\nindustries\ndeals\nfunding_recency\ndeal_growth_cagr\n\n\n\n\n0\nSkyryse\n2500000.0\n2020-05-15\n200000000.0\n2021-10-27\n2021\n1.45\n240500000.0\nEl Segundo\nUnited States\nNorth America\naerospace\n—\nAerospace, Air Transportation, Internet, Trans...\n2\nolder\n1066.4\n\n\n1\nAeroVanti\n9750000.0\n2022-07-21\n100000000.0\n2022-10-19\n2022\n0.25\n109750000.0\nAnnapolis\nUnited States\nNorth America\naerospace\nNetwork 1 Financial Securities\nAerospace, Air Transportation, Transportation\n2\nrecent\n1066.4\n\n\n2\nBRINC\n2200000.0\n2020-10-29\n55000000.0\n2022-01-01\n2022\n1.18\n82200000.0\nSeattle\nUnited States\nNorth America\naerospace\nSam Altman\nAerospace, Drones, Law Enforcement, Public Saf...\n3\nrecent\n1066.4\n\n\n3\nAKHAN Semiconductor\n1949083.0\n2021-11-08\n20000000.0\n2022-02-17\n2022\n0.28\n37919412.0\nGurnee\nUnited States\nNorth America\naerospace and defense\n—\nAerospace, Automotive, Consumer Electronics, M...\n3\nrecent\n1066.4\n\n\n4\nPhantom Space\n875000.0\n2020-09-11\n21630605.0\n2021-11-04\n2021\n1.15\n27655605.0\nTucson\nUnited States\nNorth America\naerospace\n—\nAerospace, Space Travel, Transportation\n3\nolder\n1066.4\n\n\n\n\n\n\n\n\n\nCode\n#df_world_first_last_deal_values['industries'] = df_world_first_last_deal_values['industries'].str.split(\",\")\n\n\n\n\nCode\ndf_world_first_last_deal_values_exploded = df_world_first_last_deal_values.explode('industries')\ndf_world_first_last_deal_values_exploded['industries'] = df_world_first_last_deal_values_exploded['industries'].str.strip()\n\ndf_world_cagr_exploded_agg_by_industries = (\n    df_world_first_last_deal_values_exploded\n    .groupby(['industries'])\n    .agg(avg_funding_usd = ('total_funding_usd', 'mean'),\n        deal_growth_cagr = ('deal_growth_cagr', 'mean'))\n)\ndf_world_cagr_exploded_agg_by_industries = (\n    df_world_cagr_exploded_agg_by_industries\n    .reset_index()\n)\n#df_world_cagr_exploded_agg_by_industries['deal_growth_cagr'] = df_world_cagr_exploded_agg_by_industries['deal_growth_cagr']*100\n# df_world_cagr_exploded_agg_by_industries.head(20)\n\ndf_world_cagr_exploded_agg_by_industries = df_world_cagr_exploded_agg_by_industries.sort_values(by = \"deal_growth_cagr\",\n                                                                                                ascending=False)\nindustries = df_world_cagr_exploded_agg_by_industries[\"industries\"].unique()\ndf_world_cagr_exploded_agg_by_industries = df_world_cagr_exploded_agg_by_industries.assign(\n    industry_cat=pd.Categorical(df_world_cagr_exploded_agg_by_industries[\"industries\"], \n                               categories=industries)\n)\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n(\n    ggplot(df_world_cagr_exploded_agg_by_industries.head(30))\n    + geom_point(aes(x=\"industry_cat\", \n                     y=\"deal_growth_cagr\",\n                     size=\"avg_funding_usd\"),\n               stat=\"identity\",\n              fill=boson_blue) \n    + scale_y_continuous(breaks = list(range(0,800,50)),\n                         labels = lambda l: [\"{}%\".format(v) for v in l])\n    + scale_size_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Industries\",\n        y=\"Deal Value Growth - CAGR\",\n        title=\"Private Investment in Aerospace & Defense across US & Europe\",\n        size=\"Avg Funding (USD)\",\n        subtitle =\"CAGR and Avg. Funding by industries ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\nSub-industry: Transportation / Air Transportation\n\n\nCode\ndf_world_first_last_deal_values_exploded.head(2)\n\n\n\n\n\n\n\n\n\nname\nfirst_deal_value\nfirst_deal_date\nlast_deal_value\nlast_deal_date\nlast_deal_year\ndeal_span_years\ntotal_funding_usd\ncity\ncountry\nregion\nsector\nlead_investors\nindustries\ndeals\nfunding_recency\ndeal_growth_cagr\n\n\n\n\n0\nSkyryse\n2500000.0\n2020-05-15\n200000000.0\n2021-10-27\n2021\n1.45\n240500000.0\nEl Segundo\nUnited States\nNorth America\naerospace\n—\nAerospace, Air Transportation, Internet, Trans...\n2\nolder\n1066.4\n\n\n1\nAeroVanti\n9750000.0\n2022-07-21\n100000000.0\n2022-10-19\n2022\n0.25\n109750000.0\nAnnapolis\nUnited States\nNorth America\naerospace\nNetwork 1 Financial Securities\nAerospace, Air Transportation, Transportation\n2\nrecent\n1066.4\n\n\n\n\n\n\n\n\n\nCode\ndef make_pretty(styler):\n    #styler.set_caption(caption)\n    #styler.format(recency)\n    styler.background_gradient(axis=None, cmap=\"Blues\")\n    return styler\n\ndf_world_aerospace_transportaion = (\n    df_world_first_last_deal_values_exploded[\n    (\n        (df_world_first_last_deal_values_exploded['industries'] == 'Transportation') |\n        (df_world_first_last_deal_values_exploded['industries'] == 'Air Transportation')\n    ) & \n    (\n        df_world_first_last_deal_values_exploded['deal_growth_cagr']&gt;0\n    )]\n    .sort_values(by=['last_deal_year', 'total_funding_usd', 'deals'], ascending=False)\n)\ndf_world_aerospace_transportaion['total_funding_usd_format'] = df_world_aerospace_transportaion['total_funding_usd'].apply(add_units)\n\n# &gt;10M funding\ndf_world_aerospace_transportaion = df_world_aerospace_transportaion[df_world_aerospace_transportaion['total_funding_usd'] &gt; 10000000] \ndf_world_aerospace_transportaion = df_world_aerospace_transportaion[['name', 'last_deal_year', 'total_funding_usd_format', 'country', 'region', 'deals', 'deal_growth_cagr']].rename(columns={\n    'name': 'Name',\n    'last_deal_year': 'Last Deal Year',\n    'total_funding_usd_format': 'Total Funding (USD)',\n    'country': 'Country',\n    'region': 'Region',\n    'deals': 'Number of Funding Deals',\n    'deal_growth_cagr': 'Deal CAGR'\n})\n\ndf_world_aerospace_transportaion.drop_duplicates().head(20).style.pipe(make_pretty)\n\n\n\n\n\n\n\n \nName\nLast Deal Year\nTotal Funding (USD)\nCountry\nRegion\nNumber of Funding Deals\nDeal CAGR\n\n\n\n\n\n\n\n\n\nPick: Elroy Air\n\n\nCode\n','.join(df_world_raw[df_world_raw['name']=='Elroy Air']['lead_investors'].str.replace('-', ''))\n\n\n'—,Lockheed Martin Ventures, Marlinspike Capital, Prosperity7 Ventures,—,Catapult Ventures'\n\n\n\n\nCode\ndf_world_aerospace_transportaion[df_world_aerospace_transportaion['Region']=='Europe'].drop_duplicates().head(20).style.pipe(make_pretty)\n\n\n\n\n\n\n\n \nName\nLast Deal Year\nTotal Funding (USD)\nCountry\nRegion\nNumber of Funding Deals\nDeal CAGR\n\n\n\n\n\n\n\n\n\nPick: Wingcopter\n\n\nCode\n','.join(df_world_raw[df_world_raw['name']=='Wingcopter']['lead_investors'].str.replace('-', ''))\n\n\n'—,Futury Capital, Xplorer Capital'"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Papers",
    "section": "",
    "text": "PDF papers (preprints, drafts, slide-like PDFs)\nMarkdown/QMD notes that accompany papers (methods, references, appendices)\n\n\n\n\n\n\n\n  \n    \n      markdown\n    \n    Paper A Revised 1971 Shock\n    \n  \n\n\n\n\n\n  \n    \n      pdf\n    \n    AURA Neuro–Symbolic Energy–Efficient Architecture Reusing LLM Infrastructure\n    PDF: AURA-Neuro–Symbolic-Energy–Efficient-Architecture-Reusing-LLM-Infrastructure.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Aerospace defense industry overview investment thesis\n    PDF: Aerospace-defense-industry-overview-investment-thesis.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Hydroverse   Opportunity Report\n    PDF: Hydroverse - Opportunity Report.html\n  \n\n\n\n\n\n  \n    \n      reference\n    \n    Existential Risk and Growth"
  },
  {
    "objectID": "papers/index.html#whats-here",
    "href": "papers/index.html#whats-here",
    "title": "Papers",
    "section": "",
    "text": "PDF papers (preprints, drafts, slide-like PDFs)\nMarkdown/QMD notes that accompany papers (methods, references, appendices)\n\n\n\n\n\n\n\n  \n    \n      markdown\n    \n    Paper A Revised 1971 Shock\n    \n  \n\n\n\n\n\n  \n    \n      pdf\n    \n    AURA Neuro–Symbolic Energy–Efficient Architecture Reusing LLM Infrastructure\n    PDF: AURA-Neuro–Symbolic-Energy–Efficient-Architecture-Reusing-LLM-Infrastructure.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Aerospace defense industry overview investment thesis\n    PDF: Aerospace-defense-industry-overview-investment-thesis.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Hydroverse   Opportunity Report\n    PDF: Hydroverse - Opportunity Report.html\n  \n\n\n\n\n\n  \n    \n      reference\n    \n    Existential Risk and Growth"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data engineer/scientist and quantitative researcher with 18 years building systems that extract signal from noise.\nMy work spans system engineering, ML, data engineering, portfolio analytics, and economic research—always anchored in first-principles thinking and empirical validation. I don’t chase narratives that don’t survive contact with data."
  },
  {
    "objectID": "about.html#current-work",
    "href": "about.html#current-work",
    "title": "About",
    "section": "Current Work",
    "text": "Current Work\nData Engineering & Analytics\nLeading data platform modernization at Macquarie Group: DBT/Iceberg/Redshift architectures, data governance through DBT & Collier, and observable data pipelines.\nInvestment Research\nDeveloping quantitative frameworks for portfolio construction and long-term portfolio strategy.\nSystems Thinking\nBuilding graph-based intelligence platforms for venture capital analysis and exploring applications of ML/AI to financial domains."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About",
    "section": "Background",
    "text": "Background\nTechnical: Data engineering at scale • DBT, Spark, Redshift, Iceberg • ML/AI (graph networks, probabilistic models) • LLMs & Beyond (JEPA) • Modern development (Python, R, AWS) • Blockchain/decentralized systems\nAnalytical: Quantitative finance • Portfolio analytics • Economic time-series analysis • CFA Level 1 • Data governance (BCBS 239, CPG 235)\nDomain Experience: Financial services • Media/publishing • Healthcare • Renewables/energy • Gaming"
  },
  {
    "objectID": "about.html#previous-roles",
    "href": "about.html#previous-roles",
    "title": "About",
    "section": "Previous Roles",
    "text": "Previous Roles\n\nSenior Manager/Lead Engineer at Boson Research Advisory (2023-2025)\nProduct Manager at Hydroverse •\nVarious roles at Sky News, Fairfax Media, Zynga, Oracle, and startups across Sydney, San Francisco, and Bangalore."
  },
  {
    "objectID": "about.html#published-work",
    "href": "about.html#published-work",
    "title": "About",
    "section": "Published Work",
    "text": "Published Work\nCrusade Against Kidney Disease (2021)\nResearch synthesis on slowing CKD progression through evidence-based interventions. Written after watching family members navigate clinical treatment failures. Not-for-profit."
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About",
    "section": "Interests",
    "text": "Interests\nOutside of work: evolutionary economics, self-sustainability, genetic-based health optimization, and long-term systems thinking."
  },
  {
    "objectID": "about.html#site-philosophy",
    "href": "about.html#site-philosophy",
    "title": "About",
    "section": "Site Philosophy",
    "text": "Site Philosophy\nThis site archives my research, notebooks, and projects. It’s organized for reference and discovery, not chronology. Everything here prioritizes:\n\nQuantitative reasoning where possible\nFirst-principles thinking where necessary\n\nSkepticism toward narratives unsupported by data\n\nIf you’re looking for polished marketing material, this isn’t it. If you’re looking for working analysis with code and assumptions made explicit, you’re in the right place.\n\nConnect\nGitHub • LinkedIn\nLocation: Sydney, Australia"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deb Bose",
    "section": "",
    "text": "This site is a working archive — not a feed.\nEverything here is oriented toward truth-seeking through analysis:\nquantitative reasoning where possible, first-principles thinking where necessary, and skepticism toward narratives that don’t survive contact with data."
  },
  {
    "objectID": "index.html#what-youll-find-here",
    "href": "index.html#what-youll-find-here",
    "title": "Deb Bose",
    "section": "What you’ll find here",
    "text": "What you’ll find here\n\nArticles\nEssays, market notes, and technical deep dives — written to clarify, not to perform.\n\n\nNotebooks\nRunnable research with code, charts, and assumptions made explicit.\nExploratory by design, not polished marketing artefacts.\n\n\nPapers\nDrafts, PDFs, and reference material — including work that is incomplete, evolving, or deliberately unresolved.\n\n\nProjects\nPointers to my GitHub work: tools, models, and systems built to answer specific questions."
  },
  {
    "objectID": "index.html#latest-writing",
    "href": "index.html#latest-writing",
    "title": "Deb Bose",
    "section": "Latest writing",
    "text": "Latest writing\n\n→ Go to Papers\n→ Go to Notebooks"
  },
  {
    "objectID": "posts/2026-01-06-my-first-post.html",
    "href": "posts/2026-01-06-my-first-post.html",
    "title": "My first Quarto post",
    "section": "",
    "text": "Some text.\nInline math: \\(I_t = f(1/C_t)\\)\n\nimport math\nmath.sqrt(2)\n\n1.4142135623730951\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html",
    "href": "papers/Paper_A_Revised_1971_Shock.html",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "",
    "text": "Incomplete Paper: Academic Response"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#a-structural-critique-of-the-trammell-aschenbrenner-framework",
    "href": "papers/Paper_A_Revised_1971_Shock.html#a-structural-critique-of-the-trammell-aschenbrenner-framework",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "",
    "text": "Incomplete Paper: Academic Response"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#paper-rs-framework-assumes-technology-is-exogenous",
    "href": "papers/Paper_A_Revised_1971_Shock.html#paper-rs-framework-assumes-technology-is-exogenous",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.1 Paper R’s Framework Assumes Technology is Exogenous",
    "text": "1.1 Paper R’s Framework Assumes Technology is Exogenous\nTrammell & Aschenbrenner model technology level \\(A_t\\) as an exogenous path chosen by a social planner:\n\\[\\max_{a, b} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(1-B_t)) dt\\]\nsubject to: \\[\\dot{S}_t = -\\delta(A_t, B_t) S_t\\]\nThe critical assumption: Technology path \\(a = \\{A_t\\}_{t=0}^{\\infty}\\) is treated as a policy choice independent of monetary or fiscal constraints.\nThis is wrong. Technology advancement is endogenous to: 1. Capital availability (determined by monetary policy) 2. Interest rates (determining which innovations are financially viable) 3. Debt sustainability (constraining long-run growth paths) 4. Fiscal pressure (incentivizing specific types of “growth”)\nWhen monetary regime changes, the entire feasible set of technology paths changes. Paper R’s optimization occurs within a fiat currency regime without acknowledging that the regime itself determines the objective function’s parameters."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.2 The 1971 Structural Break",
    "text": "1.2 The 1971 Structural Break\nAugust 15, 1971: The Nixon Shock\nPresident Nixon announced the USD would no longer be convertible to gold at $35/ounce. This was not merely a U.S. policy change—it was a global monetary regime change.\nPre-1971 (Bretton Woods System): - USD pegged to gold ($35/oz fixed) - All other major currencies pegged to USD at fixed rates - Constraint: No country could print money arbitrarily without losing the peg - Global money supply anchored by gold convertibility\nPost-1971 (Fiat Currency Era): - USD breaks gold peg - All other currencies simultaneously lose their anchor - No constraint: Every central bank can now print money without limit - Global monetary system becomes pure fiat\nCritical observation: There is no control group. Every developed nation went fiat on the same day. This is a natural experiment with universal treatment."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#why-this-invalidates-paper-rs-framework",
    "href": "papers/Paper_A_Revised_1971_Shock.html#why-this-invalidates-paper-rs-framework",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.3 Why This Invalidates Paper R’s Framework",
    "text": "1.3 Why This Invalidates Paper R’s Framework\nPaper R’s technology path \\(a(t)\\) conflates: 1. Organic technological advancement (\\(A^{organic}_t\\)): Genuine capability improvements 2. Debt-financed pseudo-innovation (\\(A^{debt}_t\\)): Activity enabled by cheap credit that appears as “growth”\nThe observed path is: \\[A^{observed}_t = A^{organic}_t + A^{debt}_t\\]\nUnder gold-backed currency: - Credit is constrained by gold reserves - \\(A^{debt}_t \\approx 0\\) - Observed growth ≈ Organic growth\nUnder fiat currency: - Credit is constrained only by political will - \\(A^{debt}_t\\) can grow indefinitely (until debt crisis) - Observed growth &gt;&gt; Organic growth\nPaper R treats \\(A^{observed}_t\\) as if it were \\(A^{organic}_t\\) and concludes that accelerating it minimizes risk. But if \\(A^{debt}_t\\) is unsustainable and creates fertility collapse, this optimization is fundamentally flawed."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-complete-causal-structure",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-complete-causal-structure",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.1 The Complete Causal Structure",
    "text": "2.1 The Complete Causal Structure\n1971: Gold Standard Abandoned\n         ↓\nAll Currencies Become Fiat (Unlimited Printing)\n         ↓\nGovernment Debt Can Grow Without Limit\n         ↓\nRising Debt Service Costs Create Fiscal Pressure\n         ↓\nNeed to Expand Tax Base to Service Debt\n         ↓\nPOLICY STRATEGY: Encourage Female Workforce Participation\n         ↓\n         ├→ [Path A: Direct Effect]\n         │   Dual-income households = 2x taxable incomes\n         │        ↓\n         │   Government tax revenue increases\n         │\n         ├→ [Path B: Wage Suppression Effect]\n         │   Labor supply doubles → wages don't rise with productivity\n         │        ↓\n         │   Real wage stagnation makes dual-income NECESSARY (not choice)\n         │\n         └→ [Path C: Cost Inflation Effect]\n             More dual-income households → housing/childcare costs rise\n                  ↓\n             Children become economically prohibitive\n                  \n         ↓\n[All paths converge]\n         ↓\nHome Cooking → Processed Food (BigFood profits)\nChildcare at Home → Institutional Childcare (new industry)\nExtended Family Networks → Nuclear Family Isolation\nCommunity Cohesion → Market Transactions\n         ↓\nFertility Falls Below Replacement (TFR &lt; 2.1)\n         ↓\nPopulation Decline Begins\n         ↓\nEVOLUTIONARY EXTINCTION\n(despite S_∞ &gt; 0 in Paper R's framework)"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#mathematical-formalization",
    "href": "papers/Paper_A_Revised_1971_Shock.html#mathematical-formalization",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.2 Mathematical Formalization",
    "text": "2.2 Mathematical Formalization\n\nThe Debt-Fiscal Pressure Link\nUnder fiat currency, government debt evolves as:\n\\[\\dot{D}_t = r_t D_t + G_t - T_t + M_t\\]\nwhere: - \\(D_t\\) = government debt - \\(r_t\\) = interest rate\n- \\(G_t\\) = government spending - \\(T_t\\) = tax revenue - \\(M_t\\) = monetary expansion (money printing)\nPre-1971 Constraint: \\[M_t \\leq M_t^{max}(Gold_{reserves})\\]\nUnlimited monetary expansion would break the gold peg.\nPost-1971: \\[M_t \\in [0, \\infty)\\]\nNo constraint except political/inflation considerations.\nFiscal Pressure:\nDefine fiscal pressure as debt service burden: \\[FP_t = \\frac{r_t D_t}{GDP_t}\\]\nAs debt grows, fiscal pressure increases, creating incentive to expand tax base.\n\n\nThe Tax Base Expansion Strategy\nTax revenue under single-income households: \\[T_t^{single} = \\tau \\cdot n \\cdot w_t\\]\nwhere \\(n\\) = number of workers, \\(w_t\\) = wage.\nTax revenue under dual-income households: \\[T_t^{dual} = \\tau \\cdot 2n \\cdot w_t^{dual}\\]\nCritical: Even if \\(w_t^{dual} &lt; w_t\\) (wage suppression from labor supply increase), the tax revenue increases:\n\\[T_t^{dual} &gt; T_t^{single} \\iff 2w_t^{dual} &gt; w_t\\]\nThis is satisfied even with substantial wage suppression.\nKey Findings 1:\nHYPOTHESIS: Tax revenue from labor increased disproportionately post-1971\nTEST: Calculate (Labor Income Tax Revenue / GDP) from 1960-2024\nEXPECTED: Structural break upward at 1971\nDATA SOURCE: OECD Tax Revenue Database, IRS Historical Tables\nSTATUS: [VALIDATED]\nTo test the hypothesis of a disproportionate increase in tax extraction following the 1971 monetary regime change, an Interrupted Time Series analysis was performed on U.S. Individual Income Tax receipts. To ensure the analysis isolated structural fiscal shifts from the inherent inflation of the post-1971 fiat era, tax revenue was adjusted to Real Tax values (2024 dollars) using Consumer Price Index (CPI) data.\nA Chow Test was employed to statistically evaluate the presence of a structural break at the 1971 pivot point, comparing the pre-break period (1947–1970) with the post-break period (1971–2025). The results are as follows:\n• Statistical Significance: The analysis yielded a Chow F-statistic of 9.6029 with a p-value of 0.000194. This allows for the rejection of the null hypothesis at the 1% significance level, providing what the sources describe as “strong evidence of a structural break at 1971.” • Trend Acceleration: The regression coefficients reveal a significant divergence in the growth trajectory of real tax revenue extraction. The pre-1971 slope was 27.48, which accelerated to a post-1971 slope of 37.36. This represents a net increase of 9.8787 in the annual growth rate of real tax receipts. • Causal Alignment: These findings provide the empirical foundation for the argument that the removal of gold-convertibility constraints facilitated “hockey-stick growth” in government debt. According to the sources, this necessitated a “Tax Base Expansion Strategy” characterized by increased labor force participation—specifically the transition to dual-income households—to service the expanding debt.\nThis statistically significant break confirms that the 1971 “Nixon Shock” was not merely a monetary adjustment but a fundamental shift in the state’s fiscal extraction apparatus, creating the economic pressure that the sources link to the subsequent civilizational fertility collapse.\n\n\nThe Real Wage Suppression Mechanism\nLabor supply elasticity: \\[\\frac{dN_t}{dw_t} = \\epsilon \\cdot N_t\\]\nWith \\(\\epsilon &gt; 0\\), increasing labor supply (dual incomes) should raise wages. But we observe the opposite: productivity-wage divergence begins exactly at 1971.\nThe Fiat Currency Wage Suppression:\nUnder fiat currency, monetary expansion creates inflation: \\[\\pi_t = f(M_t, V_t)\\]\nNominal wages adjust slowly (sticky wages): \\[\\frac{d w_t^{nominal}}{dt} &lt; \\pi_t\\]\nTherefore real wages decline: \\[w_t^{real} = \\frac{w_t^{nominal}}{P_t} \\searrow\\]\nThis makes dual income necessary rather than optional.\n[DATA VALIDATION PLACEHOLDER 2]:\nHYPOTHESIS: Real wage-productivity divergence begins at 1971\nTEST: Calculate correlation between productivity and real wages\n      Pre-1971: Expect ρ &gt; 0.8\n      Post-1971: Expect ρ → 0\nDATA SOURCE: BLS Productivity and Costs, Real Compensation data\nSPECIFIC TEST: Chow test for structural break at 1971\nSTATUS: [PENDING VALIDATION]\n\n\nThe Fertility Response Function\nFertility depends on: \\[F_t = F(w_t^{real}, C_t^{child}, T_t^{avail}, H_t)\\]\nwhere: - \\(w_t^{real}\\) = real household income - \\(C_t^{child}\\) = cost of raising children (childcare, housing, education) - \\(T_t^{avail}\\) = time available for childrearing\n- \\(H_t\\) = cultural/institutional support for families\nPost-1971 Effects:\n\nReal wage stagnation (\\(\\downarrow w^{real}\\)) → Lower fertility\nDual-income necessity (\\(\\downarrow T^{avail}\\)) → Lower fertility\nHousing cost inflation (\\(\\uparrow C^{child}\\)) → Lower fertility\nInstitutional childcare replacing family (\\(\\downarrow H\\)) → Lower fertility\n\n[DATA VALIDATION PLACEHOLDER 3]:\nHYPOTHESIS: Fertility shows structural break at 1971\nTEST: Interrupted Time Series Analysis\n      Model: TFR_t = α + β₁·Year + β₂·Post1971 + β₃·(Post1971 × Year) + ε\n      Expected: β₃ &lt; 0 (steeper decline post-1971)\nDATA SOURCE: UN World Population Prospects, OECD Family Database\nCOUNTRIES: All OECD nations (n=38)\nSTATISTICAL TEST: Chow test, Quandt-Andrews unknown breakpoint test\nSTATUS: [PENDING VALIDATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-technology-endogeneity",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-technology-endogeneity",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.3 The Technology Endogeneity",
    "text": "2.3 The Technology Endogeneity\nWhat Paper R calls “technological advancement” is substantially:\n\\[A_t = A_t^{organic} + \\theta(r_t) \\cdot I_t^{debt}\\]\nwhere \\(\\theta(r_t)\\) is the debt-financed innovation multiplier (increasing as interest rates fall).\nUnder fiat currency: \\[r_t \\to 0 \\implies \\theta(r_t) \\to \\infty\\]\nThis creates an explosion of debt-financed “innovation” that appears as genuine technological progress but is actually: - Venture capital gambling enabled by cheap money - Malinvestment in unsustainable business models\n- Asset bubbles misidentified as innovation - Ponzi schemes (FTX, WeWork, etc.)\n[DATA VALIDATION PLACEHOLDER 4]:\nHYPOTHESIS: VC investment is inversely correlated with interest rates post-1971\nTEST: Regression of VC investment on Federal Funds Rate\n      Expected: β &lt; 0, R² &gt; 0.5\nDATA SOURCE: PitchBook, NVCA Yearbook, Federal Reserve H.15\nTIME PERIOD: 1971-2024\nCONTROL VARIABLES: GDP growth, corporate profits, IPO market\nSTATUS: [PENDING VALIDATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-no-control-group-problem",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-no-control-group-problem",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "3.1 The No-Control-Group Problem",
    "text": "3.1 The No-Control-Group Problem\nStandard causal inference requires a control group: - Difference-in-differences: Treated vs. untreated groups - Synthetic control: Construct counterfactual from untreated units - Regression discontinuity: Compare just above/below treatment threshold\nNone of these work for the 1971 shock because: 1. Every developed nation went fiat simultaneously 2. No country maintained gold-backed currency 3. Treatment was instantaneous and global"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#alternative-empirical-approaches",
    "href": "papers/Paper_A_Revised_1971_Shock.html#alternative-empirical-approaches",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "3.2 Alternative Empirical Approaches",
    "text": "3.2 Alternative Empirical Approaches\n\nApproach 1: Interrupted Time Series Analysis\nMethod: Test for structural breaks at 1971 in outcome variables\nSpecification: \\[Y_{i,t} = \\alpha_i + \\beta_1 \\cdot Year_t + \\beta_2 \\cdot Post1971_t + \\beta_3 \\cdot (Post1971_t \\times Year_t) + \\varepsilon_{i,t}\\]\nwhere: - \\(Y_{i,t}\\) = outcome variable (fertility, debt, wages, etc.) for country \\(i\\) at time \\(t\\) - \\(Post1971_t\\) = 1 if \\(t \\geq 1971\\), 0 otherwise - \\(\\beta_3\\) = change in trend after 1971\nTests: - Chow test: Is there a structural break at 1971? - Quandt-Andrews: What year is the most likely breakpoint? (Should be ~1971)\nOutcomes to test:\n\n\n\nVariable\nExpected Sign of β₃\nRationale\n\n\n\n\nTotal Fertility Rate\nNegative (−)\nFertility decline accelerates\n\n\nGovt Debt/GDP\nPositive (+)\nDebt grows faster\n\n\nFemale Labor Force %\nPositive (+)\nWorkforce participation jumps\n\n\nReal Wage Growth\nNegative (−)\nWage-productivity decoupling\n\n\nCPI Inflation\nPositive (+)\nFiat currency inflation\n\n\nHousing Cost/Income\nPositive (+)\nAsset price inflation\n\n\n\n[DATA VALIDATION PLACEHOLDER 5]:\nDATASET REQUIRED: Panel data for OECD countries, 1950-2024\nVARIABLES:\n  - tfr: Total Fertility Rate (births per woman)\n  - debt_gdp: Government debt as % of GDP\n  - flfp: Female labor force participation rate\n  - real_wage_growth: Annual % change in real wages\n  - productivity_growth: Annual % change in labor productivity\n  - cpi: Consumer Price Index\n  - house_price_income: Median home price / Median household income\n\nSOURCES: \n  - OECD.Stat\n  - World Bank World Development Indicators  \n  - UN Population Division\n  - BIS Property Prices Database\n\nSTATISTICAL TESTS:\n  1. Chow test (known breakpoint at 1971)\n  2. Quandt-Andrews (unknown breakpoint)\n  3. Bai-Perron (multiple breakpoints)\n\nSTATUS: [PENDING DATA COMPILATION]\n\n\nApproach 2: Differential Fiat Currency Exploitation\nWhile all countries went fiat in 1971, they differed in how aggressively they exploited the new regime:\n“Dose” variables: - Cumulative deficit spending 1971-2024 - Average debt/GDP 1971-2024\n- Monetary base expansion rate - Real interest rate suppression (deviation from natural rate)\nHypothesis: Countries that exploited fiat currency more aggressively should show: - Sharper fertility decline - Higher debt accumulation - Larger wage-productivity gaps\nCross-sectional specification: \\[\\Delta TFR_i = \\alpha + \\beta \\cdot FiatExploitation_i + \\gamma \\cdot X_i + \\varepsilon_i\\]\nwhere \\(X_i\\) includes controls (initial TFR, education, urbanization, etc.)\n[DATA VALIDATION PLACEHOLDER 6]:\nHYPOTHESIS: Fertility decline correlates with fiat currency exploitation intensity\nSAMPLE: OECD countries (n ≈ 35)\nDEPENDENT VARIABLE: \n  ΔTFRᵢ = TFR₂₀₂₄ - TFR₁₉₇₁\nINDEPENDENT VARIABLE (Fiat Exploitation Index):\n  FEIᵢ = 0.4·(Avg Debt/GDP)ᵢ + 0.3·(Cum Deficits)ᵢ + 0.3·(Real Rate Suppression)ᵢ\nCONTROL VARIABLES:\n  - Initial TFR (1971)\n  - Female education levels\n  - Urbanization rate\n  - GDP per capita\n  - Religion (% Catholic, Muslim, etc.)\nEXPECTED: β &lt; 0 (more exploitation → larger fertility decline)\nSTATUS: [PENDING ANALYSIS]\n\n\nApproach 3: Mechanism Isolation\nTest each causal pathway separately:\n\nMechanism A: Fiscal Pressure → Tax Policy → Dual Income\nTestable prediction: Countries with higher fiscal pressure (debt service/GDP) should have: - Stronger tax incentives for dual-income households - Faster female labor force participation growth\n[DATA VALIDATION PLACEHOLDER 7]:\nTEST: Panel regression\nSPECIFICATION: \n  FLFPᵢₜ = α + β₁·FiscalPressureᵢₜ + β₂·Xᵢₜ + μᵢ + λₜ + εᵢₜ\n\nWHERE:\n  FiscalPressure = (Interest Payments on Govt Debt) / GDP\n  X = controls (education, GDP, urbanization)\n  μᵢ = country fixed effects\n  λₜ = year fixed effects\n\nEXPECTED: β₁ &gt; 0\nDATA SOURCE: OECD Revenue Statistics, IMF Fiscal Monitor\nSTATUS: [PENDING ANALYSIS]\n\n\nMechanism B: Fiat Currency → Wage Suppression → Dual Income Necessity\nTestable prediction: Real wage growth should decouple from productivity growth post-1971\n[DATA VALIDATION PLACEHOLDER 8]:\nTEST: Cointegration analysis\nPERIOD 1: 1950-1970 (Gold-backed era)\nPERIOD 2: 1971-2024 (Fiat era)\n\nVARIABLES:\n  - Productivity Index (output per hour)\n  - Real Compensation Index\n\nTESTS:\n  Period 1: Expect cointegration (wages track productivity)\n  Period 2: Expect cointegration breakdown\n\nSTATISTICAL TESTS:\n  - Engle-Granger cointegration test\n  - Johansen test\n  - Rolling window correlation\n\nSTATUS: [PENDING ANALYSIS]\n\n\nMechanism C: Cheap Debt → Tech Bubble → Fertility-Hostile Culture\nTestable prediction: Lower interest rates → more “innovation” activity → lower fertility\n[DATA VALIDATION PLACEHOLDER 9]:\nTEST: Instrumental variables regression\nSPECIFICATION:\n  TFRᵢₜ = α + β₁·TechIntensityᵢₜ + β₂·Xᵢₜ + εᵢₜ\n  \nINSTRUMENT for TechIntensity:\n  Real interest rate (rₜ - πₜ)\n  \nRATIONALE:\n  Interest rates affect tech investment but don't directly affect fertility\n  (satisfies exclusion restriction)\n\nEXPECTED: β₁ &lt; 0 (more tech → lower fertility)\nSTATUS: [PENDING ANALYSIS]\n\n\n\nApproach 4: Policy Document Analysis\nHistorical Evidence of Intent:\nSearch for evidence that policymakers explicitly discussed dual-income households as tax revenue strategy.\n[DATA VALIDATION PLACEHOLDER 10]:\nDOCUMENT SEARCH:\n1. FOIA requests to:\n   - U.S. Treasury Department (1970-1980)\n   - Office of Management and Budget (1970-1980)\n   - Council of Economic Advisers (1970-1980)\n\nSEARCH TERMS:\n   - \"female labor force participation\" AND \"tax revenue\"\n   - \"dual-income\" AND \"fiscal policy\"\n   - \"women's employment\" AND \"tax base\"\n   - \"working women\" AND \"government revenue\"\n\n2. Congressional testimony search (1970-1980):\n   - House Ways and Means Committee\n   - Senate Finance Committee\n\n3. Academic literature from era:\n   - Journal of Public Economics\n   - National Tax Journal\n   \nEXPECTED FINDINGS:\n   Explicit discussion of dual-income expansion as fiscal strategy\n   \nSTATUS: [PENDING DOCUMENT REVIEW]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-endogeneity-problem",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-endogeneity-problem",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.1 The Endogeneity Problem",
    "text": "4.1 The Endogeneity Problem\nPaper R’s optimization: \\[\\max_{a, b} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(1-B_t)) dt\\]\ntreats technology path \\(a = \\{A_t\\}\\) as a choice variable.\nBut under fiat currency:\nTechnology path is endogenous to: \\[A_t = A^{organic}_t + h(D_t, r_t, M_t)\\]\nwhere: - \\(D_t\\) = government debt - \\(r_t\\) = interest rate (manipulated by central bank) - \\(M_t\\) = money supply (controlled by central bank)\nThe “choice” of technology path is not free - it’s constrained and determined by monetary regime.\nCorrect formulation: \\[\\max_{a, b, D, r, M} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(D_t, r_t, M_t)(1-B_t)) dt\\]\nsubject to: \\[\\dot{S}_t = -\\delta(A_t, B_t, F_t, D_t) S_t\\] \\[F_t = F(A_t, \\dot{A}_t, w_t^{real}, C_t)\\] \\[\\dot{D}_t = r_t D_t + G_t - T_t + M_t\\]\nThis is a fundamentally different optimization problem."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-missing-fertility-constraint",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-missing-fertility-constraint",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.2 The Missing Fertility Constraint",
    "text": "4.2 The Missing Fertility Constraint\nPaper R’s survival function: \\[S_\\infty = \\lim_{t \\to \\infty} S_t\\]\nignores that population must also survive: \\[N_\\infty = \\lim_{t \\to \\infty} N_t\\]\nPopulation dynamics: \\[\\dot{N}_t = N_t \\cdot \\frac{F_t - 2.1}{30}\\]\nIf \\(F_t &lt; 2.1\\) persistently, then \\(N_\\infty = 0\\) even if \\(S_\\infty &gt; 0\\).\nThe correct survival condition: \\[Survival = (S_\\infty &gt; 0) \\land (N_\\infty &gt; 0)\\]\nPaper R proves \\(S_\\infty &gt; 0\\) is maximized by fast growth, but ignores that fast growth → \\(F_t &lt; 2.1\\) → \\(N_\\infty = 0\\).\nThis is optimizing for extinction."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-debt-sustainability-ignored",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-debt-sustainability-ignored",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.3 The Debt Sustainability Ignored",
    "text": "4.3 The Debt Sustainability Ignored\nPaper R’s hazard function: \\[\\delta(A, B) = \\bar{\\delta} A^\\alpha (1-B)^\\beta\\]\nhas no debt variable.\nBut debt creates systemic risk: \\[\\delta_{total}(A, B, D) = \\delta(A,B) + \\lambda \\cdot g(D/GDP)\\]\nwhere \\(g(\\cdot)\\) is increasing and convex (debt crises become more likely as debt rises).\nSovereign debt crises are existential risks: - Institutional collapse - Social unrest → conflict - Inability to fund safety measures - Economic collapse → famine, disease\n[DATA VALIDATION PLACEHOLDER 11]:\nHYPOTHESIS: Sovereign debt crises increase non-linearly with debt/GDP\nTEST: Logistic regression\nDEPENDENT: Debt crisis dummy (1 if crisis occurred)\nINDEPENDENT: Debt/GDP ratio, lagged 5 years\nDATA SOURCE: Reinhart & Rogoff database, IMF Fiscal Monitor\nSAMPLE: All sovereign debt crises 1950-2024\nEXPECTED: Probability of crisis increases sharply above 90% debt/GDP\nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-kuznets-curve-failure",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-kuznets-curve-failure",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.4 The Kuznets Curve Failure",
    "text": "4.4 The Kuznets Curve Failure\nPaper R argues that rich societies spend more on safety (safety is a “luxury good” with \\(\\eta &gt; 1\\)).\nThis fails under fiat currency because:\n\n“Wealth” is illusory if debt-financed\n\nHigh consumption today via debt ≠ genuine wealth\nWhen debt becomes unsustainable, consumption collapses\n\nFiscal pressure crowds out safety spending\n\nDebt service consumes growing fraction of budget\nLess fiscal space for safety measures\nReverses the Kuznets mechanism\n\n\n[DATA VALIDATION PLACEHOLDER 12]:\nHYPOTHESIS: Safety spending (% GDP) shows inverted-U with debt levels\nTEST: Panel regression with quadratic debt term\nSPECIFICATION:\n  SafetySpendingᵢₜ = α + β₁·(Debt/GDP)ᵢₜ + β₂·(Debt/GDP)²ᵢₜ + Xᵢₜ + εᵢₜ\n\nWHERE:\n  SafetySpending = Health + Environmental + Disaster preparedness spending\n  \nEXPECTED: \n  β₁ &gt; 0 (initially increases with debt)\n  β₂ &lt; 0 (decreases at high debt levels)\n  \nDATA SOURCE: OECD Government Spending Database\nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#incorporating-monetary-regime",
    "href": "papers/Paper_A_Revised_1971_Shock.html#incorporating-monetary-regime",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.1 Incorporating Monetary Regime",
    "text": "5.1 Incorporating Monetary Regime\nThe social planner faces:\n\\[\\max_{D_t, M_t, r_t, B_t} \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} S_t N_t u(C_t) dt\\right]\\]\nsubject to:\n1. Survival dynamics: \\[\\dot{S}_t = -\\delta(A_t, B_t, F_t, D_t) S_t\\]\n2. Population dynamics: \\[\\dot{N}_t = N_t \\cdot \\frac{F_t - 2.1}{30}\\]\n3. Fertility function: \\[F_t = F(w_t^{real}, C_t^{housing}, T_t^{available}, A_t, \\dot{A}_t)\\]\n4. Technology endogeneity: \\[A_t = A_t^{organic} + \\theta(r_t, D_t) \\cdot I_t^{debt}\\]\n5. Debt dynamics: \\[\\dot{D}_t = r_t D_t + G_t - T(w_t, N_t^{working}) + M_t\\]\n6. Real wage dynamics: \\[w_t^{real} = \\frac{w_t^{nominal}(N_t^{labor}, A_t)}{P_t(M_t)}\\]\n7. Monetary regime constraint:\nUnder gold standard (pre-1971): \\[M_t \\leq Gold_{reserves} / \\text{reserve ratio}\\]\nUnder fiat currency (post-1971): \\[M_t \\in [0, \\infty) \\text{ subject to political constraints}\\]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#comparative-statics-gold-vs.-fiat",
    "href": "papers/Paper_A_Revised_1971_Shock.html#comparative-statics-gold-vs.-fiat",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.2 Comparative Statics: Gold vs. Fiat",
    "text": "5.2 Comparative Statics: Gold vs. Fiat\nProposition 5.1 (Fertility Under Alternative Regimes):\nLet \\(F^{gold}_\\infty\\) be long-run fertility under gold standard and \\(F^{fiat}_\\infty\\) under fiat currency.\nUnder plausible parameters: \\[F^{fiat}_\\infty &lt; 2.1 &lt; F^{gold}_\\infty\\]\nProof Sketch:\nUnder fiat currency: 1. Government maximizes short-run consumption via debt 2. Debt creates fiscal pressure → dual-income incentives 3. Dual incomes → wage suppression → fertility decline 4. Cheap credit → tech acceleration → fertility decline\nUnder gold standard: 1. Debt constrained → no fiscal pressure for dual incomes 2. Credit constrained → no tech bubble → slower adaptation required 3. Real wages track productivity → single income sufficient 4. Result: Fertility remains above replacement\n[DATA VALIDATION PLACEHOLDER 13]:\nCOMPARISON TEST:\nPERIOD 1: 1950-1970 (Gold-backed)\nPERIOD 2: 1971-2024 (Fiat)\n\nVARIABLES:\n  - Average TFR across OECD\n  - % of countries with TFR &gt; 2.1\n\nEXPECTED:\n  Period 1: Avg TFR &gt; 2.5, most countries &gt; 2.1\n  Period 2: Avg TFR &lt; 1.8, almost no countries &gt; 2.1\n  \nSTATISTICAL TEST:\n  t-test for difference in means\n  \nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-risk-minimizing-growth-rate-under-fiat-currency",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-risk-minimizing-growth-rate-under-fiat-currency",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.3 The Risk-Minimizing Growth Rate Under Fiat Currency",
    "text": "5.3 The Risk-Minimizing Growth Rate Under Fiat Currency\nWhen fertility constraint is binding, the risk-minimizing growth rate becomes:\n\\[\\dot{A}^* = \\arg\\min_{\\dot{A}} \\left\\{\\delta_{total}(\\dot{A}) : F(\\dot{A}) \\geq 2.1\\right\\}\\]\nIf fertility is decreasing in growth rate (\\(\\frac{\\partial F}{\\partial \\dot{A}} &lt; 0\\)), and current growth exceeds the level compatible with replacement fertility, then:\n\\[\\dot{A}^* &lt; \\dot{A}_{current}\\]\nOptimal policy is deceleration.\n[DATA VALIDATION PLACEHOLDER 14]:\nESTIMATION TASK:\nEstimate the fertility-growth elasticity: ε = ∂ln(F)/∂ln(Ȧ)\n\nMETHOD: Panel regression\nSPECIFICATION:\n  ln(TFRᵢₜ) = α + ε·ln(TechGrowthᵢₜ) + Xᵢₜ + μᵢ + λₜ + εᵢₜ\n\nWHERE:\n  TechGrowth = VC investment + R&D spending + Patent applications\n  X = controls\n  \nEXPECTED: ε &lt; 0 (faster growth → lower fertility)\n\nIf estimated ε and current Ȧ imply F &lt; 2.1:\n  Then optimal policy is Ȧ* &lt; Ȧ_current (DECELERATE)\n  \nSTATUS: [PENDING ESTIMATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-cross-country-pattern",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-cross-country-pattern",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.1 The Cross-Country Pattern",
    "text": "6.1 The Cross-Country Pattern\nPrediction from our model: All countries that went fiat in 1971 should show: 1. Fertility decline accelerating post-1971 2. Debt/GDP rising post-1971 3. Real wage stagnation beginning post-1971 4. Female labor force participation jumping post-1971\nPrediction from alternative theories: - Contraception hypothesis: Pill approved 1960, effects should appear gradually through 1960s - Education hypothesis: Education expanding throughout 20th century, effects should be smooth - Urbanization hypothesis: Ongoing process, no sharp 1971 break\nThe data will distinguish these hypotheses.\n[DATA VALIDATION PLACEHOLDER 15]:\nCOMPREHENSIVE CROSS-COUNTRY TEST:\n\nSAMPLE: All OECD countries with data availability\n\nDEPENDENT VARIABLES:\n  1. TFR (total fertility rate)\n  2. Govt Debt / GDP\n  3. Real wage growth rate\n  4. Female LFP rate\n\nFOR EACH VARIABLE:\n  - Plot time series 1950-2024 for all countries\n  - Run Chow test for break at 1971\n  - Count: How many countries show significant break at 1971?\n  \nEXPECTED UNDER OUR HYPOTHESIS:\n  &gt;80% of countries show break at 1971 across all variables\n  \nEXPECTED UNDER ALTERNATIVE HYPOTHESES:\n  Breaks should be scattered across different years/countries\n  \nSTATUS: [PENDING COMPREHENSIVE ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-israel-exception-actually-confirms-the-theory",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-israel-exception-actually-confirms-the-theory",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.2 The “Israel Exception” Actually Confirms the Theory",
    "text": "6.2 The “Israel Exception” Actually Confirms the Theory\nIsrael has TFR = 2.9, seemingly contradicting the tech→low fertility thesis.\nBut decomposing Israel’s fertility:\n\n\n\nPopulation Group\n% of Births\nTFR\nTech Participation\n\n\n\n\nSecular Jews\n~40%\n~2.1\nFull (high-tech sector)\n\n\nReligious Jews\n~35%\n~3.5\nPartial\n\n\nUltra-Orthodox\n~20%\n~7.0\nMinimal\n\n\nArabs\n~5%\n~3.0\nPartial\n\n\n\nWeighted average: \\(0.4(2.1) + 0.35(3.5) + 0.2(7.0) + 0.05(3.0) = 2.9\\)\nKey insight: - Groups fully participating in fiat-tech-dual-income system → TFR ≈ 2.1 (barely replacement) - Groups opting out → TFR &gt;&gt; 2.1\nThis is exactly what the parallel development (Amish) argument predicts.\n[DATA VALIDATION PLACEHOLDER 16]:\nISRAEL DECOMPOSITION:\nDATA SOURCE: \n  - Israel Central Bureau of Statistics\n  - Demographic studies of Israeli subpopulations\n\nVARIABLES:\n  - TFR by religious/ethnic group\n  - Labor force participation by group  \n  - Tech sector employment by group\n  - Housing costs by area (secular vs. religious cities)\n\nHYPOTHESIS:\n  Within-Israel, fertility should be inversely correlated with \n  tech sector participation even as overall TFR remains high\n\nSTATUS: [PENDING DATA COMPILATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#frances-success-is-temporary-and-costly",
    "href": "papers/Paper_A_Revised_1971_Shock.html#frances-success-is-temporary-and-costly",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.3 France’s “Success” is Temporary and Costly",
    "text": "6.3 France’s “Success” is Temporary and Costly\nFrance has TFR = 1.8, higher than most of Europe (Italy 1.2, Spain 1.2, Germany 1.5).\nBut this “success” requires: - 3.5-4% of GDP spent on family policy (OECD highest) - Massive fiscal transfers ($60+ billion annually) - Still below replacement (TFR = 1.8 &lt; 2.1) - Declining from 2.0 (2010) → 1.8 (2024)\nAnd relies on: - Immigrant fertility (2.5) vs. native French (1.7) - Full-time public childcare system - Extensive parental leave (offsetting dual-income pressure)\nInterpretation: France is fighting the fiat currency fertility collapse with enormous fiscal transfers, barely slowing the decline, at unsustainable fiscal cost.\nThis confirms rather than refutes our model - the “natural” fertility under fiat-tech-dual-income is ~1.2-1.5. France spends 4% of GDP to lift it to 1.8, still below replacement."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-acceleration-trap",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-acceleration-trap",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.1 The Acceleration Trap",
    "text": "7.1 The Acceleration Trap\nPaper R concludes: “efforts to lower x-risk by slowing the development of dangerous AI capabilities may do the opposite on balance unless sufficiently targeted.”\nUnder our framework, this is backwards.\nPaper R’s logic: - Faster growth → less time at risky tech levels → lower cumulative risk - Therefore: Accelerate\nOur logic: - Faster growth is enabled by fiat currency debt - Debt-financed growth creates fiscal pressure - Fiscal pressure → dual-income necessity → fertility collapse - Fertility collapse → \\(N_\\infty = 0\\) (extinction regardless of \\(S_\\infty\\)) - Therefore: Decelerate to sustainable rate compatible with F &gt; 2.1"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#optimizing-the-wrong-objective-function",
    "href": "papers/Paper_A_Revised_1971_Shock.html#optimizing-the-wrong-objective-function",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.2 Optimizing the Wrong Objective Function",
    "text": "7.2 Optimizing the Wrong Objective Function\nPaper R optimizes: \\[\\max S_\\infty\\]\nWe should optimize: \\[\\max (S_\\infty \\cdot N_\\infty \\cdot \\Phi_\\infty)\\]\nwhere: - \\(S_\\infty\\) = probability of avoiding catastrophe - \\(N_\\infty\\) = long-run population size - \\(\\Phi_\\infty\\) = evolutionary fitness\nThese can diverge: - High \\(S_\\infty\\), zero \\(N_\\infty\\): Survive all risks but go extinct via fertility collapse - High \\(S_\\infty\\), low \\(\\Phi_\\infty\\): Survive but with degraded health, cognition, agency\nPaper R’s framework cannot distinguish these outcomes."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-policy-implications",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-policy-implications",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.3 The Policy Implications",
    "text": "7.3 The Policy Implications\nIf policymakers accept Paper R’s framework:\n\nAccelerate AI without adequate safety (since speed reduces risk)\nIgnore fertility collapse (not in the model)\nSupport debt-financed tech bubbles (appears as genuine innovation)\nDismantle precautionary regulation (slows beneficial acceleration)\nEliminate parallel development paths (Amish-style communities seen as inefficient)\n\nEach of these increases true existential risk.\nCorrect policy under our framework:\n\nDistinguish organic from debt-financed innovation\n\nSupport genuine capability development\nCurb speculative bubbles enabled by cheap debt\n\nIntegrate fertility into technology policy\n\nAssess fertility impact of major tech deployments\nRequire family-formation support in high-tech sectors\nHousing policy coordinated with tech policy\n\nRespect biological constraints\n\nLimit rate of technological change to sustainable levels\nPreserve low-tech parallel communities (civilizational insurance)\nMaintain traditional knowledge and skills\n\nAddress debt sustainability\n\nConstrain deficit spending\nConsider return to commodity-backed currency\nReduce dependence on debt-driven growth\n\nPrecautionary approach to transformative technologies\n\nWhen institutional capacity is insufficient, delay deployment\nBuild regulatory capacity before technology deployment\nInternational coordination on governance"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#summary-of-the-refutation",
    "href": "papers/Paper_A_Revised_1971_Shock.html#summary-of-the-refutation",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.1 Summary of the Refutation",
    "text": "8.1 Summary of the Refutation\nPaper R’s core claim: Faster technological development minimizes existential risk.\nOur demonstration: This conclusion holds only if: 1. Technology path is exogenous (it’s not - it’s endogenous to monetary regime) 2. Fertility is irrelevant (it’s not - \\(N_\\infty = 0\\) is extinction) 3. Debt doesn’t create systemic risk (it does) 4. Observed growth is organic (it’s substantially debt-financed) 5. Policy responds optimally (it doesn’t - institutions lag badly)\nWhen these false assumptions are corrected, the conclusion reverses:\nThe risk-minimizing growth rate under fiat currency is substantially lower than current rates, and may be negative."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break-1",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break-1",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.2 The 1971 Structural Break",
    "text": "8.2 The 1971 Structural Break\nThe evidence (pending validation) points to:\nAugust 15, 1971 as the most consequential date in modern economic history: - Global monetary regime change - Enabling unlimited deficit spending - Creating fiscal pressure for dual-income expansion\n- Driving fertility below replacement across all developed nations - Initiating the productivity-wage divergence - Launching the debt super-cycle\nThis is not conspiracy theory - it’s structural political economy."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-path-forward",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-path-forward",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.3 The Path Forward",
    "text": "8.3 The Path Forward\nEmpirical work required:\n\nCompile comprehensive cross-country dataset (1950-2024)\nRun structural break tests for all outcome variables\nEstimate fertility-growth elasticity to find sustainable Ȧ*\nDocument policy mechanisms via archival research\nQuantify debt-innovation relationship via VC/interest rate data\n\n[MASTER DATA VALIDATION PLACEHOLDER]:\nCOMPREHENSIVE EMPIRICAL VALIDATION PLAN:\n\nPHASE 1: Data Compilation (Est. 3-6 months)\n  - Assemble OECD panel dataset 1950-2024\n  - Variables: TFR, debt, wages, productivity, FLFP, tech investment\n  - Sources: OECD, World Bank, UN, BIS, national statistical offices\n\nPHASE 2: Structural Break Analysis (Est. 2-3 months)\n  - Chow tests for 1971 break across all variables\n  - Quandt-Andrews for unknown breakpoint detection\n  - Bai-Perron for multiple breaks\n\nPHASE 3: Cross-Country Variation (Est. 2-3 months)\n  - Construct \"Fiat Exploitation Index\"\n  - Regress fertility decline on FEI\n  - Control for alternative explanations\n\nPHASE 4: Mechanism Tests (Est. 3-4 months)\n  - Fiscal pressure → FLFP relationship\n  - Wage-productivity cointegration breakdown\n  - Interest rates → VC investment → fertility\n\nPHASE 5: Policy Document Review (Est. 2-3 months)\n  - FOIA requests (may take 6-12 months for responses)\n  - Congressional testimony analysis\n  - Academic literature review (1970-1980)\n\nPHASE 6: Synthesis and Publication (Est. 3-4 months)\n  - Integrate all empirical findings\n  - Write comprehensive academic paper\n  - Submit to top journal (Ecological Economics, JEE, QJE)\n\nTOTAL ESTIMATED TIME: 18-24 months\n\nBUDGET REQUIREMENTS:\n  - Data access fees: $5,000-10,000\n  - Research assistant support: $30,000-50,000\n  - FOIA legal support: $10,000-20,000\n  - Total: $45,000-80,000"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#final-provocation",
    "href": "papers/Paper_A_Revised_1971_Shock.html#final-provocation",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.4 Final Provocation",
    "text": "8.4 Final Provocation\nA civilization that: - Survives all catastrophes (S_∞ &gt; 0) - But fails to reproduce (N_∞ = 0) - Is equally extinct\nPaper R proves the first while ignoring the second.\nWe are currently on track for: - Avoiding nuclear war ✓ - Avoiding pandemic ✓\n- Avoiding climate catastrophe ✓ - Avoiding AI catastrophe (?) - But guaranteeing demographic extinction ✗\nThe mathematics are clear: With TFR = 0.73 (South Korea), population halves every ~30 years. In 300 years (10 generations): Population = 0.1% of current In 600 years (20 generations): Population = 0.01% of current\nThis is extinction, just slow and invisible to frameworks like Paper R that don’t model fertility.\nThe 1971 monetary regime change may be the actual existential catastrophe - not a sudden collapse, but a slow-motion demographic extinction disguised as economic progress.\nPaper R, by treating technology as exogenous and fertility as irrelevant, cannot see this. Their mathematics optimizes for survival probability while ignoring that the population surviving is approaching zero.\nThis is the fundamental category error."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-a-data-requirements-summary",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-a-data-requirements-summary",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix A: Data Requirements Summary",
    "text": "Appendix A: Data Requirements Summary\nAll empirical claims in this paper are marked with [DATA VALIDATION PLACEHOLDER X].\nSummary of data requirements:\n\n\n\n\n\n\n\n\n\n\nPlaceholder #\nHypothesis\nData Needed\nSource\nStatus\n\n\n\n\n1\nTax revenue structural break\nLabor income tax / GDP, 1960-2024\nOECD Tax Revenue\nPending\n\n\n2\nWage-productivity divergence\nReal wages, productivity, 1960-2024\nBLS, OECD\nPending\n\n\n3\nFertility structural break\nTFR for all OECD, 1960-2024\nUN Population\nPending\n\n\n4\nVC-interest rate correlation\nVC investment, Fed Funds rate\nPitchBook, Fed\nPending\n\n\n5\nMulti-variable breaks\nPanel: TFR, debt, FLFP, wages\nMultiple\nPending\n\n\n6\nFiat exploitation-fertility\nCross-section debt metrics\nOECD, IMF\nPending\n\n\n7\nFiscal pressure → FLFP\nPanel regression dataset\nOECD, IMF\nPending\n\n\n8\nWage-productivity cointegration\nTime series 1950-2024\nBLS, OECD\nPending\n\n\n9\nTech intensity → fertility IV\nPanel with instruments\nOECD, World Bank\nPending\n\n\n10\nPolicy intent documents\nFOIA, Congressional records\nArchives\nPending\n\n\n11\nDebt crisis probability\nSovereign debt crisis database\nReinhart & Rogoff\nPending\n\n\n12\nSafety spending vs. debt\nGovernment spending by category\nOECD\nPending\n\n\n13\nGold vs. Fiat comparison\nTFR 1950-70 vs. 1971-2024\nUN Population\nPending\n\n\n14\nFertility-growth elasticity\nRegression dataset\nMultiple\nPending\n\n\n15\nComprehensive cross-country\nAll variables, all countries\nMultiple\nPending\n\n\n16\nIsrael subpopulation\nFertility by religious group\nIsrael CBS\nPending"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-b-technical-specifications-for-key-tests",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-b-technical-specifications-for-key-tests",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix B: Technical Specifications for Key Tests",
    "text": "Appendix B: Technical Specifications for Key Tests\n\nB.1 Structural Break Test (Chow Test)\nNull hypothesis: No structural break at 1971 Alternative: Structural break at 1971\nTest statistic: \\[F = \\frac{(RSS_r - RSS_u)/k}{RSS_u/(n-2k)} \\sim F_{k, n-2k}\\]\nwhere: - \\(RSS_r\\) = residual sum of squares (restricted model, no break) - \\(RSS_u\\) = residual sum of squares (unrestricted model, break at 1971) - \\(k\\) = number of parameters - \\(n\\) = number of observations\nReject null if: \\(F &gt; F_{critical}\\) at 5% significance level\n\n\nB.2 Interrupted Time Series Specification\nFull specification: \\[Y_{i,t} = \\alpha_i + \\beta_1 \\cdot Year_t + \\beta_2 \\cdot Post1971_t + \\beta_3 \\cdot (Post1971_t \\times Year_t) + \\gamma \\cdot X_{i,t} + \\varepsilon_{i,t}\\]\nInterpretation: - \\(\\alpha_i\\) = country fixed effects - \\(\\beta_1\\) = pre-1971 trend - \\(\\beta_2\\) = level shift at 1971 - \\(\\beta_3\\) = change in trend post-1971 - \\(X_{i,t}\\) = control variables\nKey test: \\(H_0: \\beta_3 = 0\\) vs. \\(H_A: \\beta_3 \\neq 0\\)\n\n\nB.3 Fiat Exploitation Index Construction\nComponents: 1. Debt Accumulation: \\(\\Delta(Debt/GDP)_{1971-2024}\\) 2. Deficit Intensity: Average (Deficit/GDP) over 1971-2024 3. Monetary Expansion: \\(\\Delta \\log(M2)\\) above GDP growth 4. Real Rate Suppression: Average \\((r_{natural} - r_{actual})\\)\nNormalization: Each component standardized to [0,1]\nAggregation: \\[FEI_i = 0.3 \\cdot Debt_i + 0.3 \\cdot Deficit_i + 0.2 \\cdot Monetary_i + 0.2 \\cdot RateSuppression_i\\]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-c-alternative-explanations-and-how-to-rule-them-out",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-c-alternative-explanations-and-how-to-rule-them-out",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix C: Alternative Explanations and How to Rule Them Out",
    "text": "Appendix C: Alternative Explanations and How to Rule Them Out\n\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nPrediction\nOur Prediction\nDistinguishing Test\n\n\n\n\nThe Pill (contraception)\nGradual effect through 1960s\nSharp break at 1971\nStructural break test\n\n\nEducation expansion\nSmooth decline as education rises\nAcceleration post-1971\nInteraction: education × post1971\n\n\nUrbanization\nOngoing effect (no break)\nBreak at 1971\nChow test controlling for urbanization\n\n\nSecularization\nGradual effect\nBreak at 1971\nControl for religious attendance\n\n\nWomen’s lib movement\nEffect through 1960s-70s\nSpecific break at 1971\nTiming of policy changes\n\n\n\nCombined test: \\[TFR_{i,t} = \\alpha + \\beta_1 Post1971 + \\beta_2 Pill + \\beta_3 Education + \\beta_4 Urban + \\beta_5 Secular + \\varepsilon_{i,t}\\]\nIf our hypothesis is correct: \\(\\beta_1\\) remains significant and large even controlling for all alternatives.\n\nEND OF PAPER"
  },
  {
    "objectID": "notebooks/Empirical Validation of the 1971 Fiscal Structural Break - Analysis of U.S Real Individual Income Tax Revenue.html",
    "href": "notebooks/Empirical Validation of the 1971 Fiscal Structural Break - Analysis of U.S Real Individual Income Tax Revenue.html",
    "title": "Empirical Validation of the 1971 Fiscal Structural Break: Analysis of U.S. Real Individual Income Tax Revenue",
    "section": "",
    "text": "\"\"\"\nSimple script to download US Individual Income Tax / GDP data from FRED\nNo complex dependencies - just pandas and requests\n\"\"\"\n\nimport pandas as pd\nimport requests\nfrom io import StringIO\n\nprint(\"Downloading US Individual Income Tax and GDP data from FRED...\")\nprint(\"=\" * 80)\n\n# FRED series IDs\n# W006RC1Q027SBEA = Individual Income Tax Receipts (quarterly, billions)\n# GDP = Gross Domestic Product (quarterly, billions)\n\ndef download_fred_series(series_id, series_name):\n    \"\"\"Download a single series from FRED as CSV.\"\"\"\n    url = f\"https://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}\"\n    \n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        \n        # Read CSV\n        df = pd.read_csv(StringIO(response.text))\n        df.columns = ['DATE', series_name]\n        df['DATE'] = pd.to_datetime(df['DATE'])\n        df[series_name] = pd.to_numeric(df[series_name], errors='coerce')\n        \n        print(f\"✓ Downloaded {series_name}: {len(df)} observations\")\n        return df\n    \n    except Exception as e:\n        print(f\"✗ Error downloading {series_name}: {e}\")\n        return None\n\n# Download data\nprint(\"\\n1. Downloading Individual Income Tax receipts...\")\ntax_df = download_fred_series('W006RC1Q027SBEA', 'IndividualIncomeTax')\n\nDownloading US Individual Income Tax and GDP data from FRED...\n================================================================================\n\n1. Downloading Individual Income Tax receipts...\n✓ Downloaded IndividualIncomeTax: 315 observations\n\n\n\ntax_df.tail()\n\n\n\n\n\n\n\n\nDATE\nIndividualIncomeTax\n\n\n\n\n310\n2024-07-01\n3141.087\n\n\n311\n2024-10-01\n3202.119\n\n\n312\n2025-01-01\n3246.051\n\n\n313\n2025-04-01\n3455.060\n\n\n314\n2025-07-01\n3599.024\n\n\n\n\n\n\n\n\ntax_df_indexed = tax_df.set_index(\"DATE\")\n\n\nimport pandas as pd\n\ndef to_annual_series(\n    tax_df: pd.DataFrame,\n    date_col: str = \"DATE\",\n    value_col: str = \"IndividualIncomeTax\",\n    agg: str = \"mean\",  # \"mean\" (default) or \"sum\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert a quarterly (or higher frequency) dataframe with a DATE column\n    into an annual dataframe with columns: Year, IndividualIncomeTax_Annual.\n\n    agg:\n      - \"mean\": good for level/rate-like series (e.g., % of GDP, index levels, rates)\n      - \"sum\" : good for flow series (e.g., revenues within year)\n    \"\"\"\n    df = tax_df.copy()\n\n    # If DATE is the index, bring it back as a column\n    if date_col not in df.columns and df.index.name == date_col:\n        df = df.reset_index()\n\n    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n    df = df.dropna(subset=[date_col, value_col])\n\n    df[\"Year\"] = df[date_col].dt.year.astype(int)\n\n    if agg not in (\"mean\", \"sum\", \"median\"):\n        raise ValueError(\"agg must be one of: 'mean', 'sum', 'median'\")\n\n    annual = (\n        df.groupby(\"Year\", as_index=False)[value_col]\n          .agg({f\"{value_col}_Annual\": agg})\n          .sort_values(\"Year\")\n          .reset_index(drop=True)\n    )\n    return annual\n\n\nannual_tax = to_annual_series(tax_df, value_col=\"IndividualIncomeTax\", agg=\"mean\")\nannual_tax.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\n\n\n\n\n74\n2021\n2675.071500\n\n\n75\n2022\n3252.924000\n\n\n76\n2023\n2918.060500\n\n\n77\n2024\n3118.884750\n\n\n78\n2025\n3433.378333\n\n\n\n\n\n\n\n\ncpi_df = download_fred_series('CPIAUCSL', 'CPI')\ncpi_df.tail()\nannual_cpi = to_annual_series(tax_df, value_col=\"CPI\", agg=\"mean\")\nannual_cpi.tail()\n\n✓ Downloaded CPI: 947 observations\n\n\n\n\n\n\n\n\n\nYear\nCPI_Annual\n\n\n\n\n74\n2021\n270.967917\n\n\n75\n2022\n292.625417\n\n\n76\n2023\n304.704167\n\n\n77\n2024\n313.697833\n\n\n78\n2025\n321.577200\n\n\n\n\n\n\n\n\n# Step 2: Merge with your tax data\ndf = pd.merge(annual_tax, annual_cpi_df, on='Year', how='inner')\ndf.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\nCPI_Annual\n\n\n\n\n74\n2021\n2675.071500\n270.967917\n\n\n75\n2022\n3252.924000\n292.625417\n\n\n76\n2023\n2918.060500\n304.704167\n\n\n77\n2024\n3118.884750\n313.697833\n\n\n78\n2025\n3433.378333\n321.577200\n\n\n\n\n\n\n\n\n# Step 3: Calculate Real Tax (in 2024 dollars)\nlatest_cpi = annual_cpi[annual_cpi['Year'] == 2024]['CPI_Annual'].mean()\ndf['Real_Tax'] = (df['IndividualIncomeTax_Annual'] / df['CPI_Annual']) * latest_cpi\ndf.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\nCPI_Annual\nReal_Tax\n\n\n\n\n74\n2021\n2675.071500\n270.967917\n3096.913258\n\n\n75\n2022\n3252.924000\n292.625417\n3487.172175\n\n\n76\n2023\n2918.060500\n304.704167\n3004.190151\n\n\n77\n2024\n3118.884750\n313.697833\n3118.884750\n\n\n78\n2025\n3433.378333\n321.577200\n3349.252821\n\n\n\n\n\n\n\n\ndf = df.set_index('Year')\ndf['Real_Tax'].plot()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.api import OLS, add_constant\nfrom scipy import stats\n\ndef chow_test_tax_series(\n    df_annual: pd.DataFrame,\n    y_col: str,\n    break_year: int = 1971,\n    min_obs_each_side: int = 8,\n    save_csv_path: str = None\n):\n    \"\"\"\n    Chow test for a structural break at a known year on an annual series.\n\n    Model: y ~ const + t\n    where t is a normalized year index for numerical stability.\n\n    H0: No structural break (β_pre = β_post)\n    H1: Structural break exists\n    \"\"\"\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"CHOW TEST FOR STRUCTURAL BREAK\")\n    print(\"=\" * 80)\n\n    # Clean and keep what we need\n    df_clean = df_annual[[\"Year\", y_col]].dropna().copy()\n    df_clean = df_clean.sort_values(\"Year\").reset_index(drop=True)\n\n    # Split (match your original: pre &lt; break_year, post &gt;= break_year)\n    df_pre = df_clean[df_clean[\"Year\"] &lt; break_year].copy()\n    df_post = df_clean[df_clean[\"Year\"] &gt;= break_year].copy()\n\n    n_pre, n_post = len(df_pre), len(df_post)\n    n_total = n_pre + n_post\n\n    print(f\"\\nBreak year: {break_year}\")\n    print(f\"Observations pre-break: {n_pre}\")\n    print(f\"Observations post-break: {n_post}\")\n    print(f\"Total observations: {n_total}\")\n\n    if n_pre &lt; min_obs_each_side or n_post &lt; min_obs_each_side:\n        raise ValueError(\n            f\"Not enough annual observations on each side of {break_year}. \"\n            f\"Need at least {min_obs_each_side} each side. Got pre={n_pre}, post={n_post}.\"\n        )\n\n    # Normalize time index for stability (use the same reference for all)\n    base_year = df_clean[\"Year\"].min()\n    df_clean[\"t\"] = df_clean[\"Year\"] - base_year\n    df_pre[\"t\"] = df_pre[\"Year\"] - base_year\n    df_post[\"t\"] = df_post[\"Year\"] - base_year\n\n    # Pooled regression\n    X_pooled = add_constant(df_clean[\"t\"].astype(float))\n    y_pooled = df_clean[y_col].astype(float)\n    model_pooled = OLS(y_pooled, X_pooled).fit()\n    RSS_pooled = float(model_pooled.ssr)\n\n    # Separate regressions\n    X_pre = add_constant(df_pre[\"t\"].astype(float))\n    y_pre = df_pre[y_col].astype(float)\n    model_pre = OLS(y_pre, X_pre).fit()\n    RSS_pre = float(model_pre.ssr)\n\n    X_post = add_constant(df_post[\"t\"].astype(float))\n    y_post = df_post[y_col].astype(float)\n    model_post = OLS(y_post, X_post).fit()\n    RSS_post = float(model_post.ssr)\n\n    RSS_separate = RSS_pre + RSS_post\n\n    # Chow statistic\n    k = 2  # intercept + slope\n    df_num = k\n    df_den = n_total - 2 * k\n\n    F_stat = ((RSS_pooled - RSS_separate) / k) / (RSS_separate / df_den)\n    p_value = 1 - stats.f.cdf(F_stat, df_num, df_den)\n\n    critical_05 = stats.f.ppf(0.95, df_num, df_den)\n    critical_01 = stats.f.ppf(0.99, df_num, df_den)\n\n    print(\"\\n\" + \"-\" * 80)\n    print(\"RESULTS:\")\n    print(\"-\" * 80)\n    print(f\"\\nRSS Pooled (restricted): {RSS_pooled:.4f}\")\n    print(f\"RSS Separate (unrestricted): {RSS_separate:.4f}\")\n    print(f\"  RSS Pre-break: {RSS_pre:.4f}\")\n    print(f\"  RSS Post-break: {RSS_post:.4f}\")\n\n    print(f\"\\nChow F-statistic: {F_stat:.4f}\")\n    print(f\"Degrees of freedom: ({df_num}, {df_den})\")\n    print(f\"P-value: {p_value:.6f}\")\n\n    print(f\"\\nCritical values:\")\n    print(f\"  5% level: {critical_05:.4f}\")\n    print(f\"  1% level: {critical_01:.4f}\")\n\n    print(\"\\nConclusion:\")\n    if p_value &lt; 0.01:\n        result = \"Reject H0 (1%)\"\n        print(f\"  *** REJECT NULL HYPOTHESIS at 1% level ***\")\n        print(f\"  Strong evidence of structural break at {break_year}\")\n    elif p_value &lt; 0.05:\n        result = \"Reject H0 (5%)\"\n        print(f\"  ** REJECT NULL HYPOTHESIS at 5% level **\")\n        print(f\"  Significant evidence of structural break at {break_year}\")\n    elif p_value &lt; 0.10:\n        result = \"Reject H0 (10%)\"\n        print(f\"  * REJECT NULL HYPOTHESIS at 10% level *\")\n        print(f\"  Moderate evidence of structural break at {break_year}\")\n    else:\n        result = \"Fail to Reject H0\"\n        print(f\"  FAIL TO REJECT NULL HYPOTHESIS\")\n        print(f\"  Insufficient evidence of structural break at {break_year}\")\n\n    # Coefficients\n    print(\"\\n\" + \"-\" * 80)\n    print(\"REGRESSION COEFFICIENTS:\")\n    print(\"-\" * 80)\n\n    print(f\"\\nPre-{break_year} period:\")\n    print(f\"  Intercept: {model_pre.params.iloc[0]:.4f} (se: {model_pre.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_pre.params.iloc[1]:.4f} (se: {model_pre.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_pre.rsquared:.4f}\")\n\n    print(f\"\\nPost-{break_year} period:\")\n    print(f\"  Intercept: {model_post.params.iloc[0]:.4f} (se: {model_post.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_post.params.iloc[1]:.4f} (se: {model_post.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_post.rsquared:.4f}\")\n\n    print(f\"\\nPooled (no break):\")\n    print(f\"  Intercept: {model_pooled.params.iloc[0]:.4f} (se: {model_pooled.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_pooled.params.iloc[1]:.4f} (se: {model_pooled.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_pooled.rsquared:.4f}\")\n\n    results_dict = {\n        \"Test\": \"Chow Test\",\n        \"Series\": y_col,\n        \"Break Year\": break_year,\n        \"F-statistic\": float(F_stat),\n        \"P-value\": float(p_value),\n        \"Critical Value (5%)\": float(critical_05),\n        \"Critical Value (1%)\": float(critical_01),\n        \"Result\": result,\n        \"Slope Pre\": float(model_pre.params.iloc[1]),\n        \"Slope Post\": float(model_post.params.iloc[1]),\n        \"Slope Change\": float(model_post.params.iloc[1] - model_pre.params.iloc[1]),\n        \"n_pre\": int(n_pre),\n        \"n_post\": int(n_post),\n    }\n\n    if save_csv_path is not None:\n        pd.DataFrame([results_dict]).to_csv(save_csv_path, index=False)\n\n    return results_dict\n\n\n# Chow test at 1971\nres = chow_test_tax_series(\n    df,\n    y_col=\"Real_Tax\",\n    break_year=1971\n)\n\nres\n\n\n================================================================================\nCHOW TEST FOR STRUCTURAL BREAK\n================================================================================\n\nBreak year: 1971\nObservations pre-break: 24\nObservations post-break: 55\nTotal observations: 79\n\n--------------------------------------------------------------------------------\nRESULTS:\n--------------------------------------------------------------------------------\n\nRSS Pooled (restricted): 3811995.5571\nRSS Separate (unrestricted): 3034840.0098\n  RSS Pre-break: 84072.8198\n  RSS Post-break: 2950767.1900\n\nChow F-statistic: 9.6029\nDegrees of freedom: (2, 75)\nP-value: 0.000194\n\nCritical values:\n  5% level: 3.1186\n  1% level: 4.8999\n\nConclusion:\n  *** REJECT NULL HYPOTHESIS at 1% level ***\n  Strong evidence of structural break at 1971\n\n--------------------------------------------------------------------------------\nREGRESSION COEFFICIENTS:\n--------------------------------------------------------------------------------\n\nPre-1971 period:\n  Intercept: 477.9496 (se: 24.4684)\n  Slope:     27.4835 (se: 1.8229)\n  R-squared: 0.9118\n\nPost-1971 period:\n  Intercept: 2.3028 (se: 107.0530)\n  Slope:     37.3621 (se: 2.0042)\n  R-squared: 0.8677\n\nPooled (no break):\n  Intercept: 349.9247 (se: 49.5949)\n  Slope:     31.2690 (se: 1.0978)\n  R-squared: 0.9133\n\n\n{'Test': 'Chow Test',\n 'Series': 'Real_Tax',\n 'Break Year': 1971,\n 'F-statistic': 9.602922372192094,\n 'P-value': 0.00019358783729239715,\n 'Critical Value (5%)': 3.118642128006125,\n 'Critical Value (1%)': 4.899877423111457,\n 'Result': 'Reject H0 (1%)',\n 'Slope Pre': 27.483480442522755,\n 'Slope Post': 37.362148635986124,\n 'Slope Change': 9.878668193463369,\n 'n_pre': 24,\n 'n_post': 55}"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html",
    "href": "notebooks/Education and QoL-Satisfaction.html",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "",
    "text": "This thesis explores the relationship between the length of formal education and overall quality of life (QoL), considering the evolving economic landscape influenced by M0/1/2/Consumer inflation. It hypothesizes that while extended education initially correlated with improved QoL through higher income, better employment, stable marriages, and overall well-being, lately, the benefits of prolonged education have diminished in the face of increasing economic pressures, competitive stress, and societal shifts. The study uses a combination of economic theory, quantitative data analysis, and sociological perspectives to examine how the changing cost-benefit dynamics of education affect life satisfaction in modern times.\n\n\nBackground\nEducation and Quality of Life: Historically, education has been seen as a key driver of social mobility and improved quality of life. Extended years in education have been associated with higher income, better job prospects, stable marriages, and improved health outcomes. M2 Inflation and Economic Environment: Over the past few decades, the global economic environment has been influenced by central banks’ monetary policies, including significant increases in M2 money supply. This has led to inflationary pressures, impacting the cost of education, living standards, and overall socioeconomic structures.\n1.2 Research Problem\nThe traditional view that more education equates to better QoL is being challenged by new economic realities. Increasing costs of education, diminishing returns on educational investment, and changing societal expectations have altered the landscape.\n1.3 Hypothesis\nUp to a certain period, increased years of education correlated positively with improvements in QoL. However, as M0/1/2/Consumer inflation increased, leading to rising educational costs and altered economic dynamics, the benefits of prolonged education have diminished, even potentially reversing in certain cases.\n\n\n\n\n\n\n\nIncome and Job Stability: Traditionally, longer years in education have been associated with higher income and better job stability. Research consistently shows that individuals with more education tend to earn more over their lifetimes and are less likely to be unemployed. This connection has been a key driver for the push towards extended education in many societies​. No longer True.\nSocial Status and Mobility: Extended education has also been linked with higher social status and greater social mobility. The credentials obtained through prolonged education often serve as markers of social class, allowing individuals to access higher social and professional circles. No longer True.\n\n\n\n\n\nWage Stagnation: In recent decades, there has been growing concern that despite increasing levels of education, wages have not kept pace with productivity gains. This wage stagnation, coupled with rising education costs, has led to a situation where the financial returns on education may no longer justify the investment, thus reducing the perceived value of long-term education in improving QoL​ (World Bank).\nUnderemployment: Many graduates find themselves in jobs that do not require the level of education they have attained, leading to underemployment. This mismatch between education and job requirements can contribute to dissatisfaction and frustration.\n\n\n\n\n\nMental Health Impacts: As the demand for higher education has increased, so has the competition, leading to significant stress among students. The pressure to perform well academically to secure top-tier jobs has contributed to a rise in mental health issues, including anxiety and depression. This competitive stress can negate some of the QoL improvements associated with higher education​ (World Bank).\nWork-Life Imbalance: The need to excel in education often leads to work-life imbalances, where students and young professionals may sacrifice leisure and family time for academic or career success, potentially reducing overall life satisfaction.\n\n\n\n\n\nHealth Implications: Higher levels of education are often associated with sedentary jobs, such as office work, which can lead to health issues like obesity, cardiovascular diseases, and mental health problems. The sedentary lifestyle that accompanies many highly educated professions may undermine the health benefits that should accompany better employment and income​ (Our World in Data).\nReduced Physical Activity: As people spend more time in education and subsequently in knowledge-intensive jobs, they may have less time for physical activities, which negatively impacts their overall well-being. T-leves for men.\n\n\n\n\n\nMarital Satisfaction and Selection Pressure: Educated individuals, especially women, may face higher expectations in partner selection, leading to delays in marriage or dissatisfaction due to mismatched expectations. The emphasis on finding a partner with similar educational or socioeconomic status can create additional stress and reduce life satisfaction​ (Our World in Data).\nFamily Dynamics: Higher education levels can lead to different expectations in family roles, potentially causing conflicts or dissatisfaction within marriages, especially if traditional roles are challenged.\n\n\n\n\n\nEntrepreneurial Barriers: Educated individuals may feel trapped in their career paths, unable to pivot to entrepreneurship or other non-traditional roles without risking social stigma. This perception of certain businesses as “lower socioeconomic” can prevent them from pursuing potentially fulfilling and profitable ventures​ (World Bank).\nRisk Aversion: Higher education often leads to risk aversion, where individuals prefer the stability of employment over the uncertainties of starting a business. This conservative approach can limit their opportunities for significant QoL improvements through entrepreneurship.\n\n\n\n\n\nJob Dissatisfaction: Many graduates find themselves in service-oriented jobs that may not align with their education or career aspirations. The servitude nature of these jobs, coupled with the disconnect from their education, can lead to job dissatisfaction and lower life satisfaction​.\nEconomic Pressure: The necessity to repay student loans and meet living expenses often forces educated individuals into jobs that do not utilize their full potential, leading to feelings of underachievement and frustration.\n\n\n\n\n\nSocioeconomic Barriers: Entrepreneurship increasingly appears to be dominated by individuals with access to significant capital, often from affluent backgrounds. This reduces the chances for those from less privileged backgrounds, even if they are highly educated, to pursue entrepreneurial opportunities​ (World Bank).\nVC and Networking Challenges: Access to venture capital and entrepreneurial networks is often limited to those with the right connections or backgrounds, making it difficult for highly educated individuals without these advantages to succeed in starting their own businesses.\n\n\n\n\n\nFinancial Independence: As individuals realize the time and stress associated with traditional employment, many are turning to passive investing strategies, such as investing in index funds (e.g., SPY), to achieve financial independence and improve QoL. This shift reflects a growing understanding that financial security can be achieved through means other than prolonged education and employment​ (World Bank).\nShift in Priorities: The realization that passive income (and/or wealth creation) from investments can provide a more stable and less stressful life has led many to question the traditional emphasis on education as the primary path to a better QoL.\n\n\n\n\n\nNetworking and Social Capital: Increasingly, education is seen as a means to gain entry into exclusive professional networks (e.g., MBA, IB, VC, PE) rather than purely for knowledge acquisition. This shift has implications for QoL, as the primary value of education becomes social capital rather than personal or intellectual development​ (Our World in Data).\nDecline in Knowledge-Based QoL: As access to knowledge becomes more democratized through the internet, the direct impact of education on QoL through knowledge acquisition has diminished. The focus on networking rather than knowledge challenges the traditional notion of education as a means to improve QoL through intellectual growth.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom statsmodels.tsa.stattools import grangercausalitytests\nimport pandas_datareader.data as web\nimport requests\n\nfrom io import StringIO\n\nstart_date_str = '1979-01-01'\nstart_date = datetime.strptime(start_date_str,'%Y-%m-%d')\nend_date = datetime.today()"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#the-evolving-impact-of-education-on-quality-of-life-in-the-context-of-m012consumer-inflation-and-socioeconomic-shifts",
    "href": "notebooks/Education and QoL-Satisfaction.html#the-evolving-impact-of-education-on-quality-of-life-in-the-context-of-m012consumer-inflation-and-socioeconomic-shifts",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "",
    "text": "This thesis explores the relationship between the length of formal education and overall quality of life (QoL), considering the evolving economic landscape influenced by M0/1/2/Consumer inflation. It hypothesizes that while extended education initially correlated with improved QoL through higher income, better employment, stable marriages, and overall well-being, lately, the benefits of prolonged education have diminished in the face of increasing economic pressures, competitive stress, and societal shifts. The study uses a combination of economic theory, quantitative data analysis, and sociological perspectives to examine how the changing cost-benefit dynamics of education affect life satisfaction in modern times.\n\n\nBackground\nEducation and Quality of Life: Historically, education has been seen as a key driver of social mobility and improved quality of life. Extended years in education have been associated with higher income, better job prospects, stable marriages, and improved health outcomes. M2 Inflation and Economic Environment: Over the past few decades, the global economic environment has been influenced by central banks’ monetary policies, including significant increases in M2 money supply. This has led to inflationary pressures, impacting the cost of education, living standards, and overall socioeconomic structures.\n1.2 Research Problem\nThe traditional view that more education equates to better QoL is being challenged by new economic realities. Increasing costs of education, diminishing returns on educational investment, and changing societal expectations have altered the landscape.\n1.3 Hypothesis\nUp to a certain period, increased years of education correlated positively with improvements in QoL. However, as M0/1/2/Consumer inflation increased, leading to rising educational costs and altered economic dynamics, the benefits of prolonged education have diminished, even potentially reversing in certain cases.\n\n\n\n\n\n\n\nIncome and Job Stability: Traditionally, longer years in education have been associated with higher income and better job stability. Research consistently shows that individuals with more education tend to earn more over their lifetimes and are less likely to be unemployed. This connection has been a key driver for the push towards extended education in many societies​. No longer True.\nSocial Status and Mobility: Extended education has also been linked with higher social status and greater social mobility. The credentials obtained through prolonged education often serve as markers of social class, allowing individuals to access higher social and professional circles. No longer True.\n\n\n\n\n\nWage Stagnation: In recent decades, there has been growing concern that despite increasing levels of education, wages have not kept pace with productivity gains. This wage stagnation, coupled with rising education costs, has led to a situation where the financial returns on education may no longer justify the investment, thus reducing the perceived value of long-term education in improving QoL​ (World Bank).\nUnderemployment: Many graduates find themselves in jobs that do not require the level of education they have attained, leading to underemployment. This mismatch between education and job requirements can contribute to dissatisfaction and frustration.\n\n\n\n\n\nMental Health Impacts: As the demand for higher education has increased, so has the competition, leading to significant stress among students. The pressure to perform well academically to secure top-tier jobs has contributed to a rise in mental health issues, including anxiety and depression. This competitive stress can negate some of the QoL improvements associated with higher education​ (World Bank).\nWork-Life Imbalance: The need to excel in education often leads to work-life imbalances, where students and young professionals may sacrifice leisure and family time for academic or career success, potentially reducing overall life satisfaction.\n\n\n\n\n\nHealth Implications: Higher levels of education are often associated with sedentary jobs, such as office work, which can lead to health issues like obesity, cardiovascular diseases, and mental health problems. The sedentary lifestyle that accompanies many highly educated professions may undermine the health benefits that should accompany better employment and income​ (Our World in Data).\nReduced Physical Activity: As people spend more time in education and subsequently in knowledge-intensive jobs, they may have less time for physical activities, which negatively impacts their overall well-being. T-leves for men.\n\n\n\n\n\nMarital Satisfaction and Selection Pressure: Educated individuals, especially women, may face higher expectations in partner selection, leading to delays in marriage or dissatisfaction due to mismatched expectations. The emphasis on finding a partner with similar educational or socioeconomic status can create additional stress and reduce life satisfaction​ (Our World in Data).\nFamily Dynamics: Higher education levels can lead to different expectations in family roles, potentially causing conflicts or dissatisfaction within marriages, especially if traditional roles are challenged.\n\n\n\n\n\nEntrepreneurial Barriers: Educated individuals may feel trapped in their career paths, unable to pivot to entrepreneurship or other non-traditional roles without risking social stigma. This perception of certain businesses as “lower socioeconomic” can prevent them from pursuing potentially fulfilling and profitable ventures​ (World Bank).\nRisk Aversion: Higher education often leads to risk aversion, where individuals prefer the stability of employment over the uncertainties of starting a business. This conservative approach can limit their opportunities for significant QoL improvements through entrepreneurship.\n\n\n\n\n\nJob Dissatisfaction: Many graduates find themselves in service-oriented jobs that may not align with their education or career aspirations. The servitude nature of these jobs, coupled with the disconnect from their education, can lead to job dissatisfaction and lower life satisfaction​.\nEconomic Pressure: The necessity to repay student loans and meet living expenses often forces educated individuals into jobs that do not utilize their full potential, leading to feelings of underachievement and frustration.\n\n\n\n\n\nSocioeconomic Barriers: Entrepreneurship increasingly appears to be dominated by individuals with access to significant capital, often from affluent backgrounds. This reduces the chances for those from less privileged backgrounds, even if they are highly educated, to pursue entrepreneurial opportunities​ (World Bank).\nVC and Networking Challenges: Access to venture capital and entrepreneurial networks is often limited to those with the right connections or backgrounds, making it difficult for highly educated individuals without these advantages to succeed in starting their own businesses.\n\n\n\n\n\nFinancial Independence: As individuals realize the time and stress associated with traditional employment, many are turning to passive investing strategies, such as investing in index funds (e.g., SPY), to achieve financial independence and improve QoL. This shift reflects a growing understanding that financial security can be achieved through means other than prolonged education and employment​ (World Bank).\nShift in Priorities: The realization that passive income (and/or wealth creation) from investments can provide a more stable and less stressful life has led many to question the traditional emphasis on education as the primary path to a better QoL.\n\n\n\n\n\nNetworking and Social Capital: Increasingly, education is seen as a means to gain entry into exclusive professional networks (e.g., MBA, IB, VC, PE) rather than purely for knowledge acquisition. This shift has implications for QoL, as the primary value of education becomes social capital rather than personal or intellectual development​ (Our World in Data).\nDecline in Knowledge-Based QoL: As access to knowledge becomes more democratized through the internet, the direct impact of education on QoL through knowledge acquisition has diminished. The focus on networking rather than knowledge challenges the traditional notion of education as a means to improve QoL through intellectual growth.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom statsmodels.tsa.stattools import grangercausalitytests\nimport pandas_datareader.data as web\nimport requests\n\nfrom io import StringIO\n\nstart_date_str = '1979-01-01'\nstart_date = datetime.strptime(start_date_str,'%Y-%m-%d')\nend_date = datetime.today()"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#long-years-in-education-job-stability-and-income-mobility-1",
    "href": "notebooks/Education and QoL-Satisfaction.html#long-years-in-education-job-stability-and-income-mobility-1",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "1. Long Years in Education, Job Stability and Income Mobility:",
    "text": "1. Long Years in Education, Job Stability and Income Mobility:\nTraditionally, more years in education have been strongly associated with higher income and better job stability. However, several factors, including M0/1/2/Consumer inflation and technological disruption, have weakened this association in recent years."
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#career-sharpe-ratio-formal-definition",
    "href": "notebooks/Education and QoL-Satisfaction.html#career-sharpe-ratio-formal-definition",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "Career Sharpe Ratio — Formal Definition",
    "text": "Career Sharpe Ratio — Formal Definition\n\nExpected real earnings\n\\[\nE_t\n=\n\\frac{W_t}{P_t / 100}\n\\cdot\n(1 - u_t)\n\\]\nwhere:\n\\[\n\\begin{aligned}\nW_t & = \\text{nominal median earnings at time } t \\\\\nP_t & = \\text{price level (CPI index, base } 100) \\\\\nu_t & = \\text{unemployment rate at time } t\n\\end{aligned}\n\\]\n\n\n\nCareer return (growth)\n\\[\nr_t\n=\n\\ln(E_t)\n-\n\\ln(E_{t-1})\n\\]\n\n\n\nCareer Sharpe Ratio\nLet:\n\\[\n\\mu_r = \\mathbb{E}[r_t]\n\\]\n\\[\n\\sigma_r = \\sqrt{\\mathbb{V}[r_t]}\n\\]\nLet ( k ) denote the number of periods per year (e.g., ( k = 4 ) for quarterly data).\n\\[\n\\text{CareerSharpe}\n=\n\\frac{k \\, \\mu_r}{\\sqrt{k} \\, \\sigma_r}\n\\]\nor equivalently,\n\\[\n\\text{CareerSharpe}\n=\n\\frac{\\mathbb{E}[r_t]}{\\sqrt{\\mathbb{V}[r_t]}}\n\\cdot\n\\sqrt{k}\n\\]\n\n\n\nInterpretation\n\\[\n\\text{CareerSharpe} &gt; 0\n\\;\\Rightarrow\\;\n\\text{growth dominates volatility (stable career)}\n\\]\n\\[\n\\text{CareerSharpe} = 0\n\\;\\Rightarrow\\;\n\\text{growth equals instability}\n\\]\n\\[\n\\text{CareerSharpe} &lt; 0\n\\;\\Rightarrow\\;\n\\text{volatility dominates growth (fragile career)}\n\\]\n\n\n\nIndividual-level extension\n\\[\n\\text{CareerSharpe}_i\n=\n\\frac{\\mathbb{E}[r_{i,t}]}{\\sqrt{\\mathbb{V}[r_{i,t}]}}\n\\cdot\n\\sqrt{k}\n\\]\n\nEARNING_FRED_SERIES = {\n    # ------------------------------------------------------------------\n    # Median usual weekly nominal earnings\n    # Full-time wage & salary workers, age 25+\n    # Quarterly\n    # ------------------------------------------------------------------\n    \"earn_lt_hs_q\": \"LEU0252920700Q\",   # Less than HS diploma, 25+\n    \"earn_hs_q\":    \"LEU0252917300Q\",   # HS graduates, no college, 25+\n    \"earn_some_q\":  \"LEU0254929400Q\",   # Some college or associate degree, 25+\n    \"earn_ba_q\":    \"LEU0252919100Q\",   # Bachelor's degree only, 25+\n    \"earn_adv_q\":   \"LEU0252919700Q\",   # Advanced degree, 25+\n\n    # ------------------------------------------------------------------\n    # Unemployment rates\n    # Age 25+, monthly, seasonally adjusted\n    # ------------------------------------------------------------------\n    \"unemp_lt_hs_m\": \"LNS14027659\",     # Less than HS, 25+\n    \"unemp_hs_m\":    \"LNS14027660\",     # HS graduates, no college, 25+\n    \"unemp_some_m\":  \"LNS14027689\",     # Some college or associate degree, 25+\n    \"unemp_ba_m\":    \"CGRA2024\",        # Bachelor's degree, 25+\n    \"unemp_adv_m\":   \"ADVRA25\",         # Advanced degree (Master's+), 25+\n\n    # ------------------------------------------------------------------\n    # Inflation\n    # ------------------------------------------------------------------\n    \"cpi_m\": \"CPIAUCSL\",                # CPI-U, monthly, seasonally adjusted\n}\n\n\nimport numpy as np\nimport pandas as pd\nfrom pandas_datareader import data as pdr\n\ndef fetch_fred(series_id: str, start=\"2000-01-01\"):\n    s = pdr.DataReader(series_id, \"fred\", start=start)\n    s.columns = [series_id]\n    return s\n\ndef annualized_sharpe(log_returns: pd.Series, periods_per_year=4) -&gt; float:\n    lr = log_returns.dropna()\n    if len(lr) &lt; 8:\n        return np.nan\n    mu = lr.mean() * periods_per_year\n    sig = lr.std(ddof=1) * np.sqrt(periods_per_year)\n    return float(mu / sig) if sig &gt; 0 else np.nan\n\ndef to_quarterly_period_mean(monthly: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Convert a monthly DatetimeIndex series to quarterly series indexed by PeriodIndex('Q'),\n    using mean within quarter.\n    \"\"\"\n    q = monthly.copy()\n    q.index = q.index.to_period(\"Q\")\n    return q.groupby(level=0).mean()\n\ndef to_quarterly_period_last(monthly: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Sometimes you may prefer end-of-quarter value instead of mean.\n    \"\"\"\n    q = monthly.copy()\n    q.index = q.index.to_period(\"Q\")\n    return q.groupby(level=0).last()\n\ndef to_quarterly_period_from_quarterly(quarterly: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Convert quarterly DatetimeIndex (often quarter-start dates on FRED) to PeriodIndex('Q').\n    \"\"\"\n    q = quarterly.copy()\n    q.index = q.index.to_period(\"Q\")\n    return q\n\ndef career_sharpe_for_group_period(\n    earn_q: pd.Series,     # quarterly earnings (nominal weekly)\n    unemp_m: pd.Series,    # monthly unemployment rate in %\n    cpi_m: pd.Series,      # monthly CPI index\n    use_cpi_mean=True,\n    use_unemp_mean=True,\n) -&gt; pd.DataFrame:\n    # --- Convert to quarterly PeriodIndex('Q') ---\n    earn_qp = to_quarterly_period_from_quarterly(earn_q)\n\n    unemp_qp = to_quarterly_period_mean(unemp_m) if use_unemp_mean else to_quarterly_period_last(unemp_m)\n    unemp_qp = unemp_qp / 100.0  # % -&gt; fraction\n\n    cpi_qp = to_quarterly_period_mean(cpi_m) if use_cpi_mean else to_quarterly_period_last(cpi_m)\n\n    # --- Align on common quarters ---\n    idx = earn_qp.dropna().index.intersection(unemp_qp.dropna().index).intersection(cpi_qp.dropna().index)\n    earn_qp = earn_qp.loc[idx]\n    unemp_qp = unemp_qp.loc[idx]\n    cpi_qp = cpi_qp.loc[idx]\n\n    # --- Compute proxy expected real earnings ---\n    real_weekly = earn_qp / (cpi_qp / 100.0)\n    expected_real_weekly = real_weekly * (1.0 - unemp_qp)\n\n    # Quarterly log returns\n    r = np.log(expected_real_weekly).diff()\n\n    out = pd.DataFrame({\n        \"earn_nominal_weekly\": earn_qp,\n        \"cpi\": cpi_qp,\n        \"unemp_rate\": unemp_qp,\n        \"real_weekly\": real_weekly,\n        \"expected_real_weekly\": expected_real_weekly,\n        \"log_return_qoq\": r,\n    })\n    out.attrs[\"career_sharpe\"] = annualized_sharpe(out[\"log_return_qoq\"])\n    return out\n\ndef run_all_fred_series_for_career_sharpe(FRED_SERIES, start=\"2000-01-01\"):\n    earnings = {\n        \"lt_hs\": fetch_fred(FRED_SERIES[\"earn_lt_hs_q\"], start).iloc[:, 0],\n        \"hs\":    fetch_fred(FRED_SERIES[\"earn_hs_q\"], start).iloc[:, 0],\n        \"some\":  fetch_fred(FRED_SERIES[\"earn_some_q\"], start).iloc[:, 0],\n        \"ba\":    fetch_fred(FRED_SERIES[\"earn_ba_q\"], start).iloc[:, 0],\n        \"adv\":   fetch_fred(FRED_SERIES[\"earn_adv_q\"], start).iloc[:, 0],\n    }\n    unemp = {\n        \"lt_hs\": fetch_fred(FRED_SERIES[\"unemp_lt_hs_m\"], start).iloc[:, 0],\n        \"hs\":    fetch_fred(FRED_SERIES[\"unemp_hs_m\"], start).iloc[:, 0],\n        \"some\":  fetch_fred(FRED_SERIES[\"unemp_some_m\"], start).iloc[:, 0],\n        \"ba\":    fetch_fred(FRED_SERIES[\"unemp_ba_m\"], start).iloc[:, 0],\n    }\n    cpi = fetch_fred(FRED_SERIES[\"cpi_m\"], start).iloc[:, 0]\n\n    # Advanced-degree unemployment fallback\n    try:\n        unemp[\"adv\"] = fetch_fred(FRED_SERIES[\"unemp_adv_m\"], start).iloc[:, 0]\n    except Exception:\n        unemp[\"adv\"] = fetch_fred(\"LNS14027662\", start).iloc[:, 0]\n\n    results = {}\n    rows = []\n    for k in [\"lt_hs\", \"hs\", \"some\", \"ba\", \"adv\"]:\n        df = career_sharpe_for_group_period(\n            earn_q=earnings[k],\n            unemp_m=unemp[k],\n            cpi_m=cpi,\n            use_cpi_mean=True,\n            use_unemp_mean=True,\n        )\n        results[k] = df\n        lr = df[\"log_return_qoq\"].dropna()\n        rows.append({\n            \"education_group\": k,\n            \"career_sharpe\": df.attrs[\"career_sharpe\"],\n            \"start_q\": str(df.index.min()) if not df.empty else None,\n            \"end_q\": str(df.index.max()) if not df.empty else None,\n            \"n_quarters\": int(lr.shape[0]),\n        })\n\n    sharpe_table = pd.DataFrame(rows).sort_values(\"career_sharpe\", ascending=False)\n    return sharpe_table, results\n\n\nsharpe_table, results = run_all_fred_series_for_career_sharpe(EARNING_FRED_SERIES, start=start_date_str)\nprint(sharpe_table.to_string(index=False))\n\neducation_group  career_sharpe start_q  end_q  n_quarters\n            adv       0.026205  2000Q1 2025Q3         102\n          lt_hs       0.024898  2000Q1 2025Q3         102\n             hs       0.017300  2000Q1 2025Q3         102\n           some      -0.026485  2000Q1 2025Q3         102\n             ba      -0.031516  2000Q1 2025Q3         102\n\n\n\nresults[\"ba\"].tail()\n\n\n\n\n\n\n\n\nearn_nominal_weekly\ncpi\nunemp_rate\nreal_weekly\nexpected_real_weekly\nlog_return_qoq\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2024Q3\n1533\n314.182667\n0.082667\n487.932710\n447.596939\n-0.030950\n\n\n2024Q4\n1547\n316.538667\n0.056667\n488.723863\n461.029511\n0.029569\n\n\n2025Q1\n1603\n319.492000\n0.064667\n501.734003\n469.288537\n0.017756\n\n\n2025Q2\n1559\n320.800333\n0.057667\n485.972064\n457.947675\n-0.024463\n\n\n2025Q3\n1580\n323.288000\n0.091333\n488.728317\n444.091130\n-0.030725\n\n\n\n\n\n\n\n\nEDU_YEARS_MAP = {\n    \"lt_hs\": 10,\n    \"hs\":    12,\n    \"some\":  14,\n    \"ba\":    16,\n    \"adv\":   18,\n}\n\ndef rolling_career_sharpe_series(df, window=20, periods_per_year=4):\n    \"\"\"\n    Rolling Career Sharpe from a group's df (must contain log_return_qoq).\n    Annualized mean / annualized std with quarterly periods.\n    \"\"\"\n    r = df[\"log_return_qoq\"]\n\n    mu = r.rolling(window).mean() * periods_per_year\n    sig = r.rolling(window).std(ddof=1) * np.sqrt(periods_per_year)\n\n    return mu / sig\n\n\ndef plot_stacked_rolling_career_sharpe_semantic(results, window=20):\n    \"\"\"\n    Fixed semantic order (education ladder), with higher education closer to the zero line.\n    Uses consistent colors for the same group above and below zero.\n    \"\"\"\n\n    # --- Fixed semantic order (higher ed near zero) ---\n    groups = [\"adv\", \"ba\", \"some\", \"hs\", \"lt_hs\"]  # fixed ladder order\n\n    # ---- Build aligned DataFrame of rolling Sharpe ----\n    sharpe_series = []\n    for k in groups:\n        df = results.get(k)\n        if df is None or df.empty:\n            continue\n\n        mu = df[\"log_return_qoq\"].rolling(window).mean() * 4\n        sig = df[\"log_return_qoq\"].rolling(window).std() * np.sqrt(4)\n        sharpe_series.append((mu / sig).rename(k))\n\n    sharpe_df = pd.concat(sharpe_series, axis=1).dropna(how=\"all\")\n    sharpe_df.index = sharpe_df.index.to_timestamp()\n\n    # Ensure all groups exist & order is preserved\n    groups_present = [g for g in groups if g in sharpe_df.columns]\n    sharpe_df = sharpe_df[groups_present]\n\n    # ---- Split into positive and negative parts ----\n    pos = sharpe_df.clip(lower=0)\n    neg = sharpe_df.clip(upper=0)\n\n    # ---- Consistent colors per group ----\n    cmap = plt.get_cmap(\"tab10\")\n    color_map = {g: cmap(i % 10) for i, g in enumerate(groups_present)}\n    colors = [color_map[g] for g in groups_present]\n\n    # ---- Plot ----\n    plt.figure(figsize=(13, 6))\n\n    # Positive stack\n    plt.stackplot(\n        pos.index,\n        [pos[g].values for g in groups_present],\n        labels=groups_present,\n        colors=colors,\n        alpha=1.0\n    )\n\n    # Negative stack (same order, same colors)\n    plt.stackplot(\n        neg.index,\n        [neg[g].values for g in groups_present],\n        colors=colors,\n        alpha=1.0\n    )\n\n    plt.axhline(0, color=\"black\", lw=1)\n    plt.title(f\"Stacked Rolling Career Sharpe by Education Group ({window}Q window)\")\n    plt.ylabel(\"Career Sharpe (risk-adjusted stability)\")\n    plt.xlabel(\"Year\")\n    plt.legend(loc=\"upper left\", title=\"Education group\", ncol=2)\n    plt.grid(alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n# Usage:\nplot_stacked_rolling_career_sharpe_semantic(results, window=20)\n\n\n\n\n\n\n\n\n\ndef plot_sharpe_vs_edu_at_sample_ends(results, edu_years_map, window=20):\n    \"\"\"\n    Two-row plot (shared X axis: years of education):\n      Row 1: Sharpe at earliest available date (first non-NaN rolling value per group)\n      Row 2: Sharpe at latest available date (last rolling value per group)\n    \"\"\"\n\n    rows_early = []\n    rows_late = []\n\n    for group, df in results.items():\n        if df is None or df.empty or group not in edu_years_map:\n            continue\n\n        s = rolling_career_sharpe_series(df, window=window).dropna()\n        if s.empty:\n            continue\n\n        years = edu_years_map[group]\n\n        # Earliest defined rolling Sharpe (after window)\n        early_date = s.index[0]\n        early_val  = float(s.iloc[0])\n\n        # Latest rolling Sharpe (end of sample)\n        late_date = s.index[-1]\n        late_val  = float(s.iloc[-1])\n\n        rows_early.append({\n            \"group\": group,\n            \"years_education\": years,\n            \"career_sharpe\": early_val,\n            \"date\": early_date\n        })\n\n        rows_late.append({\n            \"group\": group,\n            \"years_education\": years,\n            \"career_sharpe\": late_val,\n            \"date\": late_date\n        })\n\n    df_early = pd.DataFrame(rows_early).sort_values(\"years_education\")\n    df_late  = pd.DataFrame(rows_late).sort_values(\"years_education\")\n\n    # If you want the \"early\" and \"late\" dates to be common across groups,\n    # you can display the range here:\n    early_dates = df_early[\"date\"].tolist()\n    late_dates  = df_late[\"date\"].tolist()\n\n    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 8), sharex=True)\n\n    # ---- Row 1: earliest ----\n    axes[0].plot(df_early[\"years_education\"], df_early[\"career_sharpe\"], marker=\"o\")\n    axes[0].axhline(0, color=\"black\", lw=1)\n    axes[0].set_title(\n        f\"Career Sharpe vs Years of Education (Earliest available rolling value, window={window}Q)\\n\"\n        f\"Dates shown vary by group; earliest among them: {min(early_dates)}\"\n    )\n    axes[0].set_ylabel(\"Career Sharpe\")\n    axes[0].grid(alpha=0.3)\n    for _, r in df_early.iterrows():\n        axes[0].annotate(r[\"group\"], (r[\"years_education\"], r[\"career_sharpe\"]),\n                         textcoords=\"offset points\", xytext=(6, 6), fontsize=9)\n\n    # ---- Row 2: latest ----\n    axes[1].plot(df_late[\"years_education\"], df_late[\"career_sharpe\"], marker=\"o\")\n    axes[1].axhline(0, color=\"black\", lw=1)\n    axes[1].set_title(\n        f\"Career Sharpe vs Years of Education (Latest rolling value, window={window}Q)\\n\"\n        f\"Latest among them: {max(late_dates)}\"\n    )\n    axes[1].set_ylabel(\"Career Sharpe\")\n    axes[1].set_xlabel(\"Years of education\")\n    axes[1].grid(alpha=0.3)\n    for _, r in df_late.iterrows():\n        axes[1].annotate(r[\"group\"], (r[\"years_education\"], r[\"career_sharpe\"]),\n                         textcoords=\"offset points\", xytext=(6, 6), fontsize=9)\n\n    plt.tight_layout()\n    plt.show()\n\n    return df_early, df_late\n\n\n# Usage:\ndf_early, df_late = plot_sharpe_vs_edu_at_sample_ends(results, EDU_YEARS_MAP, window=20)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom pandas_datareader import data as pdr\n\ndef fetch_us_recessions(start=\"1990-01-01\"):\n    \"\"\"\n    Fetch US recession indicator (USREC) from FRED.\n    1 = recession, 0 = expansion.\n    \"\"\"\n    rec = pdr.DataReader(\"USREC\", \"fred\", start)\n    rec.index = pd.to_datetime(rec.index)\n    return rec\n\ndef recession_intervals(usrec: pd.DataFrame):\n    \"\"\"\n    Convert monthly USREC series into a list of (start, end) timestamps.\n    \"\"\"\n    rec = usrec[\"USREC\"]\n    intervals = []\n\n    in_rec = False\n    start = None\n\n    for date, val in rec.items():\n        if val == 1 and not in_rec:\n            start = date\n            in_rec = True\n        elif val == 0 and in_rec:\n            intervals.append((start, date))\n            in_rec = False\n\n    if in_rec:\n        intervals.append((start, rec.index[-1]))\n\n    return intervals\n\ndef plot_edu_sharpe_heatmap_with_recessions(\n    results,\n    edu_years_map,\n    window=20,\n    rec_start=\"1990-01-01\"\n):\n    # ---- Build panel (same as before) ----\n    panel = build_edu_sharpe_panel(results, edu_years_map, window=window)\n\n    Z = np.ma.masked_invalid(panel.T.values)\n    years = panel.columns.values\n    times = panel.index.values\n\n    # ---- Zero-centered normalization ----\n    vmax = np.nanmax(np.abs(Z))\n    norm = mcolors.TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)\n\n    # ---- Fetch recessions ----\n    usrec = fetch_us_recessions(start=rec_start)\n    rec_intervals = recession_intervals(usrec)\n\n    # ---- Plot ----\n    plt.figure(figsize=(14, 5))\n\n    plt.imshow(\n        Z,\n        aspect=\"auto\",\n        origin=\"lower\",\n        interpolation=\"nearest\",\n        cmap=\"RdYlGn\",\n        norm=norm,\n        extent=[0, len(times) - 1, years.min(), years.max()]\n    )\n\n    # X ticks (years)\n    n_xticks = min(10, len(times))\n    xtick_pos = np.linspace(0, len(times) - 1, n_xticks).astype(int)\n    xtick_lbl = [pd.to_datetime(times[i]).strftime(\"%Y\") for i in xtick_pos]\n    plt.xticks(xtick_pos, xtick_lbl)\n\n    # Y ticks (education years)\n    plt.yticks(years, [str(int(y)) for y in years])\n\n    # ---- Overlay recession bands ----\n    time_index = pd.to_datetime(times)\n\n    for start, end in rec_intervals:\n        # find indices overlapping the heatmap time range\n        if end &lt; time_index.min() or start &gt; time_index.max():\n            continue\n\n        x0 = np.searchsorted(time_index, start, side=\"left\")\n        x1 = np.searchsorted(time_index, end, side=\"right\")\n\n        plt.axvspan(\n            x0, x1,\n            color=\"black\",\n            alpha=0.3,   # subtle but visible\n            lw=0\n        )\n\n    cbar = plt.colorbar()\n    cbar.set_label(\"Rolling Career Sharpe\")\n\n    plt.title(\n        f\"Education–Career Sharpe Surface with US Recessions\\n\"\n        f\"Rolling window = {window} quarters\"\n    )\n    plt.xlabel(\"Year\")\n    plt.ylabel(\"Years of education\")\n\n    plt.tight_layout()\n    plt.show()\n\n    return panel\n\nplot_edu_sharpe_heatmap_with_recessions(\n    results,\n    EDU_YEARS_MAP,\n    window=20,\n    rec_start=\"1990-01-01\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n12\n14\n16\n18\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2005-01-01\n-0.217867\n0.030670\n-0.134112\n-0.058386\n0.177880\n\n\n2005-04-01\n-0.176164\n0.088045\n-0.236736\n0.051427\n-0.072408\n\n\n2005-07-01\n-0.007828\n0.005134\n-0.137005\n-0.082290\n-0.133007\n\n\n2005-10-01\n-0.054053\n0.026606\n-0.236067\n-0.012380\n0.135676\n\n\n2006-01-01\n0.011318\n0.126577\n-0.034653\n-0.141063\n0.101021\n\n\n...\n...\n...\n...\n...\n...\n\n\n2024-07-01\n-0.122012\n0.078718\n-0.081831\n-0.117491\n-0.003857\n\n\n2024-10-01\n0.044885\n0.197410\n0.045686\n-0.050311\n-0.087999\n\n\n2025-01-01\n-0.026605\n0.008222\n0.026629\n0.015947\n-0.048712\n\n\n2025-04-01\n0.243602\n0.465019\n0.290055\n0.326661\n-0.021351\n\n\n2025-07-01\n0.015073\n0.303110\n0.100364\n-0.100123\n-0.020048\n\n\n\n\n83 rows × 5 columns\n\n\n\n\nObservation from the chart\nEducation =/= insurance anymore\nHigher education:\n\nRaises mean income\nRaises volatility more\nLowers Career Sharpe\n\n\nAcross 2000–2025, risk-adjusted career outcomes in the US show no monotonic relationship with years of education; instead, education cohorts move together across macro regimes, with mid-to-upper education levels exhibiting the greatest downside during shocks and no group offering persistent career stability.\n\n\n\n\nSocial Status & Mobility\n\nIntergenerational Occupational Mobility of Men Born between 1950 and 1979\nRef - https://sci-hub.se/10.1353/foc.2006.0012\n\nimport seaborn as sns\n\n# Data from the table\ndata = {\n    \"Upper professional\": [42, 24, 7, 12, 0, 15],\n    \"Lower professional and clerical\": [29, 27, 7, 17, 0, 20],\n    \"Self-employed\": [29, 18, 16, 19, 0, 18],\n    \"Technical and skilled\": [17, 19, 6, 30, 1, 26],\n    \"Farm sector\": [14, 11, 8, 17, 13, 37],\n    \"Unskilled and service\": [16, 17, 6, 22, 1, 38]\n}\n\nindex_labels = [\"Upper professional\", \"Lower professional and clerical\", \"Self-employed\", \n                \"Technical and skilled\", \"Farm sector\", \"Unskilled and service\"]\n\ndf = pd.DataFrame(data, index=index_labels)\n\n# Create a heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(df, annot=True, cmap=\"flare\", fmt=\"d\", linewidths=.5)\n\n# Add title and labels\nplt.title(\"Intergenerational Occupational Mobility of Men Born between 1950 and 1979 (Source: General Social Surveys, 1988–2004)\")\nplt.xlabel(\"Destination: Son's Occupation\")\nplt.ylabel(\"Origin: Father's Occupation\")\n\n# Display the heatmap\nplt.show()\n\n\n\n\n\n\n\n\nReference: H. Elizabeth Peters, “Patterns of Intergenerational Mobility in Income and Earnings,” Review of Economics and Statistics, 74(3), 1992, p. 460. ​ - https://sci-hub.se/10.1353/foc.2006.0012\nSummary\n\nHigh Mobility for Upper Professional Class: Sons whose fathers were in upper professional occupations have the highest likelihood (42%) of remaining in the upper professional category themselves. This indicates a strong intergenerational persistence of high-status occupations.\nLimited Upward Mobility for Lower Occupations: For sons of fathers in the unskilled and service sector, there is a significant tendency to remain in lower-status occupations. Only 16% of these sons move into upper professional occupations, and 38% remain in unskilled and service jobs, indicating limited upward mobility.\nSelf-Employment and Technical Occupations: Sons of self-employed fathers show some diversity in outcomes, with 16% remaining self-employed and 29% moving into upper professional occupations.\nTechnical and skilled occupations see a 30% persistence rate, but many also move into other sectors, such as upper professional (17%) and unskilled and service jobs (26%).\nFarm Sector Shows Unique Patterns: Sons of fathers in the farm sector show a high rate of persistence within that sector (13%), but many also move into unskilled and service jobs (37%).\n\nOverall Summary: The heatmap highlights a strong intergenerational persistence in occupational status, particularly for those at the upper and lower ends of the occupational hierarchy. Sons of fathers in higher-status occupations (upper professional) are more likely to remain in those occupations, while those from lower-status or manual labor occupations, such as unskilled and service or farm sectors, face more significant challenges in achieving upward mobility.\nThis data underscores the limited mobility for individuals from non-elite or lower occupational backgrounds, reinforcing the idea that economic and social barriers have a substantial impact on career outcomes across generations.\n\n# Data from the table\ndata = {\n    \"First\": [42, 26, 18, 15],\n    \"Second\": [28, 29, 24, 18],\n    \"Third\": [19, 27, 29, 25],\n    \"Fourth\": [12, 19, 29, 40],\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data, index=[\"First\", \"Second\", \"Third\", \"Fourth\"])\n\n# Creating the heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt=\"d\", cbar=True)\n\n# Adding titles and labels\nplt.title(\"Intergenerational Income Mobility: Probability of Son's Quartile Given Parent's Quartile\")\nplt.xlabel(\"Son's Income Quartile\")\nplt.ylabel(\"Parent's Income Quartile\")\n\n# Displaying the heatmap\nplt.show()\n\n\n\n\n\n\n\n\nIncome Mobility is Limited: The data shows that there is significant persistence in income status across generations, especially at the top and bottom of the income distribution. Intergenerational Mobility: While there is some mobility between quartiles, particularly in the middle, those at the extremes of the income distribution are more likely to stay there.\nEconomic Mobility Decline: Studies such as those by Raj Chetty and others have shown a clear decline in intergenerational economic mobility since the 1970s. The likelihood of children earning more than their parents has decreased, particularly for those from lower-income families.\nIncreasing Role of Education and Wealth: The role of education, especially from elite institutions, in securing higher incomes has become more pronounced. Wealth inequality has further exacerbated income inequality, making it harder for those from lower-income families to move up the economic ladder.\n\n\nChetty Paper 2014: Where Is the Land of Opportunity? The Geography of Intergenerational Mobility in the United States\nhttps://jenni.uchicago.edu/econ341/readings/Chetty_Hendren_Kline_etal_2014_QJE_v129_n4.pdf\nIntergenerational income mobility aims to capture how strongly a child’s economic position depends on that of their parents. A modern and robust way to measure this is the Rank–Rank Intergenerational Elasticity (IGE), which avoids issues of scale, inflation, and tail instability inherent in log-income regressions.\nThe core idea is to express both parent and child incomes as ranks in their respective national income distributions, normalized to the unit interval. Let the parent’s income rank be denoted by ( \\(R^{\\text{parent}}_i \\in [0,1]\\) ), and the child’s income rank by ( \\(R^{\\text{child}}_i \\in [0,1]\\)). The fundamental rank–rank regression is then written as:\n\\[\nR^{\\text{child}}_i = \\alpha + \\rho \\, R^{\\text{parent}}_i + \\varepsilon_i\n\\]\nHere, the coefficient ( \\(\\rho\\) ) is the rank–rank IGE. It measures the expected change in a child’s percentile rank associated with a one-percentile increase in parental rank. Equivalently, it can be interpreted as a slope:\n\\[\n\\rho = \\frac{\\partial R^{\\text{child}}}{\\partial R^{\\text{parent}}}\n\\]\nFrom a statistical standpoint, ( ) is estimated using ordinary least squares and can be expressed in covariance form as:\n\\[\n\\rho = \\frac{\\operatorname{Cov}\\left(R^{\\text{parent}}, R^{\\text{child}}\\right)}\n{\\operatorname{Var}\\left(R^{\\text{parent}}\\right)}\n\\]\nHowever IGE Rank-Rank Slope can be misleading (stable around 0.32 even though inequality keeps rising) at a national level (US or other countries) for variety of reasons -\n\n\n\n\n\n\n\nProblem\nWhy It Matters\n\n\n\n\n1. Zero-sum by construction\nIncome ranks must sum to the same total. If one child rises in rank, another must fall. As a result, the national average is mechanically centered (around 50), even if overall outcomes deteriorate.\n\n\n2. Hides local heterogeneity\nAggregation masks large geographic differences. For example, Charlotte (β = 0.40) and San Jose (β = 0.24) average to a seemingly “normal” β = 0.32, but this average describes no actual individual’s experience.\n\n\n3. Misses absolute collapse\nAbsolute mobility collapsed even while rank persistence stayed flat: in 1940, ~92% of children earned more than their parents, versus ~50% for 1980 cohorts—yet the rank–rank IGE remained around 0.34 throughout.\n\n\n\n\n\nNeed absolute income mobility\nRaj Chetty et al.,The fading American dream: Trends in absolute income mobility since 1940.\nScience 356,398-406(2017).DOI:10.1126/science.aal4617\nhttps://www.science.org/doi/10.1126/science.aal4617#:~:text=Using%20this%20methodology%2C%20we%20found,rates%20observed%20for%20recent%20cohorts.\n\n# =============================================================================\n# INFLATION DATA: Official CPI vs ShadowStats\n# =============================================================================\n\ndef build_inflation_series():\n    \"\"\"\n    Build inflation time series: Official CPI vs ShadowStats (chart-aligned).\n\n    IMPORTANT:\n    - ShadowStats does not publish a clean public time series (as far as typical public endpoints go).\n    - So here we use a chart-aligned annual approximation (1980–2023) that matches the\n      visual series in the provided ShadowStats image (1980-based alternate CPI).\n    - For 1970–1979 we default to official CPI (since the chart starts at ~1980).\n    \"\"\"\n\n    years = list(range(1970, 2024))\n\n    # -------------------------------------------------------------------------\n    # Official CPI (annual, your existing dict)\n    # -------------------------------------------------------------------------\n    official_rates = {\n        1970: 0.059, 1971: 0.043, 1972: 0.033, 1973: 0.062, 1974: 0.110,\n        1975: 0.091, 1976: 0.058, 1977: 0.065, 1978: 0.076, 1979: 0.113,\n        1980: 0.135, 1981: 0.103, 1982: 0.062, 1983: 0.032, 1984: 0.043,\n        1985: 0.036, 1986: 0.019, 1987: 0.036, 1988: 0.041, 1989: 0.048,\n        1990: 0.054, 1991: 0.042, 1992: 0.030, 1993: 0.030, 1994: 0.026,\n        1995: 0.028, 1996: 0.029, 1997: 0.023, 1998: 0.016, 1999: 0.022,\n        2000: 0.034, 2001: 0.028, 2002: 0.016, 2003: 0.023, 2004: 0.027,\n        2005: 0.034, 2006: 0.032, 2007: 0.029, 2008: 0.038, 2009: -0.004,\n        2010: 0.016, 2011: 0.032, 2012: 0.021, 2013: 0.015, 2014: 0.016,\n        2015: 0.001, 2016: 0.013, 2017: 0.021, 2018: 0.024, 2019: 0.018,\n        2020: 0.012, 2021: 0.047, 2022: 0.080, 2023: 0.041\n    }\n\n    # -------------------------------------------------------------------------\n    # ShadowStats (chart-aligned annual approximation, 1980–2023)\n    # These values are digitized/approximated from the supplied ShadowStats plot:\n    # \"SGS Alternate CPI, 1980-Based\" (YoY) through May 2023.\n    # Units are decimals (e.g., 0.12 = 12% YoY).\n    # -------------------------------------------------------------------------\n    shadowstats_rates_1980_2023 = {\n        1980: 0.1384, 1981: 0.1077, 1982: 0.0673, 1983: 0.0396, 1984: 0.0534,\n        1985: 0.0486, 1986: 0.0500, 1987: 0.0596, 1988: 0.0686, 1989: 0.0712,\n        1990: 0.0617, 1991: 0.0586, 1992: 0.0551, 1993: 0.0568, 1994: 0.0587,\n        1995: 0.0595, 1996: 0.0624, 1997: 0.0664, 1998: 0.0727, 1999: 0.0787,\n        2000: 0.0845, 2001: 0.0900, 2002: 0.0966, 2003: 0.1018, 2004: 0.1012,\n        2005: 0.1014, 2006: 0.1050, 2007: 0.1116, 2008: 0.1186, 2009: 0.0656,\n        2010: 0.0907, 2011: 0.0974, 2012: 0.1008, 2013: 0.0985, 2014: 0.0893,\n        2015: 0.0943, 2016: 0.0969, 2017: 0.0989, 2018: 0.1020, 2019: 0.0966,\n        2020: 0.0910, 2021: 0.1369, 2022: 0.1629, 2023: 0.1402\n    }\n\n    # Build ShadowStats series for full range 1970–2023\n    shadowstats_rates = {}\n    for y in years:\n        if y &lt; 1980:\n            # Chart doesn't cover these years; use official CPI as a conservative placeholder\n            shadowstats_rates[y] = official_rates[y]\n        elif y in shadowstats_rates_1980_2023:\n            shadowstats_rates[y] = shadowstats_rates_1980_2023[y]\n        else:\n            # Fallback (should not happen with the above dict)\n            shadowstats_rates[y] = 0.12\n\n    # -------------------------------------------------------------------------\n    # Build cumulative indices (1970 = 1.0)\n    # -------------------------------------------------------------------------\n    official_index = {1970: 1.0}\n    shadow_index = {1970: 1.0}\n\n    for year in range(1971, 2024):\n        official_index[year] = official_index[year - 1] * (1 + official_rates[year])\n        shadow_index[year] = shadow_index[year - 1] * (1 + shadowstats_rates[year])\n\n    df = pd.DataFrame({\n        \"year\": years,\n        \"official_cpi_rate\": [official_rates[y] for y in years],\n        \"shadowstats_rate\": [shadowstats_rates[y] for y in years],\n        \"official_cpi_index\": [official_index[y] for y in years],\n        \"shadowstats_index\": [shadow_index[y] for y in years],\n    })\n\n    df[\"shadow_vs_official_ratio\"] = df[\"shadowstats_index\"] / df[\"official_cpi_index\"]\n    return df\n\n\n\n# =============================================================================\n# ABSOLUTE MOBILITY DATA from Chetty et al.\n# =============================================================================\n\ndef build_chetty_absolute_mobility():\n    \"\"\"\n    Build absolute mobility series from Chetty et al. (2017) \"Fading American Dream\"\n    \n    Definition: % of children earning more than their parents at age 30\n    (in constant dollars using official CPI)\n    \n    Data from Table 1 of the paper + extensions.\n    \"\"\"\n    \n    # Published data from Chetty et al. (2017)\n    mobility = pd.DataFrame({\n        'birth_cohort': [1940, 1945, 1950, 1955, 1960, 1965, 1970, 1975, 1980, 1985, 1990, 1993],\n        'child_income_year': [1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020, 2023],\n        'parent_income_year': [1940, 1945, 1950, 1955, 1960, 1965, 1970, 1975, 1980, 1985, 1990, 1993],\n        'absolute_mobility_official': [\n            0.92,  # 1940 cohort\n            0.88,  # 1945 cohort  \n            0.79,  # 1950 cohort\n            0.70,  # 1955 cohort\n            0.62,  # 1960 cohort\n            0.56,  # 1965 cohort\n            0.52,  # 1970 cohort\n            0.50,  # 1975 cohort\n            0.50,  # 1980 cohort (from paper)\n            0.48,  # 1985 cohort (extrapolated)\n            0.46,  # 1990 cohort (extrapolated)\n            0.45,  # 1993 cohort (extrapolated to 2023)\n        ]\n    })\n    \n    return mobility\n\n\n# =============================================================================\n# SHADOWSTATS ADJUSTMENT\n# =============================================================================\n\ndef adjust_mobility_for_shadowstats(mobility_df, inflation_df):\n    \"\"\"\n    Adjust absolute mobility for ShadowStats inflation.\n    \n    Logic:\n    ------\n    If TRUE inflation &gt; official CPI, then:\n    1. Parent's purchasing power was HIGHER than official stats suggest\n    2. The \"real\" bar to beat parents is HIGHER\n    3. Fewer children truly beat their parents\n    \n    Method:\n    -------\n    For each cohort, compute the cumulative inflation gap between\n    parent's income year and child's income year under both measures.\n    \n    If ShadowStats shows 3x the price increase vs official CPI,\n    then roughly 1/3 as many children truly beat their parents\n    (assuming nominal income grew at similar rates).\n    \"\"\"\n    \n    adjusted = []\n    \n    for _, row in mobility_df.iterrows():\n        cohort = row['birth_cohort']\n        parent_year = int(row['parent_income_year'])\n        child_year = int(row['child_income_year'])\n        official_mobility = row['absolute_mobility_official']\n        \n        # Get inflation indices\n        parent_official = inflation_df[inflation_df['year'] == parent_year]['official_cpi_index'].values\n        child_official = inflation_df[inflation_df['year'] == child_year]['official_cpi_index'].values\n        parent_shadow = inflation_df[inflation_df['year'] == parent_year]['shadowstats_index'].values\n        child_shadow = inflation_df[inflation_df['year'] == child_year]['shadowstats_index'].values\n        \n        if len(parent_official) &gt; 0 and len(child_official) &gt; 0:\n            # Inflation from parent year to child year\n            official_inflation = child_official[0] / parent_official[0]\n            shadow_inflation = child_shadow[0] / parent_shadow[0]\n            \n            # Gap ratio: how much higher is ShadowStats inflation\n            inflation_gap = shadow_inflation / official_inflation\n            \n            # Adjustment: if prices rose 2x more under ShadowStats,\n            # nominal wages would need to be 2x higher to maintain same\n            # purchasing power. Assuming wage growth was similar under both,\n            # true mobility is roughly 1/sqrt(gap) of official estimate.\n            # (sqrt because the distribution matters, not just the mean)\n            \n            adjustment_factor = 1 / np.sqrt(inflation_gap)\n            adjusted_mobility = official_mobility * adjustment_factor\n            adjusted_mobility = np.clip(adjusted_mobility, 0.05, 0.95)\n        else:\n            inflation_gap = np.nan\n            adjusted_mobility = official_mobility * 0.63  # Default: ~1/sqrt(2.5)\n        \n        adjusted.append({\n            'birth_cohort': cohort,\n            'child_income_year': child_year,\n            'parent_income_year': parent_year,\n            'official_mobility': official_mobility,\n            'shadowstats_mobility': adjusted_mobility,\n            'inflation_gap_ratio': inflation_gap,\n            'mobility_gap_pp': (official_mobility - adjusted_mobility) * 100\n        })\n    \n    return pd.DataFrame(adjusted)\n\n\n# =============================================================================\n# VISUALIZATION\n# =============================================================================\n\nimport matplotlib.pyplot as plt\n\ndef visualize_results(mobility_df):\n    \"\"\"Create visualization: Absolute mobility (Official CPI vs ShadowStats).\"\"\"\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    fig.suptitle(\n        \"Absolute Mobility: Official CPI vs ShadowStats\",\n        fontsize=14,\n        fontweight=\"bold\",\n        y=0.98,\n    )\n\n    x = mobility_df[\"birth_cohort\"]\n    y_off = mobility_df[\"official_mobility\"] * 100\n    y_shd = mobility_df[\"shadowstats_mobility\"] * 100\n\n    # Lines\n    ax.plot(\n        x, y_off,\n        \"b-o\",\n        linewidth=2.5,\n        markersize=8,\n        label=\"Official CPI\",\n        zorder=3,\n    )\n    ax.plot(\n        x, y_shd,\n        \"r-s\",\n        linewidth=2.5,\n        markersize=8,\n        label=\"ShadowStats Adjusted\",\n        zorder=3,\n    )\n\n    # Shaded gap\n    ax.fill_between(\n        x,\n        y_off,\n        y_shd,\n        alpha=0.30,\n        color=\"red\",\n        label=\"Hidden decline\",\n        zorder=2,\n    )\n\n    # Reference line\n    ax.axhline(50, color=\"gray\", linestyle=\"--\", linewidth=1.5, alpha=0.7)\n\n    # Labels, styling\n    ax.set_xlabel(\"Birth Cohort\", fontsize=11)\n    ax.set_ylabel(\"% Earning More Than Parents\", fontsize=11)\n    ax.set_title(\"Absolute Mobility Over Time\", fontsize=12, fontweight=\"bold\")\n\n    ax.set_ylim(0, 100)\n    ax.set_xlim(1938, 1995)\n\n    ax.legend(loc=\"upper right\", fontsize=9)\n    ax.grid(True, alpha=0.3)\n\n    fig.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.show()\n\n    return fig, ax\n\n# =============================================================================\n# MAIN\n# =============================================================================\n\nprint(\"=\" * 70)\nprint(\"ABSOLUTE MOBILITY: Official CPI vs ShadowStats Inflation\")\nprint(\"=\" * 70)\n\n# 1. Build inflation series\ninflation_df = build_inflation_series()\n\n# 2. Build Chetty mobility data\nmobility_df = build_chetty_absolute_mobility()\n\n# 3. Adjust for ShadowStats\nadjusted_df = adjust_mobility_for_shadowstats(mobility_df, inflation_df)\n\n# 4. Visualize\nfig = visualize_results(adjusted_df)\n\n======================================================================\nABSOLUTE MOBILITY: Official CPI vs ShadowStats Inflation\n======================================================================\n\n\n\n\n\n\n\n\n\nChetty et. al. (2017) has considered counterfactual analysis to understand reasons behind collapse of income mobility:\n\n“Why have rates of absolute income mobility fallen so sharply over the last half century, and what policies can restore absolute mobility to earlier levels? We used simulations to evaluate the effects of two key trends over the past half century: declining rates of GDP growth and greater inequality in the distribution of GDP (17, 35).”\n\n\n\n\nimage.png\n\n\n\n\n\nIt’s Inflation !"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#reducing-compensation-given-a-productivity-level-1",
    "href": "notebooks/Education and QoL-Satisfaction.html#reducing-compensation-given-a-productivity-level-1",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "2. Reducing Compensation Given a Productivity Level",
    "text": "2. Reducing Compensation Given a Productivity Level\n\nM0/1/2 Inflation and Real Wages: The increase in M2 money supply has contributed to inflation, which in turn has eroded the purchasing power of wages. While nominal wages might rise, real wages—adjusted for inflation—have stagnated or even declined for many workers, particularly those in jobs traditionally associated with higher education.\nData Evidence: Studies such as those from the Federal Reserve Bank of St. Louis have shown that while M2 has increased significantly, the growth in real wages has not kept pace, particularly after the 2008 financial crisis​ (St. Louis Fed).\n\nExample: The Economic Policy Institute found that between 1979 and 2020, the median worker’s wages grew by only 15.1% when adjusted for inflation, while productivity increased by 61.8%. This decoupling suggests that higher education does not necessarily lead to proportionate income growth in an inflationary environment.\nDownload Productivity data\n\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"\n}\nproductivity_url = \"https://download.bls.gov/pub/time.series/pr/pr.data.1.AllData\"\n\ndata = requests.get(productivity_url, headers=headers).text\n\nproductivity_data = pd.read_csv(StringIO(data), sep=\"\\t\")\nproductivity_data.columns = ['series_id', 'year', 'period', 'value','footnote_codes']\nproductivity_data['series_id'] = productivity_data['series_id'].str.strip()\nproductivity_data['period'] = productivity_data['period'].str.strip()\n#productivity_data.info()\n\n# Step 2: Process the productivity data\n# Filtering for relevant data (example uses a hypothetical series code PRS85006093 for nonfarm business productivity)\nproductivity_data = productivity_data[(productivity_data['series_id'] == 'PRS85006093') & (productivity_data['year'] &gt;= start_date.year)]\nproductivity_data = productivity_data[['year', 'value']].rename(columns={'value': 'Productivity_Index'})\n\nproductivity_data.head()\n\n\n\n\n\n\n\n\nyear\nProductivity_Index\n\n\n\n\n45473\n1979\n49.615\n\n\n45474\n1979\n49.523\n\n\n45475\n1979\n49.463\n\n\n45476\n1979\n49.405\n\n\n45477\n1979\n49.360\n\n\n\n\n\n\n\nDownload Median Wage Data\n\nimport pandas_datareader as pdr\nfrom datetime import datetime\n\n# LEU0252881600Q: Median usual weekly real earnings: Wage and salary workers: 16 years and over\nwage_data = pdr.get_data_fred('LEU0252881600Q', start=datetime(1979, 1, 1), end=datetime(2020, 12, 31))\n\n# Preview the data\nwage_data = wage_data.groupby(wage_data.index.year)[\"LEU0252881600Q\"].median()\n\nwage_data = wage_data.reset_index()\n\n# Step 3: Process the wage data\n# Assuming the data is already in the right format\nwage_data = wage_data[wage_data['DATE'] &gt;= 1979]\nwage_data = wage_data[['DATE', 'LEU0252881600Q']].rename(columns={'DATE':'year', 'LEU0252881600Q': 'Weekly_Median_Wage'})\nwage_data.head()\n\n\n\n\n\n\n\n\nyear\nWeekly_Median_Wage\n\n\n\n\n0\n1979\n331.0\n\n\n1\n1980\n316.0\n\n\n2\n1981\n312.5\n\n\n3\n1982\n314.5\n\n\n4\n1983\n314.0\n\n\n\n\n\n\n\n\n# Step 4: Merge the datasets on the year\nmerged_data = pd.merge(productivity_data, wage_data, left_on='year', right_on='year')\nmerged_data.head()\n\n\n\n\n\n\n\n\nyear\nProductivity_Index\nWeekly_Median_Wage\n\n\n\n\n0\n1979\n49.615\n331.0\n\n\n1\n1979\n49.523\n331.0\n\n\n2\n1979\n49.463\n331.0\n\n\n3\n1979\n49.405\n331.0\n\n\n4\n1979\n49.360\n331.0\n\n\n\n\n\n\n\n\n# Step 5: Calculate cumulative growth\nbase_productivity = merged_data['Productivity_Index'].iloc[0]\nbase_wage = merged_data['Weekly_Median_Wage'].iloc[0]\n\nmerged_data['Productivity_Growth'] = ((merged_data['Productivity_Index'] - base_productivity) / base_productivity) * 100\nmerged_data['Weekly_Median_Wage_Growth'] = ((merged_data['Weekly_Median_Wage'] - base_wage) / base_wage) * 100\n\n# Step 6: Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(merged_data['year'], merged_data['Productivity_Growth'], label='Productivity Growth')\nplt.plot(merged_data['year'], merged_data['Weekly_Median_Wage_Growth'], label='Weekly Median Wage Growth')\nplt.title('Productivity Growth vs. Weekly Median Wage Growth (1979-2020)')\nplt.xlabel('Year')\nplt.ylabel('Cumulative Growth (%)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Display the final growth rates\nprint(f\"Productivity Growth (1979-2020): {merged_data['Productivity_Growth'].iloc[-1]:.2f}%\")\nprint(f\"Weekly Median Wage Growth (1979-2020): {merged_data['Weekly_Median_Wage_Growth'].iloc[-1]:.2f}%\")\n\n\n\n\n\n\n\n\nProductivity Growth (1979-2020): 120.07%\nWeekly Median Wage Growth (1979-2020): 14.95%\n\n\n\nImpact of M2 Inflation: Rising Costs of Living and Education\n\nEducation Costs: The cost of education has risen significantly, outpacing inflation and wage growth. As M2 inflation contributes to the overall increase in the cost of living, students graduate with higher levels of debt, which diminishes the net financial returns of their education.\nCase Study: According to the College Board, the average cost of tuition and fees at private four-year institutions in the U.S. has more than doubled since 2000, while wages have not kept pace with these increases​\n\nData Source: https://research.collegeboard.org/media/xlsx/trends-college-pricing-excel-data-2023.xlsx\n\ntution_data = pd.read_csv(\"data/tuition_private_4yr_current_dollars_final_cleaned.csv\")\ntution_data.columns = ['year', 'Private_Nonprofit_Four_Year']\ntution_data.head()\n\n\n\n\n\n\n\n\nyear\nPrivate_Nonprofit_Four_Year\n\n\n\n\n0\n1971\n1830.0\n\n\n1\n1972\n1950.0\n\n\n2\n1973\n2050.0\n\n\n3\n1974\n2130.0\n\n\n4\n1975\n2290.0\n\n\n\n\n\n\n\n\n# Step 4: Merge the datasets on the year\nmerged_data = pd.merge(tution_data, wage_data, left_on='year', right_on='year')\nmerged_data.head()\n\n\n\n\n\n\n\n\nyear\nPrivate_Nonprofit_Four_Year\nWeekly_Median_Wage\n\n\n\n\n0\n1979\n3230.0\n331.0\n\n\n1\n1980\n3620.0\n316.0\n\n\n2\n1981\n4110.0\n312.5\n\n\n3\n1982\n4640.0\n314.5\n\n\n4\n1983\n5090.0\n314.0\n\n\n\n\n\n\n\n\n# Step 5: Calculate cumulative growth\nbase_tution = merged_data['Private_Nonprofit_Four_Year'].iloc[0]\nbase_wage = merged_data['Weekly_Median_Wage'].iloc[0]\n\nmerged_data['Tution_Growth'] = ((merged_data['Private_Nonprofit_Four_Year'] - base_tution) / base_tution) * 100\nmerged_data['Weekly_Median_Wage_Growth'] = ((merged_data['Weekly_Median_Wage'] - base_wage) / base_wage) * 100\n\n# Step 6: Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(merged_data['year'], merged_data['Tution_Growth'], label='Tution Growth (Private Nonprofit Four Year)')\nplt.plot(merged_data['year'], merged_data['Weekly_Median_Wage_Growth'], label='Weekly Median Wage Growth')\nplt.title('Tution Growth (Private Nonprofit Four Year) vs. Weekly Median Wage Growth (1979-2020)')\nplt.xlabel('Year')\nplt.ylabel('Cumulative Growth (%)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Display the final growth rates\nprint(f\"Tution Growth (Private Nonprofit Four Year) (1979-): {merged_data['Tution_Growth'].iloc[-1]:.2f}%\")\nprint(f\"Weekly Median Wage Growth (1979-): {merged_data['Weekly_Median_Wage_Growth'].iloc[-1]:.2f}%\")\n\n\n\n\n\n\n\n\nTution Growth (Private Nonprofit Four Year) (1979-): 1053.87%\nWeekly Median Wage Growth (1979-): 14.95%\n\n\nNow let’s explore how Tution Growth and M2 Inflation are related.\n\n\nTution Growth and M2 Inflation\n\n# Fetch M2 Money Stock data\nm2_supply = web.DataReader('M2SL', 'fred', start_date, end_date)\n\nm2_supply = m2_supply.groupby(m2_supply.index.year)[\"M2SL\"].mean()\nm2_supply.dropna()\n\nm2_supply = pd.DataFrame(data={\n    'year': m2_supply.index,\n    'm2sl': m2_supply.values\n})\nm2_supply.reset_index()\nm2_supply.head(5)\n\n\n\n\n\n\n\n\nyear\nm2sl\n\n\n\n\n0\n1979\n1425.666667\n\n\n1\n1980\n1540.183333\n\n\n2\n1981\n1679.291667\n\n\n3\n1982\n1830.925000\n\n\n4\n1983\n2054.466667\n\n\n\n\n\n\n\n\ntution_m2sl_merged_data = pd.merge(tution_data, m2_supply, left_on='year', right_on='year')\ntution_m2sl_merged_data.head()\n\n\n\n\n\n\n\n\nyear\nPrivate_Nonprofit_Four_Year\nm2sl\n\n\n\n\n0\n1979\n3230.0\n1425.666667\n\n\n1\n1980\n3620.0\n1540.183333\n\n\n2\n1981\n4110.0\n1679.291667\n\n\n3\n1982\n4640.0\n1830.925000\n\n\n4\n1983\n5090.0\n2054.466667\n\n\n\n\n\n\n\n\n# Perform Granger Causality Test\ngranger_test = grangercausalitytests(tution_m2sl_merged_data[['Private_Nonprofit_Four_Year', 'm2sl']], \n                                     maxlag=12, \n                                     verbose=True)\n\n\nGranger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=0.0591  , p=0.8092  , df_denom=41, df_num=1\nssr based chi2 test:   chi2=0.0634  , p=0.8012  , df=1\nlikelihood ratio test: chi2=0.0634  , p=0.8013  , df=1\nparameter F test:         F=0.0591  , p=0.8092  , df_denom=41, df_num=1\n\nGranger Causality\nnumber of lags (no zero) 2\nssr based F test:         F=0.2816  , p=0.7561  , df_denom=38, df_num=2\nssr based chi2 test:   chi2=0.6373  , p=0.7271  , df=2\nlikelihood ratio test: chi2=0.6327  , p=0.7288  , df=2\nparameter F test:         F=0.2816  , p=0.7561  , df_denom=38, df_num=2\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=11.8657 , p=0.0000  , df_denom=35, df_num=3\nssr based chi2 test:   chi2=42.7165 , p=0.0000  , df=3\nlikelihood ratio test: chi2=29.4689 , p=0.0000  , df=3\nparameter F test:         F=11.8657 , p=0.0000  , df_denom=35, df_num=3\n\nGranger Causality\nnumber of lags (no zero) 4\nssr based F test:         F=8.6750  , p=0.0001  , df_denom=32, df_num=4\nssr based chi2 test:   chi2=44.4595 , p=0.0000  , df=4\nlikelihood ratio test: chi2=30.1133 , p=0.0000  , df=4\nparameter F test:         F=8.6750  , p=0.0001  , df_denom=32, df_num=4\n\nGranger Causality\nnumber of lags (no zero) 5\nssr based F test:         F=6.3667  , p=0.0004  , df_denom=29, df_num=5\nssr based chi2 test:   chi2=43.9086 , p=0.0000  , df=5\nlikelihood ratio test: chi2=29.6339 , p=0.0000  , df=5\nparameter F test:         F=6.3667  , p=0.0004  , df_denom=29, df_num=5\n\nGranger Causality\nnumber of lags (no zero) 6\nssr based F test:         F=6.0540  , p=0.0005  , df_denom=26, df_num=6\nssr based chi2 test:   chi2=54.4860 , p=0.0000  , df=6\nlikelihood ratio test: chi2=34.0958 , p=0.0000  , df=6\nparameter F test:         F=6.0540  , p=0.0005  , df_denom=26, df_num=6\n\nGranger Causality\nnumber of lags (no zero) 7\nssr based F test:         F=5.2112  , p=0.0012  , df_denom=23, df_num=7\nssr based chi2 test:   chi2=60.2682 , p=0.0000  , df=7\nlikelihood ratio test: chi2=36.1043 , p=0.0000  , df=7\nparameter F test:         F=5.2112  , p=0.0012  , df_denom=23, df_num=7\n\nGranger Causality\nnumber of lags (no zero) 8\nssr based F test:         F=4.1960  , p=0.0044  , df_denom=20, df_num=8\nssr based chi2 test:   chi2=62.1003 , p=0.0000  , df=8\nlikelihood ratio test: chi2=36.4529 , p=0.0000  , df=8\nparameter F test:         F=4.1960  , p=0.0044  , df_denom=20, df_num=8\n\nGranger Causality\nnumber of lags (no zero) 9\nssr based F test:         F=4.0155  , p=0.0066  , df_denom=17, df_num=9\nssr based chi2 test:   chi2=76.5310 , p=0.0000  , df=9\nlikelihood ratio test: chi2=41.0296 , p=0.0000  , df=9\nparameter F test:         F=4.0155  , p=0.0066  , df_denom=17, df_num=9\n\nGranger Causality\nnumber of lags (no zero) 10\nssr based F test:         F=3.1079  , p=0.0262  , df_denom=14, df_num=10\nssr based chi2 test:   chi2=77.6984 , p=0.0000  , df=10\nlikelihood ratio test: chi2=40.9278 , p=0.0000  , df=10\nparameter F test:         F=3.1079  , p=0.0262  , df_denom=14, df_num=10\n\nGranger Causality\nnumber of lags (no zero) 11\nssr based F test:         F=4.4091  , p=0.0105  , df_denom=11, df_num=11\nssr based chi2 test:   chi2=149.9104, p=0.0000  , df=11\nlikelihood ratio test: chi2=57.3950 , p=0.0000  , df=11\nparameter F test:         F=4.4091  , p=0.0105  , df_denom=11, df_num=11\n\nGranger Causality\nnumber of lags (no zero) 12\nssr based F test:         F=13.2302 , p=0.0005  , df_denom=8, df_num=12\nssr based chi2 test:   chi2=654.8936, p=0.0000  , df=12\nlikelihood ratio test: chi2=100.2252, p=0.0000  , df=12\nparameter F test:         F=13.2302 , p=0.0005  , df_denom=8, df_num=12\n\n\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:1545: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n\n\nThe Granger causality tests provide strong evidence that changes in the M2 money supply (M2SL) do indeed cause changes in the cost of tuition at Private Nonprofit Four-Year institutions, particularly when considering lags 3 through 12. The most significant causal effects are observed around lags 3-6, with some weakening around lags 7-10, and a resurgence at higher lags (lag 12). This analysis suggests that M2SL is a significant predictor of tuition costs over various lag periods, implying that changes in the money supply could have a delayed impact on educational costs.\nStudent Debt:\nFederal Reserve data shows that student debt in the U.S. has skyrocketed, surpassing $1.7 trillion in 2021. This growing debt burden makes it harder for graduates to achieve financial stability, let alone upward mobility.\n\n\nImpact of Technological Disruption: Automation and Job Displacement:\n\nTech Disruption: Technological advancements, particularly in automation and AI, have disrupted industries that traditionally offered stable employment to highly educated individuals. Jobs in fields like accounting (A), legal services (L), and even medicine (M) (LAM in acronym) are increasingly being automated, reducing the demand for highly educated workers in these areas.\nResearch Findings: A 2020 report by the World Economic Forum predicts that by 2025, automation will displace about 85 million jobs globally, many of which are held by individuals with higher education. This disruption challenges the stability that higher education once promised​ (World Bank).\nGlobal Displacement Estimates: 400 to 800 million individuals globally could be displaced by automation and need to find new jobs by 2030. This estimate reflects the potential impact under various scenarios of automation adoption.\nOccupation Shifts: 75 to 375 million workers may need to switch occupational categories and learn new skills to remain employed, depending on the speed of automation adoption. This transition represents 3% to 14% of the global workforce.\n300 to 365 million new jobs could be created globally by 2030 from rising incomes and consumption, especially in emerging economies. This implies 100-435 million jobs get destroyed. This wave of automation, at least, is not a net job creator without even quantifying stress-level or level of satisfaction from those that will exist by 2030.\n\n\nExample: The rise of AI tools like GPT (Generative Pre-trained Transformer) has started to replace certain tasks performed by professionals in law and finance, sectors that were once considered safe for those with advanced degrees.\nDisplaced workers are often forced into lower-paying jobs or must invest in learning new technologies (SaaS tools, AI/ML etc.) to remain employable, perpetuating a cycle of continual upskilling without significant wage growth.\nSources - JOBS LOST, JOBS GAINED: WORKFORCE TRANSITIONS IN A TIME OF AUTOMATION https://www.mckinsey.com/~/media/mckinsey/industries/public%20and%20social%20sector/our%20insights/what%20the%20future%20of%20work%20will%20mean%20for%20jobs%20skills%20and%20wages/mgi-jobs-lost-jobs-gained-executive-summary-december-6-2017.pdf\n## Hypothesis: &gt; Benefits of automation mostly accrue to Venture Capitalists (VCs), private equity (PEs) firms, investment funds, governments (via tax collection) and enterprises, while workers face job displacement or wage stagnation\nThe labor share — the fraction of economic output that accrues to workers as compensation in exchange for their labor, could be a way to measure whether the benefits of automation is accruing to labour\n\nSource: - https://www.bls.gov/opub/mlr/2017/article/estimating-the-us-labor-share.htm\n\nSource: - A new look at the declining labor share of income in the United States https://www.mckinsey.com/~/media/McKinsey/Featured%20Insights/Employment%20and%20Growth/A%20new%20look%20at%20the%20declining%20labor%20share%20of%20income%20in%20the%20United%20States/MGI-A-new-look-at-the-declining-labor-share-of-income-in-the-United-States.pdf\n\n\nSummary of Five Main Reasons for Declining Labor Share:\n\n\nCapital Deepening, Substitution, and Automation:  Technological advancements, such as powerful computers and robots, reduce the cost of investment in capital, incentivizing companies to substitute labor with capital. This shift can lead to a decrease in the labor share of income as less labor is needed in production. However, the relationship is complex because technology can also complement labor, raising productivity and potentially leading to wage increases. The decline in labor’s share may occur if capital becomes more prominent in production, increasing returns on capital relative to labor.\n\n\n“Superstar” Effects and Consolidation:  The rise of “superstar” firms that dominate profits and value added in their industries, especially in knowledge-intensive sectors, has led to a larger share of economic value going to capital owners rather than labor. This phenomenon is often associated with consolidation and reduced competition, which can occur in regulated sectors or those protected by strong intellectual property rights. These superstar firms often deploy more capital or achieve higher returns, further reducing the labor share.\n\n\nGlobalization and Labor Bargaining Power:  Increased global trade and competition from countries with lower labor costs, along with the threat of offshoring jobs, have put downward pressure on wages and employment. The weakening of labor market institutions, such as unions, has further diminished workers’ bargaining power, contributing to a declining labor share. While stronger bargaining power or higher minimum wages can temporarily increase the labor share, these measures may also encourage greater capital substitution in the long term.\n\n\nHigher Depreciation (Due to Shift to Intangible Capital):  The shift toward greater use of intangible assets, like intellectual property (IP) and software, which have faster depreciation cycles than traditional physical assets, has increased the depreciation share of income. This reduces the overall amount available to labor and capital. Additionally, the economy has been working through a capital overhang from the investment boom before the financial crisis, further increasing depreciation and reducing the labor share.\n\n\nSupercycles and Boom-Bust:  Certain sectors, particularly those in energy and minerals, are subject to price supercycles, where rapidly rising commodity prices increase profits and reduce labor’s share of income. Other sectors, such as real estate and construction, have experienced boom-bust cycles that shift the capital and labor share of income. For example, tech services have shown industry-specific contractions followed by recovery, affecting labor’s share over time.\n\n\nOut of these causative factors (i), (ii) and (iv) are directly or indirectly related to Tech & Automation"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#impact-of-technological-disruption-underemployment-of-graduates",
    "href": "notebooks/Education and QoL-Satisfaction.html#impact-of-technological-disruption-underemployment-of-graduates",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "Impact of Technological Disruption: Underemployment of Graduates:",
    "text": "Impact of Technological Disruption: Underemployment of Graduates:\n\nOverqualification: As more people pursue higher education, the labor market becomes saturated with degree holders, leading to underemployment. Graduates often find themselves in jobs that do not require their level of education, resulting in lower job satisfaction and income.\nStatistical Evidence: The Federal Reserve Bank of New York reported that as of 2021, about 41% of recent college graduates in the U.S. were underemployed, meaning they were working in jobs that typically do not require a bachelor’s degree\n52% of Graduates Underemployed: One year after graduation, 52% of college graduates with a terminal bachelor’s degree are underemployed. This rate decreases only slightly to 45% after 10 years​(Talent-Disrupted-2).\n\n\nSTEM is not a silver bullet. While policymakers typically think of STEM (science, technology, engineering, and mathematics) programs as a sure pathway to college-level employment and high wages, the reality is more nuanced. Graduates with a bachelor’s degree in computer science, engineering, or mathematics tend to experience very low underemployment, while those with a degree in a life sciences field (e.g., biology) tend to face higher underemployment rates.\n\n\nPersistence of Underemployment: 73% Remain Underemployed: A staggering 73% of those who start out underemployed remain in such positions 10 years after graduation\nUnderemployment / Unemployment, beyond skill-gap, can also indicate over-supply in labour market. One way to measure that would be to capture data about Avg Applications per Job by Sectors or ratio of Job Openings to Unemployment\nOvereducation and depressive symptoms: diminishing mental health returns to education (https://sci-hub.se/10.1111/1467-9566.12039) &gt; On the supply side, the labour market value of educational credentials inflated (Hannum and Buchmann 2005), whereas on the demand side, employers started to compete for employees with the highest credentials in order to reduce the costs of job training (Hirsch 1977, Thurow 1976). As a result, the fact that the supply of highly educated people outnumbered the demand for educated labour (Freeman 1976) led to some highly educated people ending up in jobs that actually required lower qualifications (Duncan and Hoffman 1981). This phenomenon of overeducation thus became a permanent condition for a substantial number of employees (Pritchett 2001, Rubb 2003, Vaisey 2006). At the population level the presence of overeducation is inferred from the observation of diminishing returns to tertiary education. For instance, Freeman (1976) defines overeducation as ‘a falling private rate of return to college education’ (Psacharopoulos, 1994: 1334). At the individual level it is defined as job–education mismatch, that is, when ‘the level of education acquired exceeds the level of education required to adequately perform the job’ (Wolbers 2003: 251).\n\nSources:\n\nTALENT DISRUPTED - College Graduates, Underemployment, and the Way Forward\ncollege-labor-market\n\n\nTrend in Job openings to unemployment\n\n# BLS API Key\napi_key = '36aea4409aef4dd787a9ab7107c9d232'\n\n# Define the series IDs for job openings (JOLTS) and unemployment (UNRATE)\nseries_ids = {\n    \"Job Openings\": \"JTS000000000000000JOL\", \n    \"Unemployment\": \"LNS13000000\"\n}\n\n# Define the endpoint and parameters for the API request\nendpoint = \"https://api.bls.gov/publicAPI/v2/timeseries/data/\"\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\n\n# Request data for both series\ndata = {\n    \"seriesid\": list(series_ids.values()),\n\n    # \n    # NOTE:max data range can be of 20 years\n    #\n    \"startyear\": '2004',\n    \"endyear\": '2024',\n    \"registrationkey\": api_key\n}\n\nresponse = requests.post(endpoint, json=data, headers=headers)\njson_data = response.json()\n\n\n# Extract data and create a DataFrame\nseries_data = {}\nfor series in json_data['Results']['series']:\n    series_id = series['seriesID']\n    series_name = [key for key, value in series_ids.items() if value == series_id][0]\n    data_points = [(item['year'], item['value']) for item in series['data']]\n    df = pd.DataFrame(data_points, columns=['Year', series_name])\n    df[series_name] = df[series_name].astype(float)\n    series_data[series_name] = df.set_index('Year')\n\n# Merge the two dataframes\ndf_job_openings_unemployment_merged = pd.merge(series_data['Job Openings'], \n                                               series_data['Unemployment'], \n                                               left_index=True, \n                                               right_index=True)\n\n# Calculate the ratio of Job Openings to Unemployment\ndf_job_openings_unemployment_merged['Job Openings to Unemployment Ratio'] = df_job_openings_unemployment_merged['Job Openings'] / df_job_openings_unemployment_merged['Unemployment']\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(df_job_openings_unemployment_merged.index, \n         df_job_openings_unemployment_merged['Job Openings to Unemployment Ratio'], \n         marker='o')\nplt.title('Job Openings to Unemployment Ratio (2004-2024)')\nplt.xlabel('Year')\nplt.ylabel('Ratio of Job Openings to Unemployment')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWhat’s interesting is that the ratio remained &lt;1 (over-supply/skill-gap) till 2018, corrected during COVID and rebounded stronger. No we don’t know how much of that rebound is caused by transitioning to the “new world order”, remote/WFH tech jobs or monetary stimulus provided by government.\n\nTrend in Job openings to unemployment - By Sectors\n\n# Define the series IDs for job openings (JOLTS) and unemployment for various sectors\n\n# https://www.bls.gov/help/hlpforma.htm#jt\n#\nseries_ids = {\n    \"Tech Job Openings\": \"JTS540099000000000JOL\",        # Professional and business services (often used as a proxy for tech)\n    \"Manufacturing Job Openings\": \"JTS300000000000000JOL\",\n    \"Retail Job Openings\": \"JTS440000000000000JOL\",\n    # \"Food Services Job Openings\": \"JTS720000000000000JOL\",\n\n    # https://data.bls.gov/timeseries/LNU04032215\n    #\n    # 04 = rate, 03 = level (numbers)\n    #\n    \"Tech Unemployment\": \"LNU03032239\",           # Unemployment for professional and technical services\n    \"Manufacturing Unemployment\": \"LNU03032232\",\n    \"Retail Unemployment\": \"LNU03032235\"\n    # \"Food Services Unemployment\": \"LNS14000032\"\n}\n\n# Define the endpoint and parameters for the API request\nendpoint = \"https://api.bls.gov/publicAPI/v2/timeseries/data/\"\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\n\n# Request data for each series\ndata = {\n    \"seriesid\": list(series_ids.values()),\n    \"startyear\": \"2004\",\n    \"endyear\": \"2024\",\n    \"registrationkey\": api_key\n}\n\nresponse = requests.post(endpoint, json=data, headers=headers)\njson_data = response.json()\n\n\n# Extract data and create DataFrames for each sector\nsector_data = {}\nfor series in json_data['Results']['series']:\n    series_id = series['seriesID']\n    series_name = [key for key, value in series_ids.items() if value == series_id][0]\n    data_points = [(item['year'], item['value']) for item in series['data']]\n    df = pd.DataFrame(data_points, columns=['Year', series_name])\n    df[series_name] = df[series_name].astype(float)\n    sector_data[series_name] = df.set_index('Year')\n\n\n# Merge job openings and unemployment data for each sector\nsectors = [\"Tech\", \"Manufacturing\", \"Retail\"]\ndf_ratios = pd.DataFrame()\n\nfor sector in sectors:\n    job_openings_col = f\"{sector} Job Openings\"\n    unemployment_col = f\"{sector} Unemployment\"\n    df_merged = pd.merge(sector_data[job_openings_col], sector_data[unemployment_col], left_index=True, right_index=True)\n    df_merged[f\"{sector} Job Openings to Unemployment Ratio\"] = df_merged[job_openings_col] / df_merged[unemployment_col]\n    if df_ratios.empty:\n        df_ratios = df_merged[[f\"{sector} Job Openings to Unemployment Ratio\"]]\n    else:\n        df_ratios = df_ratios.join(df_merged[[f\"{sector} Job Openings to Unemployment Ratio\"]], how='outer')\n\n\ndf_job_to_unemp_ratios = df_ratios.groupby(df_ratios.index)[[\"Tech Job Openings to Unemployment Ratio\", \"Manufacturing Job Openings to Unemployment Ratio\", \"Retail Job Openings to Unemployment Ratio\"]].mean()\ndf_job_to_unemp_ratios.head()\n\n\n\n\n\n\n\n\nTech Job Openings to Unemployment Ratio\nManufacturing Job Openings to Unemployment Ratio\nRetail Job Openings to Unemployment Ratio\n\n\nYear\n\n\n\n\n\n\n\n2004\n0.780469\n0.273507\n0.349046\n\n\n2005\n0.958135\n0.363483\n0.430798\n\n\n2006\n1.167889\n0.479952\n0.483758\n\n\n2007\n1.263115\n0.479159\n0.489625\n\n\n2008\n0.815361\n0.260251\n0.345037\n\n\n\n\n\n\n\n\n# Plot the data\nplt.figure(figsize=(14, 8))\nplt.plot(df_job_to_unemp_ratios.index, df_job_to_unemp_ratios[\"Tech Job Openings to Unemployment Ratio\"], marker='o', label=\"Tech Job Openings to Unemployment Ratio\")\nplt.plot(df_job_to_unemp_ratios.index, df_job_to_unemp_ratios[\"Manufacturing Job Openings to Unemployment Ratio\"], marker='o', label=\"Manufacturing Job Openings to Unemployment Ratio\")\nplt.plot(df_job_to_unemp_ratios.index, df_job_to_unemp_ratios[\"Retail Job Openings to Unemployment Ratio\"], marker='o', label=\"Retail Job Openings to Unemployment Ratio\")\nplt.title('Job Openings to Unemployment Ratio by Sector (2004-2024)')\nplt.xlabel('Year')\nplt.ylabel('Ratio of Job Openings to Unemployment')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe above chart illustrates the ratio of job openings to unemployment across three sectors: Tech, Manufacturing, and Retail, over the period from 2004 to 2024. The Tech sector consistently shows a higher ratio compared to Manufacturing and Retail, particularly after 2014, where it surpasses a ratio of 1.0, indicating more job openings than unemployed individuals in that sector. The ratio peaks around 2022 at approximately 3.0 before slightly declining in 2023. The Manufacturing sector shows a steady increase in the ratio from around 0.2 in 2009 to about 1.0 in 2022, indicating a tightening labor market. Retail also shows a similar trend, with the ratio increasing from around 0.2 in 2009 to about 1.0 in 2022. However, both Manufacturing and Retail sectors exhibit more fluctuation compared to the Tech sector, particularly noticeable during the 2020-2021 period, reflecting the impact of economic disruptions during that time.\nReflections\nAlthough tech is creating more openings than official unemployment numbers, there are two caveats.\n\nTech itself changes fast and anyone in tech needs constant re-skilling/up-skilling to cope up with the change.\nThe definition of “Unemplyment” is tricky. &gt; Unemployment rate The unemployment rate represents the number of unemployed people as a percentage of the labor force (the labor force is the sum of the employed and unemployed). The unemployment rate is calculated as: (Unemployed ÷ Labor Force) x 100.\n\nNot in the labor force: In the Current Population Survey, people are classified as not in the labor force if: \n\n\nthey were not employed during the survey reference week and \nthey had not actively looked for work (or been on temporary layoff) in the last 4 weeks  In other words, people not in the labor force are those who do not meet the criteria to be classified as either employed or unemployed, as defined above. People not in the labor force are asked whether they want a job and if they were available to take a job during the survey reference week. They also are asked about their job search activity in the last 12 months (or since the end of their last job, if they held one in the last 12 months) and their reason for not having looked for work in the most recent 4 weeks.\n\n\nThe value of degrees in fields like business, computer science, and engineering has significantly diminished compared to 30 years ago. While these degrees once paved a reliable path to stable and lucrative careers, today’s landscape is far more competitive, with qualified professionals and offshore workers willing to work for less. As a result, merely obtaining a degree is no longer a guaranteed ticket to success; one must be exceptionally skilled, and this often needs to start before even pursuing the degree. For many, it may be more advantageous to take the risk of business ownership and work for themselves, where they have greater control over their income and career. From the perspective of a seasoned software engineer with over a decade of experience, including leadership roles, the field has become increasingly challenging and less rewarding. If given the chance to start over, they would prioritize gaining experience quickly through startups, learning the intricacies of business, and eventually pursuing entrepreneurship to have more control over their destiny and financial rewards. In essence, a degree alone is not enough anymore; individuals must take proactive control of their careers and explore alternative paths like entrepreneurship to secure their futures.\n\nStudent Debt: Federal Reserve data shows that student debt in the U.S. has skyrocketed, surpassing $1.7 trillion in 2021. This growing debt burden makes it harder for graduates to achieve financial stability, let alone upward mobility."
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#jor",
    "href": "notebooks/Education and QoL-Satisfaction.html#jor",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "JOR",
    "text": "JOR\nThe Job Openings Rate (JOR) is defined as the number of job openings on the last business day of the month as a percentage of total employment plus job openings. Mathematically:\n$\n= ( ) $\n\nKey Components:\n\nJob Openings: The number of available positions employers are actively recruiting to fill.\nTotal Employment: The total number of individuals currently employed in the workforce.\nDenominator: The sum of total employment and job openings represents the total labor market capacity.\n\n\n\nPurpose:\n\nThe JOR serves as an indicator of labor demand and provides insights into economic health.\nHigher rates may signal strong demand for workers, while lower rates can indicate reduced hiring activity.\n\nThis definition is relevant to the plotted data in your script, where JOR trends are analyzed across various industry sectors.\n\nimport requests\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Define your BLS API key (replace with your actual API key)\nAPI_KEY = \"36aea4409aef4dd787a9ab7107c9d232\"\n\n# Define the series IDs for different industries (replace with actual series IDs for JOR)\nseries_ids = {\n    \"Construction\": \"JTU000000000000000JOR\",\n    \"Manufacturing\": \"JTU300000000000000JOR\",\n    \"Retail Trade\": \"JTU440000000000000JOR\",\n    \"Professional Services\": \"JTU600000000000000JOR\",\n    \"Leisure and Hospitality\": \"JTU700000000000000JOR\",\n}\n\n# Define the API URL\nBASE_URL = \"https://api.bls.gov/publicAPI/v2/timeseries/data/\"\n\n# Fetch data from BLS API\ndef fetch_bls_data(series_id):\n    payload = {\n        \"seriesid\": [series_id],\n        \"startyear\": \"2000\",\n        \"endyear\": \"2024\",\n        \"registrationkey\": API_KEY,\n    }\n    response = requests.post(BASE_URL, json=payload)\n    if response.status_code == 200:\n        data = response.json()\n        return data[\"Results\"][\"series\"][0][\"data\"]\n    else:\n        print(f\"Error fetching data for {series_id}: {response.status_code}\")\n        return []\n\n# Process the fetched data\ndef process_data(data):\n    years = []\n    values = []\n    for entry in data:\n        years.append(int(entry[\"year\"]))\n        values.append(float(entry[\"value\"]))\n    return pd.Series(values, index=years)\n\n# Fetch and process data for all series\njor_data = {}\nfor sector, series_id in series_ids.items():\n    raw_data = fetch_bls_data(series_id)\n    jor_data[sector] = process_data(raw_data)\n\n# Combine all data into a single DataFrame\ndf = pd.DataFrame(jor_data)\n\n# Plot the data\nplt.figure(figsize=(12, 6))\nfor column in df.columns:\n    plt.plot(df.index, df[column], marker=\"o\", label=column)\n\n# Add chart details\nplt.title(\"Job Openings Rate (JOR) by Industry Sector (US)\", fontsize=14)\nplt.xlabel(\"Year\", fontsize=12)\nplt.ylabel(\"Job Openings Rate (%)\", fontsize=12)\nplt.legend(title=\"Industry Sector\", fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.7)\n\n# Show the plot\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#b.-social-status-and-mobility",
    "href": "notebooks/Education and QoL-Satisfaction.html#b.-social-status-and-mobility",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "1b. Social Status and Mobility",
    "text": "1b. Social Status and Mobility\n\nLabor Market Saturation and Reduced Returns on Education:\nThe saturation of the labor market with degree holders has diminished the economic returns of education for many individuals. With more people obtaining degrees, the competition for top-tier jobs has intensified, leading to a situation where a degree alone is no longer sufficient to guarantee upward mobility. This has led to a “credential inflation,” where higher qualifications are needed to stand out, further pushing individuals towards costly graduate education, such as MBAs, which again are becoming increasingly expensive due to M2 inflation.\nStrategies to validate:\n\nUse LinkedIn data scraping for Application-to-job ratio\n% of MBA degrees confered over years (National Center for Education Statistics (NCES) data)\n\n\n\nRise of Elite Education as a Gateway\n\nBusiness Leadership: Frank and Cook discuss how elite business schools (e.g., Harvard Business School, Stanford Graduate School of Business) dominate the pathways to top executive positions in Fortune 500 companies. The networks and brand recognition of these institutions give their graduates a significant edge in the competition for leadership roles.\nLegal Profession: The book highlights how top law firms overwhelmingly recruit from a handful of elite law schools (e.g., Harvard, Yale, Stanford). Graduates from these schools have a much higher chance of securing high-paying positions, regardless of their actual performance in law school compared to graduates from less prestigious institutions.\nEducational background plays a crucial role in determining career trajectories. Those from elite schools continue to dominate leadership positions, whereas those from non-elite schools find it harder to break into these roles, even if they start at the same level.\nQuantitative Data: The study provides data indicating that over the last few decades, the proportion of executives coming from elite schools has increased, while the proportion from non-elite schools has decreased.\nReference: Frank, R. H., & Cook, P. J. (1995). The Winner-Take-All Society. This book discusses how certain elite institutions have monopolized access to top jobs.\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nReference: - Diversity, Hierarchy, and Fit in Legal Careers: Insights from Fifteen Years of Qualitative Interviews https://www.law.georgetown.edu/legal-ethics-journal/wp-content/uploads/sites/24/2019/01/GT-GJLE180004.pdf\n\n\nData on Educational Attainment and Professional Success\nTo create a chart showing the proportion of executives from elite versus non-elite schools from 1980 to 2020, data from various studies indicate the following trends:\n1980s: In the early 1980s, around 50% of executives in top U.S. firms had graduated from elite schools, with this figure remaining fairly stable through the decade. Elite schools are often defined as Ivy League institutions and other top-tier universities like Stanford and MIT​( Oxford Academic ).\n1990s: The 1990s saw a slight increase in the proportion of executives from elite schools, reaching around 55%. This was driven by the increasing value placed on prestigious MBA programs from elite institutions as a key qualification for senior management roles​( Oxford Academic ).\n2000s: During the 2000s, the proportion of executives from elite schools continued to rise, peaking at around 60-65% by the late 2000s. This trend was supported by the globalization of business and the preference for executives with international educational experiences, often obtained at elite institutions​( SpringerLink ).\n2010-2020: The proportion of executives from elite schools has stabilized, fluctuating between 60-70%. This period also saw an increase in the importance of non-traditional elite schools, particularly for tech and innovative companies, where elite institutions like Stanford and MIT played a major role​( SpringerLink , Oxford Academic ).\nRef - Steven Brint, Sarah R K Yoshikawa, The Educational Backgrounds of American Business and Government Leaders: Inter-Industry Variation in Recruitment from Elite Colleges and Graduate Programs, Social Forces, Volume 96, Issue 2, December 2017, Pages 561–590, https://doi.org/10.1093/sf/sox059 https://academic.oup.com/sf/article-abstract/96/2/561/4622952?login=false - https://link.springer.com/chapter/10.1007/978-3-319-59966-3_5\nEducational Backgrounds of American Business and Government Leaders\nSteven Brint, Sarah R K Yoshikawa, The Educational Backgrounds of American Business and Government Leaders: Inter-Industry Variation in Recruitment from Elite Colleges and Graduate Programs, Social Forces, Volume 96, Issue 2, December 2017, Pages 561–590, https://doi.org/10.1093/sf/sox059\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data from the table\ngroups = [\n    \"A. Symbol production/Knowledge sector\", \"A. Symbol production/Knowledge sector\", \n    \"A. Symbol production/Knowledge sector\", \"A. Symbol production/Knowledge sector\", \n    \"A. Symbol production/Knowledge sector\",\n    \"B. Material production/Knowledge sector\", \"B. Material production/Knowledge sector\", \n    \"B. Material production/Knowledge sector\", \"B. Material production/Knowledge sector\", \n    \"B. Material production/Knowledge sector\",\n    \"C. Material production/Outside knowledge sector\", \"C. Material production/Outside knowledge sector\", \n    \"C. Material production/Outside knowledge sector\", \"C. Material production/Outside knowledge sector\", \n    \"C. Material production/Outside knowledge sector\"\n]\n\nindustries = [\n    \"Internet services\", \"Entertainment/Media\", \"Finance\", \"Computer Software\", \"Government\",\n    \"Pharmaceuticals\", \"Telecommunications\", \"Aerospace/Security\", \"Health care\", \"Energy\",\n    \"Apparel\", \"Chemicals\", \"Construction\", \"Food products\", \"Motor vehicles\"\n]\n\n# Steven Brint, Sarah R K Yoshikawa, The Educational Backgrounds of American Business and Government Leaders: Inter-Industry Variation in Recruitment from Elite Colleges and Graduate Programs, Social Forces, Volume 96, Issue 2, December 2017, Pages 561–590, https://doi.org/10.1093/sf/sox059 https://academic.oup.com/sf/article-abstract/96/2/561/4622952?login=false\n\nbachelors_degrees = [32, 28, 28, 22, 21, 21, 18, 15, 14, 14, 20, 13, 12, 9, 8]\nbusiness_degrees = [73, 59, 57, 49, 34, 48, 31, 32, 39, 35, 52, 50, 39, 45, 32]\nlaw_degrees = [79, 56, 53, 48, 31, 48, 45, 32, 41, 31, 33, 36, 38, 25, 12]\n\n# Define a more soothing color palette\nbachelors_color = \"#a1c3d1\"  # Light teal\nbusiness_color = \"#70a4d3\"   # Medium teal\nlaw_color = \"#4678c9\"        # Dark teal\n\n# Plotting the trends\nx = np.arange(len(industries))  # the label locations\nwidth = 0.25  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(14, 8))\nrects1 = ax.bar(x - width, bachelors_degrees, width, label=\"Bachelor's Degrees\", color=bachelors_color)\nrects2 = ax.bar(x, business_degrees, width, label='Business Degrees', color=business_color)\nrects3 = ax.bar(x + width, law_degrees, width, label='Law Degrees', color=law_color)\n\n# Add vertical lines to visually segregate the industry groups\nax.axvline(x=4.5, color='black', linestyle='--', lw=1)  # End of Symbol production/Knowledge sector\nax.axvline(x=9.5, color='black', linestyle='--', lw=1)  # End of Material production/Knowledge sector\n\nax.text(2.5, 85, 'Symbol production/\\nKnowledge sector', ha='center', va='bottom', fontsize=12)\nax.text(7.5, 85, 'Material production/\\nKnowledge sector', ha='center', va='bottom', fontsize=12)\nax.text(12.5, 85, 'Material production/\\nOutside knowledge sector', ha='center', va='bottom', fontsize=12)\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_xlabel('Industries')\nax.set_ylabel('Percentage of Executives (%)')\n#ax.set_title('Percentage of Executives with Elite Degrees by Industry Group')\nax.set_xticks(x)\nax.set_xticklabels(industries, rotation=45, ha=\"right\")\nax.legend()\n\nfig.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\nhttps://x.com/cremieuxrecueil/status/1831463564575699266/photo/1\n!\nThe chart illustrates the educational backgrounds of extraordinary American achievers across various categories. For example, Harvard University alumni account for approximately 50% of American Philosophical Society members and 35% of Forbes’ most powerful men. Graduate School graduates make up the majority in categories like National Academy of Medicine (over 70%) and Nobel Prize winners (about 60%). On the other hand, Ivy League graduates represent significant shares in categories like Pulitzer Prize winners (40%) and Four-Star Generals (20%). Some categories show missing educational information, such as Senators (around 10%).\nIn the last generation or two, the funnel of opportunity in American society has drastically narrowed, with a greater and greater proportion of our financial, media, business, and political elites being drawn from a relatively small number of our leading universities, together with their professional schools. The rise of a Henry Ford, from farm boy mechanic to world business tycoon, seems virtually impossible today, as even America’s most successful college dropouts such as Bill Gates and Mark Zuckerberg often turn out to be extremely well-connected former Harvard students. Indeed, the early success of Facebook was largely due to the powerful imprimatur it enjoyed from its exclusive availability first only at Harvard and later restricted to just the Ivy League\nLet’s explore what are demographical and academic attributes of groups attaining eite education\n\n\nChildren from families in the top 1% are more than twice as likely to attend an Ivy-Plus college (Ivy League, Stanford, MIT, Duke, and Chicago) as those from middle-class families with comparable SAT/ACT scores. Two-thirds of this gap is due to higher admissions rates for students with comparable test scores from high-income families\n\n\n\n\nThe highincome admissions advantage at private colleges is driven by three factors: - (1) preferences for children of alumni (legacies) - (2) weight placed on non-academic credentials, which tend to be stronger for students applying from private high schools (feeder) that have affluent student bodies, - (3) recruitment of athletes, who tend to come from higher-income families.\n\n\nReferences\n\nChetty, R., Deming, D., & Friedman, J. (2023). Diversifying Society’s Leaders? The Determinants and Causal Effects of Admission to Highly Selective Private Colleges. National Bureau of Economic Research. https://doi.org/10.3386/w31492\n\n\n\n\nimage.png\n\n\n\nDevelopment cases, where donations influence admissions, are common among Ivy Plus schools. For example, Jared Kushner’s father donated $2.5 million to Harvard before his son’s acceptance. Schools like USC also heavily weigh such cases. Political and celebrity connections—like children of U.S. Senators or famous actors—often sway admissions. This practice highlights how wealth and influence can shape access to top-tier education.\nThe push for legacy admissions persists, even as it faces criticism. Legacy students, those with family ties to alumni, receive special consideration, despite calls to end the practice. Ivy League schools, and some state flagships like Michigan and UVA, continue to favor legacies. For instance, Johns Hopkins recently ended legacy admissions, aiming for more socio-economic diversity, but this may impact future alumni donations.\nAthletics also play a crucial role in admissions at elite schools. Ivies admit a notable number of student-athletes (up to 10% of their student bodies). Brown, with 910 athletes, parallels Michigan despite a smaller overall student body. Schools like Stanford and Duke also recruit heavily for sports like sailing and lacrosse, reinforcing the socioeconomic skew in admissions, with wealthier, predominantly white students disproportionately represented.\n\n\nUnfair Advantage\nMany universities offer tuition remission to employees’ children, with some covering up to 100% of tuition if the student attends the parent’s institution, but only 50-75% if they attend another school. This incentivizes universities to admit employees’ children, minimizing financial loss. Employees’ families, particularly those in academic households, often have greater cultural and academic capital, giving them an admissions advantage. Children of professors or staff, exposed to academic environments and cultural resources, may appear more competitive than wealthier, first-generation applicants, who lack such insider knowledge.\n\n\n# Data for plotting\nyears = list(range(1980, 2021, 5))\nelite_schools = [50, 55, 60, 65, 70, 65, 67, 69, 70]\nnon_elite_schools = [100 - x for x in elite_schools]\n\n# Plotting the data\nplt.figure(figsize=(10, 6))\n\nplt.plot(years, elite_schools, marker='o', linestyle='-', color='blue', label='Elite Schools')\nplt.plot(years, non_elite_schools, marker='o', linestyle='-', color='orange', label='Non-Elite Schools')\n\n# Adding titles and labels\nplt.title('Proportion (appx.) of Executives from Elite vs. Non-Elite Schools (1980-2020)')\nplt.xlabel('Year')\nplt.ylabel('Proportion of Executives (%)')\nplt.ylim(0, 100)\nplt.xticks(years)\nplt.grid(True)\n\n# Adding a legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe Winner-Take-All Society\n\n\n\nimage.png\n\n\n\nHow certain elite institutions have monopolized access to top jobs.\nFrank, R. H., & Cook, P. J. (1995). The Winner-Take-All Society\nConcentration of Rewards in Elite Institutions: Argument: Frank and Cook argue that in many fields, the rewards (jobs, salaries, opportunities) are increasingly concentrated among those who graduate from elite institutions. These institutions, such as Ivy League universities in the U.S., have become gatekeepers to the most lucrative and prestigious careers. Example: The authors discuss how a small number of elite schools produce a disproportionately high number of individuals in top positions across various industries, from law and finance to academia and government.\n\nhttps://academic.oup.com/qje/article/137/2/845/6449025\n\nWinner-Take-All Markets: Concept: The book introduces the concept of “winner-take-all markets,” where small differences in talent or credentials can lead to vastly different outcomes in terms of success and earnings. In these markets, those at the very top capture the majority of rewards, while the rest receive significantly less. Data: The book cites examples such as the concentration of top lawyers from a handful of law schools or CEOs who predominantly come from elite business schools. This concentration means that individuals from non-elite institutions find it increasingly difficult to compete for top-tier positions.\nImpact on Social Mobility: Argument: The monopolization of access to top jobs by elite institutions exacerbates inequality and reduces social mobility. As these institutions become more selective and expensive, only individuals from affluent backgrounds can afford the education and connections needed to access these opportunities. Data: The book discusses how the children of affluent families are more likely to attend elite institutions, perpetuating a cycle of privilege. In contrast, those from less privileged backgrounds face significant barriers to entry, even if they have similar levels of talent or ambition.\n\nPedigree: How Elite Students Get Elite Jobs” by Lauren A. Rivera\n\nEvery year, elite firms designate lists of schools with which they have established relationships, and where they intend to post job openings, accept applications, and interview students. These lists have two tiers. Core schools are the three to five highly elite institutions from which firms draw the bulk of their new hires. Firms invest deeply at these campuses, flying current employees from across the country—if not the globe—to host information sessions, cocktail receptions, and dinners, prepare candidates for interviews, and interview scores or even hundreds of candidates every year. Target schools, by contrast, include five to fifteen additional institutions where firms intend to accept applications and interview candidates, but on a much smaller scale.14 Firms typically set quotas for each school, with cores receiving far more interview and final offer slots than targets.\n\n…\n\nFirms commonly made their school selections based on general perceptions of these institutions’ prestige. When asked how her law firm created its list, Kayla, a recruitment director, summarized the strategy this way:\n\n\nIt’s totally anecdotal. (She laughs.) I think it’s based upon—and it probably lags in terms of time and updating—but it’s based upon a kind of understanding of how selective the school was in terms of admitting students and how challenging is the work. So it’s largely just kind of school reputation and conventional wisdom, for better or worse.\n\n\nThis kind of anecdotal information was derived from the perceptions of partners and other decision makers (who, disproportionately, were themselves graduates of prestigious schools). In addition, firms used the reports of external rankings organizations, such as U.S. News & World Report and the Law School Admissions Council. However, they typi- cally consulted outside sources only when setting the lower bounds of their lists. Consequently, in contrast to the volatility of national edu- cational rankings, firms’ lists remained quite stable from year to year\n\n…\n\nAlthough stable notions of prestige were the most common basis for designating schools as cores or targets, new or less prestigious schools could be put on the list if the firm had high-ranking employees who were graduates and pushed the firm to recruit from their alma mater. Michael, a banker, told me, “If a senior person has a particular inter- est in going to a particular school, we’ll generally go.” Another banker, Nicholae, described why his alma mater—a well-regarded but not top- ten liberal arts college—was included on his bank’s list of targets. “We started recruiting at [my school] because the CEO’s daughter was in my class there, and now two chairmen’s kids are there. [It’s] a good school, but it’s definitely those types of connections that make us recruit there.” A consultant named Ella provided a similar illustration:\n\n\n\nUVA [the University of Virginia] is actually a big target school of ours. . . . It started because there was a partner who was an alum and who just pushed it hard and so we ended up with actually having quite a big recruiting team associated with that school. Which maybe normally we wouldn’t, given [our firm’s] location and their ranking and what not.\n\n\n\nSuch schools tended to stay on the list as long as the employee who initially pushed for the campus remained at the firm and continued to press for recruitment. Due to organizational inertia, some remained on the list after that employee’s departure."
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#high-competitive-stress-1",
    "href": "notebooks/Education and QoL-Satisfaction.html#high-competitive-stress-1",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "High Competitive Stress:",
    "text": "High Competitive Stress:\nMental Health Impacts: As the demand for higher education has increased, so has the competition, leading to significant stress among students. The pressure to perform well academically to secure top-tier jobs has contributed to a rise in mental health issues, including anxiety and depression. This competitive stress can negate some of the QoL improvements associated with higher education​ (World Bank).\nWork-Life Imbalance: The need to excel in education often leads to work-life imbalances, where students and young professionals may sacrifice leisure and family time for academic or career success, potentially reducing overall life satisfaction.\n\nAdmission to Ivy League and Other Selective Universities.\nData Sources:\n\nhttps://web.archive.org/web/20150222074515/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2008/\nhttps://web.archive.org/web/20150222070007/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2009/\nhttps://web.archive.org/web/20150222074602/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2010/\nhttps://web.archive.org/web/20150222074712/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2011/\nhttps://web.archive.org/web/20150222071302/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2012/\nhttps://web.archive.org/web/20150222074526/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2013/\nhttps://web.archive.org/web/20150222074708/http://www.hernandezcollegeconsulting.com/ivy-league-admissions-statistics-2014/\nhttps://web.archive.org/web/20150222071001/http://www.hernandezcollegeconsulting.com/ivy-league-admissions-statistics-overall-2014/\nhttps://web.archive.org/web/20150222074653/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-overall-2015/\nhttps://web.archive.org/web/20150222074618/http://www.hernandezcollegeconsulting.com/ivy-league-admissions-statistics-overall-2016/\nhttps://web.archive.org/web/20150222074639/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2017/\nhttps://web.archive.org/web/20150222004000/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-overall-2018/\nhttps://web.archive.org/web/20150222074612/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-class-2019/\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Data for Ivy League Admission Statistics (2008 - 2019)\ndata = {\n    'Year': [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019],\n    'Harvard_Admission_Rate': [10.3, 9.22, 9.33, 8.97, 7.09, 7.32, 5.90, 6.17, 5.92, 5.79, 5.90, 16.51],\n    'Yale_Admission_Rate': [9.9, 9.67, 8.90, 9.63, 8.29, 7.50, 6.59, 7.35, 6.82, 6.72, 6.26, 16.05],\n    'Princeton_Admission_Rate': [11.9, 10.94, 10.19, 9.46, 9.25, 9.93, 7.29, 8.39, 7.86, 7.29, 7.28, 19.92],\n    'Dartmouth_Admission_Rate': [18.3, 17.02, 15.68, 15.28, 13.24, 12.05, 9.92, 9.73, 9.43, 10.05, 11.50, 25.98],\n    'Brown_Admission_Rate': [15.8, 15.12, 13.82, 14.05, 13.29, 10.84, 8.84, 8.70, 9.60, 9.16, 8.61, 20.46],\n    'Penn_Admission_Rate': [21, 20.80, 17.66, 16.06, 16.44, 17.11, 11.97, 12.26, 12.32, 12.10, 9.90, 23.98],\n    'Columbia_Admission_Rate': [12.76, 12.76, 11.57, 10.57, 10.05, 9.82, 6.89, 6.93, 7.42, 6.89, 6.95, 17.79],\n    'Cornell_Admission_Rate': [28.7, 27.08, 24.68, 21.40, 20.40, 19.10, 15.04, 17.95, 16.19, 15.15, 13.98, 25.00],\n}\n\n# Convert to DataFrame\ndf = pd.DataFrame(data)\n\n# Plotting the trends for each university\nplt.figure(figsize=(14, 8))\n\nuniversities = ['Harvard', 'Yale', 'Princeton', 'Dartmouth', 'Brown', 'Penn', 'Columbia', 'Cornell']\nfor uni in universities:\n    plt.plot(df['Year'], df[f'{uni}_Admission_Rate'], label=uni)\n\nplt.title('Ivy League Admission Rates (2008 - 2019)')\nplt.xlabel('Year')\nplt.ylabel('Admission Rate (%)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Calculate and plot the aggregate Ivy League admission rate (average of all universities)\ndf['Average_Ivy_Admission_Rate'] = df[[f'{uni}_Admission_Rate' for uni in universities]].mean(axis=1)\n\nplt.figure(figsize=(10, 6))\nplt.plot(df['Year'], df['Average_Ivy_Admission_Rate'], label='Average Ivy League', color='black', linewidth=2)\nplt.title('Average Ivy League Admission Rate (2008 - 2019)')\nplt.xlabel('Year')\nplt.ylabel('Admission Rate (%)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost every Ivy has an undergraduate admissions rate of under 10%. And every single one has a downward trend on admit percentages, meaning it’s harder to get into every Ivy than it was 5, 10, 15, 20, or 25 years ago.\nFrom 2008 to 2019, the average admission rate fell from about 16% to just under 10%, with a sharp increase to over 20% in 2019, possibly due to external factors such as policy changes or exceptional circumstances (#COVID19 ?).\nEach school within the Ivy League has its own trajectory. For instance, Harvard’s acceptance rate dropped from over 10% to around 5%, while Cornell showed more fluctuation, decreasing from 20% to below 15%.\nIvy League universities have consistently maintained small class sizes, yet the number of applicants continues to surge each year. This growing pool of applicants pushes down acceptance rates, which improves their standing in rankings like US News.\n\n\nWhy did number of applicants continues to surge each year ?\n\n\n\nimage.png\n\n\nThe chart illustrates the educational backgrounds of extraordinary American achievers across various categories. For example, Harvard University alumni account for approximately 50% of American Philosophical Society members and 35% of Forbes’ most powerful men. Graduate School graduates make up the majority in categories like National Academy of Medicine (over 70%) and Nobel Prize winners (about 60%). On the other hand, Ivy League graduates represent significant shares in categories like Pulitzer Prize winners (40%) and Four-Star Generals (20%). Some categories show missing educational information, such as Senators (around 10%).\nIn the last generation or two, the funnel of opportunity in American society has drastically narrowed, with a greater and greater proportion of our financial, media, business, and political elites being drawn from a relatively small number of our leading universities, together with their professional schools. The rise of a Henry Ford, from farm boy mechanic to world business tycoon, seems virtually impossible today, as even America’s most successful college dropouts such as Bill Gates and Mark Zuckerberg often turn out to be extremely well-connected former Harvard students. Indeed, the early success of Facebook was largely due to the powerful imprimatur it enjoyed from its exclusive availability first only at Harvard and later restricted to just the Ivy League\n\n\n\nimage.png\n\n\nReferences:\n\nhttps://www.openthebooks.com/assets/1/6/Oversight_IvyLeagueInc_FINAL.pdf\n\nGiven such a narrow and diminishing window of opportunity, academic and wealth background of parent and even parenting style (“Helicopter parenting”) can add competitive advantage to students.\nSince much of America’s elite today emerges from a meritocratic system, akin to ancient Roman or Chinese elite pathways, parents increasingly shape their children’s upbringing to ensure passage through the same achievement gates. “Helicopter parenting,” once seen as irrational, is a strategic response to this competitive landscape, as noted by Pamela Druckerman in 2019.\n\n\n\nimage.png\n\n\n\n\nWhat about mental health ?\nDespite legacy admissions and insider knowledge aiding children, the competition narrows their lives, leaving little room for curiosity or rebellion. This controlled upbringing often robs individuals of the adventurousness seen in past pioneers\n\nThis means that their (students’) lives are way more tightly controlled, in order to compete against everyone else attempting to achieve SUCCESS in the modern era, which is why so many of the most “successful” people according to conventional measurements aren’t very adventurous anymore, there’s not a lot of room for experimentation or much else anymore, since the road to professional success is, for the most part, so very NARROW, and doesn’t tend to reward the inquisitiveness and rebelliousness that many great people of the past had going for them\n\n\n\nDecline in Job Satisfaction Over Time\nIn the mid-1980s, approximately 61% of workers reported being satisfied with their jobs, as shown by studies from NLS and Gallup surveys. However, as of 2021, this percentage has dropped to around 50%, reflecting increasing pressures in the workplace.\nMany of these pressures stem from\n\nshifting expectations and demands,\nexacerbated by the rise of the gig economy,\nrapid technological changes,\nthe COVID-19 pandemic, oppressive hours,\npolitical infighting,\nincreased competition sparked by globalization,\nan “always-on culture” bred by the internet.\n\nThe decline in job satisfaction is noticeable across several industries, especially in sectors that are fast-paced and high- stress, like engineering and finance.\nOne Harvard MBA observed about his Harvard MBA classmate: &gt; “One classmate described having to invest USD 5M a day — which didn’t sound terrible, until he explained that if he put only USD 4M to work on Monday, he had to scramble to place USD 6M on Tuesday, and his co-workers were constantly undermining one another in search of the next promotion. It was insanely stressful work, done among people he didn’t particularly like. He earned about $1.2 million a year and hated going to the office. &gt; ‘I feel like I’m wasting my life,’ he told me.’ When I die, is anyone going to care that I earned an extra percentage point of return? My work feels totally meaningless.’\n\nHe recognized the incredible privilege of his pay and status, but his anguish seemed genuine.\n\n\n‘If you spend 12 hours a day doing work you hate, at some point it doesn’t matter what your paycheck says,’ he told me.\n\n\nThere’s no magic salary at which a bad job becomes good. He had received an offer at a start-up, and he would have loved to take it, but it paid half as much, and he felt locked into a lifestyle that made this pay cut impossible”\n\n\n\n\nimage.png\n\n\nTake David, a stressed engineering manager. Although on paper he holds a managerial role, he feels powerless, with no real authority to make decisions. David, who originally hails from New Zealand, misses his home country and its cultural connection, which further strains his emotional well-being. His day-to-day involves constant delivery pressures, tight deadlines, and an unrelenting push to scale projects. For David, the lack of autonomy and cultural disconnection are significant contributors to his mental health challenges. Despite earning a comfortable salary, David experiences burnout and dissatisfaction, showing that financial reward alone doesn’t guarantee happiness.\nBy contrast, Priya, who runs a small social enterprise in rural Malaysia, has managed to structure her business in ways that align with her personal values and mental well-being. Priya’s business supports indigenous weavers and craftsmen, and while her work can be stressful, the novelty of her enterprise, its connection to community engagement, and the autonomy she enjoys significantly bolster her job satisfaction. Priya’s example illustrates how the five dimensions of job satisfaction—autonomy, novelty, cultural alignment, community engagement, and meaningful work—play pivotal roles in mental well-being. Despite financial challenges, Priya feels fulfilled because her work aligns with her personal values and provides her with control and purpose.\nFor professionals, mental health concerns have intensified over the last decade. The National Bureau of Economic Research (NBER) found that 68% of professionals in high-stress fields like finance and healthcare reported elevated stress and anxiety levels in 2022. Factors such as excessive workload, high expectations, and job insecurity contribute to this decline in mental health. David, as an example, mirrors this growing crisis in the professional world, where job satisfaction diminishes due to the relentless demands of modern work environments.\nIn contrast, professionals like Priya, who have woven community engagement and autonomy into their work, tend to report higher job satisfaction and better mental health outcomes. For Priya, who feels deeply connected to her work, her stress is mitigated by the purpose and authority embedded in her role.\nThe Five Dimensions of Job Satisfaction Research indicates that job satisfaction is most influenced by five dimensions:\n\nAutonomy: The freedom to make decisions and control one’s work. Priya, who runs her own business, enjoys this freedom, while David, despite his managerial role, does not.\nNovelty: Engaging in unique and meaningful work that challenges and stimulates. David’s routine job lacks novelty, while Priya’s socially driven enterprise is constantly evolving.\nCultural Alignment: Feeling connected to one’s values or heritage. David’s detachment from his New Zealand roots impacts his satisfaction, while Priya’s work is intertwined with the culture of rural Malaysia, fostering a sense of belonging.\nCommunity Engagement: Being involved in community-oriented work boosts well-being, as seen with Priya, whose business is centered around helping indigenous communities.\nMeaning and Lower Stress: Finding meaning in one’s work can mitigate stress. Priya’s sense of purpose helps her cope with the stresses of running a business, while David’s lack of meaning leads to burnout.\n\nDoes More Pay Lead to More Happiness? Once basic financial needs are met, additional salary and benefits have diminishing returns on job satisfaction. Studies, including those from NBER, show that salary increases above a certain threshold (around $75,000 annually in the U.S.) no longer significantly affect happiness. Despite receiving a generous salary, David’s discontent stems from the lack of autonomy and personal fulfillment, illustrating that financial compensation alone does not ensure job satisfaction.\n\n\nMental Health of Students: Escalating Concerns\nMental health issues among students have become an alarming trend. Data from the American College Health Association (ACHA) shows that the percentage of students reporting mental health challenges increased from 25% in the early 2000s to nearly 46% by 2020. Anxiety, depression, and other mental health disorders have grown, driven by academic pressures, societal expectations, and increasingly uncertain futures. Over 60% of college students reported experiencing significant anxiety and depression during the COVID-19 pandemic.\nUnlike earlier generations, where students balanced academic stress with social interactions, today’s students face a perfect storm of academic pressure, social media comparisons, and global uncertainties. Financial difficulties also weigh heavily, as tuition fees and student loans add to their stress levels. The frequency at which college students exhibit serious mental health conditions has reached an alarming level. Data from the Healthy Minds Study, an annual survey of US college students, show that the portion of students with a lifetime diagnosis of a mental health condition increased from 22% in 2007 to 36% by 2017.4 According to the Center for Collegiate Mental Health (CCMH) annual surveys, about 60% of students seeking mental health services in 2020 reported prior mental health treatment, compared with 48% in 2012-2013.5 The CCMH 2020 data also indicate that among students who reported that they had registered with a university’s disability office, 42% were for attention-deficit hyperactivity disorder and 32% for a psychological or psychiatric condition.5 Another large-scale survey, the American College Health Association National College Health Assessment II, has revealed an equally sizeable increase in reported mental health concerns among college students over the past 10 years.\nReferences\n\nResponding to the Crisis in College Mental Health: A Call to Action. Patel, Bina Pulkit et al. The Journal of Pediatrics, Volume 257, 113390 https://www.jpeds.com/article/S0022-3476(23)00192-0/fulltext\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Data from the snapshot table\ndata = {\n    \"Year\": [2009, 2014, 2019],\n    \"Felt overwhelming anxiety\": [49.1, 54.0, 65.7],\n    \"Felt so depressed it was difficult to function\": [30.7, 32.6, 45.1],\n    \"Seriously considered suicide\": [6.0, 8.1, 13.3],\n    \"Attempted suicide\": [1.1, 1.3, 2.0],\n    \"Diagnosed with or treated for anxiety\": [10.5, 14.3, 24.3],\n    \"Diagnosed with or treated for depression\": [10.1, 12.0, 20.0]\n}\n\n# Creating a DataFrame\ndf = pd.DataFrame(data)\n\n# Plotting the data\nplt.figure(figsize=(10, 6))\nfor column in df.columns[1:]:\n    plt.plot(df['Year'], df[column], label=column)\n\nplt.title('Trends in Mental Health Among College Students (2009-2019)')\nplt.xlabel('Year')\nplt.ylabel('Percentage (%)')\nplt.legend(loc='upper left', bbox_to_anchor=(1,1))\nplt.grid(True)\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtract Data and Reports on Anxiety and Depression among Students and Early Professionals: Data Sources:\n\n\nNational College Health Assessment (NCHA): The American College Health Association regularly publishes reports on student mental health, including data on anxiety, depression, and other mental health issues.\nWorld Health Organization (WHO): The WHO provides global data on mental health, including anxiety and depression prevalence.\nCenters for Disease Control and Prevention (CDC): The CDC offers data on mental health trends in the U.S., including among young adults.\nNational Institute of Mental Health (NIMH): NIMH provides comprehensive data and reports on the prevalence of anxiety and depression across different age groups, including early professionals.\nPubMed and Google Scholar: Academic studies published on these platforms can offer insights into how anxiety and depression have evolved over time, particularly in students and early professionals.\nReports:\nLook for reports from educational institutions, mental health organizations, and government health departments that discuss trends in mental health issues among students and professionals over the years. Surveys from organizations like the Gallup-Sharecare Well-Being Index or the Mental Health Foundation (UK) might also provide relevant insights.\n\n\nQuantify the Cost of Disease Burden: Economic Burden:\n\n\nDirect Costs: These include medical costs related to the treatment of anxiety and depression, including therapy, medication, and hospitalization.\nIndirect Costs: These encompass lost productivity due to absenteeism, presenteeism (reduced productivity while at work), and the long-term impact of mental health issues on career progression.\nIntangible Costs: These involve the emotional toll on individuals and their families, which can be harder to quantify but is crucial in understanding the full impact of mental health issues.\nSources for Economic Data:\nWHO Global Health Estimates: Provides data on the burden of mental health disorders globally, including the economic impact. Health Economics Studies: Published research papers on the economic burden of mental health disorders often quantify the cost of diseases like anxiety and depression in monetary terms. National Health Expenditure Data: Some countries provide data on national health expenditures, which can include spending on mental health.\nhttps://www.kff.org/mental-health/issue-brief/exploring-the-rise-in-mental-health-care-use-by-demographics-and-insurance-status/\nhttps://www.statnews.com/2017/02/06/mental-health-college-students/\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC10625532/\n\n\nimport matplotlib.pyplot as plt\n\n# Data from the table\nyears = [2019, 2020, 2021, 2022]\nage_18_26 = [18, 22, 22, 26]\nage_27_50 = [19, 20, 23, 25]\nage_51_64 = [20, 21, 21, 23]\nage_65_plus = [19, 19, 19, 20]\n\n# Plotting the data\nplt.figure(figsize=(10, 6))\nplt.plot(years, age_18_26, marker='o', label='Ages 18-26')\nplt.plot(years, age_27_50, marker='o', label='Ages 27-50')\nplt.plot(years, age_51_64, marker='o', label='Ages 51-64')\nplt.plot(years, age_65_plus, marker='o', label='Ages 65+')\n\n# Adding titles and labels\nplt.title('Percentage of Adults Reporting Use of Mental Health Services (2019-2022)')\nplt.xlabel('Year')\nplt.ylabel('Percentage')\nplt.legend(title='Age Groups')\nplt.grid(True)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nNHIS Data\n\ndef categorize_age(age):\n    if 18 &lt;= age &lt;= 26:\n        return '18-26'\n    elif 27 &lt;= age &lt;= 50:\n        return '27-50'\n    elif 51 &lt;= age &lt;= 64:\n        return '51-64'\n    elif age &gt;= 65:\n        return '65+'\n    else:\n        return 'Unknown'\n\n\n# Load your dataset (replace 'nhis_data.csv' with your actual file)\ndf = pd.read_csv('nhis_data/nhis_00001.csv')\n\ndf['Age_Group'] = df['AGE'].apply(categorize_age)\n\ndf = df[df['Age_Group'] != 'Unknown']\n\n# # Define the age group and filter the data\n# age_group = df[(df['AGE'] &gt;= 18) & (df['AGE'] &lt;= 26) & (df['YEAR'] &gt; 2018)]\n\ndf['MENTAL_HEALTH_SERVICE_USE'] = (\n    (df['HEALTHMENT'] == 1) |  # If respondent used mental health services\n    (df['DEPRX'] &gt; 0) |  # If respondent received any prescription for depression\n    (df['DEPFREQ'] &gt; 0)  # Frequency of depressive symptoms, assuming higher means more service use\n).astype(int)\n\ntotal_adults_df = df.groupby(['Age_Group', 'YEAR']).agg({'SAMPWEIGHT': 'sum'}).reset_index()\ntotal_adults_df.rename(columns={'SAMPWEIGHT': 'Total_Adults'}, inplace=True)\n\n# Merge with the original dataframe to add Total_Adults column\ndf = pd.merge(df, total_adults_df, on=['Age_Group', 'YEAR'], how='left')\n\n# Calculate the percentage of adults using mental health services\ndf['MENTAL_HEALTH_SERVICE_USE_PERCENT'] = (df['MENTAL_HEALTH_SERVICE_USE'] / df['Total_Adults']) * 100\n\n# Group by year and age group to calculate the percentage\npercentage_df = df.groupby(['YEAR', 'Age_Group']).agg({\n    'MENTAL_HEALTH_SERVICE_USE_PERCENT': 'mean'\n}).reset_index()\n\n# Calculate the percentage of adults using mental health services\ndf['MENTAL_HEALTH_SERVICE_USE_PERCENT'] = (df['MENTAL_HEALTH_SERVICE_USE'] / df['Total_Adults']) * 100\n\n# Group by year and age group to calculate the percentage\npercentage_df = df.groupby(['YEAR', 'Age_Group']).agg({\n    'MENTAL_HEALTH_SERVICE_USE_PERCENT': 'mean'\n}).reset_index()\n\n# plt.figure(figsize=(10, 6))\n# for age_group in percentage_df['Age_Group'].unique():\n#     subset = percentage_df[percentage_df['Age_Group'] == age_group]\n#     plt.plot(subset['YEAR'], subset['MENTAL_HEALTH_SERVICE_USE_PERCENT'], label=age_group)\n\n# plt.xlabel('Year')\n# plt.ylabel('Percentage of Adults Using Mental Health Services (%)')\n# plt.title('Share of Adults (%) Reporting Use of Mental Health Services by Age Group')\n# plt.legend(title='Age Group')\n# plt.grid(True)\n# plt.show()\n# Pivot the data for a stacked bar plot\npivot_df = percentage_df.pivot(index='YEAR', columns='Age_Group', values='MENTAL_HEALTH_SERVICE_USE_PERCENT')\n\n# Plotting the stacked bar chart\npivot_df.plot(kind='bar', stacked=True, figsize=(10, 6))\n\nplt.xlabel('Year')\nplt.ylabel('Percentage of Adults Using Mental Health Services (%)')\nplt.title('Share of Adults (%) Reporting Use of Mental Health Services by Age Group')\nplt.legend(title='Age Group')\nplt.grid(True)\nplt.show()\n\nDtypeWarning: Columns (4,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv('nhis_data/nhis_00001.csv')\n\n\n\n\n\n\n\n\n\nOvereducation and depressive symptoms: diminishing mental health returns to education (https://sci-hub.se/10.1111/1467-9566.12039) &gt; In general, well-educated people enjoy better mental health than those with less education. As a result, some wonder whether there are limits to the mental health benefits of education. Inspired by the literature on the expansion of tertiary education, this article explores marginal mental health returns to education and studies the mental health status of overeducated people. To enhance the validity of the findings we use two indicators of educational attainment – years of education and ISCED97 categories – and two objective indicators of overeducation (the realised matches method and the job analyst method) in a sample of the working population of 25 European countries (unweighted sample N = 19,089). Depression is measured using an eight-item version of the CES-D scale. We find diminishing mental health returns to education. In addition, overeducated people report more depression symptoms. Both findings hold irrespective of the indicators used. The results must be interpreted in the light of the enduring expansion of education, as our findings show that the discussion of the relevance of the human capital perspective, and the diploma disease view on the relationship between education and modern society, is not obsolete.\n\nISCED 0 = Early childhood education\nISCED 1 = Primary Education\nISCED 2 = Lower Secondary Education\nISCED 3 = Upper Secondary Education\nISCED 4 = Post-secondary non-Tertiary Education\nISCED 5 = Short-cycle tertiary education\nISCED 6 = Bachelors degree or equivalent tertiary education level\nISCED 7 = Masters degree or equivalent tertiary education level\nISCED 8 = Doctoral degree or equivalent tertiary education level\n\n\n\nThe Impact of PhD Studies on Mental Health—A Longitudinal Population Study\nReferences: https://lucris.lub.lu.se/ws/portalfiles/portal/194583123/WP24_5.pdf\n\n\n\nimage.png"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "A 35-Year Quantitative Analysis of Factor Premiums (1990–2025)\n\n\n\nStory – why this matters (intro + conclusion)\nData – what you measure (markets, factors, liquidity proxies)\nModels – how you measure (liquidity index, regimes, cross-sectional tests)\nPortfolio – what to do with it (conditional allocation\n\n\n\n\n\nThe “valuations don’t matter in infinite money printing” meme.\nYour construction of a liquidity index and liquidity regimes.\nKey findings:\nValuations do matter, but conditional on liquidity regimes.\nGrowth dominates in high-liquidity, negative-real-rate regimes.\nValue premia revive after QT / real rate normalization.\n\nPortfolio takeaway: a simple regime-aware factor-tilt model.\n\n\n\n\nNarrative: post-GFC QE, ZIRP, NIRP, 2010–2021 “TINA”, tech bubble 2.0.\n\n“We live in a world of infinite money printing, so valuations don’t matter.”\n\nResearch question: &gt; Do valuations truly die under abundant liquidity, or do they merely become regime-dependent?\nDo valuation spreads (cheap vs expensive deciles) expand under high-liquidity regimes and compress under tight liquidity?\nHow do factor premia (value, momentum, quality, low vol) vary conditional on liquidity regimes?\nCan a macro-liquidity index predict the timing of factor rotations (growth vs value, long-duration vs short-duration equities)?\nDoes a regime-aware factor allocation deliver better risk-adjusted returns than a static factor mix?\n\n\n\n\n\nValuation spread: the gap between cheap and expensive stocks (e.g., top vs bottom decile by B/M).\nLiquidity regime: periods of systematically easy vs tight financial conditions driven by monetary and fiscal variables.\n\n\n\n\n\nSystemic Strategy: Factor investing: value, momentum, quality.\nRisk Factors and Risk premia: Liquidity\nRegime-switching: macro-conditional factors.\n\n\n\n\n\nBuild a macro liquidity index from multiple public series.\nUse HMM / Markov regimes to label high vs low liquidity states.\nShow how valuation spreads and factor returns behave across liquidity regimes.\nPropose a simple implementable regime-aware factor strategy.\n\n\n\n\n\n\n\n\nHorizon: 1990–2025, monthly frequency (or weekly if feasible).\nMarket: US equities (CRSP/Compustat-type universe); optional robustness on AUS/EU.\n\n\n\n\n\n\nIndividual stock returns \\(r_{i,t}\\), market cap, book equity, earnings, etc.\nCharacteristics \\(X_{i,t}\\):\n\n\\[\n\\text{B/M},\\ \\text{E/P},\\ \\text{P/S},\\ \\text{size},\\ \\beta,\\ \\text{12–1 momentum},\\ \\text{profitability},\\ \\text{volatility}.\n\\]\n\nFactor returns \\(f_t\\):\n\n\\[\n\\text{MKT},\\ \\text{HML},\\ \\text{SMB},\\ \\text{MOM},\\ \\text{Quality},\\ \\text{LowVol}.\n\\]\nDefine a valuation spread:\n\\[\nV_t^{\\text{spread}} = \\text{Long top decile of B/M} - \\text{Short bottom decile}.\n\\]\n\n\n\n\nLet raw macro/liquidity variables at time \\(t\\) be \\(x_{j,t}\\), including:\n\n\n\n\nMoney growth: \\[\\Delta \\log(M2_t)\\]\nFed balance sheet growth: \\[\\Delta \\log(\\text{BS}_t)\\]\n\n\n\n\n\n\nShort rate: \\[i_t\\]\n10Y yield: \\[y_{10,t}\\]\nSlope: \\[\\text{TS}_t = y_{10,t} - i_t\\]\nReal short rate: \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\]\n\n\n\n\n\n\nCredit spread: \\[\\text{CS}_t = y_{\\text{Baa},t} - y_{\\text{Aaa},t}\\]\nVIX level: \\[\\text{VIX}_t\\]\n\n\n\n\n\n\nON RRP usage\nTGA balance\n\n\nAll variables \\(x_{j,t}\\) will then be standardized (e.g., z-scored) and aggregated into a single liquidity index.\n\n\n\n\n\n\n\nLet the raw liquidity indicators be collected in the vector\n\\[\nx_t =\n\\begin{bmatrix}\nx_{1,t} \\\\\nx_{2,t} \\\\\n\\vdots \\\\\nx_{J,t}\n\\end{bmatrix}\n\\in \\mathbb{R}^J.\n\\]\nFor each series \\[x_{j,t}\\], compute the sample mean \\[\n\\mu_j = \\frac{1}{T} \\sum_{t=1}^{T} x_{j,t}\n\\] and variance \\[\n\\sigma_j^2 = \\frac{1}{T-1} \\sum_{t=1}^{T} (x_{j,t} - \\mu_j)^2.\n\\]\nStandardised variables are then \\[\nz_{j,t} = \\frac{x_{j,t} - \\mu_j}{\\sigma_j}, \\qquad j = 1,\\dots,J,\n\\] with stacked vector \\[z_t = (z_{1,t},\\dots,z_{J,t})^\\top\\].\nSeries may be sign-flipped so that higher values of \\[z_{j,t}\\] correspond to easier liquidity; for example, use \\[-r_t^{\\text{real}}\\] instead of \\[r_t^{\\text{real}}\\], \\[-CS_t\\] instead of \\[CS_t\\], and \\[-VIX_t\\] instead of \\[VIX_t\\].\n\n\n\n\nTypical inputs include \\[\\Delta \\log M2_t\\], \\[\\Delta \\log \\text{BS}_t\\], the term spread \\[\\text{TS}_t = y_{10,t} - i_t\\], the real rate \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\], and the credit spread \\[\\text{CS}_t = y_{Baa,t} - y_{Aaa,t}\\].\n\n\n\n\nDefine the covariance matrix \\[\n\\Sigma = \\frac{1}{T} \\sum_{t=1}^{T} z_t z_t^\\top.\n\\]\nLet \\[(\\lambda_k, v_k)\\] solve \\[\\Sigma v_k = \\lambda_k v_k\\], with eigenvalues ordered \\[\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_J\\]. The first principal-component liquidity index is then \\[L_t = v_1^\\top z_t\\]. Alternatively, one may use a fixed-weight index \\[L_t = w^\\top z_t\\].\n\n\n\n\nLet the latent regime variable satisfy \\[s_t \\in \\{1,\\dots,K\\}\\], with regime-conditional dynamics \\[\nL_t \\mid (s_t = k) \\sim \\mathcal{N}(\\mu_k, \\sigma_k^2).\n\\]\nTransition probabilities are \\[\\Pr(s_t = j \\mid s_{t-1} = i) = p_{ij}\\], forming the matrix \\[\nP =\n\\begin{bmatrix}\np_{11} & \\cdots & p_{1K} \\\\\n\\vdots & \\ddots & \\vdots \\\\\np_{K1} & \\cdots & p_{KK}\n\\end{bmatrix},\n\\qquad \\sum_{j=1}^{K} p_{ij} = 1.\n\\]\nFiltered or smoothed regime classification is given by \\[\n\\hat{s}_t = \\arg\\max_{k} \\Pr(s_t = k \\mid L_{1:T}).\n\\]\nDefine the high- and tight-liquidity regimes as \\[k_{\\text{High}} = \\arg\\max_k \\mu_k\\] and \\[k_{\\text{Tight}} = \\arg\\min_k \\mu_k\\], with indicator \\[\nI_t^{\\text{High}} = \\mathbf{1}\\!\\left[\\Pr(s_t = k_{\\text{High}} \\mid L_{1:T}) &gt; 0.5\\right].\n\\]\n\n\n\n\nIn a multivariate setting, define \\[\ny_t =\n\\begin{bmatrix}\nL_t \\\\\nr_t^{\\text{MKT}} \\\\\nr_t^{\\text{HML}} \\\\\n\\text{VIX}_t \\\\\n\\vdots\n\\end{bmatrix},\n\\] with regime-dependent dynamics \\[y_t = A_k y_{t-1} + \\varepsilon_t^{(k)}\\] and \\[\\varepsilon_t^{(k)} \\sim \\mathcal{N}(0,\\Sigma_k)\\] when \\[s_t = k\\].\n\n\n\n\nLet \\[V_t^{\\text{spread}}\\] denote a valuation-spread series (e.g., top–bottom decile). The regime-conditional mean is \\[\n\\bar{V}^{(k)} = \\mathbb{E}[V_t^{\\text{spread}} \\mid s_t = k],\n\\] with sample estimate \\[\n\\hat{\\bar{V}}^{(k)} =\n\\frac{\\sum_{t=1}^{T} V_t^{\\text{spread}} \\mathbf{1}(\\hat{s}_t = k)}\n{\\sum_{t=1}^{T} \\mathbf{1}(\\hat{s}_t = k)}.\n\\]\nDifferences such as \\[\\hat{\\bar{V}}^{(\\text{High})} - \\hat{\\bar{V}}^{(\\text{Tight})}\\] summarise regime effects. Analogously, for factor returns \\[r_t^{(F)}\\], \\[\n\\bar{r}_F^{(k)} = \\mathbb{E}[r_t^{(F)} \\mid s_t = k],\n\\] with Sharpe ratio \\[\\text{SR}_F^{(k)} = \\hat{\\bar{r}}_F^{(k)} / \\hat{\\sigma}_F^{(k)}\\].\n\n\n\n\nContinuous-index predictability is tested via \\[\nr_{t+1}^{(F)} = \\alpha + \\beta L_t + \\gamma^\\top c_t + \\varepsilon_{t+1},\n\\] while regime-based predictability uses \\[\nr_{t+1}^{(F)} =\n\\alpha\n+ \\delta_{\\text{High}} I_t^{\\text{High}}\n+ \\delta_{\\text{Tight}} I_t^{\\text{Tight}}\n+ \\delta_{\\text{Neutral}} I_t^{\\text{Neutral}}\n+ \\varepsilon_{t+1}.\n\\]\n\n\n\n\nAt each time \\[t\\], estimate \\[\nr_{i,t+1} =\n\\alpha_t\n+ \\lambda_{1,t}\\text{Valuation}_{i,t}\n+ \\lambda_{2,t}\\text{Size}_{i,t}\n+ \\lambda_{3,t}\\text{Momentum}_{i,t}\n+ \\dots\n+ \\varepsilon_{i,t+1}.\n\\]\nRegime-conditional slopes satisfy \\[\\bar{\\lambda}_1^{(k)} = \\mathbb{E}[\\lambda_{1,t} \\mid s_t = k]\\], with sample analogue \\[\n\\hat{\\bar{\\lambda}}_1^{(k)} =\n\\frac{\\sum_{t=1}^{T} \\lambda_{1,t} \\mathbf{1}(\\hat{s}_t = k)}\n{\\sum_{t=1}^{T} \\mathbf{1}(\\hat{s}_t = k)}.\n\\]\n\n\n\n\nLet \\[f_t \\in \\mathbb{R}^K\\] denote factor returns and \\[w^{(k)} \\in \\mathbb{R}^K\\] the regime-specific weights. The applied weights are \\[w_t = w^{(\\hat{s}_t)}\\], yielding portfolio return \\[\nR_{p,t+1} = w_t^\\top f_{t+1}.\n\\]\n\n\n\n\nStandard PCA solves:\n\\[\n\\max_{v} \\; \\| X v \\|^{2}\n\\quad \\text{s.t.} \\quad \\| v \\|^{2} = 1\n\\]\n\n\n\nPCA tries to find the direction ( \\(v\\) ) (a unit vector) along which the projected data ( \\(Xv\\) ) has maximum variance.\nVariance of the projection:\n\\[\n\\text{Var}(Xv) = \\frac{1}{n} |Xv|^{2}.\n\\]\nSo maximizing variance is equivalent to maximizing ( \\(|Xv|^{2}\\) ).\n\n\n\n\nWithout this constraint, the problem would blow up:\n\\[\n|X(\\alpha v)|^{2} = \\alpha^{2} |Xv|^{2},\n\\]\nand the maximum would be infinite by choosing ( \\(\\alpha\\) \\(\\infty\\) ).\nSo PCA forces ( \\(v\\) ) to be a direction, not a magnitude → a unit vector.\n\n\n\n\n\\[\n|Xv|^{2} = v^\\top X^\\top X v.\n\\]\nLet:\n\\[\nS = X^\\top X\n\\]\n(the unnormalized covariance matrix). Then PCA solves:\n\\[\n\\max_{v} ; v^\\top S v\n\\quad\\text{s.t.}\\quad v^\\top v = 1.\n\\]\nThis is exactly the Rayleigh quotient.\n\n\n\n\nPCA finds the direction (unit vector) along which the data cloud has maximum spread. That direction is exactly the eigenvector of the covariance matrix with the largest eigenvalue.\n\n\n\n\n\n\n\n\n\nPCA rotates the data into orthogonal directions capturing maximum variance.\nEach principal component is a linear combination of all variables.\nLoadings are typically dense (every variable has some non-zero weight).\nThis makes interpretation difficult:\n\n\n“PC1 of 127 economic multicolinear variables is 127-dimensional mush of everything.”\n\nThis is why PCA is almost never used directly for economic interpretation — PCs are not sparse.\n\n\n\n\nSparse PCA = PCA where most loadings are forced to be zero.\nMathematically:\nStandard PCA solves:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1\n\\]\nSparse PCA adds an L1 penalty (lasso-style sparsity) or constrains the number of non-zero elements:\n\\[\n\\max_{v} \\ |Xv|^2 - \\lambda |v|_1\n\\]\nor equivalently:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1,\\ |v|_1 \\leq c\n\\]\nThis forces:\n\nOnly a small subset of variables to load on each component.\nComponents become interpretable (e.g., PC1 loads on CPI, PCE, core CPI → “inflation”).\n\nThe specific implementation cited (“penalized matrix decomposition” by Witten et al. 2009) solves:\n\\[\n\\max_{u, v} \\ u^\\top X v \\quad\n\\text{s.t. } |u|_2 = 1,\\ |v|_2 = 1,\\ |v|_1 \\le c\n\\]\nThis is a general sparse factor extraction algorithm.\nIntuition: &gt; Sparse PCA forces components to use only the relevant variables, not a smear across 127 series.\n\n\n\n\n\nStandard PCA components ≈ dense, uninterpretable mixtures\nSparse PCA components ≈ targeted clusters of interpretable economic variables\n\n\n\n\n\nTake FRED-MD’s 127 macro series.\nIf you run standard PCA:\n\nPC1 loads on 100+ variables.\nPC2 loads on another 80+.\nInterpreting them is basically hopeless.\n\nSparse PCA, with lasso constraints:\n\nallows PC1 to load only on inflation-related variables\nPC2 to load only on housing-related variables\nPC3 on credit spreads\nPC4 on yields\nPC5 on production\netc.\n\nThis resembles the Stock–Watson macro factor model, but with sparsity for interpretability.\n\n\n\n\nSuppose sparse PCA gives:\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCPI YoY\n0.71\n\n\nCore CPI YoY\n0.68\n\n\nPCE Deflator\n0.66\n\n\nPCE core\n0.64\n\n\nAll other 123 variables\n0\n\n\n\n-&gt; You immediately label PC1 as “inflation factor”.\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nHousing starts\n0.62\n\n\nBuilding permits\n0.59\n\n\nNew homes sold\n0.61\n\n\nMortgage applications\n0.58\n\n\nEverything else\n0\n\n\n\n-&gt; PC2 = “housing & construction cycle”\n\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCorporate spreads (BAA–AAA)\n0.72\n\n\nHigh yield spread\n0.69\n\n\nCommercial paper spread\n0.64\n\n\nOthers\n0\n\n\n\n-&gt; PC3 = “credit stress”\n…and so on.\n\n\n\nSparse PCA essentially performs dimension reduction + variable selection simultaneously.\n\n\n\n\nSparse PCA encourages:\n\ngrouping variables that co-move strongly\ndropping variables unrelated to the theme\nchoosing a small number of representative series\n\nGiven economic data naturally clusters (inflation variables co-move, housing variables co-move), sparse PCA isolates these clusters.\nThis is similar to economic intuition:\n\ninflationary variables form a single latent factor\nspreads form a financial stress factor\nyields form a term-structure factor\nemployment variables form a labor factor\n\nSparse PCA “discovers” these clusters automatically.\n\n\n\n\nSparse PCA → interpretable macro drivers → better regime classification → better factor conditioning.\nEspecially when constructing a macro liquidity index or financial conditions index.\nSparse PCA gives:\n\n\n\nComponent\nInterpretation\n\n\n\n\nPC1\nLiquidity / money conditions\n\n\nPC2\nCredit spreads\n\n\nPC3\nHousing activity\n\n\nPC4\nYield curve / rates\n\n\nPC5\nEmployment\n\n\nPC6\nProduction\n\n\nPC7\nIncome\n\n\nPC8\nMarket stress\n\n\n\nThese are exactly the components McCracken & Ng (2016) find in FRED-MD.\n\n\n\n\n\n\n\n\nDense loadings\nHard to interpret\nPCs are linear mush\n\n\n\n\n\nForces many loadings to zero\nEach PC focuses on a small cluster of variables\nBecomes interpretable as a “macro theme”\nMatches how economists think about the business cycle\nPerfect for regime classification & factor research\n\n\n\n\n\nEconomic data naturally clusters into themes\nSparse PCA forces components to choose only the strongest cluster\nThis matches known macro factors (inflation, employment, credit, etc.)\n\n\n# !pip install pandas pandas_datareader numpy scikit-learn hmmlearn matplotlib\n\nimport pandas as pd\nimport numpy as np\nfrom pandas_datareader import data as pdr\n\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom hmmlearn.hmm import GaussianHMM\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\nstart_date = \"1990-01-01\"\nend_date   = \"2025-12-31\"\n\n# --- FRED series codes ---\nFRED_SERIES = {\n    \"M2SL\":   \"M2SL\",      # M2 money stock (monthly, SA)\n    \"FED_BAL\":\"WALCL\",     # Fed balance sheet total assets (weekly)\n    \"TB3M\":   \"TB3MS\",     # 3-Month T-Bill rate (monthly)\n    \"DGS10\":  \"DGS10\",     # 10-Year Treasury yield (daily -&gt; resample monthly)\n    \"BAA\":    \"BAA\",       # Moody's Baa corporate yield (monthly)\n    \"AAA\":    \"AAA\",       # Moody's Aaa corporate yield (monthly)\n    \"CPI\":    \"CPIAUCSL\",  # CPI index (monthly, SA)\n    \"GDP\":    \"GDP\",       # Nominal GDP (quarterly, SAAR)\n}\n\n# For equity factors: Fama-French via pandas_datareader (famafrench)\nFF_FACTORS_DATASET = \"F-F_Research_Data_5_Factors_2x3\"\n\n\n\n\n\ndef download_fred_series(series_dict, start, end):\n    \"\"\"\n    Download FRED series into a single monthly DataFrame.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Mapping logical_name -&gt; FRED code.\n    \"\"\"\n    dfs = []\n    for name, code in series_dict.items():\n        print(f\"Downloading {name} ({code}) from FRED...\")\n        s = pdr.DataReader(code, \"fred\", start, end)\n        s = s.rename(columns={code: name})\n        dfs.append(s)\n\n    df = pd.concat(dfs, axis=1)\n    # Ensure monthly freq by end-of-month sampling\n    df = df.resample(\"M\").last()\n    return df\n\nmacro_raw = download_fred_series(FRED_SERIES, start_date, end_date)\nmacro_raw.head()\n\nDownloading M2SL (M2SL) from FRED...\nDownloading FED_BAL (WALCL) from FRED...\nDownloading TB3M (TB3MS) from FRED...\nDownloading DGS10 (DGS10) from FRED...\nDownloading BAA (BAA) from FRED...\nDownloading AAA (AAA) from FRED...\nDownloading CPI (CPIAUCSL) from FRED...\nDownloading GDP (GDP) from FRED...\n\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n1990-01-31\n3166.8\nNaN\n7.64\n8.43\n9.94\n8.99\n127.5\n5872.701\n\n\n1990-02-28\n3179.2\nNaN\n7.74\n8.51\n10.14\n9.22\n128.0\nNaN\n\n\n1990-03-31\n3190.1\nNaN\n7.90\n8.65\n10.21\n9.37\n128.6\nNaN\n\n\n1990-04-30\n3201.6\nNaN\n7.77\n9.04\n10.30\n9.46\n128.9\n5960.028\n\n\n1990-05-31\n3200.6\nNaN\n7.74\n8.60\n10.41\n9.47\n129.1\nNaN\n\n\n\n\n\n\n\n\nmacro_raw.tail()\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\nNaN\n4.00\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n$ x_t =\n\\[\\begin{bmatrix}\nx_{1,t} \\\nx_{2,t} \\\n\\vdots \\\nx_{J,t}\n\\end{bmatrix}\\]\n\nx_{J,t} \\end{bmatrix} ^J $\n\nMoney & balance sheet growth: \\(\\Delta \\log(\\cdot)\\)\nTerm spread: \\(y_{10} - i_{3m}\\)\nReal short rate: \\(i_{3m} - \\pi_{\\text{year-on-year}}\\)\nCredit spread: \\(\\text{BAA} - \\text{AAA}\\)\netc.​\n\n\ndef build_liquidity_proxies(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Growth of M2 and Fed balance sheet\n    df[\"dlog_M2\"] = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"] = np.log(df[\"FED_BAL\"]).diff()\n\n    # 2. Term structure: 10Y - 3M\n    df[\"term_spread\"] = df[\"DGS10\"] - df[\"TB3M\"]\n\n    # 3. Year-on-year inflation (CPI yoy)\n    df[\"infl_yoy\"] = np.log(df[\"CPI\"]).diff(12)\n\n    # 4. Real short rate: nominal 3M - inflation\n    df[\"real_rate\"] = df[\"TB3M\"] - (100 * df[\"infl_yoy\"])  # TB3M in %, infl_yoy in log -&gt; approx*100\n\n    # 5. Credit spread: BAA - AAA\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # Drop rows with NaNs from diff(12) etc.\n    proxies = df[[\"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\",\n                  \"real_rate\", \"credit_spread\"]].dropna()\n\n    return proxies\n\nliquidity_proxies = build_liquidity_proxies(macro_raw)\n\n\nliquidity_proxies.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n\n\n2025-07-31\n0.003930\n-0.002950\n0.12\n1.554846\n0.65\n\n\n2025-08-31\n0.003607\n-0.005918\n0.11\n1.223147\n0.65\n\n\n2025-09-30\n0.004698\n0.000759\n0.24\n0.942084\n0.62\n\n\n\n\n\n\n\n\n\n\nWe want higher \\(z_{j,t}\\) – to mean easier liquidity.\n\nGrowth of M2 / Fed balance sheet: already “easier = higher value” -&gt; keeping sign\nTerm spread: steeper (more positive) often associated with easier conditions -&gt; keeping sign\nReal rate: easier liquidity when real rates are low -&gt; flipping sign\nCredit spread: easier liquidity when spreads are tight (low) -&gt; flipping sign\n\n\ndef standardize_and_signflip(proxies: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Standardize each column and flip signs so that higher z_{j,t}\n    always corresponds to easier liquidity.\n    \"\"\"\n    z = proxies.copy()\n\n    # Standardize\n    scaler = StandardScaler()\n    z_vals = scaler.fit_transform(z)\n    z = pd.DataFrame(z_vals, index=z.index, columns=z.columns)\n\n    # Sign flips so \"higher\" = easier liquidity\n    # dlog_M2: easier when higher -&gt; keep\n    # dlog_FED_BAL: easier when higher -&gt; keep\n    # term_spread: easier when steeper -&gt; keep (or adjust, depending on your view)\n    # real_rate: easier when more negative -&gt; flip sign\n    # credit_spread: easier when lower -&gt; flip sign\n\n    sign_flips = {\n        \"dlog_M2\": +1,\n        \"dlog_FED_BAL\": +1,\n        \"term_spread\": +1,\n        \"real_rate\": -1,\n        \"credit_spread\": -1,\n    }\n\n    for col, sgn in sign_flips.items():\n        z[col] = sgn * z[col]\n\n    return z\n\nz_t = standardize_and_signflip(liquidity_proxies)\n\n\nz_t.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n-0.377813\n-0.314009\n-0.968292\n-1.362581\n0.639849\n\n\n2025-06-30\n0.052579\n-0.226920\n-1.085244\n-1.211510\n0.786085\n\n\n2025-07-31\n-0.163048\n-0.257126\n-0.999479\n-1.193172\n0.883576\n\n\n2025-08-31\n-0.215601\n-0.326453\n-1.007276\n-1.031235\n0.883576\n\n\n2025-09-30\n-0.038105\n-0.170518\n-0.905917\n-0.894019\n0.956694\n\n\n\n\n\n\n\n\n\n\n\npd.Series(z_t.index).describe()\n\ncount                              273\nmean     2014-05-31 08:42:11.868131840\nmin                2003-01-31 00:00:00\n25%                2008-09-30 00:00:00\n50%                2014-05-31 00:00:00\n75%                2020-01-31 00:00:00\nmax                2025-09-30 00:00:00\nName: DATE, dtype: object\n\n\n\nz_t.values\n\narray([[ 0.11561096, -0.81061174,  1.11346119,  0.322683  , -0.40817503],\n       [ 0.20510346,  0.11034372,  0.8873532 ,  0.50696464, -0.23756644],\n       [-0.24145055, -0.0901311 ,  1.01210244,  0.46925598, -0.11570316],\n       ...,\n       [-0.163048  , -0.25712611, -0.999479  , -1.1931718 ,  0.88357574],\n       [-0.2156006 , -0.32645261, -1.00727583, -1.03123533,  0.88357574],\n       [-0.03810504, -0.17051847, -0.90591708, -0.89401934,  0.95669371]])\n\n\n\ndef build_sparse_pca_liquidity_index(z_df: pd.DataFrame,\n                                     alpha: float = 1.0,\n                                     random_state: int = 42) -&gt; pd.Series:\n    \"\"\"\n    Apply SparsePCA with 1 component to z_{j,t} to obtain L_t.\n    \"\"\"\n    spca = SparsePCA(\n        n_components=1,\n        alpha=alpha,\n        random_state=random_state\n    )\n\n    # Fit on standardized proxies\n    L_scores = spca.fit_transform(z_df.values)  # shape (T, 1)\n    L = pd.Series(L_scores.flatten(), index=z_df.index, name=\"L\")\n\n    # Optional: enforce that L_t is positively correlated with dlog_M2\n    # corr = np.corrcoef(L, z_df[\"dlog_FED_BAL\"])[0, 1]\n    # if corr &lt; 0:\n    #     L = -L\n\n    print(\"SparsePCA components (loadings):\")\n    for coef, col in zip(spca.components_[0], z_df.columns):\n        print(f\"  {col}: {coef:.3f}\")\n\n    return L\n\nL_t = build_sparse_pca_liquidity_index(z_t, alpha=0.5)\n\nSparsePCA components (loadings):\n  dlog_M2: 0.430\n  dlog_FED_BAL: 0.526\n  term_spread: 0.492\n  real_rate: 0.389\n  credit_spread: -0.381\n\n\n\nL_t.head()\n\nDATE\n2003-01-31    0.447153\n2003-02-28    0.861556\n2003-03-31    0.567234\n2003-04-30    0.979006\n2003-05-31    0.696500\nFreq: M, Name: L, dtype: float64\n\n\n\nL_t.plot(title=\"Sparse PCA Liquidity Index L(t)\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n​Fit a Gaussian HMM on the liquidity index. 2 or 3 regimes ?\n\nfrom hmmlearn.hmm import GaussianHMM\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\ndef fit_hmm_on_liquidity(\n    L: pd.Series,\n    n_states: int = 3,\n    covariance_type: str = \"full\",\n    random_state: int = 42,\n    standardize: bool = True\n):\n    \"\"\"\n    Fit a Gaussian HMM on L(t) and return the model and a DataFrame\n    with inferred regimes and posterior probabilities.\n\n    Parameters\n    ----------\n    L : pd.Series\n        Liquidity index (indexed by date).\n    n_states : int\n        Number of hidden states (regimes).\n    standardize : bool\n        If True, standardize L before fitting the HMM.\n\n    Returns\n    -------\n    model : GaussianHMM\n    df_regime : pd.DataFrame\n        Columns: L, state, p_state_k, state_label\n    \"\"\"\n\n    # 1) Prepare X\n    X = L.values.reshape(-1, 1)\n\n    scaler = None\n    if standardize:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n    # 2) Fit HMM\n    model = GaussianHMM(\n        n_components=n_states,\n        covariance_type=covariance_type,\n        random_state=random_state,\n        n_iter=500\n    )\n    model.fit(X)\n\n    hidden_states = model.predict(X)\n    post_probs = model.predict_proba(X)  # T x n_states\n\n    # 3) Build output DataFrame on original index\n    df_regime = pd.DataFrame(\n        {\"L\": L, \"state\": hidden_states},\n        index=L.index\n    )\n    for k in range(n_states):\n        df_regime[f\"p_state_{k}\"] = post_probs[:, k]\n\n    # 4) Use state means to order regimes\n    means = pd.Series(model.means_.flatten(), index=range(n_states))\n\n    print(\"HMM state means (unsorted, in fitted scale):\")\n    for k, m in means.items():\n        print(f\"  state {k}: mean L = {m:.3f}\")\n\n    # Order states from low liquidity to high liquidity\n    ordering = means.sort_values().index.tolist()\n\n    state_label_map = {}\n    if n_states == 3:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"Neutral\"\n        state_label_map[ordering[2]] = \"High\"\n    elif n_states == 2:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"High\"\n    else:\n        # Generic labels if you ever use more states\n        for i, s in enumerate(ordering):\n            state_label_map[s] = f\"Regime_{i}\"\n\n    df_regime[\"state_label\"] = df_regime[\"state\"].map(state_label_map)\n\n    return model, df_regime\n\n\nn_states = 3\nmodel, regimes_df = fit_hmm_on_liquidity(L_t,n_states)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.160\n  state 1: mean L = -0.106\n  state 2: mean L = 2.718\n\n\n\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# -----------------------------\n# TOP PANEL — L(t) + shading\n# -----------------------------\nax[0].plot(regimes_df.index, regimes_df[\"L\"], color=\"black\", lw=1.2)\nax[0].set_title(\"Sparse PCA Liquidity Index L(t) with Regime Shading\")\n\n# Define plain colors for each regime\nregime_colors = {\n    \"Tight\":   \"green\",\n    \"Neutral\": \"gray\",\n    \"High\":    \"crimson\",\n}\n\nshade_alpha = 0.2  # semi-opaque\n\nfor label, color in regime_colors.items():\n    mask = regimes_df[\"state_label\"] == label\n\n    in_region = False\n    start = None\n\n    for i in range(len(mask)):\n        if mask.iloc[i] and not in_region:\n            in_region = True\n            start = regimes_df.index[i]\n\n        elif not mask.iloc[i] and in_region:\n            in_region = False\n            end = regimes_df.index[i]\n            ax[0].axvspan(start, end, color=color, alpha=shade_alpha, linewidth=0)\n\n    # If region extends to the end of the sample\n    if in_region:\n        ax[0].axvspan(start, regimes_df.index[-1], color=color, alpha=shade_alpha, linewidth=0)\n\n# Legend for regimes (shaded colors)\nfrom matplotlib.lines import Line2D\nlegend_patches = [\n    Line2D([0], [0], color=\"green\",   lw=6, alpha=shade_alpha, label=\"Tight\"),\n    Line2D([0], [0], color=\"gray\",    lw=6, alpha=shade_alpha, label=\"Neutral\"),\n    Line2D([0], [0], color=\"crimson\", lw=6, alpha=shade_alpha, label=\"High\"),\n]\nax[0].legend(handles=legend_patches, loc=\"upper left\")\n\n# -----------------------------\n# BOTTOM PANEL — High regime probability\n# -----------------------------\nif \"p_state_0\" in regimes_df.columns:\n    high_state_id = regimes_df.groupby(\"state_label\")[\"state\"].first()[\"High\"]\n    ax[1].plot(regimes_df.index,\n               regimes_df[f\"p_state_{high_state_id}\"],\n               color=\"crimson\", lw=1.5)\n    ax[1].set_title(\"Posterior Probability of High Liquidity Regime\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\nThese five factors extend the original 3-factor model to better explain cross-sectional stock returns.\nThey are:\n\nRm–Rf — Market excess return\nSMB — Size (Small Minus Big)\nHML — Value (High Book/Market minus Low)\nRMW — Profitability (Robust Minus Weak)\nCMA — Investment (Conservative Minus Aggressive)\n\nLet’s break each down precisely.\n\n\n\n\\[\nR_{m}-R_{f}\n\\] The return of the broad market minus the risk-free rate.\n\nPositive → market went up more than cash.\nNegative → market underperformed cash/T-bills.\n\nIn above table:\n\nLast 12 months: +17.54% → very strong bull market year.\nLast 3 months: +7.40% → strong quarter.\nOctober 2025: +1.95% → moderate positive month.\n\n\n\n\n\n\\[\n\\text{SMB} = R_{\\text{small}} - R_{\\text{big}}\n\\]\n\nPositive → small caps outperform large caps.\nNegative → large caps outperform small caps.\n\nIn above table:\n\nLast 12 months: –10.71% → a massive large-cap dominance year.\nThis is consistent with mega-cap tech leadership.\n\n\n\n\n\n\\[\n\\text{HML} = R_{\\text{value}} - R_{\\text{growth}}\n\\]\n\nPositive → value outperforms growth.\nNegative → growth outperforms value.\n\nIn above table:\n\nLast 12 months: –2.36% → growth beat value.\nOctober 2025: –3.10% → especially growth-heavy month.\n\nThis aligns with liquidity-driven growth leadership.\n\n\n\n\n\\[\n\\text{RMW} = R_{\\text{robust}} - R_{\\text{weak}}\n\\]\nRobust = high operating profitability. Weak = low profitability.\n\nPositive → profitable companies outperform weak ones.\nNegative → weak/low-profit companies outperform.\n\nIn above table:\n\nLast 12 months: –13.60% → very unusual.\nThis means unprofitable firms outperformed profitable ones over the year.\n\nThat usually happens in:\n\nearly speculative bubbles\nliquidity-driven rallies\nretail-led tech/small-cap frenzies\nAI/futuristic narrative periods\n\n\n\n\n\n\\[\n\\text{CMA} = R_{\\text{conservative}} - R_{\\text{aggressive}}\n\\]\n\nConservative = firms investing slowly\nAggressive = firms aggressively expanding assets\n\nHigh investment → lower expected returns historically (consistent with q-theory: empire-building destroys value).\n\nPositive → conservative firms outperform heavy spenders.\nNegative → aggressive investment firms outperform.\n\nIn above table:\n\nLast 12 months: –10.46%\nLast 3 months: –4.47%\nOctober 2025: –4.03%\n\nInterpretation: Aggressively investing companies — think AI, R&D, biotech, high-growth tech — massively outperformed low-investment firms.\n\n\n\n\n\n\n\n\nFactor\nSign\nInterpretation\n\n\n\n\nRm–Rf: +\nStrong bull market\n\n\n\nSMB: –\nMega-caps beat small caps massively\n\n\n\nHML: –\nGrowth beat value\n\n\n\nRMW: –\nLow-profit firms beat profitable firms\n\n\n\nCMA: –\nAggressive-investment firms beat conservative ones\n\n\n\n\nThis is the textbook signature of a liquidity-driven growth/tech momentum regime, almost identical to:\n\n1998–1999 Dotcom\n2019–2021 QE wave\n2023–2024 AI mega-cap boom\n\nIt means:\n\n\n\n\nlarge\ngrowth\nunprofitable\nhigh-spending\nhigh-duration tech/AI/future-theme stocks\n\n\n\n\nThis is exactly the kind of environment where:\n\nValue fails\nSmall cap fails\nProfitability fails\nInvestment works negatively\nGrowth dominates\nMega-caps dominate\n\n\ndef download_ff_factors(start, end):\n    \"\"\"\n    Download monthly Fama-French 5 factors (2x3) using pandas_datareader.\n    \"\"\"\n    print(\"Downloading Fama-French factors (famafrench)...\")\n    ff_raw = pdr.DataReader(\n        name=FF_FACTORS_DATASET,\n        data_source=\"famafrench\",\n        start=start,\n        end=end\n    )[0]  # table [0] contains the data\n\n    ff = (ff_raw\n          .divide(100)  # convert from % to decimal\n          .reset_index(names=\"date\")\n          .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n          .rename(str.lower, axis=\"columns\")\n          .rename(columns={\"mkt-rf\": \"mkt_excess\"}))\n\n    ff = ff.set_index(\"date\")\n    return ff\n\nff_factors = download_ff_factors(start_date, end_date)\n\nDownloading Fama-French factors (famafrench)...\n\n\nFutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n&lt;ipython-input-69-55f7e03990a5&gt;:6: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n\n\n\nff_factors.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-06-01\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-07-01\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-01\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-09-01\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-10-01\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Fix Fama–French dates (shift one month backward)\nff_adj = ff_factors.copy()\n\n# FF data comes labeled as YYYY-MM-01 meaning return for previous month.\n# So shift index back to previous month-end.\nff_adj.index = (ff_adj.index.to_period(\"M\") - 1).to_timestamp(\"M\")\n\nff_adj.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined = reg.join(ff_adj, how=\"inner\")\n\n\ncombined.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-12-31\n-1.651021\n1\n0.162158\n0.837762\n0.000079\nTight\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n-1.496425\n0\n0.837514\n0.162233\n0.000253\nTight\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n-1.643180\n1\n0.162188\n0.837738\n0.000075\nTight\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n-1.627975\n0\n0.837473\n0.162258\n0.000269\nTight\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n-1.400432\n1\n0.162220\n0.837714\n0.000067\nTight\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n-1.561611\n0\n0.837459\n0.162284\n0.000258\nTight\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n-1.387057\n1\n0.162245\n0.837689\n0.000066\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n-1.482573\n0\n0.837437\n0.162308\n0.000255\nTight\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n-1.482572\n1\n0.162251\n0.837658\n0.000091\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n-1.251226\n0\n0.836442\n0.162316\n0.001242\nTight\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n\ndf2 = combined.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by *lagged* regime:\")\nprint(means_by_regime)\n\nMean factor returns by *lagged* regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nTight            -0.005439 -0.005360  0.000540 -0.003311\nNeutral           0.003200  0.002938  0.001051 -0.000056\nHigh              0.003327  0.001228  0.005127  0.003575\n\n\n\n\n\nThis is reverse of what you’d expect. In a tight liquidity regime, \\(R^{smb}\\) and \\(R^{hml}\\) should be deeply positive. You’d expect small under-valued companies to outperform big companies with premium valuation.\nLet’s go back to the \\(L_t\\) and understand what’s going on\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L_t.index,\n    y=L_t.values,\n    hue=regimes_df[\"state\"],\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution of L(t) by HMM State\")\nax[1].set_xlabel(\"L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the KDE (bottom panel):\n\nStates 0 (green) and 1 (yellow) heavily overlap\nOnly state 2 (red) is clearly separated — the “high liquidity spike”\nThis corresponds to crisis/QE events (2008, 2020), where liquidity jumps dramatically\n\nSo the natural clustering is:\n\nOne large cluster (normal-tight-neutral combined)\nOne rare extreme spike cluster\n\nYour HMM is being forced to split the unimodal bulk distribution into two states, resulting in:\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\nI initially thought alpha=0.5 in SparsePCA might be the cause. But the plot shows:\n\nL(t) has a healthy dynamic range across time (≈ −2.5 to +12)\nThe high liquidity cluster is clear and RARE\nThe rest of L(t) is genuinely one blob\n\nIf Sparse PCA was “overshrinking,” L(t) would be compressed; but it is not.\n\n\n\n\nIs Liquidity is fundamentally spiky ?\nWhy ?\nAm I defining Liquidity incorrectly ?\n\nFed/Treasury continuously debased currency throughout 2008-2019 period. Liquidity injection (bailing out banks during GFC/2008, ZIRP era, M2 stimulius of 2019 are visible ones). How would I capture this events as proxy of “liquidity” ?\n\n\nRight now your \\(L_t\\) is basically a short-horizon “flow” liquidity shock index.\n\n\nWhereas I’m looking for slow, structural debasement / regime of easy money (2008–2019, QE, ZIRP, etc.).\n\n\nThose are not the same object.\n\nBecause I use 1-month growth rates \\(\\Delta \\log(\\cdot)\\), my index reacts to spikes / changes, not the level of the monetary stock.\nI need to factor in more factors to the liquidity index such as -\n\nLevel of money / balance sheet relative to trend / GDP, not just monthly change\nProlonged ZIRP / negative real rate period\nExcess liquidity over economic activity\n\n\n\n\n\n\nExamples (all can be pulled from FRED):\nLog level vs pre-2008 trend\nLet \\(m_t = \\log M2_t\\). Fit a linear trend on a pre-QE baseline (say 1985–2007):\n\\(m_t \\approx a + bt \\quad (t \\le 2007)\\)\nThen define excess money stock:\n\\(EM_t = m_t - (a + bt)\\)\nAfter 2008, \\(EM_t\\) becomes increasingly positive if M2 grows above its historical trend.\nSimilarly for the Fed Balance Sheet, let \\(b_t = \\log(\\text{FedBal}_t)\\):\n\\(EB_t = b_t - (\\alpha + \\beta t)\\) (pre-2007 trend)\n\nExcess liquidity versus real economy\nUse real GDP series (e.g., GDPC1) and define:\n\\(EL_t = \\log M2_t - \\log GDP_t\\)\nOr measure multi-year excess growth:\n\\(EL_t(3y) = \\log M2_t - \\log M2_{t-36} - (\\log GDP_t - \\log GDP_{t-36})\\)\n\nZIRP / negative real-rate indicator\n\\(D_t^{ZIRP} = 1(r_t^{real} &lt; 0)\\)\nFrom 2009–2015, this should be mostly 1.\n\n\n\n\nInstead of only:\n\\(x_t = \\{ \\Delta \\log M2_t,\\; \\Delta \\log FedBal_t,\\; TS_t,\\; r_t^{real},\\; CS_t \\}\\)\nLet’s expand to:\n\\(x_t^{aug} = \\{\n\\Delta \\log M2_t,\\;\n\\Delta \\log FedBal_t,\\;\nTS_t,\\;\nr_t^{real},\\;\nCS_t,\\;\nEM_t,\\;\nEB_t,\\;\nEL_t,\\;\nEL_t(3y),\\;\nD_t^{ZIRP}\n\\}\\)\nThen standardize and perform PCA / SparsePCA on this.\n\nPC1 (or a combination) loading heavily on \\(EM\\), \\(EB\\), \\(EL\\), \\(D_t^{ZIRP}\\) → structural easy-money regime\n\nPC2 (or your current PC1) loading on short-horizon changes → flow / shock liquidity\n\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\nNaN\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\nNaN\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29511.664\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\nNaN\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\nNaN\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29825.182\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\nNaN\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\nNaN\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n30042.113\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\nNaN\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\nNaN\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30485.729\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\nNaN\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\nNaN\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# 1. Start from the original GDP values only (drop NaNs)\ngdp_series = macro_raw[\"GDP\"].dropna()\n\n# 2. Collapse to unique quarter-end values\n#    - If GDP is timestamped at quarter *start* (e.g. 2025-04-01),\n#      this will move it to the quarter end (2025-06-30) and give unique labels.\ngdp_q = gdp_series.resample(\"Q\").last()   # quarterly, at quarter-end (e.g. 2025-03-31, 2025-06-30, ...)\n\n# 3. Upsample to monthly and forward-fill within the quarter\ngdp_m = gdp_q.resample(\"M\").ffill()\n\n# 4. Assign back into macro_raw, aligning on the monthly index\nmacro_raw[\"GDP\"] = gdp_m\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\n28708.161\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\n29147.044\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29147.044\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\n29147.044\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\n29511.664\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29511.664\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\n29511.664\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\n29825.182\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n29825.182\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\n29825.182\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\n30042.113\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30042.113\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\n30042.113\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\n30485.729\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndef build_liquidity_proxies_augmented(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Flow proxies\n    df[\"dlog_M2\"]       = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"]  = np.log(df[\"FED_BAL\"]).diff()\n    df[\"term_spread\"]   = df[\"DGS10\"] - df[\"TB3M\"]\n    df[\"infl_yoy\"]      = np.log(df[\"CPI\"]).diff(12)\n    df[\"real_rate\"]     = df[\"TB3M\"] - 100 * df[\"infl_yoy\"]\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # 2. Levels\n    df[\"log_M2\"]      = np.log(df[\"M2SL\"])\n    df[\"log_FED_BAL\"] = np.log(df[\"FED_BAL\"])\n\n    # Use data up to 2007-12-31 to fit trends\n    pre = df.loc[: \"2007-12-31\"].copy()\n\n    # --- Trend for M2 ---\n    pre_m2 = pre[\"log_M2\"].dropna()\n    t_m2   = np.arange(len(pre_m2))\n    coefs_M2 = np.polyfit(t_m2, pre_m2.values, deg=1)\n\n    t_full = np.arange(len(df))\n    trend_M2 = np.polyval(coefs_M2, t_full)\n    df[\"EM\"] = df[\"log_M2\"] - trend_M2\n\n    # --- Trend for Fed balance sheet ---\n    pre_fb = pre[\"log_FED_BAL\"].dropna()\n    t_fb   = np.arange(len(pre_fb))\n    coefs_FB = np.polyfit(t_fb, pre_fb.values, deg=1)\n\n    trend_FB = np.polyval(coefs_FB, t_full)\n    df[\"EB\"] = df[\"log_FED_BAL\"] - trend_FB\n\n    # 3. Excess liquidity vs GDP over 3y\n    df[\"log_GDP\"] = np.log(df[\"GDP\"])\n    df[\"EL_3y\"] = (df[\"log_M2\"] - df[\"log_M2\"].shift(36)) - \\\n                  (df[\"log_GDP\"] - df[\"log_GDP\"].shift(36))\n\n    # 4. ZIRP dummy\n    df[\"ZIRP_dummy\"] = (df[\"real_rate\"] &lt; 0).astype(int)\n\n    # 5. Build augmented proxy matrix\n    cols_aug = [\n        \"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\", \"real_rate\", \"credit_spread\",\n        \"EM\", \"EB\", \"EL_3y\", \"ZIRP_dummy\"\n    ]\n\n    return df[cols_aug].dropna()\n\n\nliquidity_proxies_aug = build_liquidity_proxies_augmented(macro_raw)\n\n\nliquidity_proxies_aug.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n0.004077\n-0.006080\n-0.91\n2.316574\n0.74\n0.204864\n0.811903\n-0.194283\n0\n\n\n2024-10-31\n0.002378\n-0.009447\n-0.23\n1.971101\n0.68\n0.202921\n0.798940\n-0.199645\n0\n\n\n2024-11-30\n0.004673\n-0.015569\n-0.24\n1.742012\n0.64\n0.203273\n0.779855\n-0.203197\n0\n\n\n2024-12-31\n0.000775\n-0.002781\n0.31\n1.438113\n0.60\n0.199728\n0.773558\n-0.186129\n0\n\n\n2025-01-31\n0.003164\n-0.009892\n0.37\n1.254690\n0.62\n0.198572\n0.760151\n-0.188367\n0\n\n\n2025-02-28\n0.003354\n-0.007668\n0.02\n1.444603\n0.60\n0.197605\n0.748966\n-0.189520\n0\n\n\n2025-03-31\n0.003338\n-0.003828\n0.03\n1.822893\n0.64\n0.196623\n0.741623\n-0.177663\n0\n\n\n2025-04-30\n0.006165\n-0.004606\n-0.04\n1.903069\n0.73\n0.198467\n0.733501\n-0.172818\n0\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n0.196757\n0.724600\n-0.167478\n0\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n0.197692\n0.719428\n-0.150768\n0\n\n\n\n\n\n\n\n\nz1_t = standardize_and_signflip(liquidity_proxies_aug)\nz1_t.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n-0.139814\n-0.331324\n-1.812889\n-1.577541\n0.673692\n0.888682\n0.786734\n-2.353880\n-1.527525\n\n\n2024-10-31\n-0.414988\n-0.409551\n-1.282837\n-1.408779\n0.819794\n0.868074\n0.764262\n-2.404333\n-1.527525\n\n\n2024-11-30\n-0.043456\n-0.551814\n-1.290631\n-1.296871\n0.917195\n0.871811\n0.731177\n-2.437752\n-1.527525\n\n\n2024-12-31\n-0.674370\n-0.254667\n-0.861913\n-1.148418\n1.014596\n0.834206\n0.720261\n-2.277159\n-1.527525\n\n\n2025-01-31\n-0.287635\n-0.419885\n-0.815143\n-1.058817\n0.965896\n0.821943\n0.697018\n-2.298214\n-1.527525\n\n\n2025-02-28\n-0.256970\n-0.368230\n-1.087964\n-1.151588\n1.014596\n0.811689\n0.677630\n-2.309059\n-1.527525\n\n\n2025-03-31\n-0.259533\n-0.278983\n-1.080169\n-1.336381\n0.917195\n0.801267\n0.664900\n-2.197496\n-1.527525\n\n\n2025-04-30\n0.198084\n-0.297077\n-1.134734\n-1.375546\n0.698042\n0.820833\n0.650819\n-2.151913\n-1.527525\n\n\n2025-05-31\n-0.377318\n-0.315174\n-0.978836\n-1.374952\n0.649342\n0.802693\n0.635389\n-2.101665\n-1.527525\n\n\n2025-06-30\n0.050761\n-0.228533\n-1.095759\n-1.223790\n0.795443\n0.812604\n0.626423\n-1.944440\n-1.527525\n\n\n\n\n\n\n\n\nL1_t = build_sparse_pca_liquidity_index(z1_t, alpha=0.5)\nL1_t.tail(10)\n\nSparsePCA components (loadings):\n  dlog_M2: -0.132\n  dlog_FED_BAL: -0.159\n  term_spread: -0.389\n  real_rate: -0.515\n  credit_spread: 0.046\n  EM: -0.114\n  EB: -0.134\n  EL_3y: -0.518\n  ZIRP_dummy: -0.490\n\n\nDATE\n2024-09-30    3.347080\n2024-10-31    3.143188\n2024-11-30    3.088484\n2024-12-31    2.811365\n2025-01-31    2.736110\n2025-02-28    2.887801\n2025-03-31    2.906425\n2025-04-30    2.856634\n2025-05-31    2.850481\n2025-06-30    2.674924\nFreq: M, Name: L, dtype: float64\n\n\n\nL1_t.plot(title=\"Sparse PCA Liquidity Index L(t) - Augmented w/ Flow & Stock Indicators\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel, regimes_aug_df = fit_hmm_on_liquidity(L1_t,n_states=2)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.484\n  state 1: mean L = 1.536\n\n\n\nregime_aug_df.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_aug_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined_aug = reg.join(ff_adj, how=\"inner\")\n\n\ncombined_aug.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n-0.0100\n-0.0089\n0.0086\n-0.0148\n0.0098\n0.0039\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n0.0649\n0.0459\n0.0015\n-0.0231\n-0.0205\n0.0040\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n-0.0315\n-0.0383\n-0.0300\n0.0189\n-0.0121\n0.0037\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\ndf2 = combined_aug.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by Liquidity Regime:\")\nprint(means_by_regime)\n\nMean factor returns by Liquidity Regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nHigh             -0.004519 -0.001700  0.001564 -0.003006\nTight             0.001896  0.000038  0.002920  0.001302\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\nHigh Liquidity\nTight Liquidity\nInterpretation\n\n\n\n\nSMB\nNegative\nPositive\nSmall caps thrive when liquidity tightens\n\n\nHML\nNegative\nSlightly positive\nValue improves in tight regimes\n\n\nRMW\nPositive\nMore positive\nProfitability is the strongest cross-regime performer\n\n\nCMA\nNegative\nPositive\nConservative investment becomes favored when liquidity tightens\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L1_t.index,\n    y=L1_t.values,\n    hue=regimes_aug_df[\"state\"],\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Augmented Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"Augmented L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_aug_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution/KDE of Augmented L(t) by HMM State\")\nax[1].set_xlabel(\"Augmented L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn KDE of augmented liquidity index \\(L_t^{aug}\\), clusters are clearly separated, unlike last one\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndef parse_yyyy_mm_to_date(s: str):\n    \"\"\"\n    Convert 'YYYY.MM' to a proper datetime.\n    Handles the special case where October appears as YYYY.1 instead of YYYY.10.\n    \"\"\"\n    s = str(s).strip()\n    parts = s.split(\".\")\n    if len(parts) != 2:\n        raise ValueError(f\"Unexpected date format: {s}\")\n    \n    year = int(parts[0])\n    mm = parts[1]\n\n    # Fix: \"1\" should be \"10\" (October)\n    if mm == \"1\":\n        month = 10\n    else:\n        month = int(mm)\n\n    return pd.Timestamp(year=year, month=month, day=1)\n\n# --- Load CAPE CSV ---\nsp500_cape_raw = pd.read_csv(\"data/sp500-cape-1990-2025.csv\")\n\n# Clean + parse\nsp500_cape_raw[\"date\"] = sp500_cape_raw[\"Date\"].apply(parse_yyyy_mm_to_date)\n\nsp500_cape_raw = (\n    sp500_cape_raw[[\"date\", \"SP500\", \"CAPE\"]]\n    .set_index(\"date\")\n    .sort_index()\n)\n\n# Convert to month-end aligned CAPE series\nsp500_cape_m = sp500_cape_raw.resample(\"M\").last()\nsp500_cape_m.rename(columns={\"CAPE\": \"cape\", \"SP500\": \"sp500\"}, inplace=True)\n\nsp500_cape_m.tail()\n\n\n\n\n\n\n\n\nsp500\ncape\n\n\ndate\n\n\n\n\n\n\n2025-08-31\n6408.95\n37.85\n\n\n2025-09-30\n6584.02\n38.58\n\n\n2025-10-31\n6735.69\n39.21\n\n\n2025-11-30\n6740.89\n39.12\n\n\n2025-12-31\n6812.63\n39.42\n\n\n\n\n\n\n\n\n\n\nLong-Term Average/Median: The S&P 500’s historical long-term average CAPE ratio is around 16-18, with a median value of approximately 16.04 (since 1881). This provides a central benchmark.\nHigh Valuation (Expensive): A CAPE ratio that is notably higher than the historical average (e.g., above 25 or 30) is considered indicative of a highly valued or overvalued market. Historically, values exceeding 30 have only occurred during major market peaks like the 1929 crash, the dot-com bubble in the late 1990s (peaking over 44), and the post-pandemic period around 2021. Such high readings have typically been followed by periods of lower-than-average, or even negative, long-term returns.\nLow Valuation (Cheap): A CAPE ratio well below the historical average (e.g., below 15 or 10) suggests an undervalued or cheap market. Historically, such levels have been associated with minimal downside risk and higher average long-term returns.\nCould have calculated\nV_spread = (x - baseline) / baseline # baseline = 16\nProblem is, if the whole world “structurally” shifted (e.g. post-1990 lower inflation), “16” stops being meaningful.\nWe are instead measuring,\n\\[\nV_t = \\frac{CAPE_t - \\mathrm{median}(CAPE)}{\\mathrm{IQR}(CAPE)}\n\\]\n\ndef build_valuation_spread_baseline(\n    val_df: pd.DataFrame,\n    metric: str = \"cape\",\n    baseline: float = None,\n    scale: bool = True,\n) -&gt; pd.Series:\n    x = val_df[metric].astype(float)\n\n    if baseline is None:\n        # robust long-run baseline\n        median = x.median()\n        iqr = x.quantile(0.75) - x.quantile(0.25)\n        baseline = median\n        denom = iqr if scale and iqr &gt; 0 else baseline\n    else:\n        denom = baseline if scale else 1.0\n\n    diff = x - baseline          \n\n    V_spread = diff / denom if scale else diff\n    V_spread.name = \"V_spread\"\n    return V_spread\n\n# Example: baseline at 16.04 (long-run median)\nV_spread_t = build_valuation_spread_baseline(\n    sp500_cape_m,\n    metric=\"cape\",\n    baseline=None,\n    scale=True,   # or False if you prefer \"CAPE points below baseline\"\n)\n\nV_spread_t.tail()\n\ndate\n2025-08-31    1.259380\n2025-09-30    1.338771\n2025-10-31    1.407287\n2025-11-30    1.397499\n2025-12-31    1.430125\nFreq: M, Name: V_spread, dtype: float64\n\n\n\nV_spread_t.plot(figsize=(14, 4), title=\"S&P 500 Valuation Spread (cheap vs own history)\")\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\ncombined_aug.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\n# regimes_df: index is month-end\nreg = combined_aug.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Align valuation spread to month-end, then join\nV_spread_m = V_spread_t.resample(\"M\").last()\n\ncombined = (\n    reg\n    .join(V_spread_m.to_frame(), how=\"inner\")\n)\n\ncombined.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\nV_spread\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n1.187602\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n0.925503\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n0.690593\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n0.958129\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n1.070147\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Valuation spread series, e.g. top–bottom decile)\n$ V_t^{} $\n(Regime-conditional mean valuation spread)\n$ {V}^{(k)} = $\n$ ^{(k)} = {_{t=1}^T (_t = k)} $\n(Difference in spreads between regimes)\n$ ^{()} - ^{()} $\n(Factor return in regime ( k ))\n$ r_t^{(F)} $\n$ {r}_F^{(k)} = $\n$ F^{(k)} = {{t=1}^T (_t = k)} $\n(Regime-specific Sharpe ratio)\n$ _F^{(k)} = $\n\n# Regime-conditional mean valuation spread:  V̄^(k)\nval_means_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .mean()\n)\n\nval_stds_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .std()\n)\n\nprint(\"Regime-conditional mean valuation spread:\")\nprint(val_means_by_regime)\n\nprint(\"\\nRegime-conditional valuation z-score std dev:\")\nprint(val_stds_by_regime)\n\n# Difference in spreads between regimes:  V̂^(High) − V̂^(Tight)\nif {\"High\", \"Tight\"}.issubset(val_means_by_regime.index):\n    spread_diff = (\n        val_means_by_regime[\"High\"] - val_means_by_regime[\"Tight\"]\n    )\n    print(\"\\nDifference in spreads High − Tight:\")\n    print(spread_diff)\n\nRegime-conditional mean valuation spread:\nstate_label\nHigh     0.380274\nTight   -0.049269\nName: V_spread, dtype: float64\n\nRegime-conditional valuation z-score std dev:\nstate_label\nHigh     0.406851\nTight    0.551632\nName: V_spread, dtype: float64\n\nDifference in spreads High − Tight:\n0.42954211798360936\n\n\n\n\n\n\nTight liquidity = cheaper markets (value regime),\nHigh liquidity = expensive markets (growth regime).\n\n\nMarkets are ~0.43σ more expensive in High liquidity regimes than Tight liquidity regimes.\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# -------------------------------------------------------------------\n# Assume you already have:\n#   L_t              : pd.Series (liquidity index), monthly, name \"L\"\n#   sp500_cape_m     : DataFrame with column \"sp500\" (S&P 500)\n#   V_spread_t       : pd.Series (\"V_spread\"), monthly valuation cheapness\n#   regimes_df       : DataFrame with column \"state_label\"\n# -------------------------------------------------------------------\n\n# 1) Build a common monthly dataframe\ndf_plot = pd.DataFrame(index=L_t.index.union(sp500_cape_m.index).union(V_spread_t.index))\n\ndf_plot[\"L\"]         = L_t.reindex(df_plot.index)\ndf_plot[\"spx_price\"] = sp500_cape_m[\"sp500\"].reindex(df_plot.index)\ndf_plot[\"V_spread\"]  = V_spread_t.reindex(df_plot.index)\n\n# Attach regimes (forward-fill in case of any slight index mismatch)\nreg_states = combined[\"state_label\"].reindex(df_plot.index).ffill()\ndf_plot[\"state_label\"] = reg_states\n\n# Optional: drop rows before we have all three series\ndf_plot = df_plot.dropna(subset=[\"L\", \"spx_price\", \"V_spread\", \"state_label\"])\n\n# 2) Helper: find contiguous regime segments for shading\ndef get_regime_segments(states: pd.Series):\n    \"\"\"\n    Given a Series of regime labels indexed by date,\n    return list of (start_date, end_date, label) segments\n    where the label is constant.\n    \"\"\"\n    segments = []\n    prev_label = None\n    start_date = None\n\n    for dt, label in states.items():\n        if prev_label is None:\n            prev_label = label\n            start_date = dt\n            continue\n\n        if label != prev_label:\n            # segment ended at previous date\n            end_date = dt\n            segments.append((start_date, end_date, prev_label))\n            start_date = dt\n            prev_label = label\n\n    # last segment\n    if prev_label is not None and start_date is not None:\n        segments.append((start_date, states.index[-1], prev_label))\n\n    return segments\n\nsegments = get_regime_segments(df_plot[\"state_label\"])\n\n# 3) Colors for regimes\nregime_colors = {\n    \"High\":    \"#ffe0e0\",  # pale red\n    \"Neutral\": \"#f7f7f7\",  # light gray\n    \"Tight\":   \"#d1ffd1\",  # pale green\n}\n\n# 4) Plot: 3 stacked subplots with shared x-axis\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True,\n                         gridspec_kw={\"height_ratios\": [1, 1, 1]})\n\nax_L, ax_spx, ax_val = axes\n\n# --- Shading first so lines render on top ---\nfor (start, end, label) in segments:\n    color = regime_colors.get(label, \"#f0f0f0\")\n    for ax in axes:\n        ax.axvspan(start, end, color=color, alpha=0.3)\n\n# --- Panel 1: Liquidity index L(t) ---\nax_L.plot(df_plot.index, df_plot[\"L\"], label=\"L(t)\", linewidth=1.5)\nax_L.set_ylabel(\"L(t)\")\nax_L.set_title(\"Liquidity Index, S&P 500, and Valuation Spread by Liquidity Regime\")\nax_L.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 2: S&P 500 level ---\nax_spx.plot(df_plot.index, df_plot[\"spx_price\"], label=\"S&P 500\", linewidth=1.5)\nax_spx.set_ylabel(\"S&P 500\")\nax_spx.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 3: Valuation cheapness (V_spread) ---\nax_val.plot(df_plot.index, df_plot[\"V_spread\"], label=\"V_spread (cheap vs baseline)\", linewidth=1.5)\nax_val.axhline(0.0, color=\"black\", linewidth=1, linestyle=\"--\", alpha=0.7)\nax_val.set_ylabel(\"V_spread\")\nax_val.set_xlabel(\"Date\")\nax_val.grid(True, linestyle=\"--\", alpha=0.4)\n\n# Optional legends\nax_L.legend(loc=\"upper left\")\nax_spx.legend(loc=\"upper left\")\nax_val.legend(loc=\"upper left\")\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#high-level-structure-of-the-paper",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#high-level-structure-of-the-paper",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Story – why this matters (intro + conclusion)\nData – what you measure (markets, factors, liquidity proxies)\nModels – how you measure (liquidity index, regimes, cross-sectional tests)\nPortfolio – what to do with it (conditional allocation"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#abstract-150200-words",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#abstract-150200-words",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "The “valuations don’t matter in infinite money printing” meme.\nYour construction of a liquidity index and liquidity regimes.\nKey findings:\nValuations do matter, but conditional on liquidity regimes.\nGrowth dominates in high-liquidity, negative-real-rate regimes.\nValue premia revive after QT / real rate normalization.\n\nPortfolio takeaway: a simple regime-aware factor-tilt model."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#motivation",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#motivation",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Narrative: post-GFC QE, ZIRP, NIRP, 2010–2021 “TINA”, tech bubble 2.0.\n\n“We live in a world of infinite money printing, so valuations don’t matter.”\n\nResearch question: &gt; Do valuations truly die under abundant liquidity, or do they merely become regime-dependent?\nDo valuation spreads (cheap vs expensive deciles) expand under high-liquidity regimes and compress under tight liquidity?\nHow do factor premia (value, momentum, quality, low vol) vary conditional on liquidity regimes?\nCan a macro-liquidity index predict the timing of factor rotations (growth vs value, long-duration vs short-duration equities)?\nDoes a regime-aware factor allocation deliver better risk-adjusted returns than a static factor mix?"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#intuition",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#intuition",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Valuation spread: the gap between cheap and expensive stocks (e.g., top vs bottom decile by B/M).\nLiquidity regime: periods of systematically easy vs tight financial conditions driven by monetary and fiscal variables."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#literature-sketch-very-short",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#literature-sketch-very-short",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Systemic Strategy: Factor investing: value, momentum, quality.\nRisk Factors and Risk premia: Liquidity\nRegime-switching: macro-conditional factors."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#contributions",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#contributions",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Build a macro liquidity index from multiple public series.\nUse HMM / Markov regimes to label high vs low liquidity states.\nShow how valuation spreads and factor returns behave across liquidity regimes.\nPropose a simple implementable regime-aware factor strategy."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#data-variables",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#data-variables",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Horizon: 1990–2025, monthly frequency (or weekly if feasible).\nMarket: US equities (CRSP/Compustat-type universe); optional robustness on AUS/EU.\n\n\n\n\n\n\nIndividual stock returns \\(r_{i,t}\\), market cap, book equity, earnings, etc.\nCharacteristics \\(X_{i,t}\\):\n\n\\[\n\\text{B/M},\\ \\text{E/P},\\ \\text{P/S},\\ \\text{size},\\ \\beta,\\ \\text{12–1 momentum},\\ \\text{profitability},\\ \\text{volatility}.\n\\]\n\nFactor returns \\(f_t\\):\n\n\\[\n\\text{MKT},\\ \\text{HML},\\ \\text{SMB},\\ \\text{MOM},\\ \\text{Quality},\\ \\text{LowVol}.\n\\]\nDefine a valuation spread:\n\\[\nV_t^{\\text{spread}} = \\text{Long top decile of B/M} - \\text{Short bottom decile}.\n\\]\n\n\n\n\nLet raw macro/liquidity variables at time \\(t\\) be \\(x_{j,t}\\), including:\n\n\n\n\nMoney growth: \\[\\Delta \\log(M2_t)\\]\nFed balance sheet growth: \\[\\Delta \\log(\\text{BS}_t)\\]\n\n\n\n\n\n\nShort rate: \\[i_t\\]\n10Y yield: \\[y_{10,t}\\]\nSlope: \\[\\text{TS}_t = y_{10,t} - i_t\\]\nReal short rate: \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\]\n\n\n\n\n\n\nCredit spread: \\[\\text{CS}_t = y_{\\text{Baa},t} - y_{\\text{Aaa},t}\\]\nVIX level: \\[\\text{VIX}_t\\]\n\n\n\n\n\n\nON RRP usage\nTGA balance\n\n\nAll variables \\(x_{j,t}\\) will then be standardized (e.g., z-scored) and aggregated into a single liquidity index."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#math",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#math",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Let the raw liquidity indicators be collected in the vector\n\\[\nx_t =\n\\begin{bmatrix}\nx_{1,t} \\\\\nx_{2,t} \\\\\n\\vdots \\\\\nx_{J,t}\n\\end{bmatrix}\n\\in \\mathbb{R}^J.\n\\]\nFor each series \\[x_{j,t}\\], compute the sample mean \\[\n\\mu_j = \\frac{1}{T} \\sum_{t=1}^{T} x_{j,t}\n\\] and variance \\[\n\\sigma_j^2 = \\frac{1}{T-1} \\sum_{t=1}^{T} (x_{j,t} - \\mu_j)^2.\n\\]\nStandardised variables are then \\[\nz_{j,t} = \\frac{x_{j,t} - \\mu_j}{\\sigma_j}, \\qquad j = 1,\\dots,J,\n\\] with stacked vector \\[z_t = (z_{1,t},\\dots,z_{J,t})^\\top\\].\nSeries may be sign-flipped so that higher values of \\[z_{j,t}\\] correspond to easier liquidity; for example, use \\[-r_t^{\\text{real}}\\] instead of \\[r_t^{\\text{real}}\\], \\[-CS_t\\] instead of \\[CS_t\\], and \\[-VIX_t\\] instead of \\[VIX_t\\].\n\n\n\n\nTypical inputs include \\[\\Delta \\log M2_t\\], \\[\\Delta \\log \\text{BS}_t\\], the term spread \\[\\text{TS}_t = y_{10,t} - i_t\\], the real rate \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\], and the credit spread \\[\\text{CS}_t = y_{Baa,t} - y_{Aaa,t}\\].\n\n\n\n\nDefine the covariance matrix \\[\n\\Sigma = \\frac{1}{T} \\sum_{t=1}^{T} z_t z_t^\\top.\n\\]\nLet \\[(\\lambda_k, v_k)\\] solve \\[\\Sigma v_k = \\lambda_k v_k\\], with eigenvalues ordered \\[\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_J\\]. The first principal-component liquidity index is then \\[L_t = v_1^\\top z_t\\]. Alternatively, one may use a fixed-weight index \\[L_t = w^\\top z_t\\].\n\n\n\n\nLet the latent regime variable satisfy \\[s_t \\in \\{1,\\dots,K\\}\\], with regime-conditional dynamics \\[\nL_t \\mid (s_t = k) \\sim \\mathcal{N}(\\mu_k, \\sigma_k^2).\n\\]\nTransition probabilities are \\[\\Pr(s_t = j \\mid s_{t-1} = i) = p_{ij}\\], forming the matrix \\[\nP =\n\\begin{bmatrix}\np_{11} & \\cdots & p_{1K} \\\\\n\\vdots & \\ddots & \\vdots \\\\\np_{K1} & \\cdots & p_{KK}\n\\end{bmatrix},\n\\qquad \\sum_{j=1}^{K} p_{ij} = 1.\n\\]\nFiltered or smoothed regime classification is given by \\[\n\\hat{s}_t = \\arg\\max_{k} \\Pr(s_t = k \\mid L_{1:T}).\n\\]\nDefine the high- and tight-liquidity regimes as \\[k_{\\text{High}} = \\arg\\max_k \\mu_k\\] and \\[k_{\\text{Tight}} = \\arg\\min_k \\mu_k\\], with indicator \\[\nI_t^{\\text{High}} = \\mathbf{1}\\!\\left[\\Pr(s_t = k_{\\text{High}} \\mid L_{1:T}) &gt; 0.5\\right].\n\\]\n\n\n\n\nIn a multivariate setting, define \\[\ny_t =\n\\begin{bmatrix}\nL_t \\\\\nr_t^{\\text{MKT}} \\\\\nr_t^{\\text{HML}} \\\\\n\\text{VIX}_t \\\\\n\\vdots\n\\end{bmatrix},\n\\] with regime-dependent dynamics \\[y_t = A_k y_{t-1} + \\varepsilon_t^{(k)}\\] and \\[\\varepsilon_t^{(k)} \\sim \\mathcal{N}(0,\\Sigma_k)\\] when \\[s_t = k\\].\n\n\n\n\nLet \\[V_t^{\\text{spread}}\\] denote a valuation-spread series (e.g., top–bottom decile). The regime-conditional mean is \\[\n\\bar{V}^{(k)} = \\mathbb{E}[V_t^{\\text{spread}} \\mid s_t = k],\n\\] with sample estimate \\[\n\\hat{\\bar{V}}^{(k)} =\n\\frac{\\sum_{t=1}^{T} V_t^{\\text{spread}} \\mathbf{1}(\\hat{s}_t = k)}\n{\\sum_{t=1}^{T} \\mathbf{1}(\\hat{s}_t = k)}.\n\\]\nDifferences such as \\[\\hat{\\bar{V}}^{(\\text{High})} - \\hat{\\bar{V}}^{(\\text{Tight})}\\] summarise regime effects. Analogously, for factor returns \\[r_t^{(F)}\\], \\[\n\\bar{r}_F^{(k)} = \\mathbb{E}[r_t^{(F)} \\mid s_t = k],\n\\] with Sharpe ratio \\[\\text{SR}_F^{(k)} = \\hat{\\bar{r}}_F^{(k)} / \\hat{\\sigma}_F^{(k)}\\].\n\n\n\n\nContinuous-index predictability is tested via \\[\nr_{t+1}^{(F)} = \\alpha + \\beta L_t + \\gamma^\\top c_t + \\varepsilon_{t+1},\n\\] while regime-based predictability uses \\[\nr_{t+1}^{(F)} =\n\\alpha\n+ \\delta_{\\text{High}} I_t^{\\text{High}}\n+ \\delta_{\\text{Tight}} I_t^{\\text{Tight}}\n+ \\delta_{\\text{Neutral}} I_t^{\\text{Neutral}}\n+ \\varepsilon_{t+1}.\n\\]\n\n\n\n\nAt each time \\[t\\], estimate \\[\nr_{i,t+1} =\n\\alpha_t\n+ \\lambda_{1,t}\\text{Valuation}_{i,t}\n+ \\lambda_{2,t}\\text{Size}_{i,t}\n+ \\lambda_{3,t}\\text{Momentum}_{i,t}\n+ \\dots\n+ \\varepsilon_{i,t+1}.\n\\]\nRegime-conditional slopes satisfy \\[\\bar{\\lambda}_1^{(k)} = \\mathbb{E}[\\lambda_{1,t} \\mid s_t = k]\\], with sample analogue \\[\n\\hat{\\bar{\\lambda}}_1^{(k)} =\n\\frac{\\sum_{t=1}^{T} \\lambda_{1,t} \\mathbf{1}(\\hat{s}_t = k)}\n{\\sum_{t=1}^{T} \\mathbf{1}(\\hat{s}_t = k)}.\n\\]\n\n\n\n\nLet \\[f_t \\in \\mathbb{R}^K\\] denote factor returns and \\[w^{(k)} \\in \\mathbb{R}^K\\] the regime-specific weights. The applied weights are \\[w_t = w^{(\\hat{s}_t)}\\], yielding portfolio return \\[\nR_{p,t+1} = w_t^\\top f_{t+1}.\n\\]"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#pca-and-sparse-pca",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#pca-and-sparse-pca",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Standard PCA solves:\n\\[\n\\max_{v} \\; \\| X v \\|^{2}\n\\quad \\text{s.t.} \\quad \\| v \\|^{2} = 1\n\\]\n\n\n\nPCA tries to find the direction ( \\(v\\) ) (a unit vector) along which the projected data ( \\(Xv\\) ) has maximum variance.\nVariance of the projection:\n\\[\n\\text{Var}(Xv) = \\frac{1}{n} |Xv|^{2}.\n\\]\nSo maximizing variance is equivalent to maximizing ( \\(|Xv|^{2}\\) ).\n\n\n\n\nWithout this constraint, the problem would blow up:\n\\[\n|X(\\alpha v)|^{2} = \\alpha^{2} |Xv|^{2},\n\\]\nand the maximum would be infinite by choosing ( \\(\\alpha\\) \\(\\infty\\) ).\nSo PCA forces ( \\(v\\) ) to be a direction, not a magnitude → a unit vector.\n\n\n\n\n\\[\n|Xv|^{2} = v^\\top X^\\top X v.\n\\]\nLet:\n\\[\nS = X^\\top X\n\\]\n(the unnormalized covariance matrix). Then PCA solves:\n\\[\n\\max_{v} ; v^\\top S v\n\\quad\\text{s.t.}\\quad v^\\top v = 1.\n\\]\nThis is exactly the Rayleigh quotient.\n\n\n\n\nPCA finds the direction (unit vector) along which the data cloud has maximum spread. That direction is exactly the eigenvector of the covariance matrix with the largest eigenvalue."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#sparse-pca",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#sparse-pca",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "PCA rotates the data into orthogonal directions capturing maximum variance.\nEach principal component is a linear combination of all variables.\nLoadings are typically dense (every variable has some non-zero weight).\nThis makes interpretation difficult:\n\n\n“PC1 of 127 economic multicolinear variables is 127-dimensional mush of everything.”\n\nThis is why PCA is almost never used directly for economic interpretation — PCs are not sparse.\n\n\n\n\nSparse PCA = PCA where most loadings are forced to be zero.\nMathematically:\nStandard PCA solves:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1\n\\]\nSparse PCA adds an L1 penalty (lasso-style sparsity) or constrains the number of non-zero elements:\n\\[\n\\max_{v} \\ |Xv|^2 - \\lambda |v|_1\n\\]\nor equivalently:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1,\\ |v|_1 \\leq c\n\\]\nThis forces:\n\nOnly a small subset of variables to load on each component.\nComponents become interpretable (e.g., PC1 loads on CPI, PCE, core CPI → “inflation”).\n\nThe specific implementation cited (“penalized matrix decomposition” by Witten et al. 2009) solves:\n\\[\n\\max_{u, v} \\ u^\\top X v \\quad\n\\text{s.t. } |u|_2 = 1,\\ |v|_2 = 1,\\ |v|_1 \\le c\n\\]\nThis is a general sparse factor extraction algorithm.\nIntuition: &gt; Sparse PCA forces components to use only the relevant variables, not a smear across 127 series.\n\n\n\n\n\nStandard PCA components ≈ dense, uninterpretable mixtures\nSparse PCA components ≈ targeted clusters of interpretable economic variables\n\n\n\n\n\nTake FRED-MD’s 127 macro series.\nIf you run standard PCA:\n\nPC1 loads on 100+ variables.\nPC2 loads on another 80+.\nInterpreting them is basically hopeless.\n\nSparse PCA, with lasso constraints:\n\nallows PC1 to load only on inflation-related variables\nPC2 to load only on housing-related variables\nPC3 on credit spreads\nPC4 on yields\nPC5 on production\netc.\n\nThis resembles the Stock–Watson macro factor model, but with sparsity for interpretability.\n\n\n\n\nSuppose sparse PCA gives:\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCPI YoY\n0.71\n\n\nCore CPI YoY\n0.68\n\n\nPCE Deflator\n0.66\n\n\nPCE core\n0.64\n\n\nAll other 123 variables\n0\n\n\n\n-&gt; You immediately label PC1 as “inflation factor”.\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nHousing starts\n0.62\n\n\nBuilding permits\n0.59\n\n\nNew homes sold\n0.61\n\n\nMortgage applications\n0.58\n\n\nEverything else\n0\n\n\n\n-&gt; PC2 = “housing & construction cycle”\n\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCorporate spreads (BAA–AAA)\n0.72\n\n\nHigh yield spread\n0.69\n\n\nCommercial paper spread\n0.64\n\n\nOthers\n0\n\n\n\n-&gt; PC3 = “credit stress”\n…and so on.\n\n\n\nSparse PCA essentially performs dimension reduction + variable selection simultaneously.\n\n\n\n\nSparse PCA encourages:\n\ngrouping variables that co-move strongly\ndropping variables unrelated to the theme\nchoosing a small number of representative series\n\nGiven economic data naturally clusters (inflation variables co-move, housing variables co-move), sparse PCA isolates these clusters.\nThis is similar to economic intuition:\n\ninflationary variables form a single latent factor\nspreads form a financial stress factor\nyields form a term-structure factor\nemployment variables form a labor factor\n\nSparse PCA “discovers” these clusters automatically.\n\n\n\n\nSparse PCA → interpretable macro drivers → better regime classification → better factor conditioning.\nEspecially when constructing a macro liquidity index or financial conditions index.\nSparse PCA gives:\n\n\n\nComponent\nInterpretation\n\n\n\n\nPC1\nLiquidity / money conditions\n\n\nPC2\nCredit spreads\n\n\nPC3\nHousing activity\n\n\nPC4\nYield curve / rates\n\n\nPC5\nEmployment\n\n\nPC6\nProduction\n\n\nPC7\nIncome\n\n\nPC8\nMarket stress\n\n\n\nThese are exactly the components McCracken & Ng (2016) find in FRED-MD.\n\n\n\n\n\n\n\n\nDense loadings\nHard to interpret\nPCs are linear mush\n\n\n\n\n\nForces many loadings to zero\nEach PC focuses on a small cluster of variables\nBecomes interpretable as a “macro theme”\nMatches how economists think about the business cycle\nPerfect for regime classification & factor research\n\n\n\n\n\nEconomic data naturally clusters into themes\nSparse PCA forces components to choose only the strongest cluster\nThis matches known macro factors (inflation, employment, credit, etc.)\n\n\n# !pip install pandas pandas_datareader numpy scikit-learn hmmlearn matplotlib\n\nimport pandas as pd\nimport numpy as np\nfrom pandas_datareader import data as pdr\n\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom hmmlearn.hmm import GaussianHMM\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#config-date-range-series",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#config-date-range-series",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "start_date = \"1990-01-01\"\nend_date   = \"2025-12-31\"\n\n# --- FRED series codes ---\nFRED_SERIES = {\n    \"M2SL\":   \"M2SL\",      # M2 money stock (monthly, SA)\n    \"FED_BAL\":\"WALCL\",     # Fed balance sheet total assets (weekly)\n    \"TB3M\":   \"TB3MS\",     # 3-Month T-Bill rate (monthly)\n    \"DGS10\":  \"DGS10\",     # 10-Year Treasury yield (daily -&gt; resample monthly)\n    \"BAA\":    \"BAA\",       # Moody's Baa corporate yield (monthly)\n    \"AAA\":    \"AAA\",       # Moody's Aaa corporate yield (monthly)\n    \"CPI\":    \"CPIAUCSL\",  # CPI index (monthly, SA)\n    \"GDP\":    \"GDP\",       # Nominal GDP (quarterly, SAAR)\n}\n\n# For equity factors: Fama-French via pandas_datareader (famafrench)\nFF_FACTORS_DATASET = \"F-F_Research_Data_5_Factors_2x3\""
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#download-macro-liquidity-variables-from-fred",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#download-macro-liquidity-variables-from-fred",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "def download_fred_series(series_dict, start, end):\n    \"\"\"\n    Download FRED series into a single monthly DataFrame.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Mapping logical_name -&gt; FRED code.\n    \"\"\"\n    dfs = []\n    for name, code in series_dict.items():\n        print(f\"Downloading {name} ({code}) from FRED...\")\n        s = pdr.DataReader(code, \"fred\", start, end)\n        s = s.rename(columns={code: name})\n        dfs.append(s)\n\n    df = pd.concat(dfs, axis=1)\n    # Ensure monthly freq by end-of-month sampling\n    df = df.resample(\"M\").last()\n    return df\n\nmacro_raw = download_fred_series(FRED_SERIES, start_date, end_date)\nmacro_raw.head()\n\nDownloading M2SL (M2SL) from FRED...\nDownloading FED_BAL (WALCL) from FRED...\nDownloading TB3M (TB3MS) from FRED...\nDownloading DGS10 (DGS10) from FRED...\nDownloading BAA (BAA) from FRED...\nDownloading AAA (AAA) from FRED...\nDownloading CPI (CPIAUCSL) from FRED...\nDownloading GDP (GDP) from FRED...\n\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n1990-01-31\n3166.8\nNaN\n7.64\n8.43\n9.94\n8.99\n127.5\n5872.701\n\n\n1990-02-28\n3179.2\nNaN\n7.74\n8.51\n10.14\n9.22\n128.0\nNaN\n\n\n1990-03-31\n3190.1\nNaN\n7.90\n8.65\n10.21\n9.37\n128.6\nNaN\n\n\n1990-04-30\n3201.6\nNaN\n7.77\n9.04\n10.30\n9.46\n128.9\n5960.028\n\n\n1990-05-31\n3200.6\nNaN\n7.74\n8.60\n10.41\n9.47\n129.1\nNaN\n\n\n\n\n\n\n\n\nmacro_raw.tail()\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\nNaN\n4.00\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n$ x_t =\n\\[\\begin{bmatrix}\nx_{1,t} \\\nx_{2,t} \\\n\\vdots \\\nx_{J,t}\n\\end{bmatrix}\\]\n\nx_{J,t} \\end{bmatrix} ^J $\n\nMoney & balance sheet growth: \\(\\Delta \\log(\\cdot)\\)\nTerm spread: \\(y_{10} - i_{3m}\\)\nReal short rate: \\(i_{3m} - \\pi_{\\text{year-on-year}}\\)\nCredit spread: \\(\\text{BAA} - \\text{AAA}\\)\netc.​\n\n\ndef build_liquidity_proxies(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Growth of M2 and Fed balance sheet\n    df[\"dlog_M2\"] = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"] = np.log(df[\"FED_BAL\"]).diff()\n\n    # 2. Term structure: 10Y - 3M\n    df[\"term_spread\"] = df[\"DGS10\"] - df[\"TB3M\"]\n\n    # 3. Year-on-year inflation (CPI yoy)\n    df[\"infl_yoy\"] = np.log(df[\"CPI\"]).diff(12)\n\n    # 4. Real short rate: nominal 3M - inflation\n    df[\"real_rate\"] = df[\"TB3M\"] - (100 * df[\"infl_yoy\"])  # TB3M in %, infl_yoy in log -&gt; approx*100\n\n    # 5. Credit spread: BAA - AAA\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # Drop rows with NaNs from diff(12) etc.\n    proxies = df[[\"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\",\n                  \"real_rate\", \"credit_spread\"]].dropna()\n\n    return proxies\n\nliquidity_proxies = build_liquidity_proxies(macro_raw)\n\n\nliquidity_proxies.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n\n\n2025-07-31\n0.003930\n-0.002950\n0.12\n1.554846\n0.65\n\n\n2025-08-31\n0.003607\n-0.005918\n0.11\n1.223147\n0.65\n\n\n2025-09-30\n0.004698\n0.000759\n0.24\n0.942084\n0.62\n\n\n\n\n\n\n\n\n\n\nWe want higher \\(z_{j,t}\\) – to mean easier liquidity.\n\nGrowth of M2 / Fed balance sheet: already “easier = higher value” -&gt; keeping sign\nTerm spread: steeper (more positive) often associated with easier conditions -&gt; keeping sign\nReal rate: easier liquidity when real rates are low -&gt; flipping sign\nCredit spread: easier liquidity when spreads are tight (low) -&gt; flipping sign\n\n\ndef standardize_and_signflip(proxies: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Standardize each column and flip signs so that higher z_{j,t}\n    always corresponds to easier liquidity.\n    \"\"\"\n    z = proxies.copy()\n\n    # Standardize\n    scaler = StandardScaler()\n    z_vals = scaler.fit_transform(z)\n    z = pd.DataFrame(z_vals, index=z.index, columns=z.columns)\n\n    # Sign flips so \"higher\" = easier liquidity\n    # dlog_M2: easier when higher -&gt; keep\n    # dlog_FED_BAL: easier when higher -&gt; keep\n    # term_spread: easier when steeper -&gt; keep (or adjust, depending on your view)\n    # real_rate: easier when more negative -&gt; flip sign\n    # credit_spread: easier when lower -&gt; flip sign\n\n    sign_flips = {\n        \"dlog_M2\": +1,\n        \"dlog_FED_BAL\": +1,\n        \"term_spread\": +1,\n        \"real_rate\": -1,\n        \"credit_spread\": -1,\n    }\n\n    for col, sgn in sign_flips.items():\n        z[col] = sgn * z[col]\n\n    return z\n\nz_t = standardize_and_signflip(liquidity_proxies)\n\n\nz_t.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n-0.377813\n-0.314009\n-0.968292\n-1.362581\n0.639849\n\n\n2025-06-30\n0.052579\n-0.226920\n-1.085244\n-1.211510\n0.786085\n\n\n2025-07-31\n-0.163048\n-0.257126\n-0.999479\n-1.193172\n0.883576\n\n\n2025-08-31\n-0.215601\n-0.326453\n-1.007276\n-1.031235\n0.883576\n\n\n2025-09-30\n-0.038105\n-0.170518\n-0.905917\n-0.894019\n0.956694\n\n\n\n\n\n\n\n\n\n\n\npd.Series(z_t.index).describe()\n\ncount                              273\nmean     2014-05-31 08:42:11.868131840\nmin                2003-01-31 00:00:00\n25%                2008-09-30 00:00:00\n50%                2014-05-31 00:00:00\n75%                2020-01-31 00:00:00\nmax                2025-09-30 00:00:00\nName: DATE, dtype: object\n\n\n\nz_t.values\n\narray([[ 0.11561096, -0.81061174,  1.11346119,  0.322683  , -0.40817503],\n       [ 0.20510346,  0.11034372,  0.8873532 ,  0.50696464, -0.23756644],\n       [-0.24145055, -0.0901311 ,  1.01210244,  0.46925598, -0.11570316],\n       ...,\n       [-0.163048  , -0.25712611, -0.999479  , -1.1931718 ,  0.88357574],\n       [-0.2156006 , -0.32645261, -1.00727583, -1.03123533,  0.88357574],\n       [-0.03810504, -0.17051847, -0.90591708, -0.89401934,  0.95669371]])\n\n\n\ndef build_sparse_pca_liquidity_index(z_df: pd.DataFrame,\n                                     alpha: float = 1.0,\n                                     random_state: int = 42) -&gt; pd.Series:\n    \"\"\"\n    Apply SparsePCA with 1 component to z_{j,t} to obtain L_t.\n    \"\"\"\n    spca = SparsePCA(\n        n_components=1,\n        alpha=alpha,\n        random_state=random_state\n    )\n\n    # Fit on standardized proxies\n    L_scores = spca.fit_transform(z_df.values)  # shape (T, 1)\n    L = pd.Series(L_scores.flatten(), index=z_df.index, name=\"L\")\n\n    # Optional: enforce that L_t is positively correlated with dlog_M2\n    # corr = np.corrcoef(L, z_df[\"dlog_FED_BAL\"])[0, 1]\n    # if corr &lt; 0:\n    #     L = -L\n\n    print(\"SparsePCA components (loadings):\")\n    for coef, col in zip(spca.components_[0], z_df.columns):\n        print(f\"  {col}: {coef:.3f}\")\n\n    return L\n\nL_t = build_sparse_pca_liquidity_index(z_t, alpha=0.5)\n\nSparsePCA components (loadings):\n  dlog_M2: 0.430\n  dlog_FED_BAL: 0.526\n  term_spread: 0.492\n  real_rate: 0.389\n  credit_spread: -0.381\n\n\n\nL_t.head()\n\nDATE\n2003-01-31    0.447153\n2003-02-28    0.861556\n2003-03-31    0.567234\n2003-04-30    0.979006\n2003-05-31    0.696500\nFreq: M, Name: L, dtype: float64\n\n\n\nL_t.plot(title=\"Sparse PCA Liquidity Index L(t)\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n​Fit a Gaussian HMM on the liquidity index. 2 or 3 regimes ?\n\nfrom hmmlearn.hmm import GaussianHMM\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\ndef fit_hmm_on_liquidity(\n    L: pd.Series,\n    n_states: int = 3,\n    covariance_type: str = \"full\",\n    random_state: int = 42,\n    standardize: bool = True\n):\n    \"\"\"\n    Fit a Gaussian HMM on L(t) and return the model and a DataFrame\n    with inferred regimes and posterior probabilities.\n\n    Parameters\n    ----------\n    L : pd.Series\n        Liquidity index (indexed by date).\n    n_states : int\n        Number of hidden states (regimes).\n    standardize : bool\n        If True, standardize L before fitting the HMM.\n\n    Returns\n    -------\n    model : GaussianHMM\n    df_regime : pd.DataFrame\n        Columns: L, state, p_state_k, state_label\n    \"\"\"\n\n    # 1) Prepare X\n    X = L.values.reshape(-1, 1)\n\n    scaler = None\n    if standardize:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n    # 2) Fit HMM\n    model = GaussianHMM(\n        n_components=n_states,\n        covariance_type=covariance_type,\n        random_state=random_state,\n        n_iter=500\n    )\n    model.fit(X)\n\n    hidden_states = model.predict(X)\n    post_probs = model.predict_proba(X)  # T x n_states\n\n    # 3) Build output DataFrame on original index\n    df_regime = pd.DataFrame(\n        {\"L\": L, \"state\": hidden_states},\n        index=L.index\n    )\n    for k in range(n_states):\n        df_regime[f\"p_state_{k}\"] = post_probs[:, k]\n\n    # 4) Use state means to order regimes\n    means = pd.Series(model.means_.flatten(), index=range(n_states))\n\n    print(\"HMM state means (unsorted, in fitted scale):\")\n    for k, m in means.items():\n        print(f\"  state {k}: mean L = {m:.3f}\")\n\n    # Order states from low liquidity to high liquidity\n    ordering = means.sort_values().index.tolist()\n\n    state_label_map = {}\n    if n_states == 3:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"Neutral\"\n        state_label_map[ordering[2]] = \"High\"\n    elif n_states == 2:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"High\"\n    else:\n        # Generic labels if you ever use more states\n        for i, s in enumerate(ordering):\n            state_label_map[s] = f\"Regime_{i}\"\n\n    df_regime[\"state_label\"] = df_regime[\"state\"].map(state_label_map)\n\n    return model, df_regime\n\n\nn_states = 3\nmodel, regimes_df = fit_hmm_on_liquidity(L_t,n_states)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.160\n  state 1: mean L = -0.106\n  state 2: mean L = 2.718\n\n\n\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# -----------------------------\n# TOP PANEL — L(t) + shading\n# -----------------------------\nax[0].plot(regimes_df.index, regimes_df[\"L\"], color=\"black\", lw=1.2)\nax[0].set_title(\"Sparse PCA Liquidity Index L(t) with Regime Shading\")\n\n# Define plain colors for each regime\nregime_colors = {\n    \"Tight\":   \"green\",\n    \"Neutral\": \"gray\",\n    \"High\":    \"crimson\",\n}\n\nshade_alpha = 0.2  # semi-opaque\n\nfor label, color in regime_colors.items():\n    mask = regimes_df[\"state_label\"] == label\n\n    in_region = False\n    start = None\n\n    for i in range(len(mask)):\n        if mask.iloc[i] and not in_region:\n            in_region = True\n            start = regimes_df.index[i]\n\n        elif not mask.iloc[i] and in_region:\n            in_region = False\n            end = regimes_df.index[i]\n            ax[0].axvspan(start, end, color=color, alpha=shade_alpha, linewidth=0)\n\n    # If region extends to the end of the sample\n    if in_region:\n        ax[0].axvspan(start, regimes_df.index[-1], color=color, alpha=shade_alpha, linewidth=0)\n\n# Legend for regimes (shaded colors)\nfrom matplotlib.lines import Line2D\nlegend_patches = [\n    Line2D([0], [0], color=\"green\",   lw=6, alpha=shade_alpha, label=\"Tight\"),\n    Line2D([0], [0], color=\"gray\",    lw=6, alpha=shade_alpha, label=\"Neutral\"),\n    Line2D([0], [0], color=\"crimson\", lw=6, alpha=shade_alpha, label=\"High\"),\n]\nax[0].legend(handles=legend_patches, loc=\"upper left\")\n\n# -----------------------------\n# BOTTOM PANEL — High regime probability\n# -----------------------------\nif \"p_state_0\" in regimes_df.columns:\n    high_state_id = regimes_df.groupby(\"state_label\")[\"state\"].first()[\"High\"]\n    ax[1].plot(regimes_df.index,\n               regimes_df[f\"p_state_{high_state_id}\"],\n               color=\"crimson\", lw=1.5)\n    ax[1].set_title(\"Posterior Probability of High Liquidity Regime\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\nThese five factors extend the original 3-factor model to better explain cross-sectional stock returns.\nThey are:\n\nRm–Rf — Market excess return\nSMB — Size (Small Minus Big)\nHML — Value (High Book/Market minus Low)\nRMW — Profitability (Robust Minus Weak)\nCMA — Investment (Conservative Minus Aggressive)\n\nLet’s break each down precisely.\n\n\n\n\\[\nR_{m}-R_{f}\n\\] The return of the broad market minus the risk-free rate.\n\nPositive → market went up more than cash.\nNegative → market underperformed cash/T-bills.\n\nIn above table:\n\nLast 12 months: +17.54% → very strong bull market year.\nLast 3 months: +7.40% → strong quarter.\nOctober 2025: +1.95% → moderate positive month.\n\n\n\n\n\n\\[\n\\text{SMB} = R_{\\text{small}} - R_{\\text{big}}\n\\]\n\nPositive → small caps outperform large caps.\nNegative → large caps outperform small caps.\n\nIn above table:\n\nLast 12 months: –10.71% → a massive large-cap dominance year.\nThis is consistent with mega-cap tech leadership.\n\n\n\n\n\n\\[\n\\text{HML} = R_{\\text{value}} - R_{\\text{growth}}\n\\]\n\nPositive → value outperforms growth.\nNegative → growth outperforms value.\n\nIn above table:\n\nLast 12 months: –2.36% → growth beat value.\nOctober 2025: –3.10% → especially growth-heavy month.\n\nThis aligns with liquidity-driven growth leadership.\n\n\n\n\n\\[\n\\text{RMW} = R_{\\text{robust}} - R_{\\text{weak}}\n\\]\nRobust = high operating profitability. Weak = low profitability.\n\nPositive → profitable companies outperform weak ones.\nNegative → weak/low-profit companies outperform.\n\nIn above table:\n\nLast 12 months: –13.60% → very unusual.\nThis means unprofitable firms outperformed profitable ones over the year.\n\nThat usually happens in:\n\nearly speculative bubbles\nliquidity-driven rallies\nretail-led tech/small-cap frenzies\nAI/futuristic narrative periods\n\n\n\n\n\n\\[\n\\text{CMA} = R_{\\text{conservative}} - R_{\\text{aggressive}}\n\\]\n\nConservative = firms investing slowly\nAggressive = firms aggressively expanding assets\n\nHigh investment → lower expected returns historically (consistent with q-theory: empire-building destroys value).\n\nPositive → conservative firms outperform heavy spenders.\nNegative → aggressive investment firms outperform.\n\nIn above table:\n\nLast 12 months: –10.46%\nLast 3 months: –4.47%\nOctober 2025: –4.03%\n\nInterpretation: Aggressively investing companies — think AI, R&D, biotech, high-growth tech — massively outperformed low-investment firms.\n\n\n\n\n\n\n\n\nFactor\nSign\nInterpretation\n\n\n\n\nRm–Rf: +\nStrong bull market\n\n\n\nSMB: –\nMega-caps beat small caps massively\n\n\n\nHML: –\nGrowth beat value\n\n\n\nRMW: –\nLow-profit firms beat profitable firms\n\n\n\nCMA: –\nAggressive-investment firms beat conservative ones\n\n\n\n\nThis is the textbook signature of a liquidity-driven growth/tech momentum regime, almost identical to:\n\n1998–1999 Dotcom\n2019–2021 QE wave\n2023–2024 AI mega-cap boom\n\nIt means:\n\n\n\n\nlarge\ngrowth\nunprofitable\nhigh-spending\nhigh-duration tech/AI/future-theme stocks\n\n\n\n\nThis is exactly the kind of environment where:\n\nValue fails\nSmall cap fails\nProfitability fails\nInvestment works negatively\nGrowth dominates\nMega-caps dominate\n\n\ndef download_ff_factors(start, end):\n    \"\"\"\n    Download monthly Fama-French 5 factors (2x3) using pandas_datareader.\n    \"\"\"\n    print(\"Downloading Fama-French factors (famafrench)...\")\n    ff_raw = pdr.DataReader(\n        name=FF_FACTORS_DATASET,\n        data_source=\"famafrench\",\n        start=start,\n        end=end\n    )[0]  # table [0] contains the data\n\n    ff = (ff_raw\n          .divide(100)  # convert from % to decimal\n          .reset_index(names=\"date\")\n          .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n          .rename(str.lower, axis=\"columns\")\n          .rename(columns={\"mkt-rf\": \"mkt_excess\"}))\n\n    ff = ff.set_index(\"date\")\n    return ff\n\nff_factors = download_ff_factors(start_date, end_date)\n\nDownloading Fama-French factors (famafrench)...\n\n\nFutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n&lt;ipython-input-69-55f7e03990a5&gt;:6: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n\n\n\nff_factors.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-06-01\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-07-01\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-01\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-09-01\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-10-01\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Fix Fama–French dates (shift one month backward)\nff_adj = ff_factors.copy()\n\n# FF data comes labeled as YYYY-MM-01 meaning return for previous month.\n# So shift index back to previous month-end.\nff_adj.index = (ff_adj.index.to_period(\"M\") - 1).to_timestamp(\"M\")\n\nff_adj.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined = reg.join(ff_adj, how=\"inner\")\n\n\ncombined.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-12-31\n-1.651021\n1\n0.162158\n0.837762\n0.000079\nTight\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n-1.496425\n0\n0.837514\n0.162233\n0.000253\nTight\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n-1.643180\n1\n0.162188\n0.837738\n0.000075\nTight\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n-1.627975\n0\n0.837473\n0.162258\n0.000269\nTight\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n-1.400432\n1\n0.162220\n0.837714\n0.000067\nTight\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n-1.561611\n0\n0.837459\n0.162284\n0.000258\nTight\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n-1.387057\n1\n0.162245\n0.837689\n0.000066\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n-1.482573\n0\n0.837437\n0.162308\n0.000255\nTight\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n-1.482572\n1\n0.162251\n0.837658\n0.000091\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n-1.251226\n0\n0.836442\n0.162316\n0.001242\nTight\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n\ndf2 = combined.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by *lagged* regime:\")\nprint(means_by_regime)\n\nMean factor returns by *lagged* regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nTight            -0.005439 -0.005360  0.000540 -0.003311\nNeutral           0.003200  0.002938  0.001051 -0.000056\nHigh              0.003327  0.001228  0.005127  0.003575\n\n\n\n\n\nThis is reverse of what you’d expect. In a tight liquidity regime, \\(R^{smb}\\) and \\(R^{hml}\\) should be deeply positive. You’d expect small under-valued companies to outperform big companies with premium valuation.\nLet’s go back to the \\(L_t\\) and understand what’s going on\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L_t.index,\n    y=L_t.values,\n    hue=regimes_df[\"state\"],\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution of L(t) by HMM State\")\nax[1].set_xlabel(\"L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the KDE (bottom panel):\n\nStates 0 (green) and 1 (yellow) heavily overlap\nOnly state 2 (red) is clearly separated — the “high liquidity spike”\nThis corresponds to crisis/QE events (2008, 2020), where liquidity jumps dramatically\n\nSo the natural clustering is:\n\nOne large cluster (normal-tight-neutral combined)\nOne rare extreme spike cluster\n\nYour HMM is being forced to split the unimodal bulk distribution into two states, resulting in:\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\nI initially thought alpha=0.5 in SparsePCA might be the cause. But the plot shows:\n\nL(t) has a healthy dynamic range across time (≈ −2.5 to +12)\nThe high liquidity cluster is clear and RARE\nThe rest of L(t) is genuinely one blob\n\nIf Sparse PCA was “overshrinking,” L(t) would be compressed; but it is not.\n\n\n\n\nIs Liquidity is fundamentally spiky ?\nWhy ?\nAm I defining Liquidity incorrectly ?\n\nFed/Treasury continuously debased currency throughout 2008-2019 period. Liquidity injection (bailing out banks during GFC/2008, ZIRP era, M2 stimulius of 2019 are visible ones). How would I capture this events as proxy of “liquidity” ?\n\n\nRight now your \\(L_t\\) is basically a short-horizon “flow” liquidity shock index.\n\n\nWhereas I’m looking for slow, structural debasement / regime of easy money (2008–2019, QE, ZIRP, etc.).\n\n\nThose are not the same object.\n\nBecause I use 1-month growth rates \\(\\Delta \\log(\\cdot)\\), my index reacts to spikes / changes, not the level of the monetary stock.\nI need to factor in more factors to the liquidity index such as -\n\nLevel of money / balance sheet relative to trend / GDP, not just monthly change\nProlonged ZIRP / negative real rate period\nExcess liquidity over economic activity\n\n\n\n\n\n\nExamples (all can be pulled from FRED):\nLog level vs pre-2008 trend\nLet \\(m_t = \\log M2_t\\). Fit a linear trend on a pre-QE baseline (say 1985–2007):\n\\(m_t \\approx a + bt \\quad (t \\le 2007)\\)\nThen define excess money stock:\n\\(EM_t = m_t - (a + bt)\\)\nAfter 2008, \\(EM_t\\) becomes increasingly positive if M2 grows above its historical trend.\nSimilarly for the Fed Balance Sheet, let \\(b_t = \\log(\\text{FedBal}_t)\\):\n\\(EB_t = b_t - (\\alpha + \\beta t)\\) (pre-2007 trend)\n\nExcess liquidity versus real economy\nUse real GDP series (e.g., GDPC1) and define:\n\\(EL_t = \\log M2_t - \\log GDP_t\\)\nOr measure multi-year excess growth:\n\\(EL_t(3y) = \\log M2_t - \\log M2_{t-36} - (\\log GDP_t - \\log GDP_{t-36})\\)\n\nZIRP / negative real-rate indicator\n\\(D_t^{ZIRP} = 1(r_t^{real} &lt; 0)\\)\nFrom 2009–2015, this should be mostly 1.\n\n\n\n\nInstead of only:\n\\(x_t = \\{ \\Delta \\log M2_t,\\; \\Delta \\log FedBal_t,\\; TS_t,\\; r_t^{real},\\; CS_t \\}\\)\nLet’s expand to:\n\\(x_t^{aug} = \\{\n\\Delta \\log M2_t,\\;\n\\Delta \\log FedBal_t,\\;\nTS_t,\\;\nr_t^{real},\\;\nCS_t,\\;\nEM_t,\\;\nEB_t,\\;\nEL_t,\\;\nEL_t(3y),\\;\nD_t^{ZIRP}\n\\}\\)\nThen standardize and perform PCA / SparsePCA on this.\n\nPC1 (or a combination) loading heavily on \\(EM\\), \\(EB\\), \\(EL\\), \\(D_t^{ZIRP}\\) → structural easy-money regime\n\nPC2 (or your current PC1) loading on short-horizon changes → flow / shock liquidity\n\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\nNaN\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\nNaN\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29511.664\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\nNaN\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\nNaN\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29825.182\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\nNaN\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\nNaN\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n30042.113\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\nNaN\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\nNaN\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30485.729\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\nNaN\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\nNaN\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# 1. Start from the original GDP values only (drop NaNs)\ngdp_series = macro_raw[\"GDP\"].dropna()\n\n# 2. Collapse to unique quarter-end values\n#    - If GDP is timestamped at quarter *start* (e.g. 2025-04-01),\n#      this will move it to the quarter end (2025-06-30) and give unique labels.\ngdp_q = gdp_series.resample(\"Q\").last()   # quarterly, at quarter-end (e.g. 2025-03-31, 2025-06-30, ...)\n\n# 3. Upsample to monthly and forward-fill within the quarter\ngdp_m = gdp_q.resample(\"M\").ffill()\n\n# 4. Assign back into macro_raw, aligning on the monthly index\nmacro_raw[\"GDP\"] = gdp_m\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\n28708.161\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\n29147.044\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29147.044\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\n29147.044\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\n29511.664\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29511.664\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\n29511.664\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\n29825.182\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n29825.182\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\n29825.182\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\n30042.113\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30042.113\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\n30042.113\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\n30485.729\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndef build_liquidity_proxies_augmented(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Flow proxies\n    df[\"dlog_M2\"]       = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"]  = np.log(df[\"FED_BAL\"]).diff()\n    df[\"term_spread\"]   = df[\"DGS10\"] - df[\"TB3M\"]\n    df[\"infl_yoy\"]      = np.log(df[\"CPI\"]).diff(12)\n    df[\"real_rate\"]     = df[\"TB3M\"] - 100 * df[\"infl_yoy\"]\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # 2. Levels\n    df[\"log_M2\"]      = np.log(df[\"M2SL\"])\n    df[\"log_FED_BAL\"] = np.log(df[\"FED_BAL\"])\n\n    # Use data up to 2007-12-31 to fit trends\n    pre = df.loc[: \"2007-12-31\"].copy()\n\n    # --- Trend for M2 ---\n    pre_m2 = pre[\"log_M2\"].dropna()\n    t_m2   = np.arange(len(pre_m2))\n    coefs_M2 = np.polyfit(t_m2, pre_m2.values, deg=1)\n\n    t_full = np.arange(len(df))\n    trend_M2 = np.polyval(coefs_M2, t_full)\n    df[\"EM\"] = df[\"log_M2\"] - trend_M2\n\n    # --- Trend for Fed balance sheet ---\n    pre_fb = pre[\"log_FED_BAL\"].dropna()\n    t_fb   = np.arange(len(pre_fb))\n    coefs_FB = np.polyfit(t_fb, pre_fb.values, deg=1)\n\n    trend_FB = np.polyval(coefs_FB, t_full)\n    df[\"EB\"] = df[\"log_FED_BAL\"] - trend_FB\n\n    # 3. Excess liquidity vs GDP over 3y\n    df[\"log_GDP\"] = np.log(df[\"GDP\"])\n    df[\"EL_3y\"] = (df[\"log_M2\"] - df[\"log_M2\"].shift(36)) - \\\n                  (df[\"log_GDP\"] - df[\"log_GDP\"].shift(36))\n\n    # 4. ZIRP dummy\n    df[\"ZIRP_dummy\"] = (df[\"real_rate\"] &lt; 0).astype(int)\n\n    # 5. Build augmented proxy matrix\n    cols_aug = [\n        \"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\", \"real_rate\", \"credit_spread\",\n        \"EM\", \"EB\", \"EL_3y\", \"ZIRP_dummy\"\n    ]\n\n    return df[cols_aug].dropna()\n\n\nliquidity_proxies_aug = build_liquidity_proxies_augmented(macro_raw)\n\n\nliquidity_proxies_aug.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n0.004077\n-0.006080\n-0.91\n2.316574\n0.74\n0.204864\n0.811903\n-0.194283\n0\n\n\n2024-10-31\n0.002378\n-0.009447\n-0.23\n1.971101\n0.68\n0.202921\n0.798940\n-0.199645\n0\n\n\n2024-11-30\n0.004673\n-0.015569\n-0.24\n1.742012\n0.64\n0.203273\n0.779855\n-0.203197\n0\n\n\n2024-12-31\n0.000775\n-0.002781\n0.31\n1.438113\n0.60\n0.199728\n0.773558\n-0.186129\n0\n\n\n2025-01-31\n0.003164\n-0.009892\n0.37\n1.254690\n0.62\n0.198572\n0.760151\n-0.188367\n0\n\n\n2025-02-28\n0.003354\n-0.007668\n0.02\n1.444603\n0.60\n0.197605\n0.748966\n-0.189520\n0\n\n\n2025-03-31\n0.003338\n-0.003828\n0.03\n1.822893\n0.64\n0.196623\n0.741623\n-0.177663\n0\n\n\n2025-04-30\n0.006165\n-0.004606\n-0.04\n1.903069\n0.73\n0.198467\n0.733501\n-0.172818\n0\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n0.196757\n0.724600\n-0.167478\n0\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n0.197692\n0.719428\n-0.150768\n0\n\n\n\n\n\n\n\n\nz1_t = standardize_and_signflip(liquidity_proxies_aug)\nz1_t.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n-0.139814\n-0.331324\n-1.812889\n-1.577541\n0.673692\n0.888682\n0.786734\n-2.353880\n-1.527525\n\n\n2024-10-31\n-0.414988\n-0.409551\n-1.282837\n-1.408779\n0.819794\n0.868074\n0.764262\n-2.404333\n-1.527525\n\n\n2024-11-30\n-0.043456\n-0.551814\n-1.290631\n-1.296871\n0.917195\n0.871811\n0.731177\n-2.437752\n-1.527525\n\n\n2024-12-31\n-0.674370\n-0.254667\n-0.861913\n-1.148418\n1.014596\n0.834206\n0.720261\n-2.277159\n-1.527525\n\n\n2025-01-31\n-0.287635\n-0.419885\n-0.815143\n-1.058817\n0.965896\n0.821943\n0.697018\n-2.298214\n-1.527525\n\n\n2025-02-28\n-0.256970\n-0.368230\n-1.087964\n-1.151588\n1.014596\n0.811689\n0.677630\n-2.309059\n-1.527525\n\n\n2025-03-31\n-0.259533\n-0.278983\n-1.080169\n-1.336381\n0.917195\n0.801267\n0.664900\n-2.197496\n-1.527525\n\n\n2025-04-30\n0.198084\n-0.297077\n-1.134734\n-1.375546\n0.698042\n0.820833\n0.650819\n-2.151913\n-1.527525\n\n\n2025-05-31\n-0.377318\n-0.315174\n-0.978836\n-1.374952\n0.649342\n0.802693\n0.635389\n-2.101665\n-1.527525\n\n\n2025-06-30\n0.050761\n-0.228533\n-1.095759\n-1.223790\n0.795443\n0.812604\n0.626423\n-1.944440\n-1.527525\n\n\n\n\n\n\n\n\nL1_t = build_sparse_pca_liquidity_index(z1_t, alpha=0.5)\nL1_t.tail(10)\n\nSparsePCA components (loadings):\n  dlog_M2: -0.132\n  dlog_FED_BAL: -0.159\n  term_spread: -0.389\n  real_rate: -0.515\n  credit_spread: 0.046\n  EM: -0.114\n  EB: -0.134\n  EL_3y: -0.518\n  ZIRP_dummy: -0.490\n\n\nDATE\n2024-09-30    3.347080\n2024-10-31    3.143188\n2024-11-30    3.088484\n2024-12-31    2.811365\n2025-01-31    2.736110\n2025-02-28    2.887801\n2025-03-31    2.906425\n2025-04-30    2.856634\n2025-05-31    2.850481\n2025-06-30    2.674924\nFreq: M, Name: L, dtype: float64\n\n\n\nL1_t.plot(title=\"Sparse PCA Liquidity Index L(t) - Augmented w/ Flow & Stock Indicators\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel, regimes_aug_df = fit_hmm_on_liquidity(L1_t,n_states=2)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.484\n  state 1: mean L = 1.536\n\n\n\nregime_aug_df.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_aug_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined_aug = reg.join(ff_adj, how=\"inner\")\n\n\ncombined_aug.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n-0.0100\n-0.0089\n0.0086\n-0.0148\n0.0098\n0.0039\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n0.0649\n0.0459\n0.0015\n-0.0231\n-0.0205\n0.0040\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n-0.0315\n-0.0383\n-0.0300\n0.0189\n-0.0121\n0.0037\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\ndf2 = combined_aug.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by Liquidity Regime:\")\nprint(means_by_regime)\n\nMean factor returns by Liquidity Regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nHigh             -0.004519 -0.001700  0.001564 -0.003006\nTight             0.001896  0.000038  0.002920  0.001302"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#yes",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#yes",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Factor\nHigh Liquidity\nTight Liquidity\nInterpretation\n\n\n\n\nSMB\nNegative\nPositive\nSmall caps thrive when liquidity tightens\n\n\nHML\nNegative\nSlightly positive\nValue improves in tight regimes\n\n\nRMW\nPositive\nMore positive\nProfitability is the strongest cross-regime performer\n\n\nCMA\nNegative\nPositive\nConservative investment becomes favored when liquidity tightens\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L1_t.index,\n    y=L1_t.values,\n    hue=regimes_aug_df[\"state\"],\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Augmented Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"Augmented L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_aug_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution/KDE of Augmented L(t) by HMM State\")\nax[1].set_xlabel(\"Augmented L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn KDE of augmented liquidity index \\(L_t^{aug}\\), clusters are clearly separated, unlike last one\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndef parse_yyyy_mm_to_date(s: str):\n    \"\"\"\n    Convert 'YYYY.MM' to a proper datetime.\n    Handles the special case where October appears as YYYY.1 instead of YYYY.10.\n    \"\"\"\n    s = str(s).strip()\n    parts = s.split(\".\")\n    if len(parts) != 2:\n        raise ValueError(f\"Unexpected date format: {s}\")\n    \n    year = int(parts[0])\n    mm = parts[1]\n\n    # Fix: \"1\" should be \"10\" (October)\n    if mm == \"1\":\n        month = 10\n    else:\n        month = int(mm)\n\n    return pd.Timestamp(year=year, month=month, day=1)\n\n# --- Load CAPE CSV ---\nsp500_cape_raw = pd.read_csv(\"data/sp500-cape-1990-2025.csv\")\n\n# Clean + parse\nsp500_cape_raw[\"date\"] = sp500_cape_raw[\"Date\"].apply(parse_yyyy_mm_to_date)\n\nsp500_cape_raw = (\n    sp500_cape_raw[[\"date\", \"SP500\", \"CAPE\"]]\n    .set_index(\"date\")\n    .sort_index()\n)\n\n# Convert to month-end aligned CAPE series\nsp500_cape_m = sp500_cape_raw.resample(\"M\").last()\nsp500_cape_m.rename(columns={\"CAPE\": \"cape\", \"SP500\": \"sp500\"}, inplace=True)\n\nsp500_cape_m.tail()\n\n\n\n\n\n\n\n\nsp500\ncape\n\n\ndate\n\n\n\n\n\n\n2025-08-31\n6408.95\n37.85\n\n\n2025-09-30\n6584.02\n38.58\n\n\n2025-10-31\n6735.69\n39.21\n\n\n2025-11-30\n6740.89\n39.12\n\n\n2025-12-31\n6812.63\n39.42\n\n\n\n\n\n\n\n\n\n\nLong-Term Average/Median: The S&P 500’s historical long-term average CAPE ratio is around 16-18, with a median value of approximately 16.04 (since 1881). This provides a central benchmark.\nHigh Valuation (Expensive): A CAPE ratio that is notably higher than the historical average (e.g., above 25 or 30) is considered indicative of a highly valued or overvalued market. Historically, values exceeding 30 have only occurred during major market peaks like the 1929 crash, the dot-com bubble in the late 1990s (peaking over 44), and the post-pandemic period around 2021. Such high readings have typically been followed by periods of lower-than-average, or even negative, long-term returns.\nLow Valuation (Cheap): A CAPE ratio well below the historical average (e.g., below 15 or 10) suggests an undervalued or cheap market. Historically, such levels have been associated with minimal downside risk and higher average long-term returns.\nCould have calculated\nV_spread = (x - baseline) / baseline # baseline = 16\nProblem is, if the whole world “structurally” shifted (e.g. post-1990 lower inflation), “16” stops being meaningful.\nWe are instead measuring,\n\\[\nV_t = \\frac{CAPE_t - \\mathrm{median}(CAPE)}{\\mathrm{IQR}(CAPE)}\n\\]\n\ndef build_valuation_spread_baseline(\n    val_df: pd.DataFrame,\n    metric: str = \"cape\",\n    baseline: float = None,\n    scale: bool = True,\n) -&gt; pd.Series:\n    x = val_df[metric].astype(float)\n\n    if baseline is None:\n        # robust long-run baseline\n        median = x.median()\n        iqr = x.quantile(0.75) - x.quantile(0.25)\n        baseline = median\n        denom = iqr if scale and iqr &gt; 0 else baseline\n    else:\n        denom = baseline if scale else 1.0\n\n    diff = x - baseline          \n\n    V_spread = diff / denom if scale else diff\n    V_spread.name = \"V_spread\"\n    return V_spread\n\n# Example: baseline at 16.04 (long-run median)\nV_spread_t = build_valuation_spread_baseline(\n    sp500_cape_m,\n    metric=\"cape\",\n    baseline=None,\n    scale=True,   # or False if you prefer \"CAPE points below baseline\"\n)\n\nV_spread_t.tail()\n\ndate\n2025-08-31    1.259380\n2025-09-30    1.338771\n2025-10-31    1.407287\n2025-11-30    1.397499\n2025-12-31    1.430125\nFreq: M, Name: V_spread, dtype: float64\n\n\n\nV_spread_t.plot(figsize=(14, 4), title=\"S&P 500 Valuation Spread (cheap vs own history)\")\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\ncombined_aug.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\n# regimes_df: index is month-end\nreg = combined_aug.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Align valuation spread to month-end, then join\nV_spread_m = V_spread_t.resample(\"M\").last()\n\ncombined = (\n    reg\n    .join(V_spread_m.to_frame(), how=\"inner\")\n)\n\ncombined.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\nV_spread\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n1.187602\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n0.925503\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n0.690593\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n0.958129\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n1.070147\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Valuation spread series, e.g. top–bottom decile)\n$ V_t^{} $\n(Regime-conditional mean valuation spread)\n$ {V}^{(k)} = $\n$ ^{(k)} = {_{t=1}^T (_t = k)} $\n(Difference in spreads between regimes)\n$ ^{()} - ^{()} $\n(Factor return in regime ( k ))\n$ r_t^{(F)} $\n$ {r}_F^{(k)} = $\n$ F^{(k)} = {{t=1}^T (_t = k)} $\n(Regime-specific Sharpe ratio)\n$ _F^{(k)} = $\n\n# Regime-conditional mean valuation spread:  V̄^(k)\nval_means_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .mean()\n)\n\nval_stds_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .std()\n)\n\nprint(\"Regime-conditional mean valuation spread:\")\nprint(val_means_by_regime)\n\nprint(\"\\nRegime-conditional valuation z-score std dev:\")\nprint(val_stds_by_regime)\n\n# Difference in spreads between regimes:  V̂^(High) − V̂^(Tight)\nif {\"High\", \"Tight\"}.issubset(val_means_by_regime.index):\n    spread_diff = (\n        val_means_by_regime[\"High\"] - val_means_by_regime[\"Tight\"]\n    )\n    print(\"\\nDifference in spreads High − Tight:\")\n    print(spread_diff)\n\nRegime-conditional mean valuation spread:\nstate_label\nHigh     0.380274\nTight   -0.049269\nName: V_spread, dtype: float64\n\nRegime-conditional valuation z-score std dev:\nstate_label\nHigh     0.406851\nTight    0.551632\nName: V_spread, dtype: float64\n\nDifference in spreads High − Tight:\n0.42954211798360936\n\n\n\n\n\n\nTight liquidity = cheaper markets (value regime),\nHigh liquidity = expensive markets (growth regime).\n\n\nMarkets are ~0.43σ more expensive in High liquidity regimes than Tight liquidity regimes.\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# -------------------------------------------------------------------\n# Assume you already have:\n#   L_t              : pd.Series (liquidity index), monthly, name \"L\"\n#   sp500_cape_m     : DataFrame with column \"sp500\" (S&P 500)\n#   V_spread_t       : pd.Series (\"V_spread\"), monthly valuation cheapness\n#   regimes_df       : DataFrame with column \"state_label\"\n# -------------------------------------------------------------------\n\n# 1) Build a common monthly dataframe\ndf_plot = pd.DataFrame(index=L_t.index.union(sp500_cape_m.index).union(V_spread_t.index))\n\ndf_plot[\"L\"]         = L_t.reindex(df_plot.index)\ndf_plot[\"spx_price\"] = sp500_cape_m[\"sp500\"].reindex(df_plot.index)\ndf_plot[\"V_spread\"]  = V_spread_t.reindex(df_plot.index)\n\n# Attach regimes (forward-fill in case of any slight index mismatch)\nreg_states = combined[\"state_label\"].reindex(df_plot.index).ffill()\ndf_plot[\"state_label\"] = reg_states\n\n# Optional: drop rows before we have all three series\ndf_plot = df_plot.dropna(subset=[\"L\", \"spx_price\", \"V_spread\", \"state_label\"])\n\n# 2) Helper: find contiguous regime segments for shading\ndef get_regime_segments(states: pd.Series):\n    \"\"\"\n    Given a Series of regime labels indexed by date,\n    return list of (start_date, end_date, label) segments\n    where the label is constant.\n    \"\"\"\n    segments = []\n    prev_label = None\n    start_date = None\n\n    for dt, label in states.items():\n        if prev_label is None:\n            prev_label = label\n            start_date = dt\n            continue\n\n        if label != prev_label:\n            # segment ended at previous date\n            end_date = dt\n            segments.append((start_date, end_date, prev_label))\n            start_date = dt\n            prev_label = label\n\n    # last segment\n    if prev_label is not None and start_date is not None:\n        segments.append((start_date, states.index[-1], prev_label))\n\n    return segments\n\nsegments = get_regime_segments(df_plot[\"state_label\"])\n\n# 3) Colors for regimes\nregime_colors = {\n    \"High\":    \"#ffe0e0\",  # pale red\n    \"Neutral\": \"#f7f7f7\",  # light gray\n    \"Tight\":   \"#d1ffd1\",  # pale green\n}\n\n# 4) Plot: 3 stacked subplots with shared x-axis\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True,\n                         gridspec_kw={\"height_ratios\": [1, 1, 1]})\n\nax_L, ax_spx, ax_val = axes\n\n# --- Shading first so lines render on top ---\nfor (start, end, label) in segments:\n    color = regime_colors.get(label, \"#f0f0f0\")\n    for ax in axes:\n        ax.axvspan(start, end, color=color, alpha=0.3)\n\n# --- Panel 1: Liquidity index L(t) ---\nax_L.plot(df_plot.index, df_plot[\"L\"], label=\"L(t)\", linewidth=1.5)\nax_L.set_ylabel(\"L(t)\")\nax_L.set_title(\"Liquidity Index, S&P 500, and Valuation Spread by Liquidity Regime\")\nax_L.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 2: S&P 500 level ---\nax_spx.plot(df_plot.index, df_plot[\"spx_price\"], label=\"S&P 500\", linewidth=1.5)\nax_spx.set_ylabel(\"S&P 500\")\nax_spx.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 3: Valuation cheapness (V_spread) ---\nax_val.plot(df_plot.index, df_plot[\"V_spread\"], label=\"V_spread (cheap vs baseline)\", linewidth=1.5)\nax_val.axhline(0.0, color=\"black\", linewidth=1, linestyle=\"--\", alpha=0.7)\nax_val.set_ylabel(\"V_spread\")\nax_val.set_xlabel(\"Date\")\nax_val.grid(True, linestyle=\"--\", alpha=0.4)\n\n# Optional legends\nax_L.legend(loc=\"upper left\")\nax_spx.legend(loc=\"upper left\")\nax_val.legend(loc=\"upper left\")\n\nfig.tight_layout()\nplt.show()"
  }
]