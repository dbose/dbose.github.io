[
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "",
    "text": "import yfinance as yf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Calculate annualized return\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\n# Calculate volatility (annualized standard deviation)\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\n# Calculate Sharpe Ratio\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(periods_per_year)\n\n# Calculate Sortino Ratio\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / downside_returns.std() * np.sqrt(periods_per_year)\n\n# Maximum drawdown\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#the-philosophy-investing-in-enablers-not-just-innovators",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#the-philosophy-investing-in-enablers-not-just-innovators",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "The Philosophy: Investing in Enablers, Not Just Innovators",
    "text": "The Philosophy: Investing in Enablers, Not Just Innovators\nThe concept of a K-wave, named after economist Nikolai Kondratiev, describes long-term economic cycles driven by technological innovation. Each wave—spanning roughly 40-60 years—ushers in transformative advancements, from the Industrial Revolution to the Information Age. We’re now entering what many believe is the sixth K-wave, propelled by AI, clean energy, and advanced manufacturing. While companies at the forefront of these technologies (e.g., pure-play AI startups or speculative renewable energy firms) often dominate headlines, their volatility can make them risky bets for long-term investors\nDuring the 1848 California Gold Rush, it wasn’t the gold miners who reaped the most consistent rewards—it was the “shovelmakers,” the suppliers of tools and infrastructure, who built lasting wealth by enabling the frenzy. Today, as we stand on the cusp of a new technological era defined by artificial intelligence, renewable energy, and advanced manufacturing, a similar strategy can guide us toward sustainable growth.\n\n\n\nGold Rush\n\n\nInstead, our mini-fund targets companies that enable these breakthroughs—those building the “picks and shovels” of the modern era. ASML, for instance, powers the semiconductor industry with its photolithography machines, a cornerstone of AI and computing advancements. Tesla, beyond its electric vehicles, drives innovation in energy storage and autonomous driving infrastructure. Microsoft provides cloud computing and AI platforms that underpin countless applications, while Berkshire Hathaway offers stability and diversified exposure to industrial and financial sectors. AbbVie contributes healthcare innovation, a critical pillar of societal progress, and Lockheed Martin strengthens the portfolio with its leadership in aerospace and defense—sectors poised for growth amid geopolitical shifts and technological integration. This blend of high-growth and value stocks creates a portfolio that captures multiple growth trends while maintaining a solid foundation. By avoiding overexposure to speculative “gold miners,” we aim to deliver consistent returns with reduced downside risk—a strategy validated by our backtest results."
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#initial-portfolio-composition",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#initial-portfolio-composition",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Initial Portfolio Composition",
    "text": "Initial Portfolio Composition\nThe original portfolio comprises five stocks — ASML Holding N.V. (ASML), SolarEdge Technologies, Inc. (SEDG), Rockwell Automation, Inc. (ROK), Illumina, Inc. (ILMN), and Lockheed Martin Corporation (LMT) — each allocated an equal weight of 20%.\nThis portfolio was backtested using historical closing prices from January 1, 2000, to January 1, 2024, sourced via Yahoo Finance (yf.download). Daily returns were calculated with pct_change() and cleaned with dropna() to ensure data integrity. The portfolio is benchmarked against the S&P 500 (^GSPC) and Nasdaq-100 (^NDX), reflecting a strategy to measure performance against broad market and technology-focused indices.\nThis portfolio embodies the “shovelmakers of tomorrow” philosophy outlined earlier. Rather than chasing speculative leaders in emerging technologies, it targets companies that provide critical infrastructure, tools, and services enabling the next Kondratiev wave (K-wave) — such as renewable energy, automation, genomics, and aerospace.\nThe equal-weight allocation ensures diversification across the following sectors, balancing growth potential with stability:\n\nSemiconductors\nSolar Energy\nIndustrial Automation\nGenomics\nDefense\n\n\n\nCompanies in the Portfolio\n\n\n\nASML Holding N.V. (ASML)\nSector: Technology (Semiconductors)\nWeight: 20%\nRole:\nASML is the global leader in photolithography systems, essential for manufacturing integrated circuits (microchips). Its extreme ultraviolet (EUV) lithography machines are critical for producing advanced chips used in AI, 5G, and high-performance computing.\nK-Wave Relevance:\nASML is a quintessential shovelmaker, supplying the tools that power the semiconductor industry—a backbone of the sixth K-wave. As demand for AI and IoT grows, ASML’s equipment enables chipmakers like TSMC and Intel to push technological boundaries.\nPortfolio Fit:\nASML contributes high-growth potential, capitalizing on secular trends in technology, while its dominant market position adds resilience.\n\n\n\nSolarEdge Technologies, Inc. (SEDG)\nSector: Renewable Energy (Solar)\nWeight: 20%\nRole:\nSolarEdge specializes in power optimizers, inverters, and monitoring systems for solar photovoltaic (PV) installations. Its solutions maximize energy efficiency and reliability for residential, commercial, and utility-scale solar projects.\nK-Wave Relevance:\nThe transition to clean energy is a defining feature of the next K-wave, with solar power at the forefront. SolarEdge’s technologies enhance the scalability and affordability of solar energy, positioning it as an enabler of the renewable revolution. Unlike solar panel manufacturers, SolarEdge focuses on the tools that optimize energy output, aligning with the shovelmaker strategy.\nPortfolio Fit:\nSEDG introduces exposure to the fast-growing renewable energy sector, offering growth potential tempered by the volatility inherent in clean energy markets.\n\n\n\nRockwell Automation, Inc. (ROK)\nSector: Industrials (Automation)\nWeight: 20%\nRole:\nRockwell Automation provides industrial automation and digital transformation solutions, including programmable logic controllers (PLCs), sensors, and software for smart manufacturing. It serves industries like automotive, food and beverage, and pharmaceuticals.\nK-Wave Relevance:\nAutomation is a cornerstone of the sixth K-wave, driving efficiency in manufacturing and supply chains. Rockwell’s technologies enable “Industry 4.0”—the integration of IoT, AI, and robotics into production—making it a key supplier of tools for industrial innovation. Its focus on software and analytics further aligns with digital transformation trends.\nPortfolio Fit:\nROK adds a value-oriented component, balancing growth with stability. Its diversified client base mitigates sector-specific risks, enhancing portfolio resilience.\n\n\n\nIllumina, Inc. (ILMN)\nSector: Healthcare (Genomics)\nWeight: 20%\nRole:\nIllumina is a leader in DNA sequencing and genomics, providing instruments, reagents, and software for genetic analysis. Its technologies support applications in personalized medicine, cancer research, and agriculture.\nK-Wave Relevance:\nGenomics is poised to transform healthcare, a critical pillar of societal progress in the next K-wave. Illumina’s sequencing platforms are the shovels of this revolution, enabling researchers and clinicians to decode genetic data at scale. Its dominance in sequencing technology ensures long-term growth potential.\nPortfolio Fit:\nILMN brings exposure to healthcare innovation, a sector with defensive qualities and high growth. It diversifies the portfolio away from pure technology, reducing correlation with market cycles.\n\n\n\nLockheed Martin Corporation (LMT)\nSector: Aerospace and Defense\nWeight: 20%\nRole:\nLockheed Martin is a global leader in aerospace, defense, and security, known for products like the F-35 fighter jet, missile systems, and space technologies. It serves government and commercial clients worldwide.\nK-Wave Relevance:\nDefense and aerospace are integral to the next K-wave, driven by geopolitical dynamics and technological advancements (e.g., hypersonics, space exploration). Lockheed Martin’s role as a systems integrator and innovator positions it as an enabler of national security and space infrastructure—a stable shovelmaker in a high-stakes field.\nPortfolio Fit:\nLMT anchors the portfolio with defensive characteristics, offering steady cash flows and dividends. Its low correlation with tech sectors enhances diversification, mitigating downside risk during market downturns. ownside risk during market downturns.\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'SEDG', 'ROK', 'ILMN', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.2]) \n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Mini-Fund Portfolio vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\npd.set_option('display.width', 1000)\nprint(metrics_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.194025    0.269933      0.718391       0.997384     -0.411544\nS&P 500              0.100897    0.184097      0.506164       0.610701     -0.339250\nNasdaq-100           0.168178    0.225889      0.713269       0.905724     -0.355631"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#beginning",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#beginning",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Beginning",
    "text": "Beginning\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\n# stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT']\n#stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n# weights = np.array([0.1639441, 0.2852132, 0.3240187, 0.2268240])\n# weights = np.array([0.14, 0.25, 0.29, 0.20, 0.12]) \n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Portfolio-GV', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(metrics_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.367925    0.272138      1.214923       1.683422     -0.421506\nS&P 500              0.119445    0.174019      0.621038       0.762031     -0.339250\nNasdaq-100           0.181999    0.207497      0.813858       1.043914     -0.355631\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\n\n# Define the stocks and benchmark indices\n# stocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT']\n# stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.2,  0.2])\n# weights = np.array([0.1639441, 0.2852132, 0.3240187, 0.2268240])\n# weights = np.array([0.14, 0.25, 0.29, 0.20, 0.12]) \n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O1 vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(metrics_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.325947    0.240479      1.211467       1.630888     -0.371925\nS&P 500              0.113679    0.172243      0.595699       0.721428     -0.339250\nNasdaq-100           0.179487    0.210365      0.795558       1.003233     -0.355631\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\n\n# Define the stocks and benchmark indices\n#stocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\n#stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT']\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\n# weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n# weights = np.array([0.1639441, 0.2852132, 0.3240187, 0.2268240])\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O2 vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(metrics_df)\n\n[*********************100%***********************]  8 of 8 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.308158    0.218942      1.246309       1.642653     -0.350050\nS&P 500              0.113679    0.172243      0.595699       0.721428     -0.339250\nNasdaq-100           0.179487    0.210365      0.795558       1.003233     -0.355631\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Plot the density plots for Portfolio, S&P 500, and Nasdaq-100 returns\nplt.figure(figsize=(15, 8))\n\n# Full Density Plot\n\nsns.kdeplot(portfolio_returns, label='Portfolio-GV-O2', color='blue', shade=True, clip=(-0.06, 0.06))\nsns.kdeplot(returns['^GSPC'], label='S&P 500', color='orange', shade=True, clip=(-0.06, 0.06))\nsns.kdeplot(returns['^NDX'], label='Nasdaq-100', color='green', shade=True, clip=(-0.06, 0.06))\n\nplt.title('Zoomed-In Density Plot: Left and Right Tails', fontsize=16)\nplt.xlabel('Daily Return', fontsize=12)\nplt.ylabel('Density', fontsize=12)\nplt.legend(loc='upper right', fontsize=10)\n\n\n# Adjust layout and show plot\nplt.tight_layout()\nplt.grid(True)\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\n&lt;ipython-input-23-b30301a8e790&gt;:28: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(portfolio_returns, label='Mini-Fund Portfolio', color='blue', shade=True, clip=(-0.06, 0.06))\n&lt;ipython-input-23-b30301a8e790&gt;:29: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(returns['^GSPC'], label='S&P 500', color='orange', shade=True, clip=(-0.06, 0.06))\n&lt;ipython-input-23-b30301a8e790&gt;:30: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(returns['^NDX'], label='Nasdaq-100', color='green', shade=True, clip=(-0.06, 0.06))"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#perf-across-vol-regimes",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#perf-across-vol-regimes",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Perf across vol regimes",
    "text": "Perf across vol regimes\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom hmmlearn.hmm import GaussianHMM\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Fit a 3-state HMM to detect volatility regimes\nmodel = GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=1000)\nmodel.fit(portfolio_returns.values.reshape(-1, 1))\n\n# Predict regimes\nhidden_states = model.predict(portfolio_returns.values.reshape(-1, 1))\n\n# Add hidden state labels to returns DataFrame\nreturns_df = pd.DataFrame({\n    'Portfolio Returns': portfolio_returns,\n    '^GSPC Returns': returns['^GSPC'],\n    '^NDX Returns': returns['^NDX'],\n    'Hidden State': hidden_states\n})\n\n# Calculate average volatility of each regime to determine regime labels\naverage_volatility = returns_df.groupby('Hidden State')['Portfolio Returns'].std()\n\n# Sort the volatilities to label regimes\nvolatility_rank = average_volatility.sort_values().index\nregime_labels = {volatility_rank[0]: 'Low Volatility',\n                 volatility_rank[1]: 'Mid Volatility',\n                 volatility_rank[2]: 'High Volatility'}\n\n# Add regime labels to DataFrame\nreturns_df['Volatility Regime'] = returns_df['Hidden State'].map(regime_labels)\n\n# Define metric functions\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(periods_per_year)\n\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / downside_returns.std() * np.sqrt(periods_per_year)\n\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()\n\n# Calculate metrics for each volatility regime for portfolio, S&P 500, and Nasdaq-100\nmetrics = {}\n\nfor regime in ['Low Volatility', 'Mid Volatility', 'High Volatility']:\n    state_data = returns_df[returns_df['Volatility Regime'] == regime]\n    state_metrics = {}\n    \n    for col in ['Portfolio Returns', '^GSPC Returns', '^NDX Returns']:\n        state_metrics[col] = {\n            'Annualized Return': annualized_return(state_data[col]),\n            'Volatility': annualized_volatility(state_data[col]),\n            'Sharpe Ratio': sharpe_ratio(state_data[col]),\n            'Sortino Ratio': sortino_ratio(state_data[col]),\n            'Max Drawdown': max_drawdown(state_data[col])\n        }\n    \n    metrics[regime] = state_metrics\n\n# Convert metrics to a DataFrame for better visualization\nmetrics_dict = {}\nfor regime, data in metrics.items():\n    for asset, values in data.items():\n        row_key = f\"{regime} - {asset.replace('Portfolio Returns', 'Mini-Fund').replace('^GSPC Returns', 'S&P 500').replace('^NDX Returns', 'Nasdaq-100')}\"\n        metrics_dict[row_key] = values\n\nmetrics_df = pd.DataFrame(metrics_dict).T\nmetrics_df = metrics_df.rename_axis('Regime and Asset').reset_index()\n\n# Group the table display by Low/Mid/High Volatility Regimes to enhance readability\ngrouped_metrics = metrics_df.copy()\n\n# Formatting the DataFrame to show Regime and Metrics without repetition of the regime\ngrouped_metrics['Volatility Regime'] = grouped_metrics['Regime and Asset'].str.extract(r'^(Low|Mid|High) Volatility')\ngrouped_metrics['Asset'] = grouped_metrics['Regime and Asset'].str.replace(r'^(Low|Mid|High) Volatility - ', '')\ngrouped_metrics.drop(columns=['Regime and Asset'], inplace=True)\n\n# Reorder columns for better readability\ngrouped_metrics = grouped_metrics[['Volatility Regime', 'Asset', 'Annualized Return', 'Volatility', 'Sharpe Ratio', 'Sortino Ratio', 'Max Drawdown']]\n\n# Sorting the DataFrame by 'Volatility Regime' to ensure grouping\ngrouped_metrics = grouped_metrics.sort_values(by=['Volatility Regime', 'Asset']).reset_index(drop=True)\n\n# Display in a more readable format\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', '{:.4f}'.format)\npd.set_option('display.width', 1000)\nprint(\"\\nPerformance Metrics Grouped by Volatility Regime:\")\nprint(grouped_metrics)\n\n# Plotting cumulative returns for each regime\nplt.figure(figsize=(15, 8))\n\nfor regime in ['Low Volatility', 'Mid Volatility', 'High Volatility']:\n    state_data = returns_df[returns_df['Volatility Regime'] == regime]\n    cumulative_portfolio_returns = (1 + state_data['Portfolio Returns']).cumprod()\n    cumulative_sp500_returns = (1 + state_data['^GSPC Returns']).cumprod()\n    cumulative_ndx_returns = (1 + state_data['^NDX Returns']).cumprod()\n\n    plt.plot(cumulative_portfolio_returns, label=f'{regime} - Mini-Fund Cumulative Returns', linestyle='-', color=f'C{list(regime_labels.values()).index(regime)}')\n    plt.plot(cumulative_sp500_returns, label=f'{regime} - S&P 500 Cumulative Returns', linestyle='--', color=f'C{list(regime_labels.values()).index(regime)}')\n    plt.plot(cumulative_ndx_returns, label=f'{regime} - Nasdaq-100 Cumulative Returns', linestyle=':', color=f'C{list(regime_labels.values()).index(regime)}')\n\n# Add labels and title\nplt.title('Cumulative Returns of Portfolio-GV-O2, S&P 500, and Nasdaq-100 Across Volatility Regimes', fontsize=16)\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('Cumulative Growth', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\nModel is not converging.  Current: 8306.194727967259 is not greater than 8306.204205929746. Delta is -0.009477962486926117\n\n\n\nPerformance Metrics Grouped by Volatility Regime:\n  Volatility Regime                         Asset  Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\n0              High   High Volatility - Mini-Fund            -0.8314      0.8617       -1.6850        -2.4929       -0.3083\n1              High  High Volatility - Nasdaq-100            -0.7188      0.8288       -1.1669        -2.0536       -0.2297\n2              High     High Volatility - S&P 500            -0.8402      0.8390       -1.8236        -3.2338       -0.3064\n3               Low    Low Volatility - Mini-Fund             0.3906      0.1553        2.0735         3.2254       -0.1154\n4               Low   Low Volatility - Nasdaq-100             0.3058      0.1421        1.8093         2.5148       -0.1106\n5               Low      Low Volatility - S&P 500             0.2072      0.1101        1.5845         2.1873       -0.1028\n6               Mid    Mid Volatility - Mini-Fund             0.1974      0.2961        0.6896         1.0894       -0.3166\n7               Mid   Mid Volatility - Nasdaq-100            -0.0683      0.2950       -0.1602        -0.2455       -0.4023\n8               Mid      Mid Volatility - S&P 500            -0.0423      0.2274       -0.1644        -0.2437       -0.3408\n\n\n\n\n\n\n\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom hmmlearn.hmm import GaussianHMM\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Fit a 3-state HMM to detect volatility regimes\nmodel = GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=1000)\nmodel.fit(portfolio_returns.values.reshape(-1, 1))\n\n# Predict regimes\nhidden_states = model.predict(portfolio_returns.values.reshape(-1, 1))\n\n# Add hidden state labels to returns DataFrame\nreturns_df = pd.DataFrame({\n    'Date': returns.index,\n    'Portfolio Returns': portfolio_returns,\n    'S&P 500 Returns': returns['^GSPC'],\n    'Nasdaq-100 Returns': returns['^NDX'],\n    'Hidden State': hidden_states\n})\n\n# Calculate average volatility of each regime to determine regime labels\naverage_volatility = returns_df.groupby('Hidden State')['Portfolio Returns'].std()\n\n# Sort the volatilities to label regimes\nvolatility_rank = average_volatility.sort_values().index\nregime_labels = {volatility_rank[0]: 'Low Volatility',\n                 volatility_rank[1]: 'Mid Volatility',\n                 volatility_rank[2]: 'High Volatility'}\n\n# Add regime labels to DataFrame\nreturns_df['Volatility Regime'] = returns_df['Hidden State'].map(regime_labels)\n\n# Calculate cumulative returns for each asset and add to DataFrame\nreturns_df['Cumulative Portfolio Returns'] = (1 + returns_df['Portfolio Returns']).cumprod()\nreturns_df['Cumulative S&P 500 Returns'] = (1 + returns_df['S&P 500 Returns']).cumprod()\nreturns_df['Cumulative Nasdaq-100 Returns'] = (1 + returns_df['Nasdaq-100 Returns']).cumprod()\n\n# Melt the DataFrame for seaborn compatibility\ncumulative_returns_df = pd.melt(\n    returns_df,\n    id_vars=['Date', 'Volatility Regime'],\n    value_vars=['Cumulative Portfolio Returns', 'Cumulative S&P 500 Returns', 'Cumulative Nasdaq-100 Returns'],\n    var_name='Asset',\n    value_name='Cumulative Returns'\n)\n\n# Create a faceted plot using seaborn's FacetGrid\ng = sns.FacetGrid(cumulative_returns_df, col='Volatility Regime', hue='Asset', col_wrap=3, height=4, aspect=1.5)\ng.map(plt.plot, 'Date', 'Cumulative Returns').add_legend()\n\n# Adjust plot aesthetics\ng.set_axis_labels('Date', 'Cumulative Growth')\ng.set_titles(col_template='{col_name} Regime')\ng.fig.suptitle('Cumulative Returns Across Volatility Regimes for Portfolio, S&P 500, and Nasdaq-100', y=1.05)\n\nplt.tight_layout()\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\nModel is not converging.  Current: 8305.528897318622 is not greater than 8305.57159532981. Delta is -0.042698011187894735\n\n\n\n\n\n\n\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom tabulate import tabulate\n\nfrom scipy.stats import t\nfrom arch import arch_model  # To use GARCH for volatility forecasting\n\n# Define risk-free rate (approximation)\nrisk_free_rate = 0.02  # 2% annually\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']  # Example portfolio stocks\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate benchmark returns\nmarket_returns = returns['^GSPC']  # S&P 500 as benchmark\n\n# Alpha, Beta, R-Square\ncov_matrix = np.cov(portfolio_returns, market_returns)\nbeta_portfolio = cov_matrix[0, 1] / cov_matrix[1, 1]\nalpha_portfolio = np.mean(portfolio_returns) - beta_portfolio * np.mean(market_returns)\nr_square = 1 - (np.var(portfolio_returns - beta_portfolio * market_returns) / np.var(portfolio_returns))\n\n# Value at Risk (VaR) and Expected Shortfall (ES)\nimport numpy as np\nfrom scipy.stats import norm\n\ndef var_es(returns, confidence_level=0.95):\n    \"\"\"\n    Calculate VaR and Expected Shortfall using the normal distribution.\n\n    Args:\n        returns (pd.Series): A pandas Series of historical returns.\n        confidence_level (float): Confidence level for VaR and ES calculation.\n\n    Returns:\n        Tuple: VaR and ES for 1-month, 6-month, and 1-year periods.\n    \"\"\"\n    # Calculate the mean return and standard deviation (daily)\n    mean_return_daily = returns.mean()\n    std_dev_daily = returns.std()\n\n    # z-score for the given confidence level (left tail)\n    z = norm.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + z * std_dev_period)\n\n        # ES Calculation\n        es_factor = norm.pdf(z) / (1 - confidence_level)\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n\ndef var_es_tf(returns, confidence_level=0.95, forecast_volatility=False):\n    \"\"\"\n    Calculate VaR and Expected Shortfall using Student's t-distribution, with optional GARCH volatility forecasting.\n\n    Args:\n        returns (pd.Series): A pandas Series of historical returns.\n        confidence_level (float): Confidence level for VaR and ES calculation.\n        forecast_volatility (bool): If True, use GARCH(1,1) to forecast volatility.\n\n    Returns:\n        Tuple: VaR and ES for 1-month, 6-month, and 1-year periods.\n    \"\"\"\n    # Calculate the mean return (daily)\n    mean_return_daily = returns.mean()\n\n    # Volatility calculation\n    if forecast_volatility:\n        # Fit GARCH(1,1) model to the return series\n        am = arch_model(returns * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        # Use last forecasted volatility (scaled back to decimal)\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        # Use historical standard deviation\n        std_dev_daily = returns.std()\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n\n\n# Get VaR and Expected Shortfall for the portfolio\n#var_1m, var_6m, var_1y, es_1m, es_6m, es_1y = var_es(portfolio_returns)\nvar_1m, var_6m, var_1y, es_1m, es_6m, es_1y = var_es_tf(portfolio_returns, forecast_volatility=True)\n\n# Calculate standard performance metrics (already in your code)\nportfolio_metrics = {\n    'Alpha': alpha_portfolio,\n    'Beta': beta_portfolio,\n    'R-Square': r_square,\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns),\n    'VaR 1M (95%)': var_1m,\n    'VaR 6M (95%)': var_6m,\n    'VaR 1Y (95%)': var_1y,\n    'ES 1M (95%)': es_1m,\n    'ES 6M (95%)': es_6m,\n    'ES 1Y (95%)': es_1y\n}\n\n# Calculate VaR and ES for S&P 500 benchmark\n#var_1m_sp500, var_6m_sp500, var_1y_sp500, es_1m_sp500, es_6m_sp500, es_1y_sp500 = var_es(returns['^GSPC'])\nvar_1m_sp500, var_6m_sp500, var_1y_sp500, es_1m_sp500, es_6m_sp500, es_1y_sp500 = var_es_tf(returns['^GSPC'], forecast_volatility=True)\n\n# Calculate the same for S&P 500 benchmark\nsp500_metrics = {\n    'Beta': 1,\n    'R-Square': 1,\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC']),\n    'VaR 1M (95%)': var_1m_sp500,\n    'VaR 6M (95%)': var_6m_sp500,\n    'VaR 1Y (95%)': var_1y_sp500,\n    'ES 1M (95%)': es_1m_sp500,\n    'ES 6M (95%)': es_6m_sp500,\n    'ES 1Y (95%)': es_1y_sp500\n}\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics], index=['Portfolio-GV-O2', 'S&P 500'])\n\n# Adjust display settings to show all columns without wrapping\npd.set_option('display.max_columns', None)          # Show all columns\npd.set_option('display.expand_frame_repr', False)   # Prevent DataFrame from wrapping\npd.set_option('display.width', 1000)                # Set width to large enough value\npd.set_option('display.colheader_justify', 'center') # Center align the headers\npd.set_option('display.float_format', '{:.6f}'.format)  # Format float to 6 decimal places\n\nprint(metrics_df)\n\n[*********************100%***********************]  8 of 8 completed\n\n\n                  Alpha    Beta    R-Square  Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown  VaR 1M (95%)  VaR 6M (95%)  VaR 1Y (95%)  ES 1M (95%)  ES 6M (95%)  ES 1Y (95%)\nPortfolio-GV-O2 0.000635 1.083823  0.727009      0.308158        0.218942     1.246309      1.642654      -0.350050     -0.105416     -0.344853     -0.573462    -0.140598    -0.431032    -0.695336  \nS&P 500              NaN 1.000000  1.000000      0.113679        0.172243     0.595699      0.721428      -0.339250     -0.068273     -0.203504     -0.323703    -0.093486    -0.265263    -0.411043  \n\n\n\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, t\nfrom arch import arch_model\n\n# Define the stocks, benchmark indices, and TAIL ETF\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\nprotective_etf = ['TAIL']\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices + protective_etf, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Weights for portfolio without TAIL\nweights_without_tail = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns without TAIL\nportfolio_returns_without_tail = (returns[stocks] * weights_without_tail).sum(axis=1)\n\n# Weights for portfolio including TAIL\nweights_with_tail = np.array([0.13, 0.18, 0.22, 0.16, 0.10, 0.11, 0.10])  # Added 10% weight to TAIL\n\n# Calculate portfolio returns with TAIL\nportfolio_returns_with_tail = (returns[stocks + protective_etf] * weights_with_tail).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns_without_tail = (1 + portfolio_returns_without_tail).cumprod()\ncumulative_portfolio_returns_with_tail = (1 + portfolio_returns_with_tail).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n# Define metrics functions\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / returns.std() * np.sqrt(periods_per_year)\n\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / downside_returns.std() * np.sqrt(periods_per_year)\n\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()\n\ndef alpha_beta(returns, benchmark_returns):\n    cov_matrix = np.cov(returns, benchmark_returns)\n    beta = cov_matrix[0, 1] / cov_matrix[1, 1]\n    alpha = returns.mean() - beta * benchmark_returns.mean()\n    return alpha, beta\n\ndef var_es(returns, confidence_level=0.95, forecast_volatility=False):\n    # Calculate the mean return (daily)\n    mean_return_daily = returns.mean()\n\n    # Volatility calculation\n    if forecast_volatility:\n        # Fit GARCH(1,1) model to the return series\n        am = arch_model(returns * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        # Use last forecasted volatility (scaled back to decimal)\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        # Use historical standard deviation\n        std_dev_daily = returns.std()\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n# Calculate Alpha, Beta, VaR, and ES for portfolios and benchmarks\nmarket_returns = returns['^GSPC']\n\nalpha_portfolio_without_tail, beta_portfolio_without_tail = alpha_beta(portfolio_returns_without_tail, market_returns)\nalpha_portfolio_with_tail, beta_portfolio_with_tail = alpha_beta(portfolio_returns_with_tail, market_returns)\n\nvar_1m_without_tail, var_6m_without_tail, var_1y_without_tail, es_1m_without_tail, es_6m_without_tail, es_1y_without_tail = var_es(portfolio_returns_without_tail, forecast_volatility=True)\nvar_1m_with_tail, var_6m_with_tail, var_1y_with_tail, es_1m_with_tail, es_6m_with_tail, es_1y_with_tail = var_es(portfolio_returns_with_tail, forecast_volatility=True)\n\n# Calculate metrics for mini-fund portfolio without TAIL\nportfolio_metrics_without_tail = {\n    'Alpha': alpha_portfolio_without_tail,\n    'Beta': beta_portfolio_without_tail,\n    'Annualized Return': annualized_return(portfolio_returns_without_tail),\n    'Volatility': annualized_volatility(portfolio_returns_without_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_without_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_without_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_without_tail),\n    'VaR 1M (95%)': var_1m_without_tail,\n    'VaR 6M (95%)': var_6m_without_tail,\n    'VaR 1Y (95%)': var_1y_without_tail,\n    'ES 1M (95%)': es_1m_without_tail,\n    'ES 6M (95%)': es_6m_without_tail,\n    'ES 1Y (95%)': es_1y_without_tail\n}\n\n# Calculate metrics for mini-fund portfolio with TAIL\nportfolio_metrics_with_tail = {\n    'Alpha': alpha_portfolio_with_tail,\n    'Beta': beta_portfolio_with_tail,\n    'Annualized Return': annualized_return(portfolio_returns_with_tail),\n    'Volatility': annualized_volatility(portfolio_returns_with_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_with_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_with_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_with_tail),\n    'VaR 1M (95%)': var_1m_with_tail,\n    'VaR 6M (95%)': var_6m_with_tail,\n    'VaR 1Y (95%)': var_1y_with_tail,\n    'ES 1M (95%)': es_1m_with_tail,\n    'ES 6M (95%)': es_6m_with_tail,\n    'ES 1Y (95%)': es_1y_with_tail\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Beta': 1,\n    'R-Square': 1,\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC']),\n    'VaR 1M (95%)': var_es(returns['^GSPC'])[0],\n    'VaR 6M (95%)': var_es(returns['^GSPC'])[1],\n    'VaR 1Y (95%)': var_es(returns['^GSPC'])[2],\n    'ES 1M (95%)': var_es(returns['^GSPC'])[3],\n    'ES 6M (95%)': var_es(returns['^GSPC'])[4],\n    'ES 1Y (95%)': var_es(returns['^GSPC'])[5]\n}\n\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_without_tail_normalized = cumulative_portfolio_returns_without_tail / cumulative_portfolio_returns_without_tail.iloc[0] * 100\ncumulative_portfolio_returns_with_tail_normalized = cumulative_portfolio_returns_with_tail / cumulative_portfolio_returns_with_tail.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio without TAIL\nplt.plot(cumulative_portfolio_returns_without_tail_normalized, label='Portfolio-GV-O2 Portfolio without TAIL', color='red')\n\n# Plot the portfolio with TAIL\nplt.plot(cumulative_portfolio_returns_with_tail_normalized, label='Portfolio-GV-O2 Portfolio with TAIL', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O2 with and without TAIL vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics_without_tail, portfolio_metrics_with_tail, sp500_metrics, ndx_metrics], \n                          index=['Portfolio-GV-O2 without TAIL', 'Portfolio-GV-O2 with TAIL', 'S&P 500', 'Nasdaq-100'])\n\n# Adjust display settings to show all columns without wrapping\npd.set_option('display.max_columns', None)          # Show all columns\npd.set_option('display.expand_frame_repr', False)   # Prevent DataFrame from wrapping\npd.set_option('display.width', 1000)                # Set width to large enough value\npd.set_option('display.colheader_justify', 'center') # Center align the headers\npd.set_option('display.float_format', '{:.6f}'.format)  # Format float to 6 decimal places\n\nprint(metrics_df)\n\n[*********************100%***********************]  9 of 9 completed\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n                               Alpha    Beta    Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown  VaR 1M (95%)  VaR 6M (95%)  VaR 1Y (95%)  ES 1M (95%)  ES 6M (95%)  ES 1Y (95%)  R-Square\nPortfolio-GV-O2 without TAIL 0.000622 1.086780      0.299328        0.243712     1.115379      1.433413      -0.350050     -0.106745     -0.347816     -0.577361    -0.142540    -0.435495    -0.701359         NaN\nPortfolio-GV-O2 with TAIL    0.000560 0.912985      0.262029        0.208124     1.127078      1.462300      -0.300293     -0.093390     -0.304079     -0.504595    -0.124734    -0.380856    -0.613174         NaN\nS&P 500                           NaN 1.000000      0.110526        0.196321     0.530843      0.634252      -0.339250     -0.124550     -0.341836     -0.519811    -0.174143    -0.463314    -0.691607    1.000000\nNasdaq-100                        NaN 1.000000      0.183523        0.241768     0.735701      0.939876      -0.355631     -0.157124     -0.443419     -0.685044    -0.218199    -0.593020    -0.896611    1.000000"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#performance-during-recessions",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#performance-during-recessions",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Performance during recessions",
    "text": "Performance during recessions\n\nrecession_data = pd.read_csv('recession_indicator.csv', parse_dates=['DATE'])\nrecession_data[\"DATE\"]\n\n0      1854-12-01\n1      1855-01-01\n2      1855-02-01\n3      1855-03-01\n4      1855-04-01\n          ...    \n2033   2024-05-01\n2034   2024-06-01\n2035   2024-07-01\n2036   2024-08-01\n2037   2024-09-01\nName: DATE, Length: 2038, dtype: datetime64[ns]\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns.index = returns.index.tz_localize(None)\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n# Remove time zone information to ensure compatibility\nportfolio_returns.index = portfolio_returns.index.tz_localize(None)\n\n# Load recession periods from uploaded CSV file\nrecession_data = pd.read_csv('recession_indicator.csv', parse_dates=['DATE'])\nrecession_data.set_index('DATE', inplace=True)\nrecession_data.index = recession_data.index.tz_localize(None)\n\nrecession_daily = recession_data.reindex(pd.date_range(start=returns.index.min(), \n                                                       end=returns.index.max(), \n                                                       freq='D')).ffill()\n\n\n# # Align the indices of returns and recession data\n# recession_daily = recession_daily.reindex(returns.index, method='ffill')\n\n# Filter returns for recession periods (only select days when recession is indicated)\nrecession_days_index = recession_daily[recession_daily['VALUE'] == 1].index\n\n# Filter returns for recession periods (only select days when recession is indicated)\nportfolio_recession_returns = portfolio_returns.loc[portfolio_returns.index.intersection(recession_days_index)]\n\n# Define metrics functions\ndef annualized_return(returns):\n    compounded_growth = (1 + returns).prod()\n    n_years = len(returns) / 252\n    return compounded_growth ** (1 / n_years) - 1\n\ndef annualized_volatility(returns):\n    return returns.std() * np.sqrt(252)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02):\n    excess_returns = returns - risk_free_rate / 252\n    return excess_returns.mean() / returns.std() * np.sqrt(252)\n\ndef sortino_ratio(returns, risk_free_rate=0.02):\n    downside_returns = returns[returns &lt; 0]\n    downside_deviation = downside_returns.std() * np.sqrt(252)\n    return (returns.mean() - risk_free_rate / 252) / downside_deviation\n\ndef max_drawdown(returns):\n    cumulative = (1 + returns).cumprod()\n    peak = cumulative.cummax()\n    drawdown = (cumulative - peak) / peak\n    return drawdown.min()\n\n# Calculate metrics for mini-fund portfolio during recession periods\nportfolio_metrics_recession = {\n    'Annualized Return': annualized_return(portfolio_recession_returns),\n    'Volatility': annualized_volatility(portfolio_recession_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_recession_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_recession_returns),\n    'Max Drawdown': max_drawdown(portfolio_recession_returns)\n}\n\n# Calculate metrics for S&P 500 during recession periods\nsp500_recession_returns = returns['^GSPC'].loc[returns.index.intersection(recession_days_index)]\nsp500_metrics_recession = {\n    'Annualized Return': annualized_return(sp500_recession_returns),\n    'Volatility': annualized_volatility(sp500_recession_returns),\n    'Sharpe Ratio': sharpe_ratio(sp500_recession_returns),\n    'Sortino Ratio': sortino_ratio(sp500_recession_returns),\n    'Max Drawdown': max_drawdown(sp500_recession_returns)\n}\n\n# Calculate metrics for Nasdaq-100 during recession periods\nndx_recession_returns = returns['^NDX'].loc[returns.index.intersection(recession_days_index)]\nndx_metrics_recession = {\n    'Annualized Return': annualized_return(ndx_recession_returns),\n    'Volatility': annualized_volatility(ndx_recession_returns),\n    'Sharpe Ratio': sharpe_ratio(ndx_recession_returns),\n    'Sortino Ratio': sortino_ratio(ndx_recession_returns),\n    'Max Drawdown': max_drawdown(ndx_recession_returns)\n}\n\n# Combine results into a DataFrame\nmetrics_df_recession = pd.DataFrame([portfolio_metrics_recession, sp500_metrics_recession, ndx_metrics_recession], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(\"Portfolio Performance During Recession Periods:\")\nprint(metrics_df_recession)\n\n# Plot the cumulative returns during recession periods\ncumulative_portfolio_returns_recession = (1 + portfolio_recession_returns).cumprod()\ncumulative_sp500_returns_recession = (1 + sp500_recession_returns).cumprod()\ncumulative_ndx_returns_recession = (1 + ndx_recession_returns).cumprod()\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns_recession / cumulative_portfolio_returns_recession.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns_recession / cumulative_sp500_returns_recession.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns_recession / cumulative_ndx_returns_recession.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio (Recession)', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500 (Recession)', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100 (Recession)', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O2 vs S&P 500 and Nasdaq-100 During Recession Periods', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\n\n\nPortfolio Performance During Recession Periods:\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund           -0.031372    0.300771     -0.021338      -0.000109     -0.397179\nS&P 500             -0.193706    0.385337     -0.417644      -0.002256     -0.600109\nNasdaq-100          -0.109028    0.460166     -0.065060      -0.000393     -0.613428\n\n\n\n\n\n\n\n\n\n\nrecession_data\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\n\n# Download historical data for the stocks\nstocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\ndata = yf.download(stocks, start='2010-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Define the function to calculate portfolio performance\ndef portfolio_performance(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n    returns = np.sum(mean_returns * weights) * 252\n    std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n    sharpe_ratio = (returns - risk_free_rate) / std\n    return returns, std, sharpe_ratio\n\n# Define the objective function for optimization (negative Sharpe ratio)\ndef negative_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n    returns, std, sharpe_ratio = portfolio_performance(weights, mean_returns, cov_matrix, risk_free_rate)\n    return -sharpe_ratio\n\n# Constraints: sum of weights must be 1\ndef check_sum(weights):\n    return np.sum(weights) - 1\n\n# Boundaries: weights between 0 and 1\nbounds = tuple((0, 1) for stock in stocks)\n\n# Initial guess (equal weight distribution)\ninitial_weights = np.array([1/len(stocks)] * len(stocks))\n\n# Mean returns and covariance matrix\nmean_returns = returns.mean()\ncov_matrix = returns.cov()\n\n# Optimization using minimize from scipy.optimize\noptimal_solution = minimize(negative_sharpe_ratio, initial_weights, args=(mean_returns, cov_matrix),\n                            method='SLSQP', bounds=bounds, constraints={'type': 'eq', 'fun': check_sum})\n\noptimal_weights = optimal_solution.x\n\n# Display optimal weights\noptimal_weights_df = pd.DataFrame(optimal_weights, index=stocks, columns=['Optimal Weight'])\n\noptimal_weights_percent = optimal_weights_df * 10000000000\n\n# Display the weights in percentage form\nprint(optimal_weights_percent)\n\n#print(optimal_weights_df)\n\n[*********************100%***********************]  5 of 5 completed\n\n\n       Optimal Weight\nASML     1.639441e+01\nNVDA     2.097931e-15\nTSLA     2.852132e+01\nBRK-B    3.240187e+01\nMSFT     2.268240e+01\n\n\n\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\nfrom arch import arch_model\nfrom arch.__future__ import reindexing\n\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nprotective_etf = ['TAIL']\n\n# Combine stocks and protective ETF\nall_assets = stocks + protective_etf\n\n# Download historical data\ndata = yf.download(all_assets, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Define the function to calculate portfolio performance\ndef portfolio_performance(weights, returns, risk_free_rate=0.02, confidence_level=0.95, forecast_volatility=True):\n    mean_returns = returns.mean()\n    cov_matrix = returns.cov()\n    \n    portfolio_returns = np.dot(mean_returns, weights) * 252\n    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n    sharpe_ratio = (portfolio_returns - risk_free_rate) / portfolio_std\n\n    # Volatility calculation (GARCH(1,1) model)\n    if forecast_volatility:\n        am = arch_model(np.dot(returns, weights) * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        std_dev_daily = np.std(np.dot(returns, weights))\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = portfolio_returns / 252 * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m = var_list\n    es_1m, es_6m = es_list\n\n    return portfolio_returns, portfolio_std, sharpe_ratio, var_1m, var_6m, es_1m, es_6m\n\n# Define the objective function for optimization\ndef objective(weights, returns, risk_free_rate=0.02, confidence_level=0.95):\n    portfolio_returns, portfolio_std, sharpe_ratio, var_1m, var_6m, es_1m, es_6m = portfolio_performance(weights, returns, risk_free_rate, confidence_level)\n    # Weights for multi-objective optimization\n    w_sharpe, w_var, w_es = 1.0, 0.5, 0.5  # Adjust these weights as per preference\n    # Objective value to minimize: negative Sharpe ratio + VaR + ES\n    objective_value = (\n        w_sharpe * -sharpe_ratio +  # Maximizing Sharpe Ratio by minimizing its negative\n        w_var * (var_1m + var_6m) +  # Minimize VaR (1 month and 6 months)\n        w_es * (es_1m + es_6m)       # Minimize ES (1 month and 6 months)\n    )\n    return objective_value\n\n# Constraints: sum of weights must be 1\ndef check_sum(weights):\n    return np.sum(weights) - 1\n\n# Boundaries: weights between 0 and 1\nbounds = tuple((0, 1) for _ in all_assets)\n\n# Initial guess (equal weight distribution)\n# initial_weights = np.array([1 / len(all_assets)] * len(all_assets))\ninitial_weights = np.array([0.13, 0.18, 0.22, 0.16, 0.10, 0.11, 0.10])  # Added 10% weight to TAIL\n\n# Optimization using minimize from scipy.optimize\noptimal_solution = minimize(objective, initial_weights, args=(returns),\n                            method='SLSQP', bounds=bounds, constraints={'type': 'eq', 'fun': check_sum})\n\noptimal_weights = optimal_solution.x\n\n# Display optimal weights\noptimal_weights_df = pd.DataFrame(optimal_weights, index=all_assets, columns=['Optimal Weight'])\n#optimal_weights_percent = optimal_weights_df * 100\n\n# Display the weights in percentage form\nprint(optimal_weights_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n       Optimal Weight\nASML      0.107482   \nTSLA      0.182479   \nBRK-B     0.183304   \nMSFT      0.194264   \nABBV      0.083253   \nLMT       0.091662   \nTAIL      0.157556   \n**************************************************************************************\n**************************************************************************************\n\n\n\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\nprotective_etf = ['TAIL']\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices + protective_etf, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Calculate portfolio returns with TAIL\nportfolio_returns_with_tail = (returns[stocks + protective_etf] * optimal_weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns_without_tail = (1 + portfolio_returns_without_tail).cumprod()\ncumulative_portfolio_returns_with_tail = (1 + portfolio_returns_with_tail).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n# Define metrics functions\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / returns.std() * np.sqrt(periods_per_year)\n\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / downside_returns.std() * np.sqrt(periods_per_year)\n\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()\n\ndef alpha_beta(returns, benchmark_returns):\n    cov_matrix = np.cov(returns, benchmark_returns)\n    beta = cov_matrix[0, 1] / cov_matrix[1, 1]\n    alpha = returns.mean() - beta * benchmark_returns.mean()\n    return alpha, beta\n\ndef var_es(returns, confidence_level=0.95, forecast_volatility=False):\n    # Calculate the mean return (daily)\n    mean_return_daily = returns.mean()\n\n    # Volatility calculation\n    if forecast_volatility:\n        # Fit GARCH(1,1) model to the return series\n        am = arch_model(returns * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        # Use last forecasted volatility (scaled back to decimal)\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        # Use historical standard deviation\n        std_dev_daily = returns.std()\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n# Calculate Alpha, Beta, VaR, and ES for portfolios and benchmarks\nmarket_returns = returns['^GSPC']\n\nalpha_portfolio_without_tail, beta_portfolio_without_tail = alpha_beta(portfolio_returns_without_tail, market_returns)\nalpha_portfolio_with_tail, beta_portfolio_with_tail = alpha_beta(portfolio_returns_with_tail, market_returns)\n\nvar_1m_without_tail, var_6m_without_tail, var_1y_without_tail, es_1m_without_tail, es_6m_without_tail, es_1y_without_tail = var_es(portfolio_returns_without_tail, forecast_volatility=True)\nvar_1m_with_tail, var_6m_with_tail, var_1y_with_tail, es_1m_with_tail, es_6m_with_tail, es_1y_with_tail = var_es(portfolio_returns_with_tail, forecast_volatility=True)\n\n# Calculate metrics for mini-fund portfolio without TAIL\nportfolio_metrics_without_tail = {\n    'Alpha': alpha_portfolio_without_tail,\n    'Beta': beta_portfolio_without_tail,\n    'Annualized Return': annualized_return(portfolio_returns_without_tail),\n    'Volatility': annualized_volatility(portfolio_returns_without_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_without_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_without_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_without_tail),\n    'VaR 1M (95%)': var_1m_without_tail,\n    'VaR 6M (95%)': var_6m_without_tail,\n    'VaR 1Y (95%)': var_1y_without_tail,\n    'ES 1M (95%)': es_1m_without_tail,\n    'ES 6M (95%)': es_6m_without_tail,\n    'ES 1Y (95%)': es_1y_without_tail\n}\n\n# Calculate metrics for mini-fund portfolio with TAIL\nportfolio_metrics_with_tail = {\n    'Alpha': alpha_portfolio_with_tail,\n    'Beta': beta_portfolio_with_tail,\n    'Annualized Return': annualized_return(portfolio_returns_with_tail),\n    'Volatility': annualized_volatility(portfolio_returns_with_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_with_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_with_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_with_tail),\n    'VaR 1M (95%)': var_1m_with_tail,\n    'VaR 6M (95%)': var_6m_with_tail,\n    'VaR 1Y (95%)': var_1y_with_tail,\n    'ES 1M (95%)': es_1m_with_tail,\n    'ES 6M (95%)': es_6m_with_tail,\n    'ES 1Y (95%)': es_1y_with_tail\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Beta': 1,\n    'R-Square': 1,\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC']),\n    'VaR 1M (95%)': var_es(returns['^GSPC'])[0],\n    'VaR 6M (95%)': var_es(returns['^GSPC'])[1],\n    'VaR 1Y (95%)': var_es(returns['^GSPC'])[2],\n    'ES 1M (95%)': var_es(returns['^GSPC'])[3],\n    'ES 6M (95%)': var_es(returns['^GSPC'])[4],\n    'ES 1Y (95%)': var_es(returns['^GSPC'])[5]\n}\n\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_without_tail_normalized = cumulative_portfolio_returns_without_tail / cumulative_portfolio_returns_without_tail.iloc[0] * 100\ncumulative_portfolio_returns_with_tail_normalized = cumulative_portfolio_returns_with_tail / cumulative_portfolio_returns_with_tail.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio without TAIL\nplt.plot(cumulative_portfolio_returns_without_tail_normalized, label='Portfolio-GV-O2', color='red')\n\n# Plot the portfolio with TAIL\nplt.plot(cumulative_portfolio_returns_with_tail_normalized, label='Portfolio-GV-TAIL-O', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-TAIL-O vs Portfolio-GV-O2 (no TAIL) vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics_without_tail, portfolio_metrics_with_tail, sp500_metrics, ndx_metrics], \n                          index=['Portfolio-GV-O2', 'Portfolio-GV-TAIL-O', 'S&P 500', 'Nasdaq-100'])\n\n# Adjust display settings to show all columns without wrapping\npd.set_option('display.max_columns', None)          # Show all columns\npd.set_option('display.expand_frame_repr', False)   # Prevent DataFrame from wrapping\npd.set_option('display.width', 1000)                # Set width to large enough value\npd.set_option('display.colheader_justify', 'center') # Center align the headers\npd.set_option('display.float_format', '{:.6f}'.format)  # Format float to 6 decimal places\n\nprint(metrics_df)\n\n[*********************100%***********************]  9 of 9 completed\n\n\n\n\n\n\n\n\n\n                      Alpha    Beta    Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown  VaR 1M (95%)  VaR 6M (95%)  VaR 1Y (95%)  ES 1M (95%)  ES 6M (95%)  ES 1Y (95%)  R-Square\nPortfolio-GV-O2     0.000622 1.086780      0.299328        0.243712     1.115379      1.433413      -0.350050     -0.106745     -0.347816     -0.577361    -0.142540    -0.435495    -0.701359         NaN\nPortfolio-GV-TAIL-O 0.000554 0.837946      0.251487        0.196279     1.139983      1.500943      -0.275660     -0.088748     -0.289508     -0.480820    -0.118467    -0.362305    -0.583771         NaN\nS&P 500                  NaN 1.000000      0.110526        0.196321     0.530843      0.634252      -0.339250     -0.124550     -0.341836     -0.519811    -0.174143    -0.463314    -0.691607    1.000000\nNasdaq-100               NaN 1.000000      0.183523        0.241768     0.735701      0.939876      -0.355631     -0.157124     -0.443419     -0.685044    -0.218199    -0.593020    -0.896611    1.000000"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#cwarp",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#cwarp",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "CWARP",
    "text": "CWARP\nhttps://docsend.com/view/teaqrcewe7ht423x\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport matplotlib.pyplot as plt\n\n# Define a function to calculate Sortino Ratio\n# Rp: Return of Portfolio, Rf: Risk-Free Rate, sigma_dp: Downside Deviation of Portfolio\ndef sortino_ratio(Rp, Rf, sigma_dp):\n    return (Rp - Rf) / sigma_dp\n\n# Define a function to calculate Return to Max Drawdown (RMDD)\n# Rp: Return of Portfolio, Lp: Trough value of Replacement Portfolio, Pp: Peak value of Replacement Portfolio\ndef return_to_max_drawdown(Rp, Lp, Pp):\n    return (Rp - Rf) / ((Lp - Pp) / Pp)\n\n# Define the CWARP Calculation Function\n# Sn: Sortino Ratio of New Portfolio, Sp: Sortino Ratio of Replacement Portfolio\n# RMDDn: Return to Max Drawdown of New Portfolio, RMDDp: Return to Max Drawdown of Replacement Portfolio\ndef cwarp_calculation(Sn, Sp, RMDDn, RMDDp):\n    return (np.sqrt((Sn / Sp) * (RMDDn / RMDDp)) - 1) * 100\n\n# Download historical data for portfolio and benchmarks\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Risk-free rate (assumed)\nRf = 0.02 / 252  # Annual risk-free rate divided by trading days\n\n# Calculate Sortino Ratios for Portfolio, S&P 500, and Nasdaq-100\nportfolio_downside_std = portfolio_returns[portfolio_returns &lt; 0].std()\nsp500_downside_std = returns['^GSPC'][returns['^GSPC'] &lt; 0].std()\nndx_downside_std = returns['^NDX'][returns['^NDX'] &lt; 0].std()\n\nSp_portfolio = sortino_ratio(portfolio_returns.mean(), Rf, portfolio_downside_std)\nSp_sp500 = sortino_ratio(returns['^GSPC'].mean(), Rf, sp500_downside_std)\nSp_ndx = sortino_ratio(returns['^NDX'].mean(), Rf, ndx_downside_std)\n\n# Calculate Return to Max Drawdowns for Portfolio, S&P 500, and Nasdaq-100\nportfolio_peak = portfolio_returns.cummax()\nportfolio_drawdown = (portfolio_returns - portfolio_peak) / portfolio_peak\nLp_portfolio = portfolio_drawdown.min()\nPp_portfolio = portfolio_peak.max()\nRMDD_portfolio = return_to_max_drawdown(portfolio_returns.mean(), Lp_portfolio, Pp_portfolio)\n\nsp500_peak = returns['^GSPC'].cummax()\nsp500_drawdown = (returns['^GSPC'] - sp500_peak) / sp500_peak\nLp_sp500 = sp500_drawdown.min()\nPp_sp500 = sp500_peak.max()\nRMDD_sp500 = return_to_max_drawdown(returns['^GSPC'].mean(), Lp_sp500, Pp_sp500)\n\nndx_peak = returns['^NDX'].cummax()\nndx_drawdown = (returns['^NDX'] - ndx_peak) / ndx_peak\nLp_ndx = ndx_drawdown.min()\nPp_ndx = ndx_peak.max()\nRMDD_ndx = return_to_max_drawdown(returns['^NDX'].mean(), Lp_ndx, Pp_ndx)\n\n# Calculate CWARP for Portfolio vs S&P 500 and Nasdaq-100\ncwarp_portfolio_sp500 = cwarp_calculation(Sp_portfolio, Sp_sp500, RMDD_portfolio, RMDD_sp500)\ncwarp_portfolio_ndx = cwarp_calculation(Sp_portfolio, Sp_ndx, RMDD_portfolio, RMDD_ndx)\n\nprint(f\"CWARP Value (Portfolio vs S&P 500): {cwarp_portfolio_sp500}\")\nprint(f\"CWARP Value (Portfolio vs Nasdaq-100): {cwarp_portfolio_ndx}\")\n\n[*********************100%***********************]  8 of 8 completed\n\n\nCWARP Value (Portfolio vs S&P 500): 162.3071593499393\nCWARP Value (Portfolio vs Nasdaq-100): 78.84851594077611"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death of Valuations - v2.html",
    "href": "notebooks/Liquidity Regimes and the Death of Valuations - v2.html",
    "title": "3.2 Fama-French Factors and Valuation Data",
    "section": "",
    "text": "# Core libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data fetching\nimport pandas_datareader as pdr\nfrom fredapi import Fred\n\n# Statistical modeling\nfrom sklearn.decomposition import PCA, SparsePCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.covariance import LedoitWolf\nfrom hmmlearn import hmm\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.stattools import grangercausalitytests, adfuller, kpss\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom scipy import stats\nfrom scipy.stats import jarque_bera, shapiro\n\n# Plotting\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (14, 8)\nplt.rcParams['font.size'] = 11\n\nprint(\"✅ All libraries imported successfully\")\nprint(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d')}\")\n\n✅ All libraries imported successfully\nAnalysis date: 2026-01-26\nstart_date = \"1990-01-01\"\nend_date   = \"2025-12-31\"\n\n# --- FRED series codes ---\nFRED_SERIES = {\n    \"M2\":   \"M2SL\",      # M2 money stock (monthly, SA)\n    \"FED_BS\":\"WALCL\",     # Fed balance sheet total assets (weekly)\n    \"FFR\": \"DFF\",  \n    \"Real_Rate\": \"REAINTRATREARAT1YE\",\n    \"T3M\":   \"DTB3\",     # 3-Month T-Bill rate (monthly)\n    \"T10Y\":  \"DGS10\",     # 10-Year Treasury yield (daily -&gt; resample monthly)\n    \"BAA\":    \"DBAA\",       # Moody's Baa corporate yield (monthly)\n    \"AAA\":    \"DAAA\",       # Moody's Aaa corporate yield (monthly)\n    \"CPI\":    \"CPIAUCSL\",  # CPI index (monthly, SA)\n    \"CORE_CPI\": \"CPILFESL\",    # Core CPI\n    \"GDP\":    \"GDP\",       # Nominal GDP (quarterly, SAAR)\n    \n    'IORB': 'IORB',                    # Interest on Reserve Balances\n    'RRP': 'RRPONTSYD',                # Overnight Reverse Repo\n    'TGA': 'WTREGEN',                  # Treasury General Account\n    \n    'Bank_Reserves': 'TOTRESNS',       # Total Reserves\n    'CI_Loans': 'TOTBKCR',             # Bank Credit, All Commercial Banks\n    'Consumer_Credit': 'TOTALSL',      # Consumer Credit Outstanding\n    'Mortgage_Rate': 'MORTGAGE30US',   # 30Y Mortgage Rate\n    'Prime_Rate': 'DPRIME',            # Bank Prime Loan Rate\n    'STLFSI': 'STLFSI',                # St. Louis Fed Financial Stress\n    'Dollar_Index': 'DTWEXBGS',        # # Global Liquidity: Trade-Weighted Dollar\n    'Household_Debt_Service': 'TDSP',  # Household Liquidity: Household Debt Service Ratio\n}\n\n# For equity factors: Fama-French via pandas_datareader (famafrench)\nFF_FACTORS_DATASET = \"F-F_Research_Data_5_Factors_2x3\"\n\ndef download_fred_series(series_dict, start, end):\n    \"\"\"\n    Download FRED series into a single monthly DataFrame.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Mapping logical_name -&gt; FRED code.\n    \"\"\"\n    dfs = []\n    for name, code in series_dict.items():\n        print(f\"Downloading {name} ({code}) from FRED...\")\n        s = pdr.DataReader(code, \"fred\", start, end)\n        s = s.rename(columns={code: name})\n        dfs.append(s)\n\n    df = pd.concat(dfs, axis=1)\n    # Ensure monthly freq by end-of-month sampling\n    df = df.resample(\"M\").last()\n    return df\nmacro_data = download_fred_series(FRED_SERIES, start_date, end_date)\n\n# 1. Start from the original GDP values only (drop NaNs)\ngdp_series = macro_data[\"GDP\"].dropna()\n# 2. Collapse to unique quarter-end values\n#    - If GDP is timestamped at quarter *start* (e.g. 2025-04-01),\n#      this will move it to the quarter end (2025-06-30) and give unique labels.\ngdp_q = gdp_series.resample(\"Q\").last()   # quarterly, at quarter-end (e.g. 2025-03-31, 2025-06-30, ...)\n\n# 3. Downsample to monthly and forward-fill within the quarter\ngdp_m = gdp_q.resample(\"M\").ffill()\n\n# 4. Assign back into macro_raw, aligning on the monthly index\nmacro_data[\"GDP\"] = gdp_m\n\nmacro_data.tail()\n\nDownloading M2 (M2SL) from FRED...\nDownloading FED_BS (WALCL) from FRED...\nDownloading FFR (DFF) from FRED...\nDownloading Real_Rate (REAINTRATREARAT1YE) from FRED...\nDownloading T3M (DTB3) from FRED...\nDownloading T10Y (DGS10) from FRED...\nDownloading BAA (DBAA) from FRED...\nDownloading AAA (DAAA) from FRED...\nDownloading CPI (CPIAUCSL) from FRED...\nDownloading CORE_CPI (CPILFESL) from FRED...\nDownloading GDP (GDP) from FRED...\nDownloading IORB (IORB) from FRED...\nDownloading RRP (RRPONTSYD) from FRED...\nDownloading TGA (WTREGEN) from FRED...\nDownloading Bank_Reserves (TOTRESNS) from FRED...\nDownloading CI_Loans (TOTBKCR) from FRED...\nDownloading Consumer_Credit (TOTALSL) from FRED...\nDownloading Mortgage_Rate (MORTGAGE30US) from FRED...\nDownloading Prime_Rate (DPRIME) from FRED...\nDownloading STLFSI (STLFSI) from FRED...\nDownloading Dollar_Index (DTWEXBGS) from FRED...\nDownloading Household_Debt_Service (TDSP) from FRED...\n\n\n\n\n\n\n\n\n\nM2\nFED_BS\nFFR\nReal_Rate\nT3M\nT10Y\nBAA\nAAA\nCPI\nCORE_CPI\n...\nRRP\nTGA\nBank_Reserves\nCI_Loans\nConsumer_Credit\nMortgage_Rate\nPrime_Rate\nSTLFSI\nDollar_Index\nHousehold_Debt_Service\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2025-08-31\n22108.3\n6603384.0\n4.33\n1.523005\n4.05\n4.23\n6.03\n5.42\n323.364\n329.793\n...\n77.898\n589998.0\n3281.9\n18720.1524\n5059896.38\n6.56\n7.50\nNaN\n120.6028\nNaN\n\n\n2025-09-30\n22212.4\n6608395.0\n4.09\n1.149868\n3.86\n4.16\n5.83\n5.22\n324.368\n330.542\n...\n49.071\n804856.0\n3068.1\n18759.8099\n5071365.99\n6.30\n7.25\nNaN\n120.5624\nNaN\n\n\n2025-10-31\n22298.0\n6587034.0\n3.86\n0.973381\n3.73\n4.11\n5.80\n5.22\nNaN\nNaN\n...\n51.802\n957990.0\n2944.9\n18820.7752\n5080601.87\n6.17\n7.00\nNaN\n121.7715\nNaN\n\n\n2025-11-30\n22322.4\n6552419.0\n3.89\n1.001027\n3.73\n4.02\n5.80\n5.18\n325.031\n331.068\n...\n7.561\n903394.0\n2879.3\n18925.1502\n5084831.24\n6.23\n7.00\nNaN\n121.4288\nNaN\n\n\n2025-12-31\nNaN\n6640618.0\n3.64\n-0.135174\n3.57\n4.18\n5.90\n5.35\n326.030\n331.860\n...\n105.993\n837306.0\nNaN\n18934.1126\nNaN\n6.15\n6.75\nNaN\n120.1166\nNaN\n\n\n\n\n5 rows × 22 columns\ndef download_ff_factors(start, end):\n    \"\"\"\n    Download monthly Fama-French 5 factors (2x3) using pandas_datareader.\n    \"\"\"\n    print(\"Downloading Fama-French factors (famafrench)...\")\n    ff_raw = pdr.DataReader(\n        name=FF_FACTORS_DATASET,\n        data_source=\"famafrench\",\n        start=start,\n        end=end\n    )[0]  # table [0] contains the data\n\n    ff = (ff_raw\n          .divide(100)  # convert from % to decimal\n          .reset_index(names=\"date\")\n          .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n          .rename(str.lower, axis=\"columns\")\n          .rename(columns={\"mkt-rf\": \"mkt_excess\"}))\n\n    ff = ff.set_index(\"date\")\n    return ff\n\nff_factors = download_ff_factors(start_date, end_date)\nff_adj = ff_factors.copy()\nff_adj.index = ff_adj.index.to_period(\"M\").to_timestamp(\"M\")\nff_adj.tail()\n\nDownloading Fama-French factors (famafrench)...\n\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-07-31\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-31\n0.0185\n0.0488\n0.0442\n-0.0067\n0.0208\n0.0038\n\n\n2025-09-30\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-10-31\n0.0196\n-0.0131\n-0.0309\n-0.0522\n-0.0403\n0.0037\n\n\n2025-11-30\n-0.0013\n0.0147\n0.0376\n0.0143\n0.0068\n0.0030"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death of Valuations - v2.html#feature-engineering",
    "href": "notebooks/Liquidity Regimes and the Death of Valuations - v2.html#feature-engineering",
    "title": "3.2 Fama-French Factors and Valuation Data",
    "section": "4. Feature Engineering",
    "text": "4. Feature Engineering\n\n4.1 Construct Liquidity Variables\n\ndef engineer_liquidity_features(df):\n    \"\"\"\n    Create liquidity-related features from raw macro data\n    \"\"\"\n    df = df.copy()\n    \n    # 1. Growth rates\n    df['M2_growth'] = df['M2'].pct_change(12) * 100  # YoY % change\n    df['FED_BS_growth'] = df['FED_BS'].pct_change(12) * 100\n    \n    # 2. Term spread\n    df['Term_Spread'] = df['T10Y'] - df['FFR']\n    \n    # 3. Credit spread\n    df['Credit_Spread'] = df['BAA'] - df['AAA']\n    \n    # 4. Real interest rate (ex-post using CPI)\n    df['CPI_inflation'] = df['CPI'].pct_change(12) * 100  # YoY inflation\n    #df['Real_Rate'] = df['FFR'] - df['CPI_inflation']\n    \n    # 5. Valuation metrics\n    df['log_CAPE'] = np.log(df['CAPE'])\n    df['CAPE_zscore'] = (df['CAPE'] - df['CAPE'].rolling(60).mean()) / df['CAPE'].rolling(60).std()\n    \n    # 6. Risk-adjusted term spread\n    df['Risk_Adj_Spread'] = df['Term_Spread'] / (df['VIX'] / 100)\n\n    #7 Net liquidity\n    df['Net_Liq'] = df['FED_BS'] - (df['TGA'] + df['RRP'])\n    \n    return df\n\n# Apply feature engineering\ndf_features = engineer_liquidity_features(combined_df)\n\nprint(\"✅ Feature engineering complete\")\nprint(f\"\\nNew features created:\")\nnew_cols = [c for c in df_features.columns if c not in combined_df.columns]\nfor col in new_cols:\n    print(f\"  - {col}\")\n\ndf_features.tail(5)\n\n✅ Feature engineering complete\n\nNew features created:\n  - M2_growth\n  - FED_BS_growth\n  - Term_Spread\n  - Credit_Spread\n  - CPI_inflation\n  - log_CAPE\n  - CAPE_zscore\n  - Risk_Adj_Spread\n  - Net_Liq\n\n\n\n\n\n\n\n\n\nM2\nFED_BS\nFFR\nReal_Rate\nT3M\nT10Y\nBAA\nAAA\nCPI\nCORE_CPI\n...\nVIX\nM2_growth\nFED_BS_growth\nTerm_Spread\nCredit_Spread\nCPI_inflation\nlog_CAPE\nCAPE_zscore\nRisk_Adj_Spread\nNet_Liq\n\n\n\n\n2025-07-31\n22028.7\n6642578.0\n4.33\n0.838904\n4.24\n4.37\n6.04\n5.41\n322.132\n328.656\n...\n16.72\n4.480153\n-7.464249\n0.04\n0.63\n2.731801\n3.623541\n1.242869\n0.239234\n6271856.555\n\n\n2025-08-31\n22108.3\n6603384.0\n4.33\n1.523005\n4.05\n4.23\n6.03\n5.42\n323.364\n329.793\n...\nNaN\n4.427283\n-7.298001\n-0.10\n0.61\n2.939220\n3.633631\n1.306865\nNaN\n6013308.102\n\n\n2025-09-30\n22212.4\n6608395.0\n4.09\n1.149868\n3.86\n4.16\n5.83\n5.22\n324.368\n330.542\n...\n16.28\n4.492553\n-6.661865\n0.07\n0.61\n3.022700\n3.652734\n1.461587\n0.429975\n5803489.929\n\n\n2025-10-31\n22298.0\n6587034.0\n3.86\n0.973381\n3.73\n4.11\n5.80\n5.22\nNaN\nNaN\n...\n17.44\n4.645651\n-6.080511\n0.25\n0.58\n2.789925\n3.668932\n1.575675\n1.433486\n5628992.198\n\n\n2025-11-30\n22322.4\n6552419.0\n3.89\n1.001027\n3.73\n4.02\n5.80\n5.18\n325.031\n331.068\n...\nNaN\n4.271300\n-5.108093\n0.13\n0.62\n2.711969\n3.666634\n1.490940\nNaN\n5649017.439\n\n\n\n\n5 rows × 40 columns\n\n\n\n\n\n4.2 Construct Valuation Spread Measure\nWe define valuation spread as the return differential between cheap (high B/M) and expensive (low B/M) stocks. This is proxied by the HML (High Minus Low) factor from Fama-French.\n\ndef construct_valuation_spread(df):\n    \"\"\"\n    Construct valuation spread measure\n    \n    Primary measure: HML (Fama-French) represents value premium\n    Secondary: CAPE z-score as market-level valuation\n    \"\"\"\n    df = df.copy()\n    \n    # Cumulative HML performance (value vs growth)\n    df['HML_cumret'] = (1 + df['hml'] / 100).cumprod()\n    \n    # Rolling 12-month HML performance\n    df['HML_12m'] = df['hml'].rolling(12).sum()\n    \n    # Define valuation spread as:\n    # Positive = cheap stocks outperforming (value regime)\n    # Negative = expensive stocks outperforming (growth regime)\n    df['V_spread'] = df['HML_12m']  # 12-month rolling HML\n    \n    # Standardize\n    df['V_spread_z'] = (df['V_spread'] - df['V_spread'].mean()) / df['V_spread'].std()\n    \n    # Also use CAPE z-score as alternative valuation measure\n    df['V_level'] = df['CAPE_zscore']\n    \n    return df\n\ndf_features = construct_valuation_spread(df_features)\n\nprint(\"✅ Valuation spread measures constructed\")\nprint(f\"\\nV_spread (HML 12m) stats:\")\nprint(df_features['V_spread'].describe())\ndf_features.tail(5)\n\n✅ Valuation spread measures constructed\n\nV_spread (HML 12m) stats:\ncount    420.000000\nmean       0.018185\nstd        0.141636\nmin       -0.417500\n25%       -0.078100\n50%        0.016600\n75%        0.095125\nmax        0.579800\nName: V_spread, dtype: float64\n\n\n\n\n\n\n\n\n\nM2\nFED_BS\nFFR\nReal_Rate\nT3M\nT10Y\nBAA\nAAA\nCPI\nCORE_CPI\n...\nCPI_inflation\nlog_CAPE\nCAPE_zscore\nRisk_Adj_Spread\nNet_Liq\nHML_cumret\nHML_12m\nV_spread\nV_spread_z\nV_level\n\n\n\n\n2025-07-31\n22028.7\n6642578.0\n4.33\n0.838904\n4.24\n4.37\n6.04\n5.41\n322.132\n328.656\n...\n2.731801\n3.623541\n1.242869\n0.239234\n6271856.555\n1.005492\n-0.0558\n-0.0558\n-0.522355\n1.242869\n\n\n2025-08-31\n22108.3\n6603384.0\n4.33\n1.523005\n4.05\n4.23\n6.03\n5.42\n323.364\n329.793\n...\n2.939220\n3.633631\n1.306865\nNaN\n6013308.102\n1.005936\n-0.0006\n-0.0006\n-0.132625\n1.306865\n\n\n2025-09-30\n22212.4\n6608395.0\n4.09\n1.149868\n3.86\n4.16\n5.83\n5.22\n324.368\n330.542\n...\n3.022700\n3.652734\n1.461587\n0.429975\n5803489.929\n1.005831\n0.0166\n0.0166\n-0.011187\n1.461587\n\n\n2025-10-31\n22298.0\n6587034.0\n3.86\n0.973381\n3.73\n4.11\n5.80\n5.22\nNaN\nNaN\n...\n2.789925\n3.668932\n1.575675\n1.433486\n5628992.198\n1.005520\n-0.0229\n-0.0229\n-0.290070\n1.575675\n\n\n2025-11-30\n22322.4\n6552419.0\n3.89\n1.001027\n3.73\n4.02\n5.80\n5.18\n325.031\n331.068\n...\n2.711969\n3.666634\n1.490940\nNaN\n5649017.439\n1.005898\n0.0132\n0.0132\n-0.035192\n1.490940\n\n\n\n\n5 rows × 45 columns"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death of Valuations - v2.html#liquidity-index-construction-via-sparse-pca",
    "href": "notebooks/Liquidity Regimes and the Death of Valuations - v2.html#liquidity-index-construction-via-sparse-pca",
    "title": "3.2 Fama-French Factors and Valuation Data",
    "section": "5. Liquidity Index Construction via Sparse PCA",
    "text": "5. Liquidity Index Construction via Sparse PCA\n\n5.1 Methodology\nWe construct a composite liquidity index using Sparse PCA (Zou et al. 2006, Witten et al. 2009) which:\n\nMaximizes explained variance like standard PCA\nEnforces sparsity via L1 penalty for interpretability\nSelects only the most relevant liquidity indicators\n\nLiquidity proxies: - Fed balance sheet growth (QE intensity) - Real interest rates (opportunity cost) - Term spread (yield curve steepness) - Credit spreads (credit conditions) - VIX (risk appetite)\nAll variables are sign-adjusted so higher values = easier liquidity conditions.\n\ndf_features.tail(1)\n\n\n\n\n\n\n\n\nM2\nFED_BS\nFFR\nReal_Rate\nT3M\nT10Y\nBAA\nAAA\nCPI\nCORE_CPI\n...\nCredit_Spread\nCPI_inflation\nlog_CAPE\nCAPE_zscore\nRisk_Adj_Spread\nHML_cumret\nHML_12m\nV_spread\nV_spread_z\nV_level\n\n\n\n\n2025-11-30\n22322.4\n6552419.0\n3.89\n1.001027\n3.73\n4.02\n5.8\n5.18\n325.031\n331.068\n...\n0.62\n2.711969\n3.666634\n1.49094\nNaN\n1.005898\n0.0132\n0.0132\n-0.035192\n1.49094\n\n\n\n\n1 rows × 33 columns\n\n\n\n\ndef build_liquidity_index(df, method='sparse', n_components=1, alpha=1.0):\n    \"\"\"\n    Build liquidity index using PCA or Sparse PCA\n    \n    Parameters:\n    -----------\n    df : DataFrame\n        Input data\n    method : str\n        'pca' or 'sparse'\n    n_components : int\n        Number of components\n    alpha : float\n        Sparsity penalty (only for sparse PCA)\n    \"\"\"\n    # Select liquidity variables\n    liq_vars = [\n        'M2_growth',\n        'FED_BS_growth',\n        'Term_Spread',\n        'Real_Rate',\n        'Credit_Spread',\n        'VIX'\n    ]\n    \n    # Extract data and drop NaN\n    X = df[liq_vars].copy()\n    X = X.dropna()\n    \n    # Sign adjustments: higher = easier liquidity\n    X['Real_Rate'] = -X['Real_Rate']  # Lower real rates = easier\n    X['Credit_Spread'] = -X['Credit_Spread']  # Tighter spreads = easier\n    X['VIX'] = -X['VIX']  # Lower VIX = easier\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_scaled_df = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n    \n    # Fit PCA/Sparse PCA\n    if method == 'sparse':\n        model = SparsePCA(n_components=n_components, alpha=alpha, random_state=42)\n    else:\n        model = PCA(n_components=n_components)\n    \n    # Fit and transform\n    L = model.fit_transform(X_scaled)\n    \n    # Create series\n    L_series = pd.Series(L[:, 0], index=X.index, name='L')\n    \n    # Get loadings\n    loadings = pd.DataFrame(\n        model.components_.T,\n        index=X.columns,\n        columns=['PC1']\n    )\n    \n    # Calculate explained variance\n    if method == 'pca':\n        explained_var = model.explained_variance_ratio_[0]\n    else:\n        # For sparse PCA, compute manually\n        var_explained = np.var(L[:, 0]) / np.sum(np.var(X_scaled, axis=0))\n        explained_var = var_explained\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Liquidity Index Construction ({method.upper()})\")\n    print(f\"{'='*70}\")\n    print(f\"\\nVariance explained: {explained_var:.2%}\")\n    print(f\"\\nLoadings:\")\n    print(loadings.sort_values('PC1', ascending=False))\n    print(f\"\\nLiquidity index range: {L_series.min():.2f} to {L_series.max():.2f}\")\n    print(f\"Mean: {L_series.mean():.2f}, Std: {L_series.std():.2f}\")\n    \n    return L_series, loadings, model, X_scaled_df\n\n# Build liquidity index\nL_index, loadings, pca_model, scaled_features = build_liquidity_index(\n    df_features,\n    method='sparse',\n    alpha=0.5  # Moderate sparsity\n)\n\n\n======================================================================\nLiquidity Index Construction (SPARSE)\n======================================================================\n\nVariance explained: 44.98%\n\nLoadings:\n                    PC1\nFED_BS_growth  0.525122\nM2_growth      0.379190\nTerm_Spread    0.370752\nReal_Rate      0.244985\nCredit_Spread -0.414407\nVIX           -0.459623\n\nLiquidity index range: -3.15 to 7.11\nMean: 0.00, Std: 1.65\n\n\n\nLiquidity Index Augmentation: Two-layer liquidity: Flow vs Stock / Excess\n\n\nAdd level / excess variables\nExamples (all can be pulled from FRED):\nLog level vs pre-2008 trend\nLet \\(m_t = \\log M2_t\\). Fit a linear trend on a pre-QE baseline (say 1985–2007):\n\\(m_t \\approx a + bt \\quad (t \\le 2007)\\)\nThen define excess money stock:\n\\(EM_t = m_t - (a + bt)\\)\nAfter 2008, \\(EM_t\\) becomes increasingly positive if M2 grows above its historical trend.\nSimilarly for the Fed Balance Sheet, let \\(b_t = \\log(\\text{FedBal}_t)\\):\n\\(EB_t = b_t - (\\alpha + \\beta t)\\) (pre-2007 trend)\n\nExcess liquidity versus real economy\nUse real GDP series (e.g., GDPC1) and define:\n\\(EL_t = \\log M2_t - \\log GDP_t\\)\nOr measure multi-year excess growth:\n\\(EL_t(3y) = \\log M2_t - \\log M2_{t-36} - (\\log GDP_t - \\log GDP_{t-36})\\)\n\nZIRP / negative real-rate indicator\n\\(D_t^{ZIRP} = 1(r_t^{real} &lt; 0)\\)\nFrom 2009–2015, this should be mostly 1.\n\n\n\nBuild an augmented feature vector\nInstead of only:\n\\(x_t = \\{ \\Delta \\log M2_t,\\; \\Delta \\log FedBal_t,\\; TS_t,\\; r_t^{real},\\; CS_t \\}\\)\nLet’s expand to:\n\\(x_t^{aug} = \\{\n\\Delta \\log M2_t,\\;\n\\Delta \\log FedBal_t,\\;\nTS_t,\\;\nr_t^{real},\\;\nCS_t,\\;\nEM_t,\\;\nEB_t,\\;\nEL_t,\\;\nEL_t(3y),\\;\nD_t^{ZIRP}\n\\}\\)\nThen standardize and perform PCA / SparsePCA on this.\n\nPC1 (or a combination) loading heavily on \\(EM\\), \\(EB\\), \\(EL\\), \\(D_t^{ZIRP}\\) → structural easy-money regime\n\nPC2 (or your current PC1) loading on short-horizon changes → flow / shock liquidity\n\n\ndf_features.columns\n\nIndex(['M2', 'FED_BS', 'FFR', 'Real_Rate', 'T3M', 'T10Y', 'BAA', 'AAA', 'CPI',\n       'CORE_CPI', 'GDP', 'IORB', 'RRP', 'TGA', 'Bank_Reserves', 'CI_Loans',\n       'Consumer_Credit', 'Mortgage_Rate', 'Prime_Rate', 'STLFSI',\n       'Dollar_Index', 'Household_Debt_Service', 'mkt_excess', 'smb', 'hml',\n       'rmw', 'cma', 'rf', 'P', 'CAPE', 'VIX', 'M2_growth', 'FED_BS_growth',\n       'Term_Spread', 'Credit_Spread', 'CPI_inflation', 'log_CAPE',\n       'CAPE_zscore', 'Risk_Adj_Spread', 'Net_Liq', 'HML_cumret', 'HML_12m',\n       'V_spread', 'V_spread_z', 'V_level'],\n      dtype='object')\n\n\n\ndef augment_liquidity_index(df_features):\n    df = df_features.copy()\n\n    # Flow proxies\n    df[\"dlog_M2\"]      = np.log(df[\"M2\"]).diff()\n    df[\"dlog_FED_BS\"]  = np.log(df[\"FED_BS\"]).diff()\n\n    # Levels\n    df[\"log_M2\"]      = np.log(df[\"M2\"])\n    df[\"log_FED_BS\"] = np.log(df[\"FED_BS\"])\n\n    # Use data up to 2007-12-31 to fit trends\n    pre = df.loc[: \"2007-12-31\"].copy()\n\n    # --- Trend for M2 ---\n    pre_m2 = pre[\"log_M2\"].dropna()\n    t_m2   = np.arange(len(pre_m2))\n    coefs_M2 = np.polyfit(t_m2, pre_m2.values, deg=1)\n\n    t_full = np.arange(len(df))\n    trend_M2 = np.polyval(coefs_M2, t_full)\n    df[\"EM\"] = df[\"log_M2\"] - trend_M2\n\n    # --- Trend for Fed balance sheet ---\n    pre_fb = pre[\"log_FED_BS\"].dropna()\n    t_fb   = np.arange(len(pre_fb))\n    coefs_FB = np.polyfit(t_fb, pre_fb.values, deg=1)\n\n    trend_FB = np.polyval(coefs_FB, t_full)\n    df[\"EB\"] = df[\"log_FED_BS\"] - trend_FB\n\n    # 3. Excess liquidity vs GDP over 3y\n    df[\"log_GDP\"] = np.log(df[\"GDP\"])\n    df[\"EL_3y\"] = (df[\"log_M2\"] - df[\"log_M2\"].shift(36)) - \\\n                  (df[\"log_GDP\"] - df[\"log_GDP\"].shift(36))\n\n    return df\n\ndf_features_aug = augment_liquidity_index(df_features)\ndf_features_aug.tail(1)\n\n\n\n\n\n\n\n\nM2\nFED_BS\nFFR\nReal_Rate\nT3M\nT10Y\nBAA\nAAA\nCPI\nCORE_CPI\n...\nV_spread_z\nV_level\ndlog_M2\ndlog_FED_BS\nlog_M2\nlog_FED_BS\nEM\nEB\nlog_GDP\nEL_3y\n\n\n\n\n2025-11-30\n22322.4\n6552419.0\n3.89\n1.001027\n3.73\n4.02\n5.8\n5.18\n325.031\n331.068\n...\n-0.035192\n1.49094\n0.001094\n-0.005269\n10.013346\n15.695345\n0.19326\n0.685233\nNaN\nNaN\n\n\n\n\n1 rows × 53 columns\n\n\n\n\ndef build_liquidity_index_aug(df, method='sparse', n_components=1, alpha=1.0):\n    \"\"\"\n    Build liquidity index using PCA or Sparse PCA\n    \n    Parameters:\n    -----------\n    df : DataFrame\n        Input data\n    method : str\n        'pca' or 'sparse'\n    n_components : int\n        Number of components\n    alpha : float\n        Sparsity penalty (only for sparse PCA)\n    \"\"\"\n    # Select liquidity variables\n    liq_vars = [\n        'M2_growth',\n        'FED_BS_growth',\n        'EM',\n        'EB',\n        'EL_3y',\n        'Term_Spread',\n        'Real_Rate',\n        'Credit_Spread',\n        'VIX'\n    ]\n    \n    # Extract data and drop NaN\n    X = df[liq_vars].copy()\n    X = X.dropna()\n    \n    # Sign adjustments: higher = easier liquidity\n    X['Real_Rate'] = -X['Real_Rate']  # Lower real rates = easier\n    X['Credit_Spread'] = -X['Credit_Spread']  # Tighter spreads = easier\n    X['VIX'] = -X['VIX']  # Lower VIX = easier\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_scaled_df = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n    \n    # Fit PCA/Sparse PCA\n    if method == 'sparse':\n        model = SparsePCA(n_components=n_components, alpha=alpha, random_state=42)\n    else:\n        model = PCA(n_components=n_components)\n    \n    # Fit and transform\n    L = model.fit_transform(X_scaled)\n    \n    # Create series\n    L_series = pd.Series(L[:, 0], index=X.index, name='L')\n    \n    # Get loadings\n    loadings = pd.DataFrame(\n        model.components_.T,\n        index=X.columns,\n        columns=['PC1']\n    )\n    \n    # Calculate explained variance\n    if method == 'pca':\n        explained_var = model.explained_variance_ratio_[0]\n    else:\n        # For sparse PCA, compute manually\n        var_explained = np.var(L[:, 0]) / np.sum(np.var(X_scaled, axis=0))\n        explained_var = var_explained\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Liquidity Index Construction ({method.upper()})\")\n    print(f\"{'='*70}\")\n    print(f\"\\nVariance explained: {explained_var:.2%}\")\n    print(f\"\\nLoadings:\")\n    print(loadings.sort_values('PC1', ascending=False))\n    print(f\"\\nLiquidity index range: {L_series.min():.2f} to {L_series.max():.2f}\")\n    print(f\"Mean: {L_series.mean():.2f}, Std: {L_series.std():.2f}\")\n    \n    return L_series, loadings, model, X_scaled_df\n\n# Build liquidity index\nL_index, loadings, pca_model, scaled_features = build_liquidity_index_aug(\n    df_features_aug,\n    method='sparse',\n    alpha=0.5  # Moderate sparsity\n)\n\n\n======================================================================\nLiquidity Index Construction (SPARSE)\n======================================================================\n\nVariance explained: 37.40%\n\nLoadings:\n                    PC1\nEL_3y          0.479785\nFED_BS_growth  0.414107\nM2_growth      0.386169\nTerm_Spread    0.365477\nReal_Rate      0.348663\nEB             0.135796\nEM             0.091370\nCredit_Spread -0.242676\nVIX           -0.329204\n\nLiquidity index range: -4.23 to 4.97\nMean: -0.00, Std: 1.84\n\n\n\n\n\nExplained variance is too small = ~45%\n\ndef build_liquidity_index_aug_1(df, method='sparse', n_components=1, alpha=1.0):\n    \"\"\"\n    Build liquidity index using PCA or Sparse PCA\n    \n    Parameters:\n    -----------\n    df : DataFrame\n        Input data\n    method : str\n        'pca' or 'sparse'\n    n_components : int\n        Number of components\n    alpha : float\n        Sparsity penalty (only for sparse PCA)\n    \"\"\"\n    # Select liquidity variables\n    # liq_vars = [\n    #     'Net_Liq',\n    #     'M2_growth',\n    #     'FED_BS_growth',\n    #     'EM',\n    #     'EB',\n    #     'EL_3y',\n    #     'Term_Spread',\n    #     'Real_Rate',\n    #     'Credit_Spread',\n    #     'VIX',\n    #     'Dollar_Index',\n    #     'Household_Debt_Service',\n    #     'STLFSI'\n    # ]\n    liq_vars = [\n        'M2_growth',\n        'FED_BS_growth',\n        'Term_Spread',\n        'Real_Rate',\n        'Credit_Spread',\n        'VIX',\n        'EL_3y'\n    ]\n    \n    # Extract data and drop NaN\n    X = df[liq_vars].copy()\n    X = X.dropna()\n    \n    # ========================================\n    # CORRECT SIGN ADJUSTMENTS\n    # Rule: Higher value = EASIER liquidity\n    # ========================================\n    \n    # Variables to FLIP (higher originally means tighter)\n    flip_vars = {\n        'Real_Rate': 'Higher real rates = tighter → FLIP',\n        'Credit_Spread': 'Wider spreads = tighter → FLIP',\n        'VIX': 'Higher VIX = tighter → FLIP',\n        'STLFSI': 'Higher stress = tighter → FLIP',\n        'Dollar_Index': 'Stronger dollar = tighter → FLIP',\n        'Household_Debt_Service': 'Higher burden = tighter → FLIP',\n    }\n    \n    # Variables to KEEP (higher already means easier)\n    keep_vars = {\n        'Net_Liq': 'Higher net liquidity = easier → KEEP',\n        'M2_growth': 'Higher growth = easier → KEEP',\n        'FED_BS_growth': 'Higher growth = easier → KEEP',\n        'EM': 'Higher excess M2 = easier → KEEP',\n        'EB': 'Higher excess Fed BS = easier → KEEP',\n        'EL_3y': 'Higher excess liquidity = easier → KEEP',\n        'Term_Spread': 'Positive slope = easier → KEEP',\n    }\n    \n    print(\"\\n➖ Flipping (higher = tighter):\")\n    for var, reason in flip_vars.items():\n        if var in X.columns:\n            X[var] = -X[var]\n            print(f\"   {var:25s}: {reason}\")\n    \n    print(\"\\n✅ Keeping (higher = easier):\")\n    for var, reason in keep_vars.items():\n        if var in X.columns:\n            print(f\"   {var:25s}: {reason}\")\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    X_scaled_df = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\n    \n    # Fit PCA/Sparse PCA\n    if method == 'sparse':\n        model = SparsePCA(n_components=n_components, alpha=alpha, random_state=42)\n    else:\n        model = PCA(n_components=n_components)\n    \n    # Fit and transform\n    L = model.fit_transform(X_scaled)\n    \n    # Create series\n    L_series = pd.Series(L[:, 0], index=X.index, name='L')\n    \n    # Get loadings\n    loadings = pd.DataFrame(\n        model.components_.T,\n        index=X.columns,\n        columns=['PC1']\n    )\n    \n    # Calculate explained variance\n    if method == 'pca':\n        explained_var = model.explained_variance_ratio_[0]\n    else:\n        # For sparse PCA, compute manually\n        var_explained = np.var(L[:, 0]) / np.sum(np.var(X_scaled, axis=0))\n        explained_var = var_explained\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Liquidity Index Construction ({method.upper()})\")\n    print(f\"{'='*70}\")\n    print(f\"\\nVariance explained: {explained_var:.2%}\")\n    print(f\"\\nLoadings:\")\n    print(loadings.sort_values('PC1', ascending=False))\n    print(f\"\\nLiquidity index range: {L_series.min():.2f} to {L_series.max():.2f}\")\n    print(f\"Mean: {L_series.mean():.2f}, Std: {L_series.std():.2f}\")\n    \n    return L_series, loadings, model, X_scaled_df\n\n# Build liquidity index\nL_index, loadings, pca_model, scaled_features = build_liquidity_index_aug_1(\n    df_features_aug,\n    method='sparse',\n    alpha=0.5  # Moderate sparsity\n)\n\n\n➖ Flipping (higher = tighter):\n   Real_Rate                : Higher real rates = tighter → FLIP\n   Credit_Spread            : Wider spreads = tighter → FLIP\n   VIX                      : Higher VIX = tighter → FLIP\n\n✅ Keeping (higher = easier):\n   M2_growth                : Higher growth = easier → KEEP\n   FED_BS_growth            : Higher growth = easier → KEEP\n   EL_3y                    : Higher excess liquidity = easier → KEEP\n   Term_Spread              : Positive slope = easier → KEEP\n\n======================================================================\nLiquidity Index Construction (SPARSE)\n======================================================================\n\nVariance explained: 47.21%\n\nLoadings:\n                    PC1\nEL_3y          0.472608\nFED_BS_growth  0.428525\nTerm_Spread    0.385863\nM2_growth      0.381727\nReal_Rate      0.334564\nCredit_Spread -0.269403\nVIX           -0.337477\n\nLiquidity index range: -4.46 to 5.37\nMean: -0.00, Std: 1.82"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death of Valuations - v2.html#regime-detection-via-hidden-markov-models",
    "href": "notebooks/Liquidity Regimes and the Death of Valuations - v2.html#regime-detection-via-hidden-markov-models",
    "title": "3.2 Fama-French Factors and Valuation Data",
    "section": "6. Regime Detection via Hidden Markov Models",
    "text": "6. Regime Detection via Hidden Markov Models\n\n6.1 HMM Specification\nWe employ a 3-state Gaussian HMM to identify: - State 0 (Tight): Low liquidity, restrictive monetary policy - State 1 (Neutral): Normal conditions - State 2 (High): Easy liquidity, expansionary policy\nThe model assumes: \\[L_t | s_t = k \\sim \\mathcal{N}(\\mu_k, \\sigma_k^2)\\]\nWith transition matrix capturing regime persistence.\n\ndef fit_hmm_regimes(L_series, n_states=3, n_iter=1000):\n    \"\"\"\n    Fit Hidden Markov Model to identify liquidity regimes\n    \n    Parameters:\n    -----------\n    L_series : Series\n        Liquidity index\n    n_states : int\n        Number of hidden states\n    n_iter : int\n        Maximum EM iterations\n    \"\"\"\n    # Prepare data\n    X = L_series.dropna().values.reshape(-1, 1)\n    \n    # Fit HMM\n    model = hmm.GaussianHMM(\n        n_components=n_states,\n        covariance_type='full',\n        n_iter=n_iter,\n        random_state=42\n    )\n    \n    model.fit(X)\n    \n    # Predict states (Viterbi algorithm)\n    states = model.predict(X)\n    \n    # Get posterior probabilities\n    posteriors = model.predict_proba(X)\n    \n    # Create results DataFrame\n    results = pd.DataFrame({\n        'L': L_series.dropna(),\n        'state': states\n    })\n    \n    for i in range(n_states):\n        results[f'prob_state_{i}'] = posteriors[:, i]\n    \n    # Sort states by mean (0=Tight, 1=Neutral, 2=High)\n    state_means = results.groupby('state')['L'].mean().sort_values()\n    state_mapping = {old: new for new, old in enumerate(state_means.index)}\n    results['state'] = results['state'].map(state_mapping)\n    \n    # Add labels\n    state_labels = {0: 'Tight', 1: 'Neutral', 2: 'High'}\n    results['state_label'] = results['state'].map(state_labels)\n    \n    # Print diagnostics\n    print(f\"\\n{'='*70}\")\n    print(f\"HMM Regime Detection ({n_states} states)\")\n    print(f\"{'='*70}\")\n    print(f\"\\nModel converged: {model.monitor_.converged}\")\n    print(f\"Log-likelihood: {model.score(X):.2f}\")\n    \n    print(f\"\\nState statistics:\")\n    for state in range(n_states):\n        mask = results['state'] == state\n        n_obs = mask.sum()\n        pct = n_obs / len(results) * 100\n        mean_L = results.loc[mask, 'L'].mean()\n        std_L = results.loc[mask, 'L'].std()\n        label = state_labels[state]\n        print(f\"  State {state} ({label}): {n_obs} obs ({pct:.1f}%), \"\n              f\"L = {mean_L:+.2f} ± {std_L:.2f}\")\n    \n    print(f\"\\nTransition Matrix:\")\n    trans_matrix = pd.DataFrame(\n        model.transmat_,\n        index=[f\"From_{state_labels[i]}\" for i in range(n_states)],\n        columns=[f\"To_{state_labels[i]}\" for i in range(n_states)]\n    )\n    print(trans_matrix.round(3))\n    \n    # Calculate regime persistence (diagonal elements)\n    persistence = np.diag(model.transmat_)\n    print(f\"\\nRegime persistence (prob. of staying):\")\n    for i, label in state_labels.items():\n        print(f\"  {label}: {persistence[i]:.1%}\")\n    \n    return results, model, state_labels\n\n# Fit HMM\nregimes_df, hmm_model, state_labels = fit_hmm_regimes(L_index, n_states=3)\n\n\n======================================================================\nHMM Regime Detection (3 states)\n======================================================================\n\nModel converged: True\nLog-likelihood: -301.76\n\nState statistics:\n  State 0 (Tight): 80 obs (43.2%), L = -1.62 ± 0.99\n  State 1 (Neutral): 53 obs (28.6%), L = +1.21 ± 1.24\n  State 2 (High): 52 obs (28.1%), L = +1.26 ± 1.29\n\nTransition Matrix:\n              To_Tight  To_Neutral  To_High\nFrom_Tight       0.001       0.999    0.000\nFrom_Neutral     0.935       0.009    0.056\nFrom_High        0.022       0.004    0.974\n\nRegime persistence (prob. of staying):\n  Tight: 0.1%\n  Neutral: 0.9%\n  High: 97.4%\n\n\n\n\nValidation\n\ndef map_to_known_liquidity_regimes():\n    \"\"\"\n    Define known Fed policy regimes based on historical record\n    \"\"\"\n    \n    # Format: (start_date, end_date, regime_type, description)\n    known_regimes = [\n        # =====================================\n        # Pre-Crisis Era (1990-2007)\n        # =====================================\n        ('1990-01', '1992-12', 'Neutral', 'Post-1990 Recession, Gradual Easing'),\n        ('1993-01', '1994-01', 'High', 'Accommodative Policy, Economic Expansion'),\n        ('1994-02', '1995-12', 'Tight', 'Fed Tightening Cycle (3.0% → 6.0%)'),\n        ('1996-01', '1998-12', 'Neutral', 'Goldilocks Economy'),\n        ('1999-01', '2000-05', 'Tight', 'Dot-com Bubble, Y2K Tightening'),\n        ('2000-06', '2003-06', 'High', 'Dot-com Crash Response, Aggressive Easing'),\n        ('2003-07', '2004-05', 'High', 'Ultra-Low Rates (1%), Housing Boom'),\n        ('2004-06', '2006-06', 'Tight', 'Fed Hiking Cycle (1% → 5.25%)'),\n        ('2006-07', '2007-07', 'Neutral', 'Pause at Peak, Pre-Crisis'),\n        \n        # =====================================\n        # Financial Crisis & QE Era (2007-2015)\n        # =====================================\n        ('2007-08', '2008-08', 'Neutral', 'Initial Crisis Response, Rate Cuts Begin'),\n        ('2008-09', '2010-03', 'High', 'QE1: Emergency Response ($1.7T)'),\n        ('2010-04', '2010-10', 'Neutral', 'QE1 Taper, Brief Pause'),\n        ('2010-11', '2011-06', 'High', 'QE2: $600B Purchase Program'),\n        ('2011-07', '2012-08', 'Neutral', 'Operation Twist, Pre-QE3'),\n        ('2012-09', '2014-10', 'High', 'QE3: $85B/month, then taper'),\n        ('2014-11', '2015-11', 'Neutral', 'Post-QE3, ZIRP Maintained'),\n        \n        # =====================================\n        # Normalization Attempt (2015-2019)\n        # =====================================\n        ('2015-12', '2018-12', 'Tight', 'Fed Hiking + QT (0.25% → 2.50%)'),\n        ('2019-01', '2019-07', 'Neutral', 'Pause, Economic Slowdown'),\n        ('2019-08', '2019-12', 'High', 'Insurance Cuts + Repo Crisis Response'),\n        \n        # =====================================\n        # COVID Era (2020-2021)\n        # =====================================\n        ('2020-01', '2020-02', 'Neutral', 'Pre-COVID'),\n        ('2020-03', '2021-10', 'High', 'COVID QE: Unlimited purchases, ZIRP'),\n        ('2021-11', '2022-02', 'High', 'Taper Announcement but still easy'),\n        \n        # =====================================\n        # Modern Tightening (2022-2025)\n        # =====================================\n        ('2022-03', '2023-06', 'Tight', 'Aggressive Hikes (0% → 5.25%)'),\n        ('2023-07', '2024-08', 'Tight', 'Higher for Longer, QT Active'),\n        ('2024-09', '2025-01', 'Neutral', 'Easing Cycle Begins'),\n    ]\n    \n    # Convert to DataFrame\n    regime_df = pd.DataFrame(known_regimes, columns=['start', 'end', 'regime', 'description'])\n    regime_df['start'] = pd.to_datetime(regime_df['start'])\n    regime_df['end'] = pd.to_datetime(regime_df['end'])\n    \n    return regime_df\n\n\ndef validate_hmm_against_known_regimes(regimes_df, known_regimes_df):\n    \"\"\"\n    Compare HMM-detected regimes to known Fed policy periods\n    \"\"\"\n    \n    print(f\"\\n{'='*70}\")\n    print(\"REGIME VALIDATION: HMM vs Known Fed Policy\")\n    print(f\"{'='*70}\")\n    \n    # Expand known regimes to monthly observations\n    known_monthly = []\n    for _, row in known_regimes_df.iterrows():\n        date_range = pd.date_range(row['start'], row['end'], freq='M')\n        for date in date_range:\n            known_monthly.append({\n                'date': date,\n                'known_regime': row['regime'],\n                'period_description': row['description']\n            })\n    \n    known_df = pd.DataFrame(known_monthly).set_index('date')\n    \n    # Merge with HMM results\n    comparison = regimes_df.join(known_df, how='inner')\n    comparison = comparison.dropna(subset=['known_regime', 'state_label'])\n    \n    print(f\"\\n📊 Overlap period: {comparison.index[0].strftime('%Y-%m')} to {comparison.index[-1].strftime('%Y-%m')}\")\n    print(f\"   Total months: {len(comparison)}\")\n    \n    # =====================================\n    # Confusion Matrix\n    # =====================================\n    \n    print(f\"\\n{'='*70}\")\n    print(\"Confusion Matrix: HMM vs Known Regimes\")\n    print(f\"{'='*70}\")\n    \n    from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n    \n    # Create confusion matrix\n    cm = confusion_matrix(comparison['known_regime'], comparison['state_label'], \n                         labels=['Tight', 'Neutral', 'High'])\n    \n    cm_df = pd.DataFrame(cm, \n                        index=['Known_Tight', 'Known_Neutral', 'Known_High'],\n                        columns=['HMM_Tight', 'HMM_Neutral', 'HMM_High'])\n    \n    print(\"\\n\", cm_df)\n    \n    # Calculate percentages\n    cm_pct = cm_df.div(cm_df.sum(axis=1), axis=0) * 100\n    print(\"\\nPercentages (row-wise):\")\n    print(cm_pct.round(1))\n    \n    # Overall accuracy\n    accuracy = accuracy_score(comparison['known_regime'], comparison['state_label'])\n    print(f\"\\n📈 Overall Accuracy: {accuracy:.1%}\")\n    \n    # =====================================\n    # Regime-by-Regime Analysis\n    # =====================================\n    \n    print(f\"\\n{'='*70}\")\n    print(\"Detailed Regime Concordance\")\n    print(f\"{'='*70}\")\n    \n    for known_regime in ['Tight', 'Neutral', 'High']:\n        mask = comparison['known_regime'] == known_regime\n        subset = comparison[mask]\n        \n        print(f\"\\n{known_regime.upper()} REGIME ({mask.sum()} months):\")\n        \n        # How HMM classified these periods\n        hmm_dist = subset['state_label'].value_counts(normalize=True) * 100\n        for hmm_regime, pct in hmm_dist.sort_index().items():\n            status = \"✅\" if hmm_regime == known_regime else \"⚠️\"\n            print(f\"   {status} Classified as {hmm_regime:8s}: {pct:5.1f}%\")\n    \n    # =====================================\n    # Key Historical Periods Check\n    # =====================================\n    \n    print(f\"\\n{'='*70}\")\n    print(\"Key Historical Periods Validation\")\n    print(f\"{'='*70}\")\n    \n    key_periods = [\n        ('2008-09', '2009-12', 'High', 'QE1 (Financial Crisis)'),\n        ('2010-11', '2011-06', 'High', 'QE2'),\n        ('2012-09', '2014-10', 'High', 'QE3'),\n        ('2015-12', '2018-12', 'Tight', 'Fed Hiking + QT'),\n        ('2020-03', '2021-06', 'High', 'COVID QE (Unlimited)'),\n        ('2022-03', '2023-12', 'Tight', 'Aggressive Rate Hikes'),\n    ]\n    \n    for start, end, expected, description in key_periods:\n        try:\n            mask = (comparison.index &gt;= start) & (comparison.index &lt;= end)\n            subset = comparison[mask]\n            \n            if len(subset) == 0:\n                continue\n            \n            # Modal HMM regime\n            hmm_mode = subset['state_label'].mode()[0]\n            hmm_pct = (subset['state_label'] == hmm_mode).sum() / len(subset) * 100\n            \n            # Check if matches expectation\n            match = \"✅\" if hmm_mode == expected else \"❌\"\n            \n            print(f\"\\n{match} {description}\")\n            print(f\"   Period: {start} to {end}\")\n            print(f\"   Expected: {expected}\")\n            print(f\"   HMM Mode: {hmm_mode} ({hmm_pct:.0f}% of period)\")\n            \n            # Show distribution\n            dist = subset['state_label'].value_counts()\n            print(f\"   Distribution: \", end=\"\")\n            for regime, count in dist.items():\n                print(f\"{regime}={count}, \", end=\"\")\n            print()\n            \n        except Exception as e:\n            print(f\"\\n⚠️  {description}: Data not available\")\n    \n    # =====================================\n    # Mismatches Analysis\n    # =====================================\n    \n    print(f\"\\n{'='*70}\")\n    print(\"Significant Mismatches (Known ≠ HMM)\")\n    print(f\"{'='*70}\")\n    \n    mismatches = comparison[comparison['known_regime'] != comparison['state_label']]\n    \n    if len(mismatches) &gt; 0:\n        # Find consecutive mismatch periods\n        mismatch_periods = []\n        current_start = None\n        \n        for i, (date, row) in enumerate(mismatches.iterrows()):\n            if current_start is None:\n                current_start = date\n                current_known = row['known_regime']\n                current_hmm = row['state_label']\n            \n            # Check if next row continues the mismatch pattern\n            is_last = i == len(mismatches) - 1\n            if is_last or (not is_last and mismatches.index[i+1] != date + pd.DateOffset(months=1)):\n                mismatch_periods.append({\n                    'start': current_start,\n                    'end': date,\n                    'known': current_known,\n                    'hmm': current_hmm,\n                    'months': (date.year - current_start.year) * 12 + date.month - current_start.month + 1\n                })\n                current_start = None\n        \n        # Show significant mismatches (&gt;= 3 months)\n        significant = [p for p in mismatch_periods if p['months'] &gt;= 3]\n        \n        if significant:\n            print(f\"\\nFound {len(significant)} significant mismatch periods (≥3 months):\\n\")\n            for p in significant:\n                print(f\"   {p['start'].strftime('%Y-%m')} to {p['end'].strftime('%Y-%m')} ({p['months']} months)\")\n                print(f\"      Known: {p['known']:8s} | HMM: {p['hmm']:8s}\")\n                \n                # Get description for this period\n                desc_mask = (known_regimes_df['start'] &lt;= p['start']) & (known_regimes_df['end'] &gt;= p['end'])\n                if desc_mask.any():\n                    desc = known_regimes_df[desc_mask]['description'].iloc[0]\n                    print(f\"      Context: {desc}\")\n                print()\n        else:\n            print(\"\\n✅ No significant consecutive mismatches (all discrepancies &lt; 3 months)\")\n    \n    else:\n        print(\"\\n🎉 Perfect match! HMM exactly matches known regimes.\")\n    \n    # =====================================\n    # Summary Statistics\n    # =====================================\n    \n    print(f\"\\n{'='*70}\")\n    print(\"SUMMARY STATISTICS\")\n    print(f\"{'='*70}\")\n    \n    print(f\"\\n📊 Agreement Rates by Regime:\")\n    for regime in ['Tight', 'Neutral', 'High']:\n        mask = comparison['known_regime'] == regime\n        if mask.sum() &gt; 0:\n            agreement = (comparison[mask]['state_label'] == regime).sum() / mask.sum() * 100\n            print(f\"   {regime:8s}: {agreement:5.1f}% correct\")\n    \n    print(f\"\\n📊 Overall Performance:\")\n    print(f\"   Total Accuracy:        {accuracy:.1%}\")\n    print(f\"   Mismatch Rate:         {(1-accuracy):.1%}\")\n    print(f\"   Consecutive Mismatches: {len([p for p in mismatch_periods if p['months'] &gt;= 3])}\")\n    \n    # Interpretation\n    print(f\"\\n{'='*70}\")\n    print(\"INTERPRETATION\")\n    print(f\"{'='*70}\")\n    \n    if accuracy &gt;= 0.80:\n        print(\"\\n✅ EXCELLENT: HMM successfully captures Fed policy regimes\")\n        print(\"   The model shows strong alignment with known monetary policy history.\")\n    elif accuracy &gt;= 0.65:\n        print(\"\\n✔️  GOOD: HMM generally aligns with Fed policy regimes\")\n        print(\"   Some discrepancies exist but the model captures major policy shifts.\")\n    elif accuracy &gt;= 0.50:\n        print(\"\\n⚠️  MODERATE: HMM shows partial alignment with Fed policy\")\n        print(\"   Consider adjusting liquidity index composition or HMM parameters.\")\n    else:\n        print(\"\\n❌ POOR: HMM does not align well with Fed policy regimes\")\n        print(\"   Major revisions needed to liquidity index or regime detection.\")\n    \n    return comparison, cm_df, accuracy\n\n\n# Run validation\nknown_regimes_df = map_to_known_liquidity_regimes()\ncomparison_df, confusion_matrix, accuracy = validate_hmm_against_known_regimes(\n    regimes_df, \n    known_regimes_df\n)\n\n\n======================================================================\nREGIME VALIDATION: HMM vs Known Fed Policy\n======================================================================\n\n📊 Overlap period: 2003-12 to 2024-12\n   Total months: 166\n\n======================================================================\nConfusion Matrix: HMM vs Known Regimes\n======================================================================\n\n                HMM_Tight  HMM_Neutral  HMM_High\nKnown_Tight           44           11         9\nKnown_Neutral         21           12        13\nKnown_High             2           28        26\n\nPercentages (row-wise):\n               HMM_Tight  HMM_Neutral  HMM_High\nKnown_Tight         68.8         17.2      14.1\nKnown_Neutral       45.7         26.1      28.3\nKnown_High           3.6         50.0      46.4\n\n📈 Overall Accuracy: 49.4%\n\n======================================================================\nDetailed Regime Concordance\n======================================================================\n\nTIGHT REGIME (64 months):\n   ⚠️ Classified as High    :  14.1%\n   ⚠️ Classified as Neutral :  17.2%\n   ✅ Classified as Tight   :  68.8%\n\nNEUTRAL REGIME (46 months):\n   ⚠️ Classified as High    :  28.3%\n   ✅ Classified as Neutral :  26.1%\n   ⚠️ Classified as Tight   :  45.7%\n\nHIGH REGIME (56 months):\n   ✅ Classified as High    :  46.4%\n   ⚠️ Classified as Neutral :  50.0%\n   ⚠️ Classified as Tight   :   3.6%\n\n======================================================================\nKey Historical Periods Validation\n======================================================================\n\n✅ QE1 (Financial Crisis)\n   Period: 2008-09 to 2009-12\n   Expected: High\n   HMM Mode: High (50% of period)\n   Distribution: Neutral=5, High=5, \n\n✅ QE2\n   Period: 2010-11 to 2011-06\n   Expected: High\n   HMM Mode: High (50% of period)\n   Distribution: Neutral=3, High=3, \n\n✅ QE3\n   Period: 2012-09 to 2014-10\n   Expected: High\n   HMM Mode: High (50% of period)\n   Distribution: High=9, Neutral=9, \n\n✅ Fed Hiking + QT\n   Period: 2015-12 to 2018-12\n   Expected: Tight\n   HMM Mode: Tight (58% of period)\n   Distribution: Tight=15, Neutral=6, High=5, \n\n✅ COVID QE (Unlimited)\n   Period: 2020-03 to 2021-06\n   Expected: High\n   HMM Mode: High (50% of period)\n   Distribution: High=5, Neutral=5, \n\n✅ Aggressive Rate Hikes\n   Period: 2022-03 to 2023-12\n   Expected: Tight\n   HMM Mode: Tight (53% of period)\n   Distribution: Tight=8, Neutral=4, High=3, \n\n======================================================================\nSignificant Mismatches (Known ≠ HMM)\n======================================================================\n\nFound 1 significant mismatch periods (≥3 months):\n\n   2007-12 to 2008-02 (3 months)\n      Known: Neutral  | HMM: Tight   \n      Context: Initial Crisis Response, Rate Cuts Begin\n\n\n======================================================================\nSUMMARY STATISTICS\n======================================================================\n\n📊 Agreement Rates by Regime:\n   Tight   :  68.8% correct\n   Neutral :  26.1% correct\n   High    :  46.4% correct\n\n📊 Overall Performance:\n   Total Accuracy:        49.4%\n   Mismatch Rate:         50.6%\n   Consecutive Mismatches: 1\n\n======================================================================\nINTERPRETATION\n======================================================================\n\n❌ POOR: HMM does not align well with Fed policy regimes\n   Major revisions needed to liquidity index or regime detection.\n\n\n\ndef build_advanced_regime_classification_system(df, L_series):\n    \"\"\"\n    State-of-the-art regime classification with proper index alignment\n    \"\"\"\n    \n    import numpy as np\n    import pandas as pd\n    from sklearn.ensemble import IsolationForest, RandomForestClassifier\n    from xgboost import XGBClassifier\n    from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n    import shap\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    \n    print(f\"\\n{'='*80}\")\n    print(\"ADVANCED REGIME CLASSIFICATION SYSTEM\")\n    print(f\"{'='*80}\")\n    \n    # ==========================================\n    # STEP 0: DATA ALIGNMENT\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 0: DATA ALIGNMENT\")\n    print(f\"{'='*80}\")\n    \n    # Find common index\n    common_idx = df.index.intersection(L_series.index)\n    \n    if len(common_idx) == 0:\n        raise ValueError(\"No overlapping dates between df and L_series!\")\n    \n    print(f\"\\n📊 Data Coverage:\")\n    print(f\"   df range:       {df.index[0]} to {df.index[-1]} ({len(df)} obs)\")\n    print(f\"   L_series range: {L_series.index[0]} to {L_series.index[-1]} ({len(L_series)} obs)\")\n    print(f\"   Overlap range:  {common_idx[0]} to {common_idx[-1]} ({len(common_idx)} obs)\")\n    \n    # Use only overlapping data\n    df_aligned = df.loc[common_idx].copy()\n    L_aligned = L_series.loc[common_idx].copy()\n    \n    # Add L to dataframe\n    df_aligned['L'] = L_aligned\n    \n    # Check for required columns\n    required_cols = ['mkt_excess']\n    missing_cols = [c for c in required_cols if c not in df_aligned.columns]\n    if missing_cols:\n        raise ValueError(f\"Missing required columns: {missing_cols}\")\n    \n    print(f\"\\n✅ Data aligned: {len(df_aligned)} observations\")\n    \n    # ==========================================\n    # STEP 1: Crisis Detection (Isolation Forest)\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 1: CRISIS DETECTION\")\n    print(f\"{'='*80}\")\n    \n    # Create crisis features\n    df_crisis = df_aligned.copy()\n    \n    # Volatility features\n    df_crisis['ret_vol_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).std()\n    df_crisis['ret_vol_12m'] = df_crisis['mkt_excess'].rolling(12, min_periods=6).std()\n    df_crisis['L_vol_3m'] = df_crisis['L'].rolling(3, min_periods=2).std()\n    df_crisis['L_vol_6m'] = df_crisis['L'].rolling(6, min_periods=3).std()\n    \n    # Return features\n    df_crisis['abs_ret'] = df_crisis['mkt_excess'].abs()\n    df_crisis['cum_ret_3m'] = df_crisis['mkt_excess'].rolling(3, min_periods=2).sum()\n    df_crisis['cum_ret_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).sum()\n    \n    # Drawdown\n    cumret = (1 + df_crisis['mkt_excess']/100).cumprod()\n    running_max = cumret.expanding().max()\n    df_crisis['drawdown'] = (cumret / running_max - 1) * 100\n    df_crisis['max_dd_6m'] = df_crisis['drawdown'].rolling(6, min_periods=3).min()\n    \n    # VIX if available\n    crisis_features = ['ret_vol_6m', 'ret_vol_12m', 'L_vol_3m', 'L_vol_6m', \n                       'abs_ret', 'cum_ret_3m', 'cum_ret_6m', 'max_dd_6m']\n    \n    if 'VIX' in df_crisis.columns:\n        df_crisis['VIX_level'] = df_crisis['VIX']\n        df_crisis['VIX_zscore'] = (df_crisis['VIX'] - df_crisis['VIX'].rolling(60, min_periods=30).mean()) / \\\n                                   df_crisis['VIX'].rolling(60, min_periods=30).std()\n        crisis_features.extend(['VIX_level', 'VIX_zscore'])\n    \n    # Check data availability\n    print(f\"\\n📊 Crisis Features:\")\n    for feat in crisis_features:\n        if feat in df_crisis.columns:\n            n_valid = df_crisis[feat].notna().sum()\n            print(f\"   {feat:20s}: {n_valid}/{len(df_crisis)} valid\")\n    \n    # Extract crisis features and drop NaN\n    crisis_data = df_crisis[crisis_features].copy()\n    \n    # Fill forward initial NaN from rolling windows (up to 12 months)\n    crisis_data = crisis_data.fillna(method='bfill', limit=12)\n    crisis_data = crisis_data.dropna()\n    \n    print(f\"\\n📊 Crisis detection dataset: {len(crisis_data)} observations\")\n    \n    if len(crisis_data) &lt; 20:\n        print(f\"\\n⚠️  WARNING: Not enough data for crisis detection ({len(crisis_data)} obs)\")\n        print(f\"   Skipping crisis detection, will use all data for regime classification\")\n        \n        df_crisis['is_crisis'] = False\n        df_noncrisis = df_crisis.copy()\n        \n    else:\n        # Fit Isolation Forest\n        iso_forest = IsolationForest(\n            contamination=0.05,\n            random_state=42,\n            n_estimators=100,\n            max_samples='auto'\n        )\n        \n        outlier_labels = iso_forest.fit_predict(crisis_data.values)\n        \n        # Crisis = outlier + negative return\n        is_crisis_candidate = (outlier_labels == -1)\n        is_negative_return = (df_crisis.loc[crisis_data.index, 'mkt_excess'] &lt; -2)\n        \n        df_crisis['is_crisis'] = False\n        df_crisis.loc[crisis_data.index, 'is_crisis'] = is_crisis_candidate & is_negative_return\n        \n        n_crisis = df_crisis['is_crisis'].sum()\n        crisis_pct = n_crisis / len(df_crisis) * 100\n        \n        print(f\"\\n✅ Crisis Detection Complete\")\n        print(f\"   Crisis periods:     {n_crisis} months ({crisis_pct:.1f}%)\")\n        print(f\"   Non-crisis periods: {len(df_crisis) - n_crisis} months ({100-crisis_pct:.1f}%)\")\n        \n        # Show crisis periods\n        if n_crisis &gt; 0:\n            crisis_periods = df_crisis[df_crisis['is_crisis']].index\n            print(f\"\\n📅 Detected Crisis Months:\")\n            for i, date in enumerate(crisis_periods[:10], 1):\n                ret = df_crisis.loc[date, 'mkt_excess']\n                print(f\"   {i:2d}. {date.strftime('%Y-%m')}: Return = {ret:+.2f}%\")\n            if len(crisis_periods) &gt; 10:\n                print(f\"   ... and {len(crisis_periods) - 10} more\")\n        \n        # Filter to non-crisis\n        df_noncrisis = df_crisis[~df_crisis['is_crisis']].copy()\n    \n    # ==========================================\n    # STEP 2: Regime Classification Setup\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 2: REGIME CLASSIFICATION (NON-CRISIS PERIODS)\")\n    print(f\"{'='*80}\")\n    \n    # Create regime labels based on liquidity quantiles\n    L_noncrisis = df_noncrisis['L']\n    \n    if len(L_noncrisis) &lt; 30:\n        raise ValueError(f\"Not enough non-crisis data: {len(L_noncrisis)} observations\")\n    \n    q33 = L_noncrisis.quantile(0.33)\n    q67 = L_noncrisis.quantile(0.67)\n    \n    print(f\"\\n📊 Liquidity Quantiles:\")\n    print(f\"   33rd percentile (q33): {q33:+.2f}\")\n    print(f\"   67th percentile (q67): {q67:+.2f}\")\n    \n    def assign_regime(L_val):\n        if L_val &lt; q33:\n            return 0  # Tight\n        elif L_val &lt; q67:\n            return 1  # Neutral\n        else:\n            return 2  # High\n    \n    df_noncrisis['regime'] = L_noncrisis.apply(assign_regime)\n    df_noncrisis['regime_label'] = df_noncrisis['regime'].map({0: 'Tight', 1: 'Neutral', 2: 'High'})\n    \n    print(f\"\\n📊 Non-Crisis Regime Distribution:\")\n    for regime, label in [(0, 'Tight'), (1, 'Neutral'), (2, 'High')]:\n        mask = df_noncrisis['regime'] == regime\n        count = mask.sum()\n        pct = count / len(df_noncrisis) * 100\n        mean_L = df_noncrisis[mask]['L'].mean()\n        print(f\"   {label:8s}: {count:4d} months ({pct:5.1f}%), Mean L = {mean_L:+.2f}\")\n    \n    # ==========================================\n    # STEP 3: Feature Engineering\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 3: FEATURE ENGINEERING\")\n    print(f\"{'='*80}\")\n    \n    # Build feature set\n    feature_cols = []\n    \n    # Core liquidity components\n    liq_features = ['dlog_M2', 'dlog_FED_BS', 'EM', 'EB', 'EL_3y']\n    if 'Net_Liq' in df_noncrisis.columns:\n        liq_features.append('Net_Liq')\n    \n    for feat in liq_features:\n        if feat in df_noncrisis.columns:\n            feature_cols.append(feat)\n    \n    # Rates and spreads\n    rate_features = ['Term_Spread', 'Credit_Spread']\n    if 'Real_Rate' in df_noncrisis.columns:\n        rate_features.append('Real_Rate')\n    \n    for feat in rate_features:\n        if feat in df_noncrisis.columns:\n            feature_cols.append(feat)\n    \n    # Market indicators\n    if 'VIX' in df_noncrisis.columns:\n        feature_cols.append('VIX')\n    \n    # Growth rates\n    if 'M2_growth' in df_noncrisis.columns:\n        feature_cols.append('M2_growth')\n    if 'FED_BS_growth' in df_noncrisis.columns:\n        feature_cols.append('FED_BS_growth')\n    \n    # Technical features on L\n    df_noncrisis['L_ma_3'] = df_noncrisis['L'].rolling(3, min_periods=1).mean()\n    df_noncrisis['L_ma_6'] = df_noncrisis['L'].rolling(6, min_periods=3).mean()\n    df_noncrisis['L_momentum'] = df_noncrisis['L'].diff(3)\n    \n    feature_cols.extend(['L_ma_3', 'L_ma_6', 'L_momentum'])\n    \n    # Valuation if available\n    if 'V_spread' in df_noncrisis.columns:\n        feature_cols.append('V_spread')\n    \n    print(f\"\\n✅ Selected {len(feature_cols)} features:\")\n    for i, feat in enumerate(feature_cols, 1):\n        if feat in df_noncrisis.columns:\n            n_valid = df_noncrisis[feat].notna().sum()\n            print(f\"   {i:2d}. {feat:20s}: {n_valid}/{len(df_noncrisis)} valid\")\n    \n    # Prepare feature matrix\n    X = df_noncrisis[feature_cols].copy()\n    y = df_noncrisis['regime'].copy()\n    \n    # Forward fill initial NaN from rolling windows\n    X = X.fillna(method='ffill', limit=6)\n    \n    # Drop remaining NaN\n    valid_idx = X.notna().all(axis=1) & y.notna()\n    X = X[valid_idx]\n    y = y[valid_idx]\n    \n    print(f\"\\n📊 Final ML dataset: {len(X)} samples × {len(feature_cols)} features\")\n    \n    if len(X) &lt; 30:\n        raise ValueError(f\"Not enough valid data for ML: {len(X)} samples\")\n    \n    # ==========================================\n    # STEP 4: Train ML Models\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 4: MODEL TRAINING\")\n    print(f\"{'='*80}\")\n    \n    # Time series split\n    tscv = TimeSeriesSplit(n_splits=min(5, len(X) // 20))\n    \n    # Models\n    models = {\n        'Random Forest': RandomForestClassifier(\n            n_estimators=100,\n            max_depth=8,\n            min_samples_split=10,\n            min_samples_leaf=5,\n            class_weight='balanced',\n            random_state=42,\n            n_jobs=-1\n        ),\n        'XGBoost': XGBClassifier(\n            n_estimators=100,\n            max_depth=5,\n            learning_rate=0.1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42,\n            eval_metric='mlogloss',\n            use_label_encoder=False,\n            n_jobs=-1\n        )\n    }\n    \n    results = {}\n    \n    for name, model in models.items():\n        print(f\"\\n{'='*70}\")\n        print(f\"Training {name}\")\n        print(f\"{'='*70}\")\n        \n        # Cross-validation\n        try:\n            cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy', n_jobs=-1)\n            print(f\"\\n📊 CV Accuracy: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})\")\n        except Exception as e:\n            print(f\"\\n⚠️  CV failed: {e}\")\n            cv_scores = np.array([])\n        \n        # Train full model\n        model.fit(X, y)\n        \n        # Predictions\n        y_pred = model.predict(X)\n        y_pred_proba = model.predict_proba(X)\n        \n        # Metrics\n        accuracy = (y_pred == y).mean()\n        \n        try:\n            roc_auc = roc_auc_score(y, y_pred_proba, multi_class='ovr', average='weighted')\n        except:\n            roc_auc = np.nan\n        \n        print(f\"\\n📈 Performance:\")\n        print(f\"   Accuracy: {accuracy:.3f}\")\n        if not np.isnan(roc_auc):\n            print(f\"   ROC-AUC:  {roc_auc:.3f}\")\n        \n        # Classification report\n        print(f\"\\n📋 Classification Report:\")\n        print(classification_report(y, y_pred, target_names=['Tight', 'Neutral', 'High'], zero_division=0))\n        \n        # Confusion matrix\n        cm = confusion_matrix(y, y_pred)\n        cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n        \n        print(f\"\\n🎯 Confusion Matrix (%):\")\n        cm_df = pd.DataFrame(\n            cm_pct,\n            index=['True_Tight', 'True_Neutral', 'True_High'],\n            columns=['Pred_Tight', 'Pred_Neutral', 'Pred_High']\n        )\n        print(cm_df.round(1))\n        \n        results[name] = {\n            'model': model,\n            'cv_scores': cv_scores,\n            'accuracy': accuracy,\n            'roc_auc': roc_auc,\n            'y_pred': y_pred,\n            'y_pred_proba': y_pred_proba\n        }\n    \n    # ==========================================\n    # STEP 5: Feature Importance\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 5: FEATURE IMPORTANCE\")\n    print(f\"{'='*80}\")\n    \n    best_model = results['XGBoost']['model']\n    \n    feature_importance = pd.DataFrame({\n        'feature': X.columns,\n        'importance': best_model.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    print(f\"\\n📊 Top 10 Features:\")\n    for i, row in feature_importance.head(10).iterrows():\n        print(f\"   {row['feature']:20s}: {row['importance']:.4f}\")\n    \n    # ==========================================\n    # STEP 6: Final Labels\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 6: FINAL REGIME ASSIGNMENT\")\n    print(f\"{'='*80}\")\n    \n    df_final = df_crisis.copy()\n    df_final['regime'] = np.nan\n    df_final['regime_label'] = 'Unknown'\n    \n    # Crisis\n    if 'is_crisis' in df_final.columns:\n        df_final.loc[df_final['is_crisis'], 'regime'] = 3\n        df_final.loc[df_final['is_crisis'], 'regime_label'] = 'Crisis'\n    \n    # ML predictions\n    ml_idx = X.index\n    df_final.loc[ml_idx, 'regime'] = results['XGBoost']['y_pred']\n    df_final.loc[ml_idx, 'regime_label'] = pd.Series(\n        results['XGBoost']['y_pred'],\n        index=ml_idx\n    ).map({0: 'Tight', 1: 'Neutral', 2: 'High'})\n    \n    print(f\"\\n📊 Final Distribution:\")\n    dist = df_final['regime_label'].value_counts()\n    for regime, count in dist.items():\n        pct = count / len(df_final) * 100\n        print(f\"   {regime:8s}: {count:4d} ({pct:5.1f}%)\")\n    \n    print(f\"\\n{'='*80}\")\n    print(\"✅ COMPLETE\")\n    print(f\"{'='*80}\")\n    \n    return df_final, results, feature_importance\n\n\n# Run with proper inputs\ndf_regimes_advanced, ml_results, feature_importance = build_advanced_regime_classification_system(\n    df_features_aug,\n    L_index\n)\n\n\n================================================================================\nADVANCED REGIME CLASSIFICATION SYSTEM\n================================================================================\n\n================================================================================\nSTEP 0: DATA ALIGNMENT\n================================================================================\n\n📊 Data Coverage:\n   df range:       1990-01-31 00:00:00 to 2025-11-30 00:00:00 (431 obs)\n   L_series range: 2003-12-31 00:00:00 to 2025-09-30 00:00:00 (185 obs)\n   Overlap range:  2003-12-31 00:00:00 to 2025-09-30 00:00:00 (185 obs)\n\n✅ Data aligned: 185 observations\n\n================================================================================\nSTEP 1: CRISIS DETECTION\n================================================================================\n\n📊 Crisis Features:\n   ret_vol_6m          : 183/185 valid\n   ret_vol_12m         : 180/185 valid\n   L_vol_3m            : 184/185 valid\n   L_vol_6m            : 183/185 valid\n   abs_ret             : 185/185 valid\n   cum_ret_3m          : 184/185 valid\n   cum_ret_6m          : 183/185 valid\n   max_dd_6m           : 183/185 valid\n   VIX_level           : 185/185 valid\n   VIX_zscore          : 156/185 valid\n\n📊 Crisis detection dataset: 168 observations\n\n✅ Crisis Detection Complete\n   Crisis periods:     0 months (0.0%)\n   Non-crisis periods: 185 months (100.0%)\n\n================================================================================\nSTEP 2: REGIME CLASSIFICATION (NON-CRISIS PERIODS)\n================================================================================\n\n📊 Liquidity Quantiles:\n   33rd percentile (q33): -0.80\n   67th percentile (q67): +0.69\n\n📊 Non-Crisis Regime Distribution:\n   Tight   :   61 months ( 33.0%), Mean L = -1.95\n   Neutral :   63 months ( 34.1%), Mean L = +0.00\n   High    :   61 months ( 33.0%), Mean L = +1.95\n\n================================================================================\nSTEP 3: FEATURE ENGINEERING\n================================================================================\n\n✅ Selected 16 features:\n    1. dlog_M2             : 185/185 valid\n    2. dlog_FED_BS         : 185/185 valid\n    3. EM                  : 185/185 valid\n    4. EB                  : 185/185 valid\n    5. EL_3y               : 185/185 valid\n    6. Net_Liq             : 118/185 valid\n    7. Term_Spread         : 185/185 valid\n    8. Credit_Spread       : 185/185 valid\n    9. Real_Rate           : 185/185 valid\n   10. VIX                 : 185/185 valid\n   11. M2_growth           : 185/185 valid\n   12. FED_BS_growth       : 185/185 valid\n   13. L_ma_3              : 185/185 valid\n   14. L_ma_6              : 183/185 valid\n   15. L_momentum          : 182/185 valid\n   16. V_spread            : 185/185 valid\n\n📊 Final ML dataset: 155 samples × 16 features\n\n================================================================================\nSTEP 4: MODEL TRAINING\n================================================================================\n\n======================================================================\nTraining Random Forest\n======================================================================\n\n📊 CV Accuracy: 0.800 (±0.104)\n\n📈 Performance:\n   Accuracy: 0.935\n   ROC-AUC:  0.995\n\n📋 Classification Report:\n              precision    recall  f1-score   support\n\n       Tight       0.95      0.95      0.95        42\n     Neutral       0.89      0.93      0.91        54\n        High       0.96      0.93      0.95        59\n\n    accuracy                           0.94       155\n   macro avg       0.94      0.94      0.94       155\nweighted avg       0.94      0.94      0.94       155\n\n\n🎯 Confusion Matrix (%):\n              Pred_Tight  Pred_Neutral  Pred_High\nTrue_Tight          95.2           4.8        0.0\nTrue_Neutral         3.7          92.6        3.7\nTrue_High            0.0           6.8       93.2\n\n======================================================================\nTraining XGBoost\n======================================================================\n\n📊 CV Accuracy: 0.744 (±0.151)\n\n📈 Performance:\n   Accuracy: 1.000\n   ROC-AUC:  1.000\n\n📋 Classification Report:\n              precision    recall  f1-score   support\n\n       Tight       1.00      1.00      1.00        42\n     Neutral       1.00      1.00      1.00        54\n        High       1.00      1.00      1.00        59\n\n    accuracy                           1.00       155\n   macro avg       1.00      1.00      1.00       155\nweighted avg       1.00      1.00      1.00       155\n\n\n🎯 Confusion Matrix (%):\n              Pred_Tight  Pred_Neutral  Pred_High\nTrue_Tight         100.0           0.0        0.0\nTrue_Neutral         0.0         100.0        0.0\nTrue_High            0.0           0.0      100.0\n\n================================================================================\nSTEP 5: FEATURE IMPORTANCE\n================================================================================\n\n📊 Top 10 Features:\n   L_ma_6              : 0.3007\n   L_ma_3              : 0.2085\n   Term_Spread         : 0.1153\n   FED_BS_growth       : 0.0552\n   EL_3y               : 0.0481\n   EM                  : 0.0375\n   L_momentum          : 0.0371\n   Real_Rate           : 0.0302\n   Credit_Spread       : 0.0288\n   VIX                 : 0.0272\n\n================================================================================\nSTEP 6: FINAL REGIME ASSIGNMENT\n================================================================================\n\n📊 Final Distribution:\n   High    :   59 ( 31.9%)\n   Neutral :   54 ( 29.2%)\n   Tight   :   42 ( 22.7%)\n   Unknown :   30 ( 16.2%)\n\n================================================================================\n✅ COMPLETE\n================================================================================\n\n\n\ndf_regimes_advanced.tail(2)\n\n\n\n\n\n\n\n\nM2\nFED_BS\nFFR\nReal_Rate\nT3M\nT10Y\nBAA\nAAA\nCPI\nCORE_CPI\n...\nabs_ret\ncum_ret_3m\ncum_ret_6m\ndrawdown\nmax_dd_6m\nVIX_level\nVIX_zscore\nis_crisis\nregime\nregime_label\n\n\n\n\n2025-07-31\n22028.7\n6642578.0\n4.33\n0.838904\n4.24\n4.37\n6.04\n5.41\n322.132\n328.656\n...\n0.0198\n0.0600\n-0.0002\n-0.041738\n-0.110072\n16.72\n-0.536469\nFalse\n0.0\nTight\n\n\n2025-09-30\n22212.4\n6608395.0\n4.09\n1.149868\n3.86\n4.16\n5.83\n5.22\n324.368\n330.542\n...\n0.0339\n0.1023\n0.0057\n-0.007852\n-0.110072\n16.28\n-0.613043\nFalse\n0.0\nTight\n\n\n\n\n2 rows × 68 columns\n\n\n\n\n# factor_returns_by_regime = (\n#     df_regimes_advanced.groupby(\"regime_label\")[[\"mkt_excess\", \"hml\", \"rmw\", \"cma\"]]\n#     .mean() * 100\n# )\n# print(factor_returns_by_regime)\n\n\ndef diagnose_unknown_regimes(df_regimes_advanced, df_features_aug, L_index):\n    \"\"\"\n    Understand why some periods are marked as Unknown\n    \"\"\"\n    \n    print(f\"\\n{'='*80}\")\n    print(\"DIAGNOSIS: Unknown Regime Periods\")\n    print(f\"{'='*80}\")\n    \n    # Find Unknown periods\n    unknown_mask = df_regimes_advanced['regime_label'] == 'Unknown'\n    unknown_dates = df_regimes_advanced[unknown_mask].index\n    \n    print(f\"\\n📊 Unknown Periods: {len(unknown_dates)} months ({len(unknown_dates)/len(df_regimes_advanced)*100:.1f}%)\")\n    \n    if len(unknown_dates) &gt; 0:\n        print(f\"\\n📅 Unknown Date Range:\")\n        print(f\"   First: {unknown_dates[0].strftime('%Y-%m')}\")\n        print(f\"   Last:  {unknown_dates[-1].strftime('%Y-%m')}\")\n        \n        # Check which features are missing\n        print(f\"\\n🔍 Missing Data Analysis:\")\n        \n        feature_cols = ['dlog_M2', 'dlog_FED_BS', 'EM', 'EB', 'EL_3y', 'Net_Liq',\n                       'Term_Spread', 'Credit_Spread', 'Real_Rate', 'VIX',\n                       'M2_growth', 'FED_BS_growth', 'V_spread']\n        \n        for feat in feature_cols:\n            if feat in df_features_aug.columns:\n                # Check missing in unknown periods\n                unknown_data = df_features_aug.loc[unknown_dates, feat]\n                n_missing = unknown_data.isna().sum()\n                pct_missing = n_missing / len(unknown_dates) * 100\n                \n                if n_missing &gt; 0:\n                    print(f\"   {feat:20s}: {n_missing:3d}/{len(unknown_dates)} missing ({pct_missing:5.1f}%)\")\n        \n        # Check if Unknown periods cluster at start/end\n        print(f\"\\n📍 Unknown Period Distribution:\")\n        \n        # Group consecutive unknowns\n        unknown_groups = []\n        current_group = [unknown_dates[0]]\n        \n        for i in range(1, len(unknown_dates)):\n            if (unknown_dates[i] - unknown_dates[i-1]).days &lt; 60:  # Within 2 months\n                current_group.append(unknown_dates[i])\n            else:\n                unknown_groups.append(current_group)\n                current_group = [unknown_dates[i]]\n        unknown_groups.append(current_group)\n        \n        print(f\"   Found {len(unknown_groups)} clusters of unknown periods:\")\n        for i, group in enumerate(unknown_groups, 1):\n            print(f\"   Cluster {i}: {group[0].strftime('%Y-%m')} to {group[-1].strftime('%Y-%m')} ({len(group)} months)\")\n\n# Run diagnosis\ndiagnose_unknown_regimes(df_regimes_advanced, df_features_aug, L_index)\n\n\n================================================================================\nDIAGNOSIS: Unknown Regime Periods\n================================================================================\n\n📊 Unknown Periods: 30 months (16.2%)\n\n📅 Unknown Date Range:\n   First: 2003-12\n   Last:  2012-10\n\n🔍 Missing Data Analysis:\n   Net_Liq             :  29/30 missing ( 96.7%)\n\n📍 Unknown Period Distribution:\n   Found 14 clusters of unknown periods:\n   Cluster 1: 2003-12 to 2003-12 (1 months)\n   Cluster 2: 2004-03 to 2004-04 (2 months)\n   Cluster 3: 2004-12 to 2005-03 (4 months)\n   Cluster 4: 2005-05 to 2005-06 (2 months)\n   Cluster 5: 2005-08 to 2005-11 (4 months)\n   Cluster 6: 2006-01 to 2006-03 (3 months)\n   Cluster 7: 2006-05 to 2006-08 (4 months)\n   Cluster 8: 2006-10 to 2006-11 (2 months)\n   Cluster 9: 2007-01 to 2007-02 (2 months)\n   Cluster 10: 2008-02 to 2008-02 (1 months)\n   Cluster 11: 2009-11 to 2009-11 (1 months)\n   Cluster 12: 2012-05 to 2012-05 (1 months)\n   Cluster 13: 2012-07 to 2012-08 (2 months)\n   Cluster 14: 2012-10 to 2012-10 (1 months)\n\n\n\ndf_features_aug.columns\n\nIndex(['M2', 'FED_BS', 'FFR', 'Real_Rate', 'T3M', 'T10Y', 'BAA', 'AAA', 'CPI',\n       'CORE_CPI', 'GDP', 'IORB', 'RRP', 'TGA', 'Bank_Reserves', 'CI_Loans',\n       'Consumer_Credit', 'Mortgage_Rate', 'Prime_Rate', 'STLFSI',\n       'Dollar_Index', 'Household_Debt_Service', 'mkt_excess', 'smb', 'hml',\n       'rmw', 'cma', 'rf', 'P', 'CAPE', 'VIX', 'M2_growth', 'FED_BS_growth',\n       'Term_Spread', 'Credit_Spread', 'CPI_inflation', 'log_CAPE',\n       'CAPE_zscore', 'Risk_Adj_Spread', 'Net_Liq', 'HML_cumret', 'HML_12m',\n       'V_spread', 'V_spread_z', 'V_level', 'dlog_M2', 'dlog_FED_BS', 'log_M2',\n       'log_FED_BS', 'EM', 'EB', 'log_GDP', 'EL_3y'],\n      dtype='object')\n\n\n\ndef build_hml_regime_classification_with_crisis(df, L_series):\n    \"\"\"\n    1. Detect crisis periods (Isolation Forest)\n    2. Classify NON-CRISIS periods by HML regimes\n    3. Predict HML regimes from liquidity features\n    \"\"\"\n    \n    import numpy as np\n    import pandas as pd\n    from sklearn.ensemble import IsolationForest, RandomForestClassifier\n    from xgboost import XGBClassifier\n    from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n    from sklearn.metrics import classification_report, confusion_matrix\n    \n    print(f\"\\n{'='*80}\")\n    print(\"HML REGIME CLASSIFICATION (WITH CRISIS DETECTION)\")\n    print(f\"{'='*80}\")\n    \n    # ==========================================\n    # STEP 0: Data Alignment\n    # ==========================================\n    \n    common_idx = df.index.intersection(L_series.index)\n    df_aligned = df.loc[common_idx].copy()\n    df_aligned['L'] = L_series.loc[common_idx]\n    \n    print(f\"\\n✅ Aligned: {len(df_aligned)} observations\")\n    \n    # ==========================================\n    # STEP 1: Crisis Detection (Isolation Forest)\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 1: CRISIS DETECTION\")\n    print(f\"{'='*80}\")\n    \n    df_crisis = df_aligned.copy()\n    \n    # Create crisis features (volatility, drawdowns, tail events)\n    df_crisis['ret_vol_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).std()\n    df_crisis['ret_vol_12m'] = df_crisis['mkt_excess'].rolling(12, min_periods=6).std()\n    df_crisis['L_vol_3m'] = df_crisis['L'].rolling(3, min_periods=2).std()\n    df_crisis['L_vol_6m'] = df_crisis['L'].rolling(6, min_periods=3).std()\n    \n    # Return features\n    df_crisis['abs_ret'] = df_crisis['mkt_excess'].abs()\n    df_crisis['cum_ret_3m'] = df_crisis['mkt_excess'].rolling(3, min_periods=2).sum()\n    df_crisis['cum_ret_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).sum()\n    \n    # Drawdown\n    cumret = (1 + df_crisis['mkt_excess']/100).cumprod()\n    running_max = cumret.expanding().max()\n    df_crisis['drawdown'] = (cumret / running_max - 1) * 100\n    df_crisis['max_dd_6m'] = df_crisis['drawdown'].rolling(6, min_periods=3).min()\n    \n    # Crisis features list\n    crisis_features = ['ret_vol_6m', 'ret_vol_12m', 'L_vol_3m', 'L_vol_6m', \n                       'abs_ret', 'cum_ret_3m', 'cum_ret_6m', 'max_dd_6m']\n    \n    # Add VIX if available\n    if 'VIX' in df_crisis.columns:\n        df_crisis['VIX_level'] = df_crisis['VIX']\n        df_crisis['VIX_zscore'] = (df_crisis['VIX'] - df_crisis['VIX'].rolling(60, min_periods=30).mean()) / \\\n                                   df_crisis['VIX'].rolling(60, min_periods=30).std()\n        crisis_features.extend(['VIX_level', 'VIX_zscore'])\n    \n    print(f\"\\n📊 Crisis Features Created: {len(crisis_features)}\")\n    \n    # Extract and clean crisis data\n    crisis_data = df_crisis[crisis_features].copy()\n    crisis_data = crisis_data.fillna(method='bfill', limit=12)\n    crisis_data = crisis_data.dropna()\n    \n    print(f\"   Valid crisis detection samples: {len(crisis_data)}/{len(df_crisis)}\")\n    \n    if len(crisis_data) &gt;= 20:\n        # Fit Isolation Forest\n        iso_forest = IsolationForest(\n            contamination=0.05,  # Expect ~5% crisis periods\n            random_state=42,\n            n_estimators=100,\n            max_samples='auto'\n        )\n        \n        outlier_labels = iso_forest.fit_predict(crisis_data.values)\n        \n        # Crisis = outlier + negative return (&lt; -2%)\n        is_crisis_candidate = (outlier_labels == -1)\n        is_negative_return = (df_crisis.loc[crisis_data.index, 'mkt_excess'] &lt; -2)\n        \n        df_crisis['is_crisis'] = False\n        df_crisis.loc[crisis_data.index, 'is_crisis'] = is_crisis_candidate & is_negative_return\n        \n        n_crisis = df_crisis['is_crisis'].sum()\n        crisis_pct = n_crisis / len(df_crisis) * 100\n        \n        print(f\"\\n✅ Crisis Detection Complete:\")\n        print(f\"   Crisis periods:     {n_crisis} months ({crisis_pct:.1f}%)\")\n        print(f\"   Non-crisis periods: {len(df_crisis) - n_crisis} months ({100-crisis_pct:.1f}%)\")\n        \n        # Show detected crisis periods\n        if n_crisis &gt; 0:\n            crisis_periods = df_crisis[df_crisis['is_crisis']].index\n            print(f\"\\n📅 Detected Crisis Months:\")\n            for i, date in enumerate(crisis_periods[:10], 1):\n                ret = df_crisis.loc[date, 'mkt_excess']\n                L_val = df_crisis.loc[date, 'L']\n                print(f\"   {i:2d}. {date.strftime('%Y-%m')}: Return = {ret:+.2f}%, L = {L_val:+.2f}\")\n            if len(crisis_periods) &gt; 10:\n                print(f\"   ... and {len(crisis_periods) - 10} more\")\n    else:\n        print(f\"\\n⚠️  Insufficient data for crisis detection, marking all as non-crisis\")\n        df_crisis['is_crisis'] = False\n    \n    # Filter to non-crisis periods for HML regime analysis\n    df_noncrisis = df_crisis[~df_crisis['is_crisis']].copy()\n    \n    print(f\"\\n📊 Non-Crisis Data: {len(df_noncrisis)} observations\")\n    \n    # ==========================================\n    # STEP 2: Define HML Regimes (Non-Crisis Only)\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 2: DEFINE HML REGIMES (NON-CRISIS PERIODS)\")\n    print(f\"{'='*80}\")\n    \n    # Check for HML data\n    if 'hml' not in df_noncrisis.columns and 'HML' not in df_noncrisis.columns:\n        raise ValueError(\"No HML column found in dataframe\")\n    \n    hml_col = 'hml' if 'hml' in df_noncrisis.columns else 'HML'\n    \n    # Use 12-month rolling HML for smoother regimes\n    if 'HML_12m' in df_noncrisis.columns:\n        hml_metric = df_noncrisis['HML_12m']\n        metric_col = 'HML_12m'\n        metric_name = \"12-month rolling HML\"\n    else:\n        df_noncrisis['HML_12m'] = df_noncrisis[hml_col].rolling(12, min_periods=6).sum()\n        hml_metric = df_noncrisis['HML_12m']\n        metric_col = 'HML_12m'\n        metric_name = \"12-month rolling HML (created)\"\n    \n    print(f\"\\n📊 Using: {metric_name}\")\n    print(f\"   Range: {hml_metric.min():.2f} to {hml_metric.max():.2f}\")\n    print(f\"   Mean:  {hml_metric.mean():.2f}\")\n    print(f\"   Std:   {hml_metric.std():.2f}\")\n    \n    # Define regime thresholds (tertiles on NON-CRISIS data)\n    q33 = hml_metric.quantile(0.33)\n    q67 = hml_metric.quantile(0.67)\n    \n    print(f\"\\n📊 HML Regime Thresholds (Non-Crisis):\")\n    print(f\"   33rd percentile: {q33:.2f}\")\n    print(f\"   67th percentile: {q67:.2f}\")\n    \n    # Create regime labels\n    df_noncrisis['hml_regime'] = pd.cut(\n        hml_metric,\n        bins=[-np.inf, q33, q67, np.inf],\n        labels=[0, 1, 2]\n    ).astype(int)\n    \n    df_noncrisis['hml_regime_label'] = df_noncrisis['hml_regime'].map({\n        0: 'Growth-Friendly',\n        1: 'Neutral', \n        2: 'Value-Friendly'\n    })\n    \n    # Remove NaN regimes\n    df_regimes = df_noncrisis.dropna(subset=['hml_regime'])\n    \n    print(f\"\\n📊 HML Regime Distribution (Non-Crisis):\")\n    for regime, label in [(0, 'Growth-Friendly'), (1, 'Neutral'), (2, 'Value-Friendly')]:\n        mask = df_regimes['hml_regime'] == regime\n        count = mask.sum()\n        pct = count / len(df_regimes) * 100\n        mean_hml = df_regimes.loc[mask, metric_col].mean()\n        mean_L = df_regimes.loc[mask, 'L'].mean()\n        print(f\"   {label:16s}: {count:4d} ({pct:5.1f}%)\")\n        print(f\"      Mean HML: {mean_hml:+.2f}, Mean L: {mean_L:+.2f}\")\n    \n    # ==========================================\n    # STEP 3: Feature Engineering (Liquidity)\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 3: LIQUIDITY FEATURES\")\n    print(f\"{'='*80}\")\n    \n    feature_cols = []\n    \n    # Core liquidity components\n    liq_features = ['dlog_M2', 'dlog_FED_BS', 'EM', 'EB', 'EL_3y']\n    feature_cols.extend([f for f in liq_features if f in df_regimes.columns])\n    \n    # Rates and spreads\n    rate_features = ['Term_Spread', 'Credit_Spread', 'Real_Rate']\n    feature_cols.extend([f for f in rate_features if f in df_regimes.columns])\n    \n    # Market indicators\n    if 'VIX' in df_regimes.columns:\n        feature_cols.append('VIX')\n    \n    # Growth rates\n    if 'M2_growth' in df_regimes.columns:\n        feature_cols.append('M2_growth')\n    if 'FED_BS_growth' in df_regimes.columns:\n        feature_cols.append('FED_BS_growth')\n    \n    # LIQUIDITY INDEX (L)\n    feature_cols.append('L')\n    \n    # Technical on L\n    df_regimes['L_ma_3'] = df_regimes['L'].rolling(3, min_periods=1).mean()\n    df_regimes['L_ma_6'] = df_regimes['L'].rolling(6, min_periods=3).mean()\n    df_regimes['L_momentum'] = df_regimes['L'].diff(3)\n    df_regimes['L_accel'] = df_regimes['L_momentum'].diff(3)\n    \n    feature_cols.extend(['L_ma_3', 'L_ma_6', 'L_momentum', 'L_accel'])\n    \n    # Lagged HML (control for momentum)\n    df_regimes['HML_lag3'] = df_regimes[hml_col].shift(3)\n    df_regimes['HML_lag6'] = df_regimes[hml_col].shift(6)\n    feature_cols.extend(['HML_lag3', 'HML_lag6'])\n    \n    print(f\"\\n✅ Selected {len(feature_cols)} features\")\n    \n    # ==========================================\n    # STEP 4: Prepare ML Dataset\n    # ==========================================\n    \n    X_raw = df_regimes[feature_cols].copy()\n    y = df_regimes['hml_regime'].copy()\n    \n    # Imputation\n    X_imputed = X_raw.fillna(method='ffill', limit=6).fillna(method='bfill', limit=6)\n    \n    for col in X_imputed.columns:\n        if X_imputed[col].isna().any():\n            X_imputed[col] = X_imputed[col].fillna(X_imputed[col].median())\n    \n    # Final dataset\n    valid_idx = X_imputed.notna().all(axis=1) & y.notna()\n    X = X_imputed[valid_idx]\n    y = y[valid_idx]\n    \n    print(f\"\\n📊 ML Dataset: {len(X)} samples × {len(feature_cols)} features\")\n    \n    if len(X) &lt; 50:\n        raise ValueError(f\"Insufficient data: {len(X)} samples\")\n    \n    # ==========================================\n    # STEP 5: Train Models with CV\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 4: MODEL TRAINING & CROSS-VALIDATION\")\n    print(f\"{'='*80}\")\n    \n    tscv = TimeSeriesSplit(n_splits=5)\n    \n    models = {\n        'Random Forest': RandomForestClassifier(\n            n_estimators=200,\n            max_depth=8,\n            min_samples_split=10,\n            min_samples_leaf=5,\n            class_weight='balanced',\n            random_state=42,\n            n_jobs=-1\n        ),\n        'XGBoost': XGBClassifier(\n            n_estimators=200,\n            max_depth=6,\n            learning_rate=0.1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42,\n            eval_metric='mlogloss',\n            use_label_encoder=False,\n            n_jobs=-1\n        )\n    }\n    \n    results = {}\n    \n    for name, model in models.items():\n        print(f\"\\n{'='*70}\")\n        print(f\"{name}\")\n        print(f\"{'='*70}\")\n        \n        # Cross-validation\n        cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy', n_jobs=-1)\n        \n        print(f\"\\n📊 Cross-Validation (Time Series):\")\n        print(f\"   Mean Accuracy: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})\")\n        print(f\"   Fold Scores:   {[f'{s:.3f}' for s in cv_scores]}\")\n        \n        baseline = 1/3\n        improvement = cv_scores.mean() - baseline\n        print(f\"   vs Baseline:   {improvement:+.1%}\")\n        \n        # Train full model\n        model.fit(X, y)\n        \n        # In-sample\n        y_pred = model.predict(X)\n        train_acc = (y_pred == y).mean()\n        \n        print(f\"\\n📈 In-Sample Accuracy: {train_acc:.3f}\")\n        \n        # Classification report\n        print(f\"\\n📋 Classification Report:\")\n        print(classification_report(\n            y, y_pred,\n            target_names=['Growth-Friendly', 'Neutral', 'Value-Friendly'],\n            zero_division=0\n        ))\n        \n        # Confusion matrix\n        cm = confusion_matrix(y, y_pred)\n        cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n        \n        print(f\"\\n🎯 Confusion Matrix (%):\")\n        cm_df = pd.DataFrame(\n            cm_pct,\n            index=['True_Growth', 'True_Neutral', 'True_Value'],\n            columns=['Pred_Growth', 'Pred_Neutral', 'Pred_Value']\n        )\n        print(cm_df.round(1))\n        \n        results[name] = {\n            'model': model,\n            'cv_scores': cv_scores,\n            'cv_mean': cv_scores.mean(),\n            'cv_std': cv_scores.std(),\n            'train_acc': train_acc,\n            'y_pred': y_pred\n        }\n    \n    # ==========================================\n    # STEP 6: Feature Importance\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 5: FEATURE IMPORTANCE\")\n    print(f\"{'='*80}\")\n    \n    best_model_name = max(results, key=lambda k: results[k]['cv_mean'])\n    best_model = results[best_model_name]['model']\n    \n    print(f\"\\n🏆 Best Model: {best_model_name} (CV: {results[best_model_name]['cv_mean']:.3f})\")\n    \n    feature_importance = pd.DataFrame({\n        'feature': X.columns,\n        'importance': best_model.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    print(f\"\\n📊 Top 15 Features:\")\n    for i, row in feature_importance.head(15).iterrows():\n        is_liquidity = any(liq in row['feature'] for liq in ['L', 'M2', 'FED', 'EM', 'EB', 'EL'])\n        marker = \"💧\" if is_liquidity else \"  \"\n        print(f\"   {marker} {row['feature']:20s}: {row['importance']:.4f}\")\n    \n    # Liquidity importance\n    liquidity_features = [f for f in feature_importance['feature'] \n                         if any(liq in f for liq in ['L', 'M2', 'FED', 'EM', 'EB', 'EL'])]\n    liquidity_importance = feature_importance[feature_importance['feature'].isin(liquidity_features)]['importance'].sum()\n    \n    print(f\"\\n💧 Total Liquidity Importance: {liquidity_importance:.1%}\")\n    \n    # ==========================================\n    # STEP 7: Hypothesis Test\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"HYPOTHESIS TEST: LIQUIDITY → HML REGIMES\")\n    print(f\"{'='*80}\")\n    \n    best_cv_acc = results[best_model_name]['cv_mean']\n    baseline = 1/3\n    improvement = best_cv_acc - baseline\n    \n    print(f\"\\n📊 Results:\")\n    print(f\"   Baseline:      {baseline:.1%}\")\n    print(f\"   CV Accuracy:   {best_cv_acc:.1%}\")\n    print(f\"   Improvement:   {improvement:+.1%}\")\n    print(f\"   Lift:          {best_cv_acc/baseline:.2f}x\")\n    \n    if best_cv_acc &gt;= 0.50:\n        print(f\"\\n   ✅ STRONG SUPPORT for hypothesis\")\n    elif best_cv_acc &gt;= 0.42:\n        print(f\"\\n   ✔️  MODERATE SUPPORT for hypothesis\")\n    else:\n        print(f\"\\n   ❌ WEAK SUPPORT for hypothesis\")\n    \n    # ==========================================\n    # STEP 8: Final Assignment\n    # ==========================================\n    \n    df_final = df_crisis.copy()\n    df_final['final_regime'] = np.nan\n    df_final['final_regime_label'] = 'Unknown'\n    \n    # Crisis\n    df_final.loc[df_final['is_crisis'], 'final_regime'] = 3\n    df_final.loc[df_final['is_crisis'], 'final_regime_label'] = 'Crisis'\n    \n    # HML regimes (predicted)\n    df_final.loc[X.index, 'final_regime'] = best_model.predict(X)\n    df_final.loc[X.index, 'final_regime_label'] = pd.Series(\n        best_model.predict(X),\n        index=X.index\n    ).map({0: 'Growth-Friendly', 1: 'Neutral', 2: 'Value-Friendly'})\n    \n    # Also keep actual HML regime\n    df_final.loc[df_regimes.index, 'actual_hml_regime'] = df_regimes['hml_regime']\n    df_final.loc[df_regimes.index, 'actual_hml_regime_label'] = df_regimes['hml_regime_label']\n    \n    print(f\"\\n📊 Final Distribution:\")\n    dist = df_final['final_regime_label'].value_counts()\n    for regime in ['Crisis', 'Growth-Friendly', 'Neutral', 'Value-Friendly', 'Unknown']:\n        if regime in dist.index:\n            count = dist[regime]\n            pct = count / len(df_final) * 100\n            print(f\"   {regime:16s}: {count:4d} ({pct:5.1f}%)\")\n    \n    print(f\"\\n{'='*80}\")\n    print(\"✅ COMPLETE\")\n    print(f\"{'='*80}\")\n    \n    return df_final, results, feature_importance\n\n\n# Run with crisis detection\ndf_hml_regimes, hml_results, hml_feat_imp = build_hml_regime_classification_with_crisis(\n    df_features_aug,\n    L_index\n)\n\n\n================================================================================\nHML REGIME CLASSIFICATION (WITH CRISIS DETECTION)\n================================================================================\n\n✅ Aligned: 185 observations\n\n================================================================================\nSTEP 1: CRISIS DETECTION\n================================================================================\n\n📊 Crisis Features Created: 10\n   Valid crisis detection samples: 168/185\n\n✅ Crisis Detection Complete:\n   Crisis periods:     0 months (0.0%)\n   Non-crisis periods: 185 months (100.0%)\n\n📊 Non-Crisis Data: 185 observations\n\n================================================================================\nSTEP 2: DEFINE HML REGIMES (NON-CRISIS PERIODS)\n================================================================================\n\n📊 Using: 12-month rolling HML\n   Range: -0.42 to 0.31\n   Mean:  -0.00\n   Std:   0.13\n\n📊 HML Regime Thresholds (Non-Crisis):\n   33rd percentile: -0.07\n   67th percentile: 0.06\n\n📊 HML Regime Distribution (Non-Crisis):\n   Growth-Friendly :   61 ( 33.0%)\n      Mean HML: -0.14, Mean L: +0.16\n   Neutral         :   63 ( 34.1%)\n      Mean HML: -0.00, Mean L: -0.27\n   Value-Friendly  :   61 ( 33.0%)\n      Mean HML: +0.13, Mean L: +0.12\n\n================================================================================\nSTEP 3: LIQUIDITY FEATURES\n================================================================================\n\n✅ Selected 18 features\n\n📊 ML Dataset: 185 samples × 18 features\n\n================================================================================\nSTEP 4: MODEL TRAINING & CROSS-VALIDATION\n================================================================================\n\n======================================================================\nRandom Forest\n======================================================================\n\n📊 Cross-Validation (Time Series):\n   Mean Accuracy: 0.293 (±0.068)\n   Fold Scores:   ['0.300', '0.300', '0.167', '0.367', '0.333']\n   vs Baseline:   -4.0%\n\n📈 In-Sample Accuracy: 0.924\n\n📋 Classification Report:\n                 precision    recall  f1-score   support\n\nGrowth-Friendly       0.94      0.97      0.95        61\n        Neutral       0.92      0.89      0.90        63\n Value-Friendly       0.92      0.92      0.92        61\n\n       accuracy                           0.92       185\n      macro avg       0.92      0.92      0.92       185\n   weighted avg       0.92      0.92      0.92       185\n\n\n🎯 Confusion Matrix (%):\n              Pred_Growth  Pred_Neutral  Pred_Value\nTrue_Growth          96.7           3.3         0.0\nTrue_Neutral          3.2          88.9         7.9\nTrue_Value            3.3           4.9        91.8\n\n======================================================================\nXGBoost\n======================================================================\n\n📊 Cross-Validation (Time Series):\n   Mean Accuracy: 0.293 (±0.108)\n   Fold Scores:   ['0.433', '0.167', '0.167', '0.333', '0.367']\n   vs Baseline:   -4.0%\n\n📈 In-Sample Accuracy: 1.000\n\n📋 Classification Report:\n                 precision    recall  f1-score   support\n\nGrowth-Friendly       1.00      1.00      1.00        61\n        Neutral       1.00      1.00      1.00        63\n Value-Friendly       1.00      1.00      1.00        61\n\n       accuracy                           1.00       185\n      macro avg       1.00      1.00      1.00       185\n   weighted avg       1.00      1.00      1.00       185\n\n\n🎯 Confusion Matrix (%):\n              Pred_Growth  Pred_Neutral  Pred_Value\nTrue_Growth         100.0           0.0         0.0\nTrue_Neutral          0.0         100.0         0.0\nTrue_Value            0.0           0.0       100.0\n\n================================================================================\nSTEP 5: FEATURE IMPORTANCE\n================================================================================\n\n🏆 Best Model: Random Forest (CV: 0.293)\n\n📊 Top 15 Features:\n   💧 EB                  : 0.1444\n   💧 EM                  : 0.1342\n      Credit_Spread       : 0.0739\n      VIX                 : 0.0566\n   💧 FED_BS_growth       : 0.0543\n   💧 L_momentum          : 0.0534\n      Term_Spread         : 0.0515\n   💧 EL_3y               : 0.0507\n   💧 dlog_FED_BS         : 0.0505\n   💧 L_ma_6              : 0.0471\n   💧 L_accel             : 0.0464\n   💧 HML_lag6            : 0.0436\n   💧 dlog_M2             : 0.0347\n   💧 L_ma_3              : 0.0329\n   💧 HML_lag3            : 0.0327\n\n💧 Total Liquidity Importance: 78.5%\n\n================================================================================\nHYPOTHESIS TEST: LIQUIDITY → HML REGIMES\n================================================================================\n\n📊 Results:\n   Baseline:      33.3%\n   CV Accuracy:   29.3%\n   Improvement:   -4.0%\n   Lift:          0.88x\n\n   ❌ WEAK SUPPORT for hypothesis\n\n📊 Final Distribution:\n   Growth-Friendly :   63 ( 34.1%)\n   Neutral         :   61 ( 33.0%)\n   Value-Friendly  :   61 ( 33.0%)\n\n================================================================================\n✅ COMPLETE\n================================================================================\n\n\n\nfactor_returns_by_regime = (\n    df_regimes_fixed.groupby(\"regime_label\")[[\"mkt_excess\", \"hml\", \"rmw\", \"cma\"]]\n    .mean() * 100\n)\nprint(factor_returns_by_regime)\n\n              mkt_excess       hml       rmw       cma\nregime_label                                          \nHigh            0.692131  0.374426  0.289180  0.525246\nNeutral         0.826190  0.099524  0.430159 -0.144762\nTight           1.039672 -0.323115  0.007049 -0.536230\n\n\n\ndef build_hml_regime_classification_with_crisis(df, L_series):\n    \"\"\"\n    1. Detect crisis periods (Isolation Forest)\n    2. Classify NON-CRISIS periods by HML regimes\n    3. Predict HML regimes from liquidity features\n    \"\"\"\n    \n    import numpy as np\n    import pandas as pd\n    from sklearn.ensemble import IsolationForest, RandomForestClassifier\n    from xgboost import XGBClassifier\n    from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n    from sklearn.metrics import classification_report, confusion_matrix\n    \n    print(f\"\\n{'='*80}\")\n    print(\"HML REGIME CLASSIFICATION (WITH CRISIS DETECTION)\")\n    print(f\"{'='*80}\")\n    \n    # ==========================================\n    # STEP 0: Data Alignment\n    # ==========================================\n    \n    common_idx = df.index.intersection(L_series.index)\n    df_aligned = df.loc[common_idx].copy()\n    df_aligned['L'] = L_series.loc[common_idx]\n    \n    print(f\"\\n✅ Aligned: {len(df_aligned)} observations\")\n    \n    # ==========================================\n    # STEP 1: Crisis Detection (Isolation Forest)\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 1: CRISIS DETECTION\")\n    print(f\"{'='*80}\")\n    \n    df_crisis = df_aligned.copy()\n    \n    # Create crisis features (volatility, drawdowns, tail events)\n    df_crisis['ret_vol_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).std()\n    df_crisis['ret_vol_12m'] = df_crisis['mkt_excess'].rolling(12, min_periods=6).std()\n    df_crisis['L_vol_3m'] = df_crisis['L'].rolling(3, min_periods=2).std()\n    df_crisis['L_vol_6m'] = df_crisis['L'].rolling(6, min_periods=3).std()\n    \n    # Return features\n    df_crisis['abs_ret'] = df_crisis['mkt_excess'].abs()\n    df_crisis['cum_ret_3m'] = df_crisis['mkt_excess'].rolling(3, min_periods=2).sum()\n    df_crisis['cum_ret_6m'] = df_crisis['mkt_excess'].rolling(6, min_periods=3).sum()\n    \n    # Drawdown\n    cumret = (1 + df_crisis['mkt_excess']/100).cumprod()\n    running_max = cumret.expanding().max()\n    df_crisis['drawdown'] = (cumret / running_max - 1) * 100\n    df_crisis['max_dd_6m'] = df_crisis['drawdown'].rolling(6, min_periods=3).min()\n    \n    # Crisis features list\n    crisis_features = ['ret_vol_6m', 'ret_vol_12m', 'L_vol_3m', 'L_vol_6m', \n                       'abs_ret', 'cum_ret_3m', 'cum_ret_6m', 'max_dd_6m']\n    \n    # Add VIX if available\n    if 'VIX' in df_crisis.columns:\n        df_crisis['VIX_level'] = df_crisis['VIX']\n        df_crisis['VIX_zscore'] = (df_crisis['VIX'] - df_crisis['VIX'].rolling(60, min_periods=30).mean()) / \\\n                                   df_crisis['VIX'].rolling(60, min_periods=30).std()\n        crisis_features.extend(['VIX_level', 'VIX_zscore'])\n    \n    print(f\"\\n📊 Crisis Features Created: {len(crisis_features)}\")\n    \n    # Extract and clean crisis data\n    crisis_data = df_crisis[crisis_features].copy()\n    crisis_data = crisis_data.fillna(method='bfill', limit=12)\n    crisis_data = crisis_data.dropna()\n    \n    print(f\"   Valid crisis detection samples: {len(crisis_data)}/{len(df_crisis)}\")\n    \n    if len(crisis_data) &gt;= 20:\n        # Fit Isolation Forest\n        iso_forest = IsolationForest(\n            contamination=0.05,  # Expect ~5% crisis periods\n            random_state=42,\n            n_estimators=100,\n            max_samples='auto'\n        )\n        \n        outlier_labels = iso_forest.fit_predict(crisis_data.values)\n        \n        # Crisis = outlier + negative return (&lt; -2%)\n        is_crisis_candidate = (outlier_labels == -1)\n        is_negative_return = (df_crisis.loc[crisis_data.index, 'mkt_excess'] &lt; -2)\n        \n        df_crisis['is_crisis'] = False\n        df_crisis.loc[crisis_data.index, 'is_crisis'] = is_crisis_candidate\n        \n        n_crisis = df_crisis['is_crisis'].sum()\n        crisis_pct = n_crisis / len(df_crisis) * 100\n        \n        print(f\"\\n✅ Crisis Detection Complete:\")\n        print(f\"   Crisis periods:     {n_crisis} months ({crisis_pct:.1f}%)\")\n        print(f\"   Non-crisis periods: {len(df_crisis) - n_crisis} months ({100-crisis_pct:.1f}%)\")\n        \n        # Show detected crisis periods\n        if n_crisis &gt; 0:\n            crisis_periods = df_crisis[df_crisis['is_crisis']].index\n            print(f\"\\n📅 Detected Crisis Months:\")\n            for i, date in enumerate(crisis_periods[:10], 1):\n                ret = df_crisis.loc[date, 'mkt_excess']\n                L_val = df_crisis.loc[date, 'L']\n                print(f\"   {i:2d}. {date.strftime('%Y-%m')}: Return = {ret:+.2f}%, L = {L_val:+.2f}\")\n            if len(crisis_periods) &gt; 10:\n                print(f\"   ... and {len(crisis_periods) - 10} more\")\n    else:\n        print(f\"\\n⚠️  Insufficient data for crisis detection, marking all as non-crisis\")\n        df_crisis['is_crisis'] = False\n    \n    # Filter to non-crisis periods for HML regime analysis\n    df_noncrisis = df_crisis[~df_crisis['is_crisis']].copy()\n    \n    print(f\"\\n📊 Non-Crisis Data: {len(df_noncrisis)} observations\")\n    \n    # ==========================================\n    # STEP 2: Define HML Regimes (Non-Crisis Only)\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 2: DEFINE HML REGIMES (NON-CRISIS PERIODS)\")\n    print(f\"{'='*80}\")\n    \n    # Check for HML data\n    if 'hml' not in df_noncrisis.columns and 'HML' not in df_noncrisis.columns:\n        raise ValueError(\"No HML column found in dataframe\")\n    \n    hml_col = 'hml' if 'hml' in df_noncrisis.columns else 'HML'\n    \n    # Use 12-month rolling HML for smoother regimes\n    # if 'HML_12m' in df_noncrisis.columns:\n    #     hml_metric = df_noncrisis['HML_12m']\n    #     metric_col = 'HML_12m'\n    #     metric_name = \"12-month rolling HML\"\n    # else:\n    #     df_noncrisis['HML_12m'] = df_noncrisis[hml_col].rolling(12, min_periods=6).sum()\n    #     hml_metric = df_noncrisis['HML_12m']\n    #     metric_col = 'HML_12m'\n    #     metric_name = \"12-month rolling HML (created)\"\n    df_noncrisis['HML_forward_6m'] = df_noncrisis[hml_col].shift(-6).rolling(\n        6, min_periods=3\n    ).sum()\n    hml_metric = df_noncrisis['HML_forward_6m']\n    metric_col = 'HML_forward_6m'\n    metric_name = \"6-month FORWARD HML\"\n    \n    print(f\"\\n📊 Using: {metric_name}\")\n    print(f\"   Range: {hml_metric.min():.2f} to {hml_metric.max():.2f}\")\n    print(f\"   Mean:  {hml_metric.mean():.2f}\")\n    print(f\"   Std:   {hml_metric.std():.2f}\")\n\n    # Remove NaN BEFORE creating regimes\n    df_noncrisis_valid = df_noncrisis.dropna(subset=['HML_forward_6m']).copy()\n    hml_metric_valid = df_noncrisis_valid['HML_forward_6m']\n    \n    print(f\"\\n📊 After removing NaN: {len(df_noncrisis_valid)} observations\")\n    \n    # Define regime thresholds (tertiles on NON-CRISIS data)\n    q33 = hml_metric.quantile(0.33)\n    q67 = hml_metric.quantile(0.67)\n    \n    print(f\"\\n📊 HML Regime Thresholds (Non-Crisis):\")\n    print(f\"   33rd percentile: {q33:.2f}\")\n    print(f\"   67th percentile: {q67:.2f}\")\n    \n    # Create regime labels\n    df_noncrisis['hml_regime'] = pd.cut(\n        hml_metric_valid,\n        bins=[-np.inf, q33, q67, np.inf],\n        labels=[0, 1, 2]\n    ).astype(int)\n    \n    df_noncrisis['hml_regime_label'] = df_noncrisis['hml_regime'].map({\n        0: 'Growth-Friendly',\n        1: 'Neutral', \n        2: 'Value-Friendly'\n    })\n    \n    # Remove NaN regimes\n    df_regimes = df_noncrisis.dropna(subset=['hml_regime'])\n    \n    print(f\"\\n📊 HML Regime Distribution (Non-Crisis):\")\n    for regime, label in [(0, 'Growth-Friendly'), (1, 'Neutral'), (2, 'Value-Friendly')]:\n        mask = df_regimes['hml_regime'] == regime\n        count = mask.sum()\n        pct = count / len(df_regimes) * 100\n        mean_hml = df_regimes.loc[mask, metric_col].mean()\n        mean_L = df_regimes.loc[mask, 'L'].mean()\n        print(f\"   {label:16s}: {count:4d} ({pct:5.1f}%)\")\n        print(f\"      Mean HML: {mean_hml:+.2f}, Mean L: {mean_L:+.2f}\")\n    \n    # ==========================================\n    # STEP 3: Feature Engineering (Liquidity)\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 3: LIQUIDITY FEATURES\")\n    print(f\"{'='*80}\")\n    \n    feature_cols = []\n    \n    # Core liquidity components\n    liq_features = ['dlog_M2', 'dlog_FED_BS', 'EM', 'EB', 'EL_3y']\n    feature_cols.extend([f for f in liq_features if f in df_regimes.columns])\n    \n    # Rates and spreads\n    rate_features = ['Term_Spread', 'Credit_Spread', 'Real_Rate']\n    feature_cols.extend([f for f in rate_features if f in df_regimes.columns])\n    \n    # Market indicators\n    if 'VIX' in df_regimes.columns:\n        feature_cols.append('VIX')\n    \n    # Growth rates\n    if 'M2_growth' in df_regimes.columns:\n        feature_cols.append('M2_growth')\n    if 'FED_BS_growth' in df_regimes.columns:\n        feature_cols.append('FED_BS_growth')\n    \n    # LIQUIDITY INDEX (L)\n    feature_cols.append('L')\n    \n    # Technical on L\n    df_regimes['L_ma_3'] = df_regimes['L'].rolling(3, min_periods=1).mean()\n    df_regimes['L_ma_6'] = df_regimes['L'].rolling(6, min_periods=3).mean()\n    df_regimes['L_momentum'] = df_regimes['L'].diff(3)\n    df_regimes['L_accel'] = df_regimes['L_momentum'].diff(3)\n    \n    feature_cols.extend(['L_ma_3', 'L_ma_6', 'L_momentum', 'L_accel'])\n    \n    # Lagged HML (control for momentum)\n    df_regimes['HML_lag3'] = df_regimes[hml_col].shift(3)\n    df_regimes['HML_lag6'] = df_regimes[hml_col].shift(6)\n    feature_cols.extend(['HML_lag3', 'HML_lag6'])\n    \n    print(f\"\\n✅ Selected {len(feature_cols)} features\")\n    \n    # ==========================================\n    # STEP 4: Prepare ML Dataset\n    # ==========================================\n    \n    X_raw = df_regimes[feature_cols].copy()\n    y = df_regimes['hml_regime'].copy()\n    \n    # Imputation\n    X_imputed = X_raw.fillna(method='ffill', limit=6).fillna(method='bfill', limit=6)\n    \n    for col in X_imputed.columns:\n        if X_imputed[col].isna().any():\n            X_imputed[col] = X_imputed[col].fillna(X_imputed[col].median())\n    \n    # Final dataset\n    valid_idx = X_imputed.notna().all(axis=1) & y.notna()\n    X = X_imputed[valid_idx]\n    y = y[valid_idx]\n    \n    print(f\"\\n📊 ML Dataset: {len(X)} samples × {len(feature_cols)} features\")\n    \n    if len(X) &lt; 50:\n        raise ValueError(f\"Insufficient data: {len(X)} samples\")\n    \n    # ==========================================\n    # STEP 5: Train Models with CV\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 4: MODEL TRAINING & CROSS-VALIDATION\")\n    print(f\"{'='*80}\")\n    \n    tscv = TimeSeriesSplit(n_splits=5)\n    \n    models = {\n        'Random Forest': RandomForestClassifier(\n            n_estimators=200,\n            max_depth=8,\n            min_samples_split=10,\n            min_samples_leaf=5,\n            class_weight='balanced',\n            random_state=42,\n            n_jobs=-1\n        ),\n        'XGBoost': XGBClassifier(\n            n_estimators=200,\n            max_depth=6,\n            learning_rate=0.1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42,\n            eval_metric='mlogloss',\n            use_label_encoder=False,\n            n_jobs=-1\n        )\n    }\n    \n    results = {}\n    \n    for name, model in models.items():\n        print(f\"\\n{'='*70}\")\n        print(f\"{name}\")\n        print(f\"{'='*70}\")\n        \n        # Cross-validation\n        cv_scores = cross_val_score(model, X, y, cv=tscv, scoring='accuracy', n_jobs=-1)\n        \n        print(f\"\\n📊 Cross-Validation (Time Series):\")\n        print(f\"   Mean Accuracy: {cv_scores.mean():.3f} (±{cv_scores.std():.3f})\")\n        print(f\"   Fold Scores:   {[f'{s:.3f}' for s in cv_scores]}\")\n        \n        baseline = 1/3\n        improvement = cv_scores.mean() - baseline\n        print(f\"   vs Baseline:   {improvement:+.1%}\")\n        \n        # Train full model\n        model.fit(X, y)\n        \n        # In-sample\n        y_pred = model.predict(X)\n        train_acc = (y_pred == y).mean()\n        \n        print(f\"\\n📈 In-Sample Accuracy: {train_acc:.3f}\")\n        \n        # Classification report\n        print(f\"\\n📋 Classification Report:\")\n        print(classification_report(\n            y, y_pred,\n            target_names=['Growth-Friendly', 'Neutral', 'Value-Friendly'],\n            zero_division=0\n        ))\n        \n        # Confusion matrix\n        cm = confusion_matrix(y, y_pred)\n        cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n        \n        print(f\"\\n🎯 Confusion Matrix (%):\")\n        cm_df = pd.DataFrame(\n            cm_pct,\n            index=['True_Growth', 'True_Neutral', 'True_Value'],\n            columns=['Pred_Growth', 'Pred_Neutral', 'Pred_Value']\n        )\n        print(cm_df.round(1))\n        \n        results[name] = {\n            'model': model,\n            'cv_scores': cv_scores,\n            'cv_mean': cv_scores.mean(),\n            'cv_std': cv_scores.std(),\n            'train_acc': train_acc,\n            'y_pred': y_pred\n        }\n    \n    # ==========================================\n    # STEP 6: Feature Importance\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"STEP 5: FEATURE IMPORTANCE\")\n    print(f\"{'='*80}\")\n    \n    best_model_name = max(results, key=lambda k: results[k]['cv_mean'])\n    best_model = results[best_model_name]['model']\n    \n    print(f\"\\n🏆 Best Model: {best_model_name} (CV: {results[best_model_name]['cv_mean']:.3f})\")\n    \n    feature_importance = pd.DataFrame({\n        'feature': X.columns,\n        'importance': best_model.feature_importances_\n    }).sort_values('importance', ascending=False)\n    \n    print(f\"\\n📊 Top 15 Features:\")\n    for i, row in feature_importance.head(15).iterrows():\n        is_liquidity = any(liq in row['feature'] for liq in ['L', 'M2', 'FED', 'EM', 'EB', 'EL'])\n        marker = \"💧\" if is_liquidity else \"  \"\n        print(f\"   {marker} {row['feature']:20s}: {row['importance']:.4f}\")\n    \n    # Liquidity importance\n    liquidity_features = [f for f in feature_importance['feature'] \n                         if any(liq in f for liq in ['L', 'M2', 'FED', 'EM', 'EB', 'EL'])]\n    liquidity_importance = feature_importance[feature_importance['feature'].isin(liquidity_features)]['importance'].sum()\n    \n    print(f\"\\n💧 Total Liquidity Importance: {liquidity_importance:.1%}\")\n    \n    # ==========================================\n    # STEP 7: Hypothesis Test\n    # ==========================================\n    \n    print(f\"\\n{'='*80}\")\n    print(\"HYPOTHESIS TEST: LIQUIDITY → HML REGIMES\")\n    print(f\"{'='*80}\")\n    \n    best_cv_acc = results[best_model_name]['cv_mean']\n    baseline = 1/3\n    improvement = best_cv_acc - baseline\n    \n    print(f\"\\n📊 Results:\")\n    print(f\"   Baseline:      {baseline:.1%}\")\n    print(f\"   CV Accuracy:   {best_cv_acc:.1%}\")\n    print(f\"   Improvement:   {improvement:+.1%}\")\n    print(f\"   Lift:          {best_cv_acc/baseline:.2f}x\")\n    \n    if best_cv_acc &gt;= 0.50:\n        print(f\"\\n   ✅ STRONG SUPPORT for hypothesis\")\n    elif best_cv_acc &gt;= 0.42:\n        print(f\"\\n   ✔️  MODERATE SUPPORT for hypothesis\")\n    else:\n        print(f\"\\n   ❌ WEAK SUPPORT for hypothesis\")\n    \n    # ==========================================\n    # STEP 8: Final Assignment\n    # ==========================================\n    \n    df_final = df_crisis.copy()\n    df_final['final_regime'] = np.nan\n    df_final['final_regime_label'] = 'Unknown'\n    \n    # Crisis\n    df_final.loc[df_final['is_crisis'], 'final_regime'] = 3\n    df_final.loc[df_final['is_crisis'], 'final_regime_label'] = 'Crisis'\n    \n    # HML regimes (predicted)\n    df_final.loc[X.index, 'final_regime'] = best_model.predict(X)\n    df_final.loc[X.index, 'final_regime_label'] = pd.Series(\n        best_model.predict(X),\n        index=X.index\n    ).map({0: 'Growth-Friendly', 1: 'Neutral', 2: 'Value-Friendly'})\n    \n    # Also keep actual HML regime\n    df_final.loc[df_regimes.index, 'actual_hml_regime'] = df_regimes['hml_regime']\n    df_final.loc[df_regimes.index, 'actual_hml_regime_label'] = df_regimes['hml_regime_label']\n    \n    print(f\"\\n📊 Final Distribution:\")\n    dist = df_final['final_regime_label'].value_counts()\n    for regime in ['Crisis', 'Growth-Friendly', 'Neutral', 'Value-Friendly', 'Unknown']:\n        if regime in dist.index:\n            count = dist[regime]\n            pct = count / len(df_final) * 100\n            print(f\"   {regime:16s}: {count:4d} ({pct:5.1f}%)\")\n    \n    print(f\"\\n{'='*80}\")\n    print(\"✅ COMPLETE\")\n    print(f\"{'='*80}\")\n    \n    return df_final, results, feature_importance\n\n\n# Run with crisis detection\ndf_hml_regimes, hml_results, hml_feat_imp = build_hml_regime_classification_with_crisis(\n    df_features_aug,\n    L_index\n)\n\n\n================================================================================\nHML REGIME CLASSIFICATION (WITH CRISIS DETECTION)\n================================================================================\n\n✅ Aligned: 185 observations\n\n================================================================================\nSTEP 1: CRISIS DETECTION\n================================================================================\n\n📊 Crisis Features Created: 10\n   Valid crisis detection samples: 168/185\n\n✅ Crisis Detection Complete:\n   Crisis periods:     9 months (4.9%)\n   Non-crisis periods: 176 months (95.1%)\n\n📅 Detected Crisis Months:\n    1. 2008-09: Return = -0.09%, L = +2.15\n    2. 2008-10: Return = -0.17%, L = +5.37\n    3. 2008-12: Return = +0.02%, L = +4.80\n    4. 2009-03: Return = +0.09%, L = +4.66\n    5. 2009-04: Return = +0.10%, L = +4.80\n    6. 2009-06: Return = +0.00%, L = +3.73\n    7. 2009-07: Return = +0.08%, L = +3.40\n    8. 2020-03: Return = -0.13%, L = +1.94\n    9. 2020-04: Return = +0.14%, L = +2.11\n\n📊 Non-Crisis Data: 176 observations\n\n================================================================================\nSTEP 2: DEFINE HML REGIMES (NON-CRISIS PERIODS)\n================================================================================\n\n📊 Using: 6-month FORWARD HML\n   Range: -0.20 to 0.25\n   Mean:  0.00\n   Std:   0.07\n\n📊 After removing NaN: 171 observations\n\n📊 HML Regime Thresholds (Non-Crisis):\n   33rd percentile: -0.03\n   67th percentile: 0.03\n\n📊 HML Regime Distribution (Non-Crisis):\n   Growth-Friendly :   57 ( 33.3%)\n      Mean HML: -0.07, Mean L: -0.54\n   Neutral         :   58 ( 33.9%)\n      Mean HML: +0.00, Mean L: +0.03\n   Value-Friendly  :   56 ( 32.7%)\n      Mean HML: +0.08, Mean L: +0.03\n\n================================================================================\nSTEP 3: LIQUIDITY FEATURES\n================================================================================\n\n✅ Selected 18 features\n\n📊 ML Dataset: 171 samples × 18 features\n\n================================================================================\nSTEP 4: MODEL TRAINING & CROSS-VALIDATION\n================================================================================\n\n======================================================================\nRandom Forest\n======================================================================\n\n📊 Cross-Validation (Time Series):\n   Mean Accuracy: 0.307 (±0.107)\n   Fold Scores:   ['0.286', '0.214', '0.179', '0.464', '0.393']\n   vs Baseline:   -2.6%\n\n📈 In-Sample Accuracy: 0.895\n\n📋 Classification Report:\n                 precision    recall  f1-score   support\n\nGrowth-Friendly       0.89      0.95      0.92        57\n        Neutral       0.84      0.84      0.84        58\n Value-Friendly       0.96      0.89      0.93        56\n\n       accuracy                           0.89       171\n      macro avg       0.90      0.90      0.90       171\n   weighted avg       0.90      0.89      0.89       171\n\n\n🎯 Confusion Matrix (%):\n              Pred_Growth  Pred_Neutral  Pred_Value\nTrue_Growth          94.7           5.3         0.0\nTrue_Neutral         12.1          84.5         3.4\nTrue_Value            0.0          10.7        89.3\n\n======================================================================\nXGBoost\n======================================================================\n\n📊 Cross-Validation (Time Series):\n   Mean Accuracy: 0.214 (±0.106)\n   Fold Scores:   ['0.179', '0.071', '0.179', '0.250', '0.393']\n   vs Baseline:   -11.9%\n\n📈 In-Sample Accuracy: 1.000\n\n📋 Classification Report:\n                 precision    recall  f1-score   support\n\nGrowth-Friendly       1.00      1.00      1.00        57\n        Neutral       1.00      1.00      1.00        58\n Value-Friendly       1.00      1.00      1.00        56\n\n       accuracy                           1.00       171\n      macro avg       1.00      1.00      1.00       171\n   weighted avg       1.00      1.00      1.00       171\n\n\n🎯 Confusion Matrix (%):\n              Pred_Growth  Pred_Neutral  Pred_Value\nTrue_Growth         100.0           0.0         0.0\nTrue_Neutral          0.0         100.0         0.0\nTrue_Value            0.0           0.0       100.0\n\n================================================================================\nSTEP 5: FEATURE IMPORTANCE\n================================================================================\n\n🏆 Best Model: Random Forest (CV: 0.307)\n\n📊 Top 15 Features:\n   💧 FED_BS_growth       : 0.1034\n      Term_Spread         : 0.0878\n   💧 M2_growth           : 0.0815\n   💧 EM                  : 0.0766\n   💧 EB                  : 0.0755\n   💧 HML_lag6            : 0.0583\n   💧 L                   : 0.0580\n      Credit_Spread       : 0.0547\n   💧 EL_3y               : 0.0530\n   💧 L_ma_3              : 0.0501\n   💧 L_ma_6              : 0.0485\n   💧 dlog_M2             : 0.0465\n   💧 L_momentum          : 0.0414\n      Real_Rate           : 0.0399\n   💧 dlog_FED_BS         : 0.0364\n\n💧 Total Liquidity Importance: 78.5%\n\n================================================================================\nHYPOTHESIS TEST: LIQUIDITY → HML REGIMES\n================================================================================\n\n📊 Results:\n   Baseline:      33.3%\n   CV Accuracy:   30.7%\n   Improvement:   -2.6%\n   Lift:          0.92x\n\n   ❌ WEAK SUPPORT for hypothesis\n\n📊 Final Distribution:\n   Crisis          :    9 (  4.9%)\n   Growth-Friendly :   61 ( 33.0%)\n   Neutral         :   58 ( 31.4%)\n   Value-Friendly  :   52 ( 28.1%)\n   Unknown         :    5 (  2.7%)\n\n================================================================================\n✅ COMPLETE\n================================================================================"
  },
  {
    "objectID": "notebooks/Follow-The-Money.html",
    "href": "notebooks/Follow-The-Money.html",
    "title": "Follow the Money: Analyzing Capital Flows from Central Banks to Key Sectors",
    "section": "",
    "text": "import pandas as pd\nimport requests\nfrom datetime import datetime\n\n\nFRED_API_KEY = '2548a719cdd51baaa5e013839926fccc'\nstart_date = '2019-01-01'\nend_date = '2025-01-01'\n\n\ndef fetch_fred_series(series_id, start_date, end_date):\n    series_id = series_id  \n    url = f\"https://api.stlouisfed.org/fred/series/observations?series_id={series_id}&api_key={FRED_API_KEY}&file_type=json&observation_start={start_date}&observation_end={end_date}\"\n    response = requests.get(url)\n    data = response.json()\n    df = pd.DataFrame(data['observations'])\n    df['value'] = df['value'].astype(float)\n    return df\n\ndef fetch_world_bank_indicators(series_id, start_year, end_year):\n    url = f\"http://api.worldbank.org/v2/country/USA/indicator/{series_id}?format=json&date={start_year}:{end_year}\"\n    response = requests.get(url)\n    data = response.json()\n    df = pd.DataFrame(data[1])\n    df['value'] = df['value'].astype(float)\n    return df\n\n# Function to fetch money supply data from FRED\ndef fetch_money_supply(start_date, end_date):\n    return fetch_fred_series('M2SL', start_date, end_date)\n\ndef fetch_real_estate_investment(start_date, end_date):\n    res_date = fetch_fred_series('A012RC1Q027SBEA', start_date, end_date)\n    nonres_data = fetch_fred_series('B009RC1Q027SBEA', start_date, end_date)\n    return res_date['value'].sum() + nonres_data['value'].sum()\n\ndef fetch_tech_investment(start_date, end_date):\n    # Real Gross Private Domestic Investment: Fixed Investment: Nonresidential: Intellectual Property Products (Y001RX1Q020SBEA)\n    direct_investment_ip_data = fetch_fred_series('Y001RX1Q020SBEA', start_date, end_date)\n\n    # Private fixed investment in information processing equipment and software\n    direct_investment_tech_data = fetch_fred_series('A679RC1Q027SBEA', start_date, end_date)\n\n    # # Households and Nonprofit Organizations; Corporate Equities; Asset, Level/1000 (HNOCEAQ027S)\n    # indirect_retail_investment = fetch_fred_series('HNOCEAQ027S', start_date, end_date)\n\n    return direct_investment_ip_data['value'].sum() + direct_investment_tech_data['value'].sum()\n\ndef fetch_healthcare_investment(start_date, end_date):\n    start_year = datetime.strptime(start_date, '%Y-%m-%d').year\n    end_year = datetime.strptime(end_date, '%Y-%m-%d').year\n    return fetch_world_bank_indicators('SH.XPD.CHEX.GD.ZS', start_year, end_year)\n\n\n0.1 Step 2: Fetching Sector Investment Data from FRED and World Bank\nFor fetching sector-specific investment data, we will use series IDs from FRED and the World Bank API. Here are some examples:\n\nmoney_supply_data = fetch_money_supply(start_date, end_date)\ntotal_money_supply = money_supply_data['value'].sum()\nprint(f\"Total Money Supply: {total_money_supply}\")\n\nTotal Money Supply: 1419632.0\n\n\n\nreal_estate_data = fetch_real_estate_investment(start_date, end_date)\nprint(f\"Total Real Estate Investment: {real_estate_data}\")\n\nTotal Real Estate Investment: 45236.283\n\n\n\ntotal_tech_investment = fetch_tech_investment(start_date, end_date)\nprint(f\"Total Tech Investment: {total_tech_investment}\")\n\nTotal Tech Investment: 58251.611000000004\n\n\n\n# healthcare_data = fetch_healthcare_investment(start_date, end_date)\n# total_healthcare_investment = healthcare_data['value'].sum()\n# print(f\"Total Healthcare Investment: {total_healthcare_investment}\")\n\npharmaceuticals_data = fetch_fred_series('Y009RC1A027NBEA', start_date, end_date)  # Example series for pharmaceuticals\n\n# Private fixed investment in equipment and software: Nonresidential: Information processing equipment and software: Medical equipment and instruments (W176RC1A027NBEA)\nmedical_devices_data = fetch_fred_series('W176RC1A027NBEA', start_date, end_date)  # Example series for medical devices\n\n# Private fixed investment: Nonresidential: Structures: Commercial and health care (W001RC1Q027SBEA)\nservices_data = fetch_fred_series('W001RC1Q027SBEA', start_date, end_date)  # Example series for healthcare services\n\n# Calculate total investments for pharmaceuticals, medical devices, and services\ntotal_pharmaceuticals_investment = pharmaceuticals_data['value'].sum()\ntotal_medical_devices_investment = medical_devices_data['value'].sum()\ntotal_services_investment = services_data['value'].sum()\ntotal_healthcare_investment = total_pharmaceuticals_investment + total_medical_devices_investment + total_services_investment\nprint(f\"Total Healthcare Investment: {total_healthcare_investment}\")\n\nTotal Healthcare Investment: 7629.786999999999\n\n\n\n\n0.2 Step 3: Sector Breakdowns\n\n0.2.1 Real Estate\n\n\n# Total Private Construction Spending: Residential in the United States (PRRESCONS)\n# Private fixed investment: Residential: Structures (A012RC1Q027SBEA)\nresidential_data = fetch_fred_series('A012RC1Q027SBEA', start_date, end_date)\n\n# Total Private Construction Spending: Nonresidential in the United States (PNRESCONS)\n# Private fixed investment: Nonresidential: Structures (B009RC1Q027SBEA)\ncommercial_data = fetch_fred_series('B009RC1Q027SBEA', start_date, end_date)\n\n# Calculate total investments for residential and commercial real estate\ntotal_residential_investment = residential_data['value'].sum()\ntotal_commercial_investment = commercial_data['value'].sum()\ntotal_real_estate_investment = total_residential_investment + total_commercial_investment\n\n# Calculate the ratios\nresidential_ratio = total_residential_investment / total_real_estate_investment\ncommercial_ratio = total_commercial_investment / total_real_estate_investment\n\nprint(f\"Residential Ratio: {residential_ratio}\")\nprint(f\"Commercial Ratio: {commercial_ratio}\")\n\nResidential Ratio: 0.5777855576683876\nCommercial Ratio: 0.4222144423316124\n\n\n\n\n0.2.2 Tech Investments\n\n# Fetching data for Direct Investment in Technology and Tech Stocks\n\n# Real Gross Private Domestic Investment: Fixed Investment: Nonresidential: Intellectual Property Products (Y001RX1Q020SBEA)\ndirect_investment_ip_data = fetch_fred_series('Y001RX1Q020SBEA', start_date, end_date)\n\n# Real Gross Private Domestic Investment: Fixed Investment: Nonresidential: Equipment (Y033RX1Q020SBEA)\ndirect_investment_eqp_data = fetch_fred_series('Y033RX1Q020SBEA', start_date, end_date)\n\n# Private fixed investment in information processing equipment and software\ndirect_investment_tech_data = fetch_fred_series('A679RC1Q027SBEA', start_date, end_date)\n\n# Calculate total investments for direct investment and tech stocks\ntotal_direct_investment = direct_investment_ip_data['value'].sum() + direct_investment_tech_data['value'].sum()\n\n# Calculate the ratios\ntech_and_ip_ratio = direct_investment_tech_data['value'].sum() / total_direct_investment\nip_ratio = 1 - tech_and_ip_ratio\n\n\nprint(f\"Tech & IP Ratio: {tech_and_ip_ratio}\")\nprint(f\"IP Ratio: {ip_ratio}\")\n\nTech & IP Ratio: 0.44079311385911707\nIP Ratio: 0.559206886140883\n\n\n\n# Quarterly Financial Report: U.S. Corporations: Pharmaceuticals and Medicines: Net Sales, Receipts, and Operating Revenues (QFR101385USNO)\n\n# Fetching data for Pharmaceuticals, Medical Devices, and Healthcare Services\n\n# Private Fixed Investment in Intellectual Property Products: Research and development: Business: Manufacturing: Pharmaceutical and medicine manufacturing (Y009RC1A027NBEA)\npharmaceuticals_data = fetch_fred_series('Y009RC1A027NBEA', start_date, end_date)  # Example series for pharmaceuticals\n\n# Private fixed investment in equipment and software: Nonresidential: Information processing equipment and software: Medical equipment and instruments (W176RC1A027NBEA)\nmedical_devices_data = fetch_fred_series('W176RC1A027NBEA', start_date, end_date)  # Example series for medical devices\n\n# Private fixed investment: Nonresidential: Structures: Commercial and health care (W001RC1Q027SBEA)\nservices_data = fetch_fred_series('W001RC1Q027SBEA', start_date, end_date)  # Example series for healthcare services\n\n# Calculate total investments for pharmaceuticals, medical devices, and services\ntotal_pharmaceuticals_investment = pharmaceuticals_data['value'].sum()\ntotal_medical_devices_investment = medical_devices_data['value'].sum()\ntotal_services_investment = services_data['value'].sum()\ntotal_healthcare_investment = total_pharmaceuticals_investment + total_medical_devices_investment + total_services_investment\n\n# Calculate the ratios\npharmaceuticals_ratio = total_pharmaceuticals_investment / total_healthcare_investment\nmedical_devices_ratio = total_medical_devices_investment / total_healthcare_investment\nservices_ratio = total_services_investment / total_healthcare_investment\n\nprint(f\"Pharmaceuticals Ratio: {pharmaceuticals_ratio}\")\nprint(f\"Medical Devices Ratio: {medical_devices_ratio}\")\nprint(f\"Services Ratio: {services_ratio}\")\n\nPharmaceuticals Ratio: 0.101622758276214\nMedical Devices Ratio: 0.09037329613526564\nServices Ratio: 0.8080039455885203\n\n\n\n\n\n0.3 Step 4: Aggregate Data with Calculated Ratios\n\nsector_data = {\n    'Real Estate': total_real_estate_investment,\n    'Residential': total_real_estate_investment * residential_ratio,\n    'Commercial': total_real_estate_investment * commercial_ratio,\n    'Tech': total_tech_investment,\n    'Tech & Software': total_tech_investment * tech_and_ip_ratio,\n    'IP': total_tech_investment * ip_ratio,\n    'Healthcare': total_healthcare_investment,\n    'Pharmaceuticals': total_healthcare_investment * pharmaceuticals_ratio,\n    'Medical Devices': total_healthcare_investment * medical_devices_ratio,\n    'Services': total_healthcare_investment * services_ratio\n}\n\n\nsector_data\n\n{'Real Estate': 45236.283,\n 'Residential': 26136.871000000003,\n 'Commercial': 19099.412,\n 'Tech': 58251.611000000004,\n 'Tech & Software': 25676.909,\n 'IP': 32574.70200000001,\n 'Healthcare': 7629.786999999999,\n 'Pharmaceuticals': 775.3599999999999,\n 'Medical Devices': 689.529,\n 'Services': 6164.897999999999}\n\n\n\n\n0.4 Step 5: Create the Sankey Diagram\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.renderers.default = \"notebook_connected\"\n#pio.renderers.default = \"notebook_connected\" # Or \"iframe_connected\"\n\n# Define nodes\nnodes = [\n    'Central Banks',\n    'Real Estate',\n    'Residential',\n    'Commercial',\n    'Tech',\n    'Tech & Software',\n    'IP',\n    'Healthcare',\n    'Pharmaceuticals',\n    'Medical Devices',\n    'Services'\n]\n\n\n# Define links based on fetched data and calculated ratios\nlinks = [\n    {'source': 'Central Banks', 'target': 'Real Estate', 'value': sector_data['Real Estate']},\n    {'source': 'Central Banks', 'target': 'Tech', 'value': sector_data['Tech']},\n    {'source': 'Central Banks', 'target': 'Healthcare', 'value': sector_data['Healthcare']},\n    {'source': 'Real Estate', 'target': 'Residential', 'value': sector_data['Residential']},\n    {'source': 'Real Estate', 'target': 'Commercial', 'value': sector_data['Commercial']},\n    {'source': 'Tech', 'target': 'Tech & Software', 'value': sector_data['Tech & Software']},\n    {'source': 'Tech', 'target': 'IP', 'value': sector_data['IP']},\n    {'source': 'Healthcare', 'target': 'Pharmaceuticals', 'value': sector_data['Pharmaceuticals']},\n    {'source': 'Healthcare', 'target': 'Medical Devices', 'value': sector_data['Medical Devices']},\n    {'source': 'Healthcare', 'target': 'Services', 'value': sector_data['Services']}\n]\n\n# Convert node names to indices for plotly\nnode_indices = {node: i for i, node in enumerate(nodes)}\n\nsankey_links = {\n    'source': [node_indices[link['source']] for link in links],\n    'target': [node_indices[link['target']] for link in links],\n    'value': [link['value'] for link in links]\n}\n\n# Create Sankey diagram\nfig = go.Figure(go.Sankey(\n    node=dict(\n        pad=15,\n        thickness=20,\n        line=dict(color=\"black\", width=0.5),\n        label=nodes\n    ),\n    link=dict(\n        source=sankey_links['source'],\n        target=sankey_links['target'],\n        value=sankey_links['value']\n    )\n))\n\nfig.update_layout(title_text=\"Follow the Money: Central Banks to Various Sectors (2019-2024)\", font_size=10)\nfig.show()\n\n\n\n\n\n# Define links based on fetched data and calculated ratios\nlinks = [\n    #{'source': 'Central Banks', 'target': 'Real Estate', 'value': sector_data['Real Estate']},\n    {'source': 'Central Banks', 'target': 'Tech', 'value': sector_data['Tech']},\n    {'source': 'Central Banks', 'target': 'Healthcare', 'value': sector_data['Healthcare']},\n    #{'source': 'Real Estate', 'target': 'Residential', 'value': sector_data['Residential']},\n    #{'source': 'Real Estate', 'target': 'Commercial', 'value': sector_data['Commercial']},\n    {'source': 'Tech', 'target': 'Tech & Software', 'value': sector_data['Tech & Software']},\n    {'source': 'Tech', 'target': 'IP', 'value': sector_data['IP']},\n    {'source': 'Healthcare', 'target': 'Pharmaceuticals', 'value': sector_data['Pharmaceuticals']},\n    {'source': 'Healthcare', 'target': 'Medical Devices', 'value': sector_data['Medical Devices']},\n    {'source': 'Healthcare', 'target': 'Services', 'value': sector_data['Services']}\n]\n\n# Convert node names to indices for plotly\nnode_indices = {node: i for i, node in enumerate(nodes)}\n\nsankey_links = {\n    'source': [node_indices[link['source']] for link in links],\n    'target': [node_indices[link['target']] for link in links],\n    'value': [link['value'] for link in links]\n}\n\n# Create Sankey diagram\nfig = go.Figure(go.Sankey(\n    node=dict(\n        pad=15,\n        thickness=20,\n        line=dict(color=\"black\", width=0.5),\n        label=nodes\n    ),\n    link=dict(\n        source=sankey_links['source'],\n        target=sankey_links['target'],\n        value=sankey_links['value']\n    )\n))\n\nfig.update_layout(title_text=\"Follow the Money (w/o RE): Central Banks to Various Sectors (2019-2024)\", font_size=10)\nfig.show()"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Notebooks",
    "section": "",
    "text": "Browse my research notebooks below.\n\n\n\n\n\n\n\n\n\nLiquidity Regimes and the Death (and Return) of Valuations:\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 Fama-French Factors and Valuation Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nMosaic-20 Cancer Analysis\n\n\nThis notebook investigates temporal patterns in cancer-related mortality and diagnosis proxies in the post–COVID-19 vaccination period using multiple complementary data sources, including U.S. CDC mortality statistics, aggregated insurance claims indicators, and exploratory molecular in-silico modeling of candidate biological mechanisms. The analysis focuses on identifying whether observable population-level shifts or safety signals warrant further clinical and epidemiological scrutiny. Emphasis is placed on careful interpretation of observational trends, adjustment for major confounders such as pandemic-era healthcare disruption, delayed screening, demographic effects, and baseline secular cancer trends. The molecular modeling component is presented as hypothesis-generating rather than confirmatory. Overall, the notebook provides a reproducible framework for exploring associations across datasets while distinguishing correlation from causal inference in complex public health settings\n\n\n\n\n\nFeb 22, 2026\n\n\n\n\n\n\n\nThe Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling\n\n\nThis analysis challenges the traditional association between extended formal education and improved quality of life (QoL) by examining the structural breakdown of professional labor returns. Utilizing longitudinal data from 1979 to 2025, the study identifies a significant decoupling of real wages from productivity growth and a staggering disparity between tuition inflation and median income. Through Granger Causality tests, the research validates that expansions in the M2 money supply serve as a primary predictor for rising educational costs, effectively devaluing the net financial return of degrees. Furthermore, the application of a “Career Sharpe Ratio” framework reveals that higher education no longer functions as career insurance; rather, increased credentials often lead to higher income volatility that offsets marginal gains. The findings suggest a shift in the role of tertiary education from a tool for knowledge acquisition to a mechanism for “elite inclusion,” a transition that correlates with rising underemployment and a documented decline in the mental health and job satisfaction of highly educated professionals.\n\n\n\n\n\nJan 9, 2026\n\n\n\n\n\n\n\nEmpirical Validation of the 1971 Fiscal Structural Break: Analysis of U.S. Real Individual Income Tax Revenue\n\n\nThis document provides empirical validation for a structural break in U.S. fiscal extraction occurring at the 1971 monetary pivot point. Utilizing an Interrupted Time Series framework and a Chow Test on inflation-adjusted U.S. Individual Income Tax receipts (normalized to 2024 dollars), the analysis confirms a significant shift in the growth trajectory of tax extraction following the transition to a pure fiat regime.\n\n\n\n\n\nJan 2, 2026\n\n\n\n\n\n\n\nFollow the Money: Analyzing Capital Flows from Central Banks to Key Sectors\n\n\nThis notebook provides a comprehensive technical framework for tracing capital distribution from central banks into the real estate, technology, and healthcare sectors. By utilizing Python to interface with the Federal Reserve Economic Data (FRED) and World Bank APIs, the analysis retrieves critical economic indicators for the period spanning 2019 to 2025. The study quantifies total investment volumes and calculates specific ratios for sub-sectors, including residential versus commercial real estate, information processing equipment versus software, and pharmaceutical research versus medical services. The final output utilizes the Plotly library to construct interactive Sankey diagrams that visualize the transition of liquidity through the financial system, highlighting how money moves from central sources to diverse economic pillars.\n\n\n\n\n\nMay 22, 2025\n\n\n\n\n\n\n\nPrivate Investment Trends in Aerospace and Defense (2019–2024)\n\n\nThis report analyzes private investment activity in the aerospace and defense sectors across North America and Europe from January 2019 to May 2024. Despite a sectoral investment CAGR of -20% since 2019, the market remains dominated by high-value “scaleup” funding rounds exceeding $100M for industry leaders such as SpaceX, Anduril Industries, and Sierra Space. California serves as the primary geographic epicenter for this capital, with Hawthorne and Costa Mesa leading in total money raised. Key investment drivers include space travel, satellite communications, and advanced manufacturing, backed by prominent lead investors like Valor Equity Partners, Sequoia Capital, and BlackRock. Furthermore, the report highlights the disparity in military spending as a percentage of GDP, with the United States maintaining a significantly higher ratio than its European counterparts, providing a stable backdrop for continued private sector innovation.\n\n\n\n\n\nMay 31, 2024\n\n\n\n\n\n\n\nMini-Fund Volatility Regime Backtest Analysis (2000–2024)\n\n\nThis study evaluates the long-term performance and risk metrics of a high-growth “Mini-Fund” portfolio—consisting of ASML, TSLA, BRK-B, MSFT, ABBV, and LMT—relative to the S&P 500 and Nasdaq-100 benchmarks. Using Hidden Markov Models to categorize market behavior into low, mid, and high volatility regimes, the analysis demonstrates that the optimized “Portfolio-GV-O2” consistently outpaced benchmarks, delivering an annualized return of 30.8% and a Sharpe ratio of 1.25, compared to 11.4% and 0.60 for the S&P 500. The portfolio proved highly resilient during historical recessions, yielding a -2.9% return while the S&P 500 declined by 19.4%. Furthermore, the strategic inclusion of the protective TAIL ETF reduced the maximum drawdown from -35.0% to -27.6%, enhancing downside protection without sacrificing significant growth. CWARP values of 162.3 against the S&P 500 and 78.8 against the Nasdaq-100 further validate the portfolio’s superior risk-adjusted efficiency.\n\n\n\n\n\nMay 22, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/aerospace-defense.html",
    "href": "notebooks/aerospace-defense.html",
    "title": "Private Investment Trends in Aerospace and Defense (2019–2024)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport adjustText\nCode\n####### WORKING #######\n#!pip install easy-exchange-rates\n#!pip install tqdm\nCode\ndf = pd.read_csv(\"Recipe_CB_Aerospace_and_Defense_Since2019_2.csv\")\nCode\ndf.head(5)\n\n\n\n\n\n\n\n\n\nname\nmoney_raised\nannounced_date\nlocation\nindustries\ndescription\nfunding_type\nlead_investors\ntotal_funding\n\n\n\n\n0\nSpaceX\n$1,724,965,480\nMay 24, 2022\nHawthorne, California, United States, North Am...\nAdvanced Materials, Aerospace, Manufacturing, ...\nSpaceX is an aviation and aerospace company th...\nVenture - Series Unknown\n—\n$9,779,343,846\n\n\n1\nAnduril Industries\n$1,480,000,000\nDec 2, 2022\nCosta Mesa, California, United States, North A...\nAerospace, Government, Military, National Secu...\nAnduril Industries is a defense product compan...\nSeries E\nValor Equity Partners\n$2,171,000,000\n\n\n2\nSierra Space\n$1,400,000,000\nNov 19, 2021\nLouisville, Colorado, United States, North Ame...\nAdvanced Materials, Aerospace, Industrial Manu...\nSierra Space is a commercial space company tha...\nSeries A\nCoatue, General Atlantic, Moore Strategic Vent...\n$1,690,000,000\n\n\n3\nOneWeb\n$1,250,000,000\nMar 18, 2019\nLondon, England, United Kingdom, Europe\nAerospace, Internet, Satellite Communication, ...\nOneWeb is building a space-based communication...\nVenture - Series Unknown\nSoftBank\n$4,700,000,000\n\n\n4\nSpaceX\n$850,000,000\nFeb 16, 2021\nHawthorne, California, United States, North Am...\nAdvanced Materials, Aerospace, Manufacturing, ...\nSpaceX is an aviation and aerospace company th...\nVenture - Series Unknown\nSequoia Capital\n$9,779,343,846"
  },
  {
    "objectID": "notebooks/aerospace-defense.html#plots",
    "href": "notebooks/aerospace-defense.html#plots",
    "title": "Private Investment Trends in Aerospace and Defense (2019–2024)",
    "section": "Plots",
    "text": "Plots\n\n\nCode\nfrom plotnine import ggplot, geom_bar, scale_x_date, scale_y_continuous, aes, stat_smooth, facet_wrap, options, theme_classic, labs, theme, element_text, geom_line, coord_flip, scale_size_continuous, geom_text, geom_label, scale_fill_manual, geom_tile, scale_colour_continuous, theme_bw, scale_colour_manual, scale_color_discrete, geom_point, geom_histogram, after_stat"
  },
  {
    "objectID": "notebooks/aerospace-defense.html#cagr-in-deal-value---preparing-a-derived-dataset",
    "href": "notebooks/aerospace-defense.html#cagr-in-deal-value---preparing-a-derived-dataset",
    "title": "Private Investment Trends in Aerospace and Defense (2019–2024)",
    "section": "CAGR in Deal Value - Preparing a derived dataset",
    "text": "CAGR in Deal Value - Preparing a derived dataset\n\n\nCode\ndf_spacex = df_world_raw[df_world_raw['name'] == 'Hadrian']\n\n\n'Andreessen Horowitz, Lux Capital,Founders Fund'\n\n\n\n\nCode\n# def weighted_average(data):\n#     d = {}\n#     d['d1_wa'] = np.average(data['d1'], weights=data['weights'])\n#     d['d2_wa'] = np.average(data['d2'], weights=data['weights'])\n#     return pd.Series(d)\n# Call the groupby apply method with our custom function:\n\n# df.groupby('group').apply(weighted_average)\n\n#        d1_wa  d2_wa\n# group              \n# a        9.0    2.2\n# b       58.0   13.2\n\ndef first_last_deal_flow(data):\n    d = {}\n    data_sorted = data.sort_values(by = [\"announced_date\"])\n    d['first_deal_value'] = data_sorted.loc[data_sorted.index[0], 'money_raised_usd']\n    d['first_deal_date'] = data_sorted.loc[data_sorted.index[0], 'announced_date']\n    d['last_deal_value'] = data_sorted.loc[data_sorted.index[-1], 'money_raised_usd']\n    d['last_deal_date'] = data_sorted.loc[data_sorted.index[-1], 'announced_date']\n    d['last_deal_year'] = data_sorted.loc[data_sorted.index[-1], 'announced_year']\n    d['deal_span_years'] = round((d['last_deal_date'] - d['first_deal_date'])/pd.Timedelta(days=365),2)\n    d['total_funding_usd'] = data_sorted.loc[data_sorted.index[0], 'total_funding_usd']\n    d['city'] = data_sorted.loc[data_sorted.index[0], 'city']\n    d['country'] = data_sorted.loc[data_sorted.index[0], 'country']\n    d['region'] = data_sorted.loc[data_sorted.index[0], 'region']\n    d['sector'] = data_sorted.loc[data_sorted.index[0], 'sector']\n    d['lead_investors'] = ','.join(pd.Series(data_sorted['lead_investors'].str.split(\",\").explode().unique()).where(lambda x: x != \"—\").dropna())\n    d['industries'] = data_sorted.loc[data_sorted.index[0], 'industries']\n    d['deals'] = len(data_sorted)\n    d['funding_recency'] = \"recent\" if (abs(d['last_deal_year'] - datetime.now().year) &lt;= 2) else \"older\"\n    d['deal_growth_cagr'] = round((d['last_deal_value']/d['first_deal_value'])**(1./d['deal_span_years'])-1,\n                                  2)*100 if d['deal_span_years'] &gt; 0 else 0\n    return pd.Series(d)\n\ndf_world_first_last_deal_values = ( \n    df_world_raw\n    .groupby(['name'])\n    .apply(first_last_deal_flow)\n)\n\n# Ignore outliers - top 2 percentile\ndf_world_first_last_deal_values['deal_growth_cagr'] = df_world_first_last_deal_values['deal_growth_cagr'].clip(upper=df_world_first_last_deal_values['deal_growth_cagr'].quantile(0.98))\n\n# Sort\ndf_world_first_last_deal_values = df_world_first_last_deal_values.sort_values(by = [\"deal_growth_cagr\", \n                                                                                    \"total_funding_usd\", \n                                                                                    \"last_deal_date\"], \n                                                                              ascending=False)\n\ndf_world_first_last_deal_values = df_world_first_last_deal_values.reset_index()\ndf_world_first_last_deal_values.head(10)\n\n\n\n\n\n\n\n\n\nname\nfirst_deal_value\nfirst_deal_date\nlast_deal_value\nlast_deal_date\nlast_deal_year\ndeal_span_years\ntotal_funding_usd\ncity\ncountry\nregion\nsector\nlead_investors\nindustries\ndeals\nfunding_recency\ndeal_growth_cagr\n\n\n\n\n0\nSkyryse\n2500000.0\n2020-05-15\n200000000.0\n2021-10-27\n2021\n1.45\n240500000.0\nEl Segundo\nUnited States\nNorth America\naerospace\nFidelity, Monashee Investment Management\n[Aerospace, Air Transportation, Internet, Transportation]\n2\nolder\n1066.4\n\n\n1\nAeroVanti\n9750000.0\n2022-07-21\n100000000.0\n2022-10-19\n2022\n0.25\n109750000.0\nAnnapolis\nUnited States\nNorth America\naerospace\nNetwork 1 Financial Securities\n[Aerospace, Air Transportation, Transportation]\n2\nrecent\n1066.4\n\n\n2\nBRINC\n2200000.0\n2020-10-29\n55000000.0\n2022-01-01\n2022\n1.18\n82200000.0\nSeattle\nUnited States\nNorth America\naerospace\nSam Altman,Index Ventures,Alameda Research\n[Aerospace, Drones, Law Enforcement, Public Safety, Robotics]\n3\nrecent\n1066.4\n\n\n3\nAKHAN Semiconductor\n1949083.0\n2021-11-08\n20000000.0\n2022-02-17\n2022\n0.28\n37919412.0\nGurnee\nUnited States\nNorth America\naerospace and defense\n\n[Aerospace, Automotive, Consumer Electronics, Manufacturing, Military, Semiconductor, Telecommunications]\n3\nrecent\n1066.4\n\n\n4\nPhantom Space\n875000.0\n2020-09-11\n21630605.0\n2021-11-04\n2021\n1.15\n27655605.0\nTucson\nUnited States\nNorth America\naerospace\nChenel Capital\n[Aerospace, Space Travel, Transportation]\n3\nolder\n1066.4\n\n\n5\nKarman+\n1000000.0\n2022-01-01\n25000000.0\n2023-03-01\n2023\n1.16\n26000000.0\nDenver\nUnited States\nNorth America\naerospace\n\n[Aerospace, Robotics]\n2\nrecent\n1066.4\n\n\n6\nApogee Semiconductor\n468792.0\n2022-07-21\n8606581.0\n2023-04-19\n2023\n0.75\n10373924.0\nPlano\nUnited States\nNorth America\naerospace\n\n[Aerospace, Industrial Manufacturing, Semiconductor]\n2\nrecent\n1066.4\n\n\n7\nRadical\n500000.0\n2023-04-05\n4465000.0\n2024-01-05\n2024\n0.75\n4965000.0\nSeattle\nUnited States\nNorth America\naerospace\nY Combinator,Scout Ventures\n[Aerospace, Internet, Telecommunications]\n2\nrecent\n1066.4\n\n\n8\nBasalt Tech\n500000.0\n2024-04-03\n3500000.0\n2024-05-30\n2024\n0.16\n4000000.0\nSan Francisco\nUnited States\nNorth America\naerospace\nY Combinator,Initialized Capital\n[Aerospace, Industrial Automation, Space Travel]\n2\nrecent\n1066.4\n\n\n9\nThe Exploration Company\n1740300.0\n2021-10-05\n43576000.0\n2023-02-01\n2023\n1.33\n54297360.0\nMunich\nGermany\nEurope\naerospace\nPromus Ventures,EQT Ventures, Red River West\n[Aerospace, Air Transportation, Manufacturing, Space Travel]\n3\nrecent\n1026.0\n\n\n\n\n\n\n\n\n\nCode\noptions.figure_size = (1200 / options.dpi, 780 / options.dpi)\n\n(\n    ggplot(df_world_first_last_deal_values.head(50))\n    + geom_point(aes(x=\"name\",\n                    y=\"deal_growth_cagr\",\n                    size=\"total_funding_usd\",\n                    group=\"region\",\n                    fill=\"factor(funding_recency)\",\n                    colour=\"factor(funding_recency)\"))\n    #+ geom_text(aes(label=\"total_money_raised\"), size=9)\n    + scale_y_continuous(labels = lambda l: ['{}%'.format(v) for v in l])\n    + scale_size_continuous(labels = lambda l: [add_units(v) for v in l])\n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_colour_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue }, guide=False)\n    + labs(\n        x=\"Companies\",\n        y=\"Deal Value Growth - CAGR\",\n        title=\"Private Investment in Aerospace & Defense across World\",\n        size=\"Total Funding\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Top 50 companies by Total Funding and Deal Growth CAGR ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption\n    )\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"}),\n        figure_size=(8, 8)\n    )\n    + coord_flip() \n    + facet_wrap(\"region\")\n    + theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\noptions.figure_size = (1200 / options.dpi, 780 / options.dpi)\n\n(\n    ggplot(df_world_first_last_deal_values[(df_world_first_last_deal_values['total_funding_usd']&gt;10000000) & \n           (df_world_first_last_deal_values['total_funding_usd']&lt;50000000)].head(50))\n    + geom_point(aes(x=\"name\",\n                    y=\"deal_growth_cagr\",\n                    size=\"total_funding_usd\",\n                    group=\"region\",\n                    fill=\"factor(funding_recency)\",\n                    colour=\"factor(funding_recency)\"))\n    #+ geom_text(aes(label=\"total_money_raised\"), size=9)\n    + scale_y_continuous(labels = lambda l: ['{}%'.format(v) for v in l])\n    + scale_size_continuous(labels = lambda l: [add_units(v) for v in l])\n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_colour_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue }, \n                          guide=False)\n    + labs(\n        x=\"Companies\",\n        y=\"Deal Value Growth - CAGR\",\n        title=\"Private Investment in Aerospace & Defense across World\",\n        size=\"Total Funding\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Companies w/ 10-50M total funding, by Total Funding and Deal Growth CAGR ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption\n    )\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"}),\n        figure_size=(8, 8)\n    )\n    + coord_flip() \n    + facet_wrap(\"region\")\n    + theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_stage = (\n    df_world_raw\n    .groupby(['region', 'funding_type', 'funding_recency'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_world_funding_by_stage = df_world_funding_by_stage.reset_index()\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n(\n    ggplot(df_world_funding_by_stage)\n    + geom_bar(aes(x=\"funding_type\", \n                   y=\"total_money_raised\", \n                   group=\"region\",\n                   fill = \"factor(funding_recency)\"),\n               \n               stat=\"identity\") \n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Funding Stages\",\n        y=\"Total Money Raised (in USD)\",\n        title=\"Private Investment in Aerospace & Defense across US & Europe\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Funding by stages ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n    + facet_wrap(\"region\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_countries = (\n    df_world_raw\n    .groupby(['region', 'country', 'funding_recency'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_world_funding_by_countries = df_world_funding_by_countries.reset_index()\ndf_world_funding_by_countries.head(5)\n\n\n\n\n\n\n\n\n\nregion\ncountry\nfunding_recency\ntotal_money_raised\ndeals\n\n\n\n\n0\nEurope\nAustria\nolder\n22542000.0\n1\n\n\n1\nEurope\nAustria\nrecent\n204462.0\n1\n\n\n2\nEurope\nBelgium\nolder\n56542900.0\n4\n\n\n3\nEurope\nBelgium\nrecent\n75873000.0\n3\n\n\n4\nEurope\nBulgaria\nolder\n4453420.0\n2\n\n\n\n\n\n\n\n\nMilitary spending to GDP\nSource: https://data.worldbank.org/indicator/MS.MIL.XPND.GD.ZS?end=2022&start=2022&view=bar\n\n\nCode\nmilitary_to_gdp_df = pd.read_csv(\"Military_to_GDP.csv\")\nmilitary_to_gdp_df = military_to_gdp_df[['Country Name', '2022']]\nmilitary_to_gdp_df.rename(columns = {\"Country Name\": \"country\", \"2022\": \"military_spending_to_gdp\"}, \n                          inplace=True)\nmilitary_to_gdp_df = military_to_gdp_df.dropna()\nmilitary_to_gdp_df.head(5)\n\n\n\n\n\n\n\n\n\ncountry\nmilitary_spending_to_gdp\n\n\n\n\n1\nAfrica Eastern and Southern\n1.001660\n\n\n3\nAfrica Western and Central\n0.975188\n\n\n4\nAngola\n1.328722\n\n\n5\nAlbania\n1.584881\n\n\n7\nArab World\n4.968286\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_and_spending_by_countries = df_world_funding_by_countries.merge(military_to_gdp_df, on = \"country\")\ndf_world_funding_and_spending_by_countries['military_spending_to_gdp'] = pd.to_numeric(df_world_funding_and_spending_by_countries['military_spending_to_gdp'])\ndf_world_funding_and_spending_by_countries.head(5)\n\n\n\n\n\n\n\n\n\nregion\ncountry\nfunding_recency\ntotal_money_raised\ndeals\nmilitary_spending_to_gdp\n\n\n\n\n0\nEurope\nAustria\nolder\n22542000.0\n1\n0.772607\n\n\n1\nEurope\nAustria\nrecent\n204462.0\n1\n0.772607\n\n\n2\nEurope\nBelgium\nolder\n56542900.0\n4\n1.179737\n\n\n3\nEurope\nBelgium\nrecent\n75873000.0\n3\n1.179737\n\n\n4\nEurope\nBulgaria\nolder\n4453420.0\n2\n1.508123\n\n\n\n\n\n\n\n\n\nCode\n# Ordering the bar plot\ndf_world_funding_and_spending_by_countries = df_world_funding_and_spending_by_countries.sort_values(by = \"military_spending_to_gdp\", \n                                                                          ascending=False)\ncountries_by_funding = df_world_funding_and_spending_by_countries[\"country\"].unique()\ndf_world_funding_and_spending_by_countries = df_world_funding_and_spending_by_countries.assign(\n    country_cat=pd.Categorical(df_world_funding_and_spending_by_countries[\"country\"], \n                               categories=countries_by_funding)\n)\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n(\n    ggplot(df_world_funding_and_spending_by_countries[1:])\n    + geom_bar(aes(x=\"country_cat\",\n                   y=\"military_spending_to_gdp\"),\n               fill=boson_blue,\n               stat=\"identity\") \n    #+ scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(breaks = list([x * .5 for x in range(20)]), \n                         labels = lambda l: [\"{}%\".format(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Military Spending to GDP (in %)\",\n        title=\"Military Spending to GDP (US & Europe)\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Source: https://data.worldbank.org/indicator/MS.MIL.XPND.GD.ZS?end=2022&start=2022&view=bar\",\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_and_spending_by_countries.head(2)\n\n\n\n\n\n\n\n\n\nregion\ncountry\nfunding_recency\ntotal_money_raised\ndeals\nmilitary_spending_to_gdp\ncountry_cat\n\n\n\n\n28\nEurope\nUkraine\nolder\n3.500000e+05\n1\n33.546573\nUkraine\n\n\n32\nNorth America\nUnited States\nrecent\n1.026982e+10\n273\n3.454920\nUnited States\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_countries = (\n    df_world_funding_and_spending_by_countries\n    .groupby([\"funding_recency\", \"country\"])\n    .agg(total_money_raised = ('total_money_raised', 'sum'))\n    .sort_values(by=[\"total_money_raised\"], \n                 ascending=False)\n)\ndf_world_funding_by_countries = df_world_funding_by_countries.reset_index()\ndf_world_funding_by_countries.head(5)\n\n\n\n\n\n\n\n\n\nfunding_recency\ncountry\ntotal_money_raised\n\n\n\n\n0\nolder\nUnited States\n1.038508e+10\n\n\n1\nrecent\nUnited States\n1.026982e+10\n\n\n2\nolder\nUnited Kingdom\n3.027402e+09\n\n\n3\nrecent\nGermany\n1.091632e+09\n\n\n4\nolder\nGermany\n7.439148e+08\n\n\n\n\n\n\n\n\n\nCode\n# Ordering the bar plot\ndf_world_funding_by_countries = df_world_funding_by_countries.sort_values(by = \"total_money_raised\", \n                                                                          ascending=False)\ncountries_by_funding = df_world_funding_by_countries[\"country\"].unique()\ndf_world_funding_by_countries = df_world_funding_by_countries.assign(\n    country_cat=pd.Categorical(df_world_funding_by_countries[\"country\"], \n                               categories=countries_by_funding)\n)\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n\n# Annotate &gt;1B countries\ndf_labels = df_world_funding_by_countries[df_world_funding_by_countries['total_money_raised'] &gt; 1000000000]\ndf_labels = (\n    df_labels\n    .groupby([\"country_cat\"])\n    .agg(total_money_raised = ('total_money_raised', 'sum'))\n    .reset_index()\n)\ndf_labels['total_money_raised_format'] = df_labels['total_money_raised'].apply(add_units)\n\n(\n    ggplot(df_world_funding_by_countries)\n    + geom_bar(aes(x=\"country_cat\", \n                   y=\"total_money_raised\",\n                  fill=\"factor(funding_recency)\"),\n               stat=\"identity\") \n    + geom_text(aes(x = \"country_cat\",\n                    y = \"total_money_raised - 300000000\",\n                    label = \"total_money_raised_format\"),\n                color=\"white\",\n                va = \"top\",\n                size = 8,\n               data=df_labels)\n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Total Money Raised (in USD)\",\n        title=\"Private Investment in Aerospace & Defense across US & Europe\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Funding by countries ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_countries = (\n    df_world_raw\n    .groupby(['region', 'country', 'announced_year'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_world_funding_by_countries = df_world_funding_by_countries.reset_index()\n\ndf_europe_funding_by_countries = (\n    df_world_funding_by_countries[\n        (df_world_funding_by_countries['region'] == current_region['region']) & \n        ((df_world_funding_by_countries['announced_year'] &gt;= 2022))\n    ]\n    .groupby([\"country\", \"announced_year\"])\n    .agg(total_money_raised = ('total_money_raised', 'sum'))\n    .sort_values(by=[\"total_money_raised\"], \n                 ascending=False)\n)\ndf_europe_funding_by_countries = df_europe_funding_by_countries.reset_index()\n\n# Ordering the bar plot\ndf_europe_funding_by_countries = df_europe_funding_by_countries.sort_values(by = \"total_money_raised\", \n                                                                          ascending=False)\ncountries_by_funding = df_europe_funding_by_countries[\"country\"].unique()\ndf_europe_funding_by_countries = df_europe_funding_by_countries.assign(\n    country_cat=pd.Categorical(df_europe_funding_by_countries[\"country\"], \n                               categories=countries_by_funding)\n)\n\ndf_labels = (\n    df_world_raw[\n        (df_world_raw['region'] == current_region['region']) & \n        (df_world_raw['announced_year'] &gt;= 2022)\n    ]\n    .groupby(['country', 'name'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_labels = df_labels.reset_index()\ndf_labels = df_labels.assign(\n    country_cat=pd.Categorical(df_labels[\"country\"], \n                               categories=countries_by_funding)\n)\n\n\n(\n    ggplot(df_europe_funding_by_countries)\n    + geom_bar(aes(x=\"country_cat\", \n                   y=\"total_money_raised\"),\n               stat=\"identity\",\n              fill=boson_blue)\n    # + geom_text(aes(x = \"country_cat\",\n    #             y = \"total_money_raised * 1.2 + 1000000\",\n    #             label = \"name\"),\n    #             va = \"bottom\",\n    #             size = 8,\n    #             nudge_y = 0.5,\n    #             colour=\"#780000\",\n    #             #adjust_text=adjust_text_dict,\n    #            data=df_labels[df_labels['total_money_raised'] &gt; 100000000])\n    + scale_y_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Total Money Raised (in USD)\",\n        title=\"Private Investment in Aerospace & Defense across Europe\",\n        subtitle =\"Funding by countries (2022-2024)\",\n        caption=caption,\n    )\n    + coord_flip()\n    + facet_wrap(\"announced_year\", ncol=3)\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_labels.sort_values(by=['total_money_raised'], ascending=False).head(20)\n\n\n\n\n\n\n\n\n\ncountry\nname\ntotal_money_raised\ndeals\ncountry_cat\n\n\n\n\n157\nUnited States\nSpaceX\n1.974965e+09\n2\nUnited States\n\n\n17\nUnited States\nAnduril Industries\n1.480000e+09\n1\nUnited States\n\n\n55\nUnited States\nDivergent\n4.900000e+08\n3\nUnited States\n\n\n195\nUnited States\nWisk Aero\n4.500000e+08\n1\nUnited States\n\n\n37\nUnited States\nBeta Technologies\n3.750000e+08\n1\nUnited States\n\n\n67\nUnited States\nFirefly Aerospace\n3.750000e+08\n2\nUnited States\n\n\n32\nUnited States\nAxiom Space\n3.500000e+08\n1\nUnited States\n\n\n150\nUnited States\nSierra Space\n2.900000e+08\n1\nUnited States\n\n\n62\nUnited States\nEpirus\n2.000000e+08\n1\nUnited States\n\n\n24\nUnited States\nAstranis\n2.000000e+08\n1\nUnited States\n\n\n81\nUnited States\nHadrian\n1.820000e+08\n2\nUnited States\n\n\n77\nUnited States\nGecko Robotics\n1.730000e+08\n2\nUnited States\n\n\n42\nUnited States\nCapella Space\n1.570000e+08\n2\nUnited States\n\n\n202\nUnited States\nZeroAvia\n1.460000e+08\n2\nUnited States\n\n\n124\nUnited States\nOverair\n1.450000e+08\n1\nUnited States\n\n\n178\nUnited States\nUrsa Major\n1.380000e+08\n2\nUnited States\n\n\n173\nUnited States\nTrue Anomaly\n1.330000e+08\n3\nUnited States\n\n\n6\nUnited States\nAeroVanti\n1.097500e+08\n2\nUnited States\n\n\n83\nUnited States\nHermeus\n1.000000e+08\n1\nUnited States\n\n\n165\nUnited States\nStoke Space\n1.000000e+08\n1\nUnited States\n\n\n\n\n\n\n\n\n\nCode\ndf_world_spending_by_countries = (\n    df_world_funding_and_spending_by_countries\n    .groupby([\"country\"])\n    .agg(military_spending_to_gdp = ('military_spending_to_gdp', 'sum'))\n    .sort_values(by=[\"military_spending_to_gdp\"], \n                 ascending=False)\n)\ndf_world_spending_by_countries = df_world_spending_by_countries.reset_index()\n\ncountries_by_spending = df_world_spending_by_countries[\"country\"].unique()\ndf_world_spending_by_countries = df_world_spending_by_countries.assign(\n    country_cat=pd.Categorical(df_world_spending_by_countries[\"country\"], \n                               categories=countries_by_spending)\n)\ndf_world_spending_by_countries.head(5)\n\n\n\n\n\n\n\n\n\ncountry\nmilitary_spending_to_gdp\ncountry_cat\n\n\n\n\n0\nUkraine\n33.546573\nUkraine\n\n\n1\nUnited States\n6.909840\nUnited States\n\n\n2\nLithuania\n5.045247\nLithuania\n\n\n3\nUnited Kingdom\n4.454368\nUnited Kingdom\n\n\n4\nFrance\n3.877447\nFrance\n\n\n\n\n\n\n\n\n\nCode\n(\n    ggplot(df_world_spending_by_countries[1:])\n    + geom_bar(aes(x=\"country_cat\", \n                   y=\"military_spending_to_gdp\"),\n               stat=\"identity\",\n            fill=boson_blue) \n    #+ scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(labels = lambda l: ['{}%'.format(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Military Spending to GDP (%()\",\n        title=\"Military Spending to GDP\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Funding by countries ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_raw.head(2)\n\n\n\n\n\n\n\n\n\nindex\nname\nmoney_raised\nannounced_date\nindustries\ndescription\nfunding_type\nlead_investors\ntotal_funding\ncity\n...\ntotal_funding_currency\ntotal_funding_amount\nmoney_raised_usd\ntotal_funding_usd\naerospace\ndefense\nannounced_date_trunc_month\nannounced_date_trunc_year\nsector\nfunding_recency\n\n\n\n\n0\n0\nSpaceX\n$1724965480\n2022-05-24\nAdvanced Materials, Aerospace, Manufacturing, ...\nSpaceX is an aviation and aerospace company th...\nVenture - Series Unknown\n—\n$9779343846\nHawthorne\n...\n$\n9779343846\n1.724965e+09\n9.779344e+09\nTrue\nFalse\n2022-05-01\n2022-01-01\naerospace\nrecent\n\n\n1\n1\nAnduril Industries\n$1480000000\n2022-12-02\nAerospace, Government, Military, National Secu...\nAnduril Industries is a defense product compan...\nSeries E\nValor Equity Partners\n$2171000000\nCosta Mesa\n...\n$\n2171000000\n1.480000e+09\n2.171000e+09\nTrue\nTrue\n2022-12-01\n2022-01-01\naerospace and defense\nrecent\n\n\n\n\n2 rows × 27 columns\n\n\n\n\n\nCode\ndf_world_raw['industries'] = df_world_raw['industries'].str.split(\",\")\n\n\n\n\nCode\ndf_world_raw_exploded = df_world_raw.explode('industries')\ndf_world_raw_exploded['industries'] = df_world_raw_exploded['industries'].str.strip()\n\ndf_world_raw_exploded_agg_by_industries = (\n    df_world_raw_exploded\n    .groupby(['industries'])\n    .agg(money_raised_usd = ('money_raised_usd', 'sum'))\n)\ndf_world_raw_exploded_agg_by_industries = (\n    df_world_raw_exploded_agg_by_industries\n    .reset_index()\n)\n# df_world_raw_exploded_agg_by_industries.head(2)\n\n\ndf_world_raw_exploded_agg_by_industries['money_raised_usd_format'] = df_world_raw_exploded_agg_by_industries['money_raised_usd'].apply(add_units)\ndf_world_raw_exploded_agg_by_industries.head(5)\n\n\n\n\n\n\n\n\n\nindustries\nmoney_raised_usd\nmoney_raised_usd_format\n\n\n\n\n0\n3D Printing\n6.628812e+08\n663M\n\n\n1\n3D Technology\n1.342485e+09\n1B\n\n\n2\nAdvanced Materials\n6.570686e+09\n7B\n\n\n3\nAerospace\n2.641824e+10\n26B\n\n\n4\nAgTech\n7.591975e+07\n76M\n\n\n\n\n\n\n\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook\"\n\nboson_blue = \"#04024B\"\nboson_royal = \"#1d2b71\"\nboson_yinmin = '#375496'\nboson_glaucous = '#507dbc'\nboson_powder_blue = '#a1c6ea'\nboson_columbia_blue = '#bbd1ea'\nboson_blue_faded = '#dae3e5'\n\nfig = px.treemap(df_world_raw_exploded_agg_by_industries[20:], \n                 path=['industries'],\n                 values='money_raised_usd',\n                 color='money_raised_usd',\n                \n                 # https://coolors.co/generate\n                 # https://coolors.co/04024b-1d2b71-375496-507dbc-a1c6ea-bbd1ea-dae3e5\n                 color_continuous_scale=[(0,boson_blue_faded),\n                                         (0.14, boson_columbia_blue),\n                                         (0.28, boson_powder_blue),\n                                         (0.42, boson_glaucous),\n                                         (0.56, boson_yinmin),\n                                         (0.75, boson_royal),\n                                         (1,boson_blue)],\n\n                 custom_data=['money_raised_usd_format'],\n                 \n                 width=1200, \n                 height=650)\nfig.update_layout(margin = dict(t=20, l=0, r=0, b=0))\nfig.update_traces(marker = dict(\n                    line = dict(width = 0)\n                  ),\n                 tiling = dict(pad = 0))\nfig.data[0].texttemplate = \"&lt;b&gt;%{label}&lt;/b&gt;&lt;br&gt;%{customdata[0]}\"\nfig.show()\n\n\n                                                \n\n\n\n\nCAGR by sub-sectors\n\n\nCode\ndf_world_first_last_deal_values.head(5)\n\n\n\n\n\n\n\n\n\nname\nfirst_deal_value\nfirst_deal_date\nlast_deal_value\nlast_deal_date\nlast_deal_year\ndeal_span_years\ntotal_funding_usd\ncity\ncountry\nregion\nsector\nlead_investors\nindustries\ndeals\nfunding_recency\ndeal_growth_cagr\n\n\n\n\n0\nSkyryse\n2500000.0\n2020-05-15\n200000000.0\n2021-10-27\n2021\n1.45\n240500000.0\nEl Segundo\nUnited States\nNorth America\naerospace\n—\nAerospace, Air Transportation, Internet, Trans...\n2\nolder\n1066.4\n\n\n1\nAeroVanti\n9750000.0\n2022-07-21\n100000000.0\n2022-10-19\n2022\n0.25\n109750000.0\nAnnapolis\nUnited States\nNorth America\naerospace\nNetwork 1 Financial Securities\nAerospace, Air Transportation, Transportation\n2\nrecent\n1066.4\n\n\n2\nBRINC\n2200000.0\n2020-10-29\n55000000.0\n2022-01-01\n2022\n1.18\n82200000.0\nSeattle\nUnited States\nNorth America\naerospace\nSam Altman\nAerospace, Drones, Law Enforcement, Public Saf...\n3\nrecent\n1066.4\n\n\n3\nAKHAN Semiconductor\n1949083.0\n2021-11-08\n20000000.0\n2022-02-17\n2022\n0.28\n37919412.0\nGurnee\nUnited States\nNorth America\naerospace and defense\n—\nAerospace, Automotive, Consumer Electronics, M...\n3\nrecent\n1066.4\n\n\n4\nPhantom Space\n875000.0\n2020-09-11\n21630605.0\n2021-11-04\n2021\n1.15\n27655605.0\nTucson\nUnited States\nNorth America\naerospace\n—\nAerospace, Space Travel, Transportation\n3\nolder\n1066.4\n\n\n\n\n\n\n\n\n\nCode\n#df_world_first_last_deal_values['industries'] = df_world_first_last_deal_values['industries'].str.split(\",\")\n\n\n\n\nCode\ndf_world_first_last_deal_values_exploded = df_world_first_last_deal_values.explode('industries')\ndf_world_first_last_deal_values_exploded['industries'] = df_world_first_last_deal_values_exploded['industries'].str.strip()\n\ndf_world_cagr_exploded_agg_by_industries = (\n    df_world_first_last_deal_values_exploded\n    .groupby(['industries'])\n    .agg(avg_funding_usd = ('total_funding_usd', 'mean'),\n        deal_growth_cagr = ('deal_growth_cagr', 'mean'))\n)\ndf_world_cagr_exploded_agg_by_industries = (\n    df_world_cagr_exploded_agg_by_industries\n    .reset_index()\n)\n#df_world_cagr_exploded_agg_by_industries['deal_growth_cagr'] = df_world_cagr_exploded_agg_by_industries['deal_growth_cagr']*100\n# df_world_cagr_exploded_agg_by_industries.head(20)\n\ndf_world_cagr_exploded_agg_by_industries = df_world_cagr_exploded_agg_by_industries.sort_values(by = \"deal_growth_cagr\",\n                                                                                                ascending=False)\nindustries = df_world_cagr_exploded_agg_by_industries[\"industries\"].unique()\ndf_world_cagr_exploded_agg_by_industries = df_world_cagr_exploded_agg_by_industries.assign(\n    industry_cat=pd.Categorical(df_world_cagr_exploded_agg_by_industries[\"industries\"], \n                               categories=industries)\n)\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n(\n    ggplot(df_world_cagr_exploded_agg_by_industries.head(30))\n    + geom_point(aes(x=\"industry_cat\", \n                     y=\"deal_growth_cagr\",\n                     size=\"avg_funding_usd\"),\n               stat=\"identity\",\n              fill=boson_blue) \n    + scale_y_continuous(breaks = list(range(0,800,50)),\n                         labels = lambda l: [\"{}%\".format(v) for v in l])\n    + scale_size_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Industries\",\n        y=\"Deal Value Growth - CAGR\",\n        title=\"Private Investment in Aerospace & Defense across US & Europe\",\n        size=\"Avg Funding (USD)\",\n        subtitle =\"CAGR and Avg. Funding by industries ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\nSub-industry: Transportation / Air Transportation\n\n\nCode\ndf_world_first_last_deal_values_exploded.head(2)\n\n\n\n\n\n\n\n\n\nname\nfirst_deal_value\nfirst_deal_date\nlast_deal_value\nlast_deal_date\nlast_deal_year\ndeal_span_years\ntotal_funding_usd\ncity\ncountry\nregion\nsector\nlead_investors\nindustries\ndeals\nfunding_recency\ndeal_growth_cagr\n\n\n\n\n0\nSkyryse\n2500000.0\n2020-05-15\n200000000.0\n2021-10-27\n2021\n1.45\n240500000.0\nEl Segundo\nUnited States\nNorth America\naerospace\n—\nAerospace, Air Transportation, Internet, Trans...\n2\nolder\n1066.4\n\n\n1\nAeroVanti\n9750000.0\n2022-07-21\n100000000.0\n2022-10-19\n2022\n0.25\n109750000.0\nAnnapolis\nUnited States\nNorth America\naerospace\nNetwork 1 Financial Securities\nAerospace, Air Transportation, Transportation\n2\nrecent\n1066.4\n\n\n\n\n\n\n\n\n\nCode\ndef make_pretty(styler):\n    #styler.set_caption(caption)\n    #styler.format(recency)\n    styler.background_gradient(axis=None, cmap=\"Blues\")\n    return styler\n\ndf_world_aerospace_transportaion = (\n    df_world_first_last_deal_values_exploded[\n    (\n        (df_world_first_last_deal_values_exploded['industries'] == 'Transportation') |\n        (df_world_first_last_deal_values_exploded['industries'] == 'Air Transportation')\n    ) & \n    (\n        df_world_first_last_deal_values_exploded['deal_growth_cagr']&gt;0\n    )]\n    .sort_values(by=['last_deal_year', 'total_funding_usd', 'deals'], ascending=False)\n)\ndf_world_aerospace_transportaion['total_funding_usd_format'] = df_world_aerospace_transportaion['total_funding_usd'].apply(add_units)\n\n# &gt;10M funding\ndf_world_aerospace_transportaion = df_world_aerospace_transportaion[df_world_aerospace_transportaion['total_funding_usd'] &gt; 10000000] \ndf_world_aerospace_transportaion = df_world_aerospace_transportaion[['name', 'last_deal_year', 'total_funding_usd_format', 'country', 'region', 'deals', 'deal_growth_cagr']].rename(columns={\n    'name': 'Name',\n    'last_deal_year': 'Last Deal Year',\n    'total_funding_usd_format': 'Total Funding (USD)',\n    'country': 'Country',\n    'region': 'Region',\n    'deals': 'Number of Funding Deals',\n    'deal_growth_cagr': 'Deal CAGR'\n})\n\ndf_world_aerospace_transportaion.drop_duplicates().head(20).style.pipe(make_pretty)\n\n\n\n\n\n\n\n \nName\nLast Deal Year\nTotal Funding (USD)\nCountry\nRegion\nNumber of Funding Deals\nDeal CAGR\n\n\n\n\n\n\n\n\n\nPick: Elroy Air\n\n\nCode\n','.join(df_world_raw[df_world_raw['name']=='Elroy Air']['lead_investors'].str.replace('-', ''))\n\n\n'—,Lockheed Martin Ventures, Marlinspike Capital, Prosperity7 Ventures,—,Catapult Ventures'\n\n\n\n\nCode\ndf_world_aerospace_transportaion[df_world_aerospace_transportaion['Region']=='Europe'].drop_duplicates().head(20).style.pipe(make_pretty)\n\n\n\n\n\n\n\n \nName\nLast Deal Year\nTotal Funding (USD)\nCountry\nRegion\nNumber of Funding Deals\nDeal CAGR\n\n\n\n\n\n\n\n\n\nPick: Wingcopter\n\n\nCode\n','.join(df_world_raw[df_world_raw['name']=='Wingcopter']['lead_investors'].str.replace('-', ''))\n\n\n'—,Futury Capital, Xplorer Capital'"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Papers",
    "section": "",
    "text": "PDF papers (preprints, drafts, slide-like PDFs)\nMarkdown/QMD notes that accompany papers (methods, references, appendices)\n\n\n\n\n\n\n\n  \n    \n      markdown\n    \n    Paper A Revised 1971 Shock\n    \n  \n\n\n\n\n\n  \n    \n      pdf\n    \n    AURA Neuro–Symbolic Energy–Efficient Architecture Reusing LLM Infrastructure\n    PDF: AURA-Neuro–Symbolic-Energy–Efficient-Architecture-Reusing-LLM-Infrastructure.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Aerospace defense industry overview investment thesis\n    PDF: Aerospace-defense-industry-overview-investment-thesis.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Hydroverse   Opportunity Report\n    PDF: Hydroverse - Opportunity Report.html\n  \n\n\n\n\n\n  \n    \n      reference\n    \n    Existential Risk and Growth"
  },
  {
    "objectID": "papers/index.html#whats-here",
    "href": "papers/index.html#whats-here",
    "title": "Papers",
    "section": "",
    "text": "PDF papers (preprints, drafts, slide-like PDFs)\nMarkdown/QMD notes that accompany papers (methods, references, appendices)\n\n\n\n\n\n\n\n  \n    \n      markdown\n    \n    Paper A Revised 1971 Shock\n    \n  \n\n\n\n\n\n  \n    \n      pdf\n    \n    AURA Neuro–Symbolic Energy–Efficient Architecture Reusing LLM Infrastructure\n    PDF: AURA-Neuro–Symbolic-Energy–Efficient-Architecture-Reusing-LLM-Infrastructure.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Aerospace defense industry overview investment thesis\n    PDF: Aerospace-defense-industry-overview-investment-thesis.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Hydroverse   Opportunity Report\n    PDF: Hydroverse - Opportunity Report.html\n  \n\n\n\n\n\n  \n    \n      reference\n    \n    Existential Risk and Growth"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data engineer/scientist and quantitative researcher with 18 years building systems that extract signal from noise.\nMy work spans system engineering, ML, data engineering, portfolio analytics, and economic research—always anchored in first-principles thinking and empirical validation. I don’t chase narratives that don’t survive contact with data."
  },
  {
    "objectID": "about.html#current-work",
    "href": "about.html#current-work",
    "title": "About",
    "section": "Current Work",
    "text": "Current Work\nData Engineering & Analytics\nLeading data platform modernization at Macquarie Group: DBT/Iceberg/Redshift architectures, data governance through DBT & Collier, and observable data pipelines.\nInvestment Research\nDeveloping quantitative frameworks for portfolio construction and long-term portfolio strategy.\nSystems Thinking\nBuilding graph-based intelligence platforms for venture capital analysis and exploring applications of ML/AI to financial domains."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About",
    "section": "Background",
    "text": "Background\nTechnical: Data engineering at scale • DBT, Spark, Redshift, Iceberg • ML/AI (graph networks, probabilistic models) • LLMs & Beyond (JEPA) • Modern development (Python, R, AWS) • Blockchain/decentralized systems\nAnalytical: Quantitative finance • Portfolio analytics • Economic time-series analysis • CFA Level 1 • Data governance (BCBS 239, CPG 235)\nDomain Experience: Financial services • Media/publishing • Healthcare • Renewables/energy • Gaming"
  },
  {
    "objectID": "about.html#previous-roles",
    "href": "about.html#previous-roles",
    "title": "About",
    "section": "Previous Roles",
    "text": "Previous Roles\n\nSenior Manager/Lead Engineer at Boson Research Advisory (2023-2025)\nProduct Manager at Hydroverse •\nVarious roles at Sky News, Fairfax Media, Zynga, Oracle, and startups across Sydney, San Francisco, and Bangalore."
  },
  {
    "objectID": "about.html#published-work",
    "href": "about.html#published-work",
    "title": "About",
    "section": "Published Work",
    "text": "Published Work\nCrusade Against Kidney Disease (2021)\nResearch synthesis on slowing CKD progression through evidence-based interventions. Written after watching family members navigate clinical treatment failures. Not-for-profit."
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About",
    "section": "Interests",
    "text": "Interests\nOutside of work: evolutionary economics, self-sustainability, genetic-based health optimization, and long-term systems thinking."
  },
  {
    "objectID": "about.html#site-philosophy",
    "href": "about.html#site-philosophy",
    "title": "About",
    "section": "Site Philosophy",
    "text": "Site Philosophy\nThis site archives my research, notebooks, and projects. It’s organized for reference and discovery, not chronology. Everything here prioritizes:\n\nQuantitative reasoning where possible\nFirst-principles thinking where necessary\n\nSkepticism toward narratives unsupported by data\n\nIf you’re looking for polished marketing material, this isn’t it. If you’re looking for working analysis with code and assumptions made explicit, you’re in the right place.\n\nConnect\nGitHub • LinkedIn\nLocation: Sydney, Australia"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Articles",
    "section": "",
    "text": "Browse my articles and working papers below.\n\n\n\n\n\n\n\n\n\nPageRank as a Popularity Contest: Structural Vulnerabilities, Collusion Dynamics, and a Byzantine Fault-Tolerant Alternative\n\n\nA formal critique of PageRank as an inter-subjective popularity contest incapable of surfacing marginal truth, with a proof of collusion-susceptibility for O(log N) coalitions, analysis of the institutional epistemic loop, and a proposed BFT TruthChain architecture combining independence-weighted citation scoring, Brier-anchored content quality functions, and adversarial audit governance as a collusion-resistant retrieval alternative.\n\n\n\n\n\nFeb 22, 2026\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deb Bose",
    "section": "",
    "text": "This site is a working archive — not a feed.\nEverything here is oriented toward truth-seeking through analysis:\nquantitative reasoning where possible, first-principles thinking where necessary, and skepticism toward narratives that don’t survive contact with data."
  },
  {
    "objectID": "index.html#what-youll-find-here",
    "href": "index.html#what-youll-find-here",
    "title": "Deb Bose",
    "section": "What you’ll find here",
    "text": "What you’ll find here\n\nArticles\nEssays, market notes, and technical deep dives — written to clarify, not to perform.\n\n\nNotebooks\nRunnable research with code, charts, and assumptions made explicit.\nExploratory by design, not polished marketing artefacts.\n\n\nPapers\nDrafts, PDFs, and reference material — including work that is incomplete, evolving, or deliberately unresolved.\n\n\nProjects\nPointers to my GitHub work: tools, models, and systems built to answer specific questions."
  },
  {
    "objectID": "index.html#latest-writing",
    "href": "index.html#latest-writing",
    "title": "Deb Bose",
    "section": "Latest writing",
    "text": "Latest writing\n\n→ Go to Papers\n→ Go to Notebooks"
  },
  {
    "objectID": "posts/pagerank_critique.html",
    "href": "posts/pagerank_critique.html",
    "title": "PageRank as a Popularity Contest: Structural Vulnerabilities, Collusion Dynamics, and a Byzantine Fault-Tolerant Alternative",
    "section": "",
    "text": "v3.0"
  },
  {
    "objectID": "posts/pagerank_critique.html#the-pagerank-formalism-elegance-and-hidden-assumptions",
    "href": "posts/pagerank_critique.html#the-pagerank-formalism-elegance-and-hidden-assumptions",
    "title": "PageRank as a Popularity Contest: Structural Vulnerabilities, Collusion Dynamics, and a Byzantine Fault-Tolerant Alternative",
    "section": "1 1. The PageRank Formalism: Elegance and Hidden Assumptions",
    "text": "1 1. The PageRank Formalism: Elegance and Hidden Assumptions\n\n1.1 1.1 The Stationary Distribution Interpretation\nThe classic PageRank score for a page \\(p\\) is defined as:\n\\[PR(p) = \\frac{1-d}{N} + d \\sum_{i \\in \\mathcal{B}(p)} \\frac{PR(i)}{L(i)}\\]\nwhere \\(d \\approx 0.85\\) is the damping factor, \\(N\\) is the total number of pages, \\(\\mathcal{B}(p)\\) is the set of pages linking to \\(p\\), and \\(L(i)\\) is the out-degree of page \\(i\\). This is equivalent to the stationary distribution \\(\\boldsymbol{\\pi}\\) of a modified random-walk Markov chain with transition matrix:\n\\[\\mathbf{M} = d\\mathbf{A}^T \\mathbf{D}^{-1} + \\frac{(1-d)}{N}\\mathbf{1}\\mathbf{1}^T\\]\nwhere \\(\\mathbf{A}\\) is the adjacency matrix of the web graph and \\(\\mathbf{D} = \\text{diag}(L(1), \\ldots, L(N))\\). By the Perron–Frobenius theorem, since \\(\\mathbf{M}\\) is a positive stochastic matrix, a unique stationary distribution \\(\\boldsymbol{\\pi}\\) satisfying \\(\\mathbf{M}\\boldsymbol{\\pi} = \\boldsymbol{\\pi}\\) exists. Power iteration converges geometrically at rate \\(|\\lambda_2/\\lambda_1|\\) where \\(\\lambda_1 = 1\\) and \\(|\\lambda_2| \\leq d\\).\nThe beautiful mathematics conceals a profound epistemological assumption: that the long-run visit frequency of a random web surfer is a proxy for informational quality. This is the original sin of PageRank — popularity is conflated with truth.\n\n\n1.2 1.2 What PageRank Actually Measures\nLet \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) be the web graph. The PageRank vector \\(\\boldsymbol{\\pi}\\) measures the eigenvector centrality of nodes in a teleporting random walk. It is a property of the graph topology alone — not of the content at each node. Formally:\n\\[\\boldsymbol{\\pi} = \\lim_{k \\to \\infty} \\mathbf{M}^k \\boldsymbol{\\pi}_0, \\quad \\forall\\, \\boldsymbol{\\pi}_0 \\in \\Delta^{N-1}\\]\nThe content of page \\(p\\) enters the ranking only indirectly, through the linking decisions of other human agents whose motivations are social, commercial, institutional, and political — not epistemically calibrated. PageRank therefore measures inter-subjective consensus within the link graph, not objective informational quality."
  },
  {
    "objectID": "posts/pagerank_critique.html#the-collusion-problem-a-mathematical-proof",
    "href": "posts/pagerank_critique.html#the-collusion-problem-a-mathematical-proof",
    "title": "PageRank as a Popularity Contest: Structural Vulnerabilities, Collusion Dynamics, and a Byzantine Fault-Tolerant Alternative",
    "section": "2 2. The Collusion Problem: A Mathematical Proof",
    "text": "2 2. The Collusion Problem: A Mathematical Proof\n\n2.1 2.1 Authority Injection via Coalition\nProposition. Let \\(S = \\{v_1, \\ldots, v_k\\}\\) be a coalition of pages with PageRank scores \\(\\{PR(v_j)\\}\\) and let \\(t\\) be a target page with \\(PR(t) \\approx 0\\). If every \\(v_j \\in S\\) adds an outgoing link to \\(t\\), the new PageRank of \\(t\\) satisfies:\n\\[PR'(t) \\geq \\frac{d}{N} \\sum_{j=1}^{k} \\frac{PR(v_j)}{L(v_j) + 1}\\]\nProof sketch. After link injection, the transition matrix \\(\\mathbf{M}\\) gains non-zero entries \\(M_{t, v_j} = d / (L(v_j)+1)\\) for each \\(j\\). In the updated stationary distribution, the contribution to \\(\\pi_t\\) from \\(S\\) alone is \\(d \\sum_{j} \\pi_{v_j}/(L(v_j)+1)\\). Adding the teleportation floor \\((1-d)/N &gt; 0\\) yields the lower bound. \\(\\square\\)\nCorollary. If \\(S\\) consists of high-authority domains with \\(PR(v_j) \\gg 1/N\\), even a small coalition \\(|S| = O(\\log N)\\) can elevate \\(PR'(t)\\) to the top decile of the distribution, regardless of the informational content of \\(t\\).\n\n\n2.2 2.2 The De Facto Institutional Authority Premium\nA common misconception in the SEO industry is that Google applies an explicit TLD-level bonus to .gov and .edu domains. Google’s John Mueller has stated directly that TLD does not factor into ranking preference, and that many .edu links are actively discounted due to spam. This claim, as typically stated, is incorrect and we retract the hardcoded \\(\\alpha_{\\text{tld}}\\) formulation from earlier versions of this paper.\nHowever, the weaker and more defensible version of the argument is fully sufficient. Government and major institutional domains accumulate inbound link density organically and at scale: the FDA (DA 86), NASA (DA 93), and California state government (DA 97) attract high-PageRank inbound links continuously without active link-building, simply because they are the authoritative primary sources for their subject matter. Formally, let \\(\\bar{D}(v)\\) denote the mean inbound link PageRank for domain \\(v\\). Then for institutional domains \\(v_{\\text{inst}}\\):\n\\[\\mathbb{E}[\\bar{D}(v_{\\text{inst}})] \\gg \\mathbb{E}[\\bar{D}(v_{\\text{com}})]\\]\nThis produces a structurally equivalent outcome to a hardcoded TLD premium: claims published by institutional domains, and claims endorsed by IFCN-affiliated .org fact-checkers linking to those domains, sit at the eigenvector centre of the web graph by construction of how the web was built, not by algorithmic fiat. The closed epistemic loop operates identically in either case — it is just that the lock-in is sociological rather than algorithmic.\n\n\n2.3 2.3 The Eigenvector of Power, Not Truth\nDefine the truth score of page \\(p\\) as \\(\\tau(p) \\in [0,1]\\), representing factual accuracy, and let \\(\\boldsymbol{\\tau} = (\\tau(p))_{p \\in \\mathcal{V}} \\in [0,1]^N\\) be the truth vector over all \\(N\\) pages. The stationary PageRank distribution \\(\\boldsymbol{\\pi} = (\\pi(p))_{p \\in \\mathcal{V}} \\in \\Delta^{N-1}\\) is similarly a vector over pages. We define the cross-sectional correlation between rank and truth across the full page population as:\n\\[\\text{Corr}(\\boldsymbol{\\pi}, \\boldsymbol{\\tau}) = \\frac{\\displaystyle\\sum_{p \\in \\mathcal{V}} \\bigl(\\pi(p) - \\bar{\\pi}\\bigr)\\bigl(\\tau(p) - \\bar{\\tau}\\bigr)}{\\sqrt{\\displaystyle\\sum_{p \\in \\mathcal{V}}\\bigl(\\pi(p)-\\bar{\\pi}\\bigr)^2} \\cdot \\sqrt{\\displaystyle\\sum_{p \\in \\mathcal{V}}\\bigl(\\tau(p)-\\bar{\\tau}\\bigr)^2}}\\]\nwhere \\(\\bar{\\pi} = \\frac{1}{N}\\sum_{p} \\pi(p) = \\frac{1}{N}\\) and \\(\\bar{\\tau} = \\frac{1}{N}\\sum_{p} \\tau(p)\\).\nThere is no mechanism in the PageRank formalism that forces \\(\\text{Corr}(\\boldsymbol{\\pi}, \\boldsymbol{\\tau}) &gt; 0\\). The transition matrix \\(\\mathbf{M}\\) is a function of the adjacency matrix \\(\\mathbf{A}\\) alone, and \\(\\mathbf{A}\\) is determined entirely by the linking behaviour of web authors — shaped by social proof, financial incentives, institutional mandates, and network effects — none of which are calibrated to \\(\\boldsymbol{\\tau}\\). Since \\(\\boldsymbol{\\pi}\\) is the leading eigenvector of \\(\\mathbf{M}\\), and \\(\\mathbf{M}\\) contains no information about \\(\\boldsymbol{\\tau}\\), the cross-sectional correlation is structurally unconstrained. In adversarial environments where high-PR actors systematically misrepresent \\(\\tau\\), it can become negative."
  },
  {
    "objectID": "posts/pagerank_critique.html#the-closed-epistemic-loop",
    "href": "posts/pagerank_critique.html#the-closed-epistemic-loop",
    "title": "PageRank as a Popularity Contest: Structural Vulnerabilities, Collusion Dynamics, and a Byzantine Fault-Tolerant Alternative",
    "section": "3 3. The Closed Epistemic Loop",
    "text": "3 3. The Closed Epistemic Loop\n\n3.1 3.1 Institutional Gatekeeping Architecture\nThe modern web truth-ranking stack operates as a self-reinforcing three-layer system. At layer 1, primary nodes are government agencies and institutional bodies that accumulate high PageRank through natural link density (see Section 2.2). At layer 2, IFCN-affiliated fact-checkers — organisations whose funding relationships include the Gates Foundation, Open Society Foundations, and others — link to and amplify layer 1 claims, further concentrating link authority. At layer 3, platform policies on YouTube and Facebook enforce “community guidelines” that suppress pages contradicting layer 1 and 2 content, reducing their effective out-degree in the crawlable graph. The result is a closed loop: the link graph structurally encodes a specific epistemic orthodoxy. Any heterodox page faces a near-insurmountable eigenvector disadvantage — not because Google engineered this outcome, but because the sociology of institutional link-building produced it.\n\n\n3.2 3.2 The SEO Industry as Revealed Preference\nThe existence of a multi-billion dollar SEO and link-building industry is the most compelling empirical proof that PageRank does not measure truth. If PageRank were correlated with content quality, optimising it would require improving content. Instead, the dominant SEO strategy is topology manipulation. The market price of a backlink from a DA-90+ domain (circa 2024: $500–$5,000) represents the market’s estimate of \\(\\partial PR(t)/\\partial \\text{link}_{v \\to t}\\) — the marginal authority injection value of a single edge in the graph. Truth has no such price signal."
  },
  {
    "objectID": "posts/pagerank_critique.html#llms-as-retrieval-systems-promise-and-inherited-bias",
    "href": "posts/pagerank_critique.html#llms-as-retrieval-systems-promise-and-inherited-bias",
    "title": "PageRank as a Popularity Contest: Structural Vulnerabilities, Collusion Dynamics, and a Byzantine Fault-Tolerant Alternative",
    "section": "4 4. LLMs as Retrieval Systems: Promise and Inherited Bias",
    "text": "4 4. LLMs as Retrieval Systems: Promise and Inherited Bias\n\n4.1 4.1 The Parametric Knowledge Advantage\nA large language model with parameters \\(\\boldsymbol{\\theta} \\in \\mathbb{R}^d\\) (e.g., \\(d \\sim 7 \\times 10^{11}\\) for GPT-4 class models) encodes a compressed, lossy representation of a training corpus \\(\\mathcal{D}\\):\n\\[\\boldsymbol{\\theta}^* = \\arg\\min_{\\boldsymbol{\\theta}} \\sum_{x \\in \\mathcal{D}} \\mathcal{L}(x; \\boldsymbol{\\theta})\\]\nwhere \\(\\mathcal{L}\\) is the next-token prediction loss. Critically, the training objective encourages internal consistency across documents. A factual contradiction present in \\(\\mathcal{D}\\) creates a higher loss signal than a consistent claim, exerting gradient pressure toward coherent representations — a mechanism entirely absent from PageRank.\n\n\n4.2 4.2 Retrieval-Augmented Generation as Graph-Free Ranking\nIn RAG architectures, a query \\(q\\) retrieves a context set \\(\\mathcal{C}(q) \\subset \\mathcal{D}\\) via dense retrieval:\n\\[\\mathcal{C}(q) = \\text{TopK}_k\\left\\{ d \\in \\mathcal{D} : \\text{sim}(\\mathbf{e}_q, \\mathbf{e}_d) \\right\\}, \\quad \\mathbf{e}_x = f_{\\boldsymbol{\\phi}}(x) \\in \\mathbb{R}^m\\]\nThe retrieval score is a function of semantic content, not link topology. A scientific preprint with zero inbound links but high semantic relevance outscores a heavily linked but off-topic domain. The LLM’s generative step then performs implicit multi-document reasoning, synthesising retrieved context to identify contradictions and qualify uncertainty.\n\n\n4.3 4.3 The Corpus Contamination Problem\nHowever, RAG pipelines are not epistemically clean by default. Training corpora (Common Crawl, C4, The Pile) are dominated by high-PageRank content — because web crawlers weight coverage by link density. The probability that a document \\(d\\) appears in the training corpus is approximately:\n\\[P(d \\in \\mathcal{D}) \\propto PR(d)^\\eta, \\quad \\eta &gt; 0\\]\nThis means the LLM’s parametric knowledge inherits PageRank’s inter-subjective bias through a different channel. The bias is laundered — it no longer looks like a link graph, but the underlying causal structure is the same. This corpus contamination problem motivates the architecture in Section 6."
  },
  {
    "objectID": "posts/pagerank_critique.html#the-economic-disruption-llms-vs.-google-serp",
    "href": "posts/pagerank_critique.html#the-economic-disruption-llms-vs.-google-serp",
    "title": "PageRank as a Popularity Contest: Structural Vulnerabilities, Collusion Dynamics, and a Byzantine Fault-Tolerant Alternative",
    "section": "5 5. The Economic Disruption: LLMs vs. Google SERP",
    "text": "5 5. The Economic Disruption: LLMs vs. Google SERP\n\n5.1 5.1 Click Erosion by Query Type\nThe structural impact of LLMs and AI Overviews on SERP engagement is now empirically well-documented, with effects that vary sharply by query intent. Table 1 synthesises the key data points across sources.\n\nTable 1: CTR and Zero-Click Rates by Query Type and Era (US Desktop + Mobile)\n\n\n\n\n\n\n\n\n\n\n\n\nQuery Type\nZero-Click Rate 2022\nZero-Click Rate 2024\nZero-Click Rate 2025\nPos-1 CTR (pre-AIO)\nPos-1 CTR (with AIO)\nSource\n\n\n\n\nInformational\n~45%\n~65%\n~83%\n7.6%\n1.6%\nAhrefs 2025; BrightEdge 2023\n\n\nNavigational\n~20%\n~35%\n~40%\n15–20%\n8–10%\nPew Research 2025; SparkToro 2024\n\n\nCommercial\n~15%\n~25%\n~32%\n10–14%\n4–6%\nSeer Interactive 2025 (25.1M impressions)\n\n\nTransactional\n~10%\n~18%\n~22%\n18–22%\n8–12%\nSemrush AIO Study, Oct 2025\n\n\nAll queries (blended)\n~26%\n~58.5%\n~65%\n—\n—\nSparkToro/Datos 2024; Onely 2025\n\n\n\nNotes: AIO = AI Overviews. Pew Research (March 2025, n = 68,879 real queries) found CTR of 8% with AI summaries vs. 15% without — a 47% relative reduction. Seer Interactive (42 organisations, 25.1M impressions) found paid CTR dropped 68% (19.7% → 6.34%) when AIO appeared. Position-1 CTR collapse for informational queries (7.6% → 1.6%) from Ahrefs comparing December 2023 vs. December 2025. Zero-click rate for queries triggering AI Overviews specifically reached 83% (WordStream/BrightEdge 2024).\n\nThe table reveals the asymmetry clearly: informational queries — precisely those where epistemically contested content lives — have been most devastated. Transactional queries, which drive Google’s advertising revenue, have been more resilient but are now also under AIO pressure as Semrush data shows commercial intent queries growing from 8.7% to 42.9% of AIO appearances between January and October 2025.\n\n\n5.2 5.2 A Simple Model of Click Displacement\nLet \\(\\beta_t \\in [0,1]\\) be the LLM/AIO query capture rate at time \\(t\\). Google’s effective paid-click inventory satisfies:\n\\[Q_t = Q_0 \\cdot (1 - \\beta_t) \\cdot \\text{CTR}_{\\text{paid}}(\\mathbf{k})\\]\nWith logistic adoption dynamics:\n\\[\\beta_t = \\frac{\\beta_{\\max}}{1 + e^{-r(t - t_0)}}\\]\nSetting \\(\\beta_{\\max} = 0.45\\), \\(r = 0.8\\ \\text{yr}^{-1}\\), \\(t_0 = 2024.5\\), and anchoring to the observed paid CTR collapse of 68% for AIO-present queries (Seer Interactive), the model implies \\(Q_t\\) declining ~35% from 2024 to 2027 for informational and research query categories. The self-cannibalisation paradox is worth noting: Google’s AI Overviews are simultaneously defending search market share and destroying the paid-click monetisation model that finances the entire infrastructure."
  },
  {
    "objectID": "posts/pagerank_critique.html#towards-a-bft-truthchain-a-collusion-resistant-architecture",
    "href": "posts/pagerank_critique.html#towards-a-bft-truthchain-a-collusion-resistant-architecture",
    "title": "PageRank as a Popularity Contest: Structural Vulnerabilities, Collusion Dynamics, and a Byzantine Fault-Tolerant Alternative",
    "section": "6 6. Towards a BFT TruthChain: A Collusion-Resistant Architecture",
    "text": "6 6. Towards a BFT TruthChain: A Collusion-Resistant Architecture\n\n6.1 6.1 The Byzantine Analogy\nPageRank has no Byzantine Fault Tolerance. Lamport, Pease and Shostak (1982) showed that a distributed system of \\(n\\) validators can tolerate up to \\(f &lt; n/3\\) Byzantine (malicious or faulty) nodes while preserving consensus integrity. The web’s truth consensus offers no equivalent guarantee — a coordinated minority of high-PR nodes can corrupt the global ranking indefinitely, with no slashing mechanism and no quorum requirement.\nWe propose a three-layer architecture — the BFT TruthChain — that addresses this structural gap.\n\n\n6.2 6.2 Layer 1 — Independence-Weighted Citation Graph\nThe first departure from PageRank is to weight citations by the independence of the citing agent rather than by their PageRank. Two validators are not independent if they share funding, institutional affiliation, or editorial board membership. Define the funder/affiliate graph \\(\\mathcal{F} = (\\mathcal{V}, \\mathcal{E}_F)\\) where an edge \\((u, v) \\in \\mathcal{E}_F\\) exists if entities \\(u\\) and \\(v\\) share a funding source or institutional parent. Apply community detection (e.g., Louvain algorithm) to \\(\\mathcal{F}\\) to partition validators into clusters \\(\\{C_1, \\ldots, C_m\\}\\).\nEach validator \\(v \\in \\mathcal{W}(X)\\) carries a weight \\(w_v \\in (0,1]\\) representing its accumulated epistemic reputation — initialised from a domain authority prior (e.g., normalised Moz DA score) and updated over time by the reputation staking mechanism in Section 6.5, so that validators with a history of attesting falsified claims carry lower \\(w_v\\). The independence score of a claim \\(X\\) endorsed by validator set \\(\\mathcal{W}(X)\\) is then:\n\\[\\text{Independence}(X) = 1 - \\frac{\\max_j \\sum_{v \\in \\mathcal{W}(X) \\cap C_j} w_v}{\\sum_{v \\in \\mathcal{W}(X)} w_v}\\]\nThe numerator captures the total weight controlled by the single most dominant cluster \\(C_j\\); the denominator is the total endorsement weight. This ratio equals 1 when a single cluster owns all endorsement weight (minimum independence) and 0 when weight is spread uniformly across clusters (maximum independence). The score therefore equals 1 when endorsers are maximally diverse and approaches 0 when a single institutional cluster dominates — directly penalising the IFCN-style coordinated endorsement pattern. The authority injection lower bound from Section 2.1 is replaced by:\n\\[\\text{TrustScore}_{\\text{layer1}}(X) \\geq \\text{Independence}(X) \\cdot \\sum_{j=1}^{k} \\frac{w_j}{L(v_j) + 1}\\]\nwhich approaches zero as the endorsing coalition becomes a single cluster, even if individual validator weights \\(w_j\\) are high.\n\n\n6.3 6.3 Layer 2 — Topology-Independent Content Quality Scoring\nThe second layer scores content, independent of who links to it. We define a composite quality function:\n\\[Q(p) = \\alpha_1 \\cdot V(p) + \\alpha_2 \\cdot \\Gamma(p) + \\alpha_3 \\cdot \\Pi(p) + \\alpha_4 \\cdot R(p)\\]\nVerifiability \\(V(p) \\in [0,1]\\): the fraction of factual claims in \\(p\\) cross-referenceable against structured external databases (PubMed, Cochrane Reviews, FRED, pre-registration registries):\n\\[V(p) = \\frac{|\\{k_i \\in \\mathcal{K}(p) : \\exists\\, e \\in \\mathcal{E}_{\\text{ext}},\\ \\text{entails}(e, k_i)\\}|}{|\\mathcal{K}(p)|}\\]\nLogical Consistency \\(\\Gamma(p) \\in [0,1]\\): absence of internal contradiction, via NLI:\n\\[\\Gamma(p) = 1 - \\frac{|\\{(k_i, k_j) \\in \\mathcal{K}(p)^2 : \\text{contradicts}(k_i, k_j)\\}|}{|\\mathcal{K}(p)|^2}\\]\nPredictive Accuracy (Brier Score) \\(\\Pi(p) \\in [0,1]\\): for documents making falsifiable predictions, the historical calibration score:\n\\[\\Pi(p) = 1 - \\frac{1}{|\\mathcal{P}(p)|} \\sum_{i=1}^{|\\mathcal{P}(p)|} (f_i - o_i)^2\\]\nwhere \\(f_i\\) is the stated probability of prediction \\(i\\) and \\(o_i \\in \\{0,1\\}\\) is the observed outcome. This is the Brier score for content — skin-in-the-game quality measurement.\nReproducibility \\(R(p) \\in [0,1]\\): a weighted score for data/code availability, pre-registration, and independent replication.\nCritically, none of these components are topological and thus none can be gamed by acquiring backlinks.\n\n\n6.4 6.4 Layer 3 — Epistemic Diversity Preservation\nAny ranking system that converges too quickly on consensus kills the marginal truth it is designed to surface. We introduce an explicit epistemic diversity bonus:\n\\[\\text{FinalScore}(p) = Q(p) \\cdot \\left(1 + \\lambda \\cdot \\text{Novelty}(p)\\right)\\]\nwhere \\(\\text{Novelty}(p)\\) measures semantic distance from the current consensus cluster:\n\\[\\text{Novelty}(p) = \\frac{1}{K} \\sum_{k=1}^{K} \\left(1 - \\text{sim}\\left(\\mathbf{e}_p,\\ \\mathbf{e}_{p_k^*}\\right)\\right)\\]\nwith \\(\\{p_1^*, \\ldots, p_K^*\\}\\) being the current top-\\(K\\) ranked documents. A high-quality heterodox document receives a bonus \\(\\lambda \\cdot \\text{Novelty}(p) &gt; 0\\), reversing PageRank’s structural penalty against dissenting views.\n\n\n6.5 6.5 The Reputation Staking Mechanism\nThe BFT analogy is completed by a Proof of Epistemic Work consensus layer. Validators stake reputation tokens \\(\\rho_v\\) to attest claims, with slashing conditions on falsification:\n\\[\\rho_v^{(t+1)} = \\begin{cases} \\rho_v^{(t)} \\cdot (1 + r_{\\text{reward}}) & \\text{if } \\tau(X) \\geq \\tau_{\\min} \\text{ verified at } t+\\Delta \\\\ \\rho_v^{(t)} \\cdot (1 - r_{\\text{slash}}) & \\text{if } \\tau(X) &lt; \\tau_{\\min} \\text{ verified at } t+\\Delta \\end{cases}\\]\nThis creates an economic incentive aligned with truth. Unlike PageRank — where the cost of a link is purely the opportunity cost of outbound link equity — staking imposes a direct cost on false endorsement. The Byzantine fault tolerance guarantee follows: with \\(n\\) validators and at most \\(f\\) coordinated Byzantine validators, TrustScore converges to the correct value provided \\(f &lt; n/3\\).\n\n\n6.6 6.6 Goodhart’s Law: Why the Brier Score is the Adversarial Anchor\nAny quality signal that becomes a public ranking target will be gamed — Goodhart’s Law applied to epistemology. If NLI consistency scoring \\(\\Gamma(p)\\) is known to be a ranking factor, adversaries will optimise documents to pass NLI checks while remaining misleading. If verifiability \\(V(p)\\) is the target, adversaries will write around the structured databases. These concerns are real and motivate a partially hidden ensemble.\nBut the Brier score component \\(\\Pi(p)\\) is categorically different and is the architecture’s adversarial anchor. To game a Brier score, an author must assign well-calibrated probabilities to future observable events and then those events must actually occur as predicted. There is no syntactic or topological manipulation path to a good Brier score — you cannot write your way to calibrated predictions. You can only earn them by being epistemically accurate over time. This is the property that makes \\(\\Pi(p)\\) robust where \\(\\Gamma(p)\\) and \\(V(p)\\) are not: it demands demonstrated predictive competence across a temporal horizon, not just internal consistency at a single point in time.\nThe practical implication is weighting: in environments under active adversarial pressure, \\(\\alpha_3\\) (the Brier weight) should be increased relative to \\(\\alpha_1\\) and \\(\\alpha_2\\), trading off coverage (fewer documents have sufficient prediction history) against manipulation resistance. This tradeoff is explicit, tunable, and — crucially — does not exist in PageRank at all.\nThe remaining vulnerability is the bootstrapping of \\(\\Pi(p)\\) for recent documents. The provisional tier addresses this: recent claims are scored on \\(V\\), \\(\\Gamma\\), \\(R\\) only, with uncertainty bounds displayed to the user and a flag indicating that Brier calibration is pending. Provisional scores are explicitly marked as awaiting temporal validation — the epistemically correct posture.\n\n\n6.7 6.7 The Bootstrapping Problem and Adversarial Audit Governance\nThe seed corpus for \\(\\mathcal{E}_{\\text{ext}}\\) must be chosen without circularity. We propose seeding exclusively from pre-registered clinical and scientific trials, independently replicated empirical results, and primary government statistical databases stripped of interpretive text. These have objective falsifiability criteria that do not depend on institutional authority — a trial either met its pre-specified endpoints or it did not.\nThis does not fully escape the regress: someone must maintain the pre-registration registries and decide which replication standards qualify. Rather than accepting this as an unavoidable axiom, we propose a concrete governance structure modelled on adversarial auditing with rotating challenger committees. The seed corpus is maintained by an independent foundation (structured analogously to the Internet Archive or ICANN, with multi-stakeholder governance and no single controlling funder) whose registry decisions are subject to annual adversarial challenge: any party may petition to add or remove a source, and petitions are adjudicated by a rotating panel of domain experts drawn from a pool that explicitly excludes current funders of registered sources. Approved challenges trigger re-scoring of all documents whose \\(\\mathcal{E}_{\\text{ext}}\\) citations trace to the contested source, propagating corrections forward automatically. Critically, all registry decisions, challenge histories, and adjudicator conflict-of-interest disclosures are published in an append-only public ledger — making the axiom not just minimal and explicit, but auditable and contestable over time. This converts the bootstrapping problem from an epistemological dead-end into an ongoing adversarial process whose integrity is maintained by the same skin-in-the-game mechanism as the Brier score: participants who systematically advocate for low-quality sources lose credibility in future adjudications."
  },
  {
    "objectID": "posts/pagerank_critique.html#discussion-implementation-pathway-and-institutional-resistance",
    "href": "posts/pagerank_critique.html#discussion-implementation-pathway-and-institutional-resistance",
    "title": "PageRank as a Popularity Contest: Structural Vulnerabilities, Collusion Dynamics, and a Byzantine Fault-Tolerant Alternative",
    "section": "7 7. Discussion: Implementation Pathway and Institutional Resistance",
    "text": "7 7. Discussion: Implementation Pathway and Institutional Resistance\n\n7.1 7.1 A Tractable Near-Term Version\nThe full BFT TruthChain is a long-term research programme. A tractable near-term implementation targets scientific and empirical claims specifically: Semantic Scholar + PubMed + pre-registered trials + Cochrane Reviews → embedded with \\(Q(p)\\) scores → RAG retrieval weighted by \\(Q(p)\\) rather than \\(PR(p)\\). This is buildable today using existing NLP infrastructure and represents a parallel index rather than a replacement for general web search.\n\n\n7.2 7.2 Why Incumbents Will Not Build This\nThe PageRank-plus-institutional-amplifier architecture is not merely a technical flaw — it reflects a stable alignment between commercial interests (Google’s advertiser relationships depend on the current authority hierarchy), institutional interests (governments and NGOs seek information control), and platform incentives (liability reduction through moderation). This alignment is self-reinforcing. The COVID-19 pandemic illustrated the failure mode vividly: pages contradicting WHO guidance were de-ranked irrespective of evidential basis, because the institutional link topology overwhelmingly endorsed the consensus position and the BFT guarantee was violated — the fraction of coordinated institutional validators vastly exceeded \\(n/3\\). No incumbent with a profitable stake in the current hierarchy will build a system designed to surface heterodox high-quality claims.\nA partial counterpoint exists: the EU Digital Markets Act (DMA), enforced from March 2024, has demonstrably shifted some SERP composition by requiring Google to present third-party comparison services and reduce self-preferencing of Google-owned properties. SparkToro’s 2024 data shows the EU open-web click share (374 per 1,000 searches) marginally exceeds the US figure (360 per 1,000), a gap attributed by analysts specifically to DMA-mandated diversification. This is encouraging evidence that external structural pressure can move the topology — but the DMA addresses commercial self-preferencing, not epistemic self-preferencing. It compels Google to show a rival price-comparison site; it says nothing about the ranking of heterodox scientific claims. Regulatory intervention is a necessary but not sufficient condition. The BFT TruthChain must emerge from outside: academic infrastructure, open-source communities, or adversarial startups with no legacy ad inventory to protect."
  },
  {
    "objectID": "posts/pagerank_critique.html#conclusion",
    "href": "posts/pagerank_critique.html#conclusion",
    "title": "PageRank as a Popularity Contest: Structural Vulnerabilities, Collusion Dynamics, and a Byzantine Fault-Tolerant Alternative",
    "section": "8 8. Conclusion",
    "text": "8 8. Conclusion\nWe have demonstrated that PageRank is mathematically equivalent to eigenvector centrality in a human-constructed link graph with no forcing mechanism toward truth, proven collusion-susceptibility for \\(O(\\log N)\\) coalitions, corrected the overstated .gov TLD claim to its accurate and sufficient form (structural link-density advantage rather than algorithmic preference), and shown that RAG pipelines inherit the same bias through corpus contamination. The proposed BFT TruthChain addresses these failures across three layers: independence-weighted citation scoring that penalises institutional clustering; topology-independent content quality scoring with the Brier calibration score as the manipulation-resistant temporal anchor; and an epistemic diversity bonus reversing PageRank’s structural penalty against heterodox claims. The empirical disruption is already underway — Table 1 documents position-1 CTR collapse from 7.6% to 1.6% for informational queries under AI Overviews, zero-click rates reaching 65% by mid-2025, and paid CTR dropping 68% in AIO-present SERPs. The question is no longer whether the link-graph era ends, but whether the architecture that replaces it will be epistemically honest."
  },
  {
    "objectID": "posts/pagerank_critique.html#references",
    "href": "posts/pagerank_critique.html#references",
    "title": "PageRank as a Popularity Contest: Structural Vulnerabilities, Collusion Dynamics, and a Byzantine Fault-Tolerant Alternative",
    "section": "9 References",
    "text": "9 References\n\nPage, L., Brin, S., Motwani, R., & Winograd, T. (1999). The PageRank Citation Ranking: Bringing Order to the Web. Stanford InfoLab Technical Report.\nLangville, A. N., & Meyer, C. D. (2006). Google’s PageRank and Beyond: The Science of Search Engine Rankings. Princeton University Press.\nPerron, O. (1907). Zur Theorie der Matrices. Mathematische Annalen, 64(2), 248–263.\nLamport, L., Pease, M., & Shostak, R. (1982). Byzantine Generals Problem. ACM Transactions on Programming Languages and Systems, 4(3), 382–401.\nBlondel, V. D., et al. (2008). Fast unfolding of communities in large networks. Journal of Statistical Mechanics, 2008(10), P10008.\nBrier, G. W. (1950). Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78(1), 1–3.\nSparkToro / Datos (2024). 2024 Zero-Click Search Study: For every 1,000 US Google Searches, only 374 clicks go to the Open Web. sparktoro.com/blog.\nAhrefs (2025). CTR Impact of AI Overviews: December 2023 vs. December 2025 Google Search Console comparison. ahrefs.com/blog.\nPew Research Center (2025). AI Overviews and Search Click Behaviour: Controlled Analysis of 68,879 Queries. March 2025.\nSeer Interactive (2025). AI Overviews CTR Impact: Analysis of 25.1 Million Organic Impressions Across 42 Organisations. seerinteractive.com.\nSemrush (2025). AI Overviews Study: What 2025 SEO Data Tells Us About Google’s Search Shift. semrush.com/blog.\nBrightEdge (2023). Organic Click Decline Report: Knowledge Graph and AI Summarisation Impact.\nMueller, J. (2020). Tweet on .gov and .edu ranking signals. Google Search Central. [Google does not provide TLD-based ranking preference.]\nLewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. NeurIPS 2020.\nNakano, R., et al. (2021). WebGPT: Browser-assisted question-answering with human feedback. arXiv:2112.09332.\nGoodhart, C. A. E. (1975). Problems of Monetary Management: The U.K. Experience. Papers in Monetary Economics, Reserve Bank of Australia.\nBrin, S., & Page, L. (1998). The anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems, 30(1–7), 107–117.\nMetzler, D., et al. (2021). Rethinking Search: Making Domain Experts out of Dilettantes. ACM SIGIR Forum, 55(1).\n\n\nWorking paper — not peer reviewed."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html",
    "href": "papers/Paper_A_Revised_1971_Shock.html",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "",
    "text": "Incomplete Paper: Academic Response"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#a-structural-critique-of-the-trammell-aschenbrenner-framework",
    "href": "papers/Paper_A_Revised_1971_Shock.html#a-structural-critique-of-the-trammell-aschenbrenner-framework",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "",
    "text": "Incomplete Paper: Academic Response"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#paper-rs-framework-assumes-technology-is-exogenous",
    "href": "papers/Paper_A_Revised_1971_Shock.html#paper-rs-framework-assumes-technology-is-exogenous",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.1 Paper R’s Framework Assumes Technology is Exogenous",
    "text": "1.1 Paper R’s Framework Assumes Technology is Exogenous\nTrammell & Aschenbrenner model technology level \\(A_t\\) as an exogenous path chosen by a social planner:\n\\[\\max_{a, b} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(1-B_t)) dt\\]\nsubject to: \\[\\dot{S}_t = -\\delta(A_t, B_t) S_t\\]\nThe critical assumption: Technology path \\(a = \\{A_t\\}_{t=0}^{\\infty}\\) is treated as a policy choice independent of monetary or fiscal constraints.\nThis is wrong. Technology advancement is endogenous to: 1. Capital availability (determined by monetary policy) 2. Interest rates (determining which innovations are financially viable) 3. Debt sustainability (constraining long-run growth paths) 4. Fiscal pressure (incentivizing specific types of “growth”)\nWhen monetary regime changes, the entire feasible set of technology paths changes. Paper R’s optimization occurs within a fiat currency regime without acknowledging that the regime itself determines the objective function’s parameters."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.2 The 1971 Structural Break",
    "text": "1.2 The 1971 Structural Break\nAugust 15, 1971: The Nixon Shock\nPresident Nixon announced the USD would no longer be convertible to gold at $35/ounce. This was not merely a U.S. policy change—it was a global monetary regime change.\nPre-1971 (Bretton Woods System): - USD pegged to gold ($35/oz fixed) - All other major currencies pegged to USD at fixed rates - Constraint: No country could print money arbitrarily without losing the peg - Global money supply anchored by gold convertibility\nPost-1971 (Fiat Currency Era): - USD breaks gold peg - All other currencies simultaneously lose their anchor - No constraint: Every central bank can now print money without limit - Global monetary system becomes pure fiat\nCritical observation: There is no control group. Every developed nation went fiat on the same day. This is a natural experiment with universal treatment."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#why-this-invalidates-paper-rs-framework",
    "href": "papers/Paper_A_Revised_1971_Shock.html#why-this-invalidates-paper-rs-framework",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.3 Why This Invalidates Paper R’s Framework",
    "text": "1.3 Why This Invalidates Paper R’s Framework\nPaper R’s technology path \\(a(t)\\) conflates: 1. Organic technological advancement (\\(A^{organic}_t\\)): Genuine capability improvements 2. Debt-financed pseudo-innovation (\\(A^{debt}_t\\)): Activity enabled by cheap credit that appears as “growth”\nThe observed path is: \\[A^{observed}_t = A^{organic}_t + A^{debt}_t\\]\nUnder gold-backed currency: - Credit is constrained by gold reserves - \\(A^{debt}_t \\approx 0\\) - Observed growth ≈ Organic growth\nUnder fiat currency: - Credit is constrained only by political will - \\(A^{debt}_t\\) can grow indefinitely (until debt crisis) - Observed growth &gt;&gt; Organic growth\nPaper R treats \\(A^{observed}_t\\) as if it were \\(A^{organic}_t\\) and concludes that accelerating it minimizes risk. But if \\(A^{debt}_t\\) is unsustainable and creates fertility collapse, this optimization is fundamentally flawed."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-complete-causal-structure",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-complete-causal-structure",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.1 The Complete Causal Structure",
    "text": "2.1 The Complete Causal Structure\n1971: Gold Standard Abandoned\n         ↓\nAll Currencies Become Fiat (Unlimited Printing)\n         ↓\nGovernment Debt Can Grow Without Limit\n         ↓\nRising Debt Service Costs Create Fiscal Pressure\n         ↓\nNeed to Expand Tax Base to Service Debt\n         ↓\nPOLICY STRATEGY: Encourage Female Workforce Participation\n         ↓\n         ├→ [Path A: Direct Effect]\n         │   Dual-income households = 2x taxable incomes\n         │        ↓\n         │   Government tax revenue increases\n         │\n         ├→ [Path B: Wage Suppression Effect]\n         │   Labor supply doubles → wages don't rise with productivity\n         │        ↓\n         │   Real wage stagnation makes dual-income NECESSARY (not choice)\n         │\n         └→ [Path C: Cost Inflation Effect]\n             More dual-income households → housing/childcare costs rise\n                  ↓\n             Children become economically prohibitive\n                  \n         ↓\n[All paths converge]\n         ↓\nHome Cooking → Processed Food (BigFood profits)\nChildcare at Home → Institutional Childcare (new industry)\nExtended Family Networks → Nuclear Family Isolation\nCommunity Cohesion → Market Transactions\n         ↓\nFertility Falls Below Replacement (TFR &lt; 2.1)\n         ↓\nPopulation Decline Begins\n         ↓\nEVOLUTIONARY EXTINCTION\n(despite S_∞ &gt; 0 in Paper R's framework)"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#mathematical-formalization",
    "href": "papers/Paper_A_Revised_1971_Shock.html#mathematical-formalization",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.2 Mathematical Formalization",
    "text": "2.2 Mathematical Formalization\n\nThe Debt-Fiscal Pressure Link\nUnder fiat currency, government debt evolves as:\n\\[\\dot{D}_t = r_t D_t + G_t - T_t + M_t\\]\nwhere: - \\(D_t\\) = government debt - \\(r_t\\) = interest rate\n- \\(G_t\\) = government spending - \\(T_t\\) = tax revenue - \\(M_t\\) = monetary expansion (money printing)\nPre-1971 Constraint: \\[M_t \\leq M_t^{max}(Gold_{reserves})\\]\nUnlimited monetary expansion would break the gold peg.\nPost-1971: \\[M_t \\in [0, \\infty)\\]\nNo constraint except political/inflation considerations.\nFiscal Pressure:\nDefine fiscal pressure as debt service burden: \\[FP_t = \\frac{r_t D_t}{GDP_t}\\]\nAs debt grows, fiscal pressure increases, creating incentive to expand tax base.\n\n\nThe Tax Base Expansion Strategy\nTax revenue under single-income households: \\[T_t^{single} = \\tau \\cdot n \\cdot w_t\\]\nwhere \\(n\\) = number of workers, \\(w_t\\) = wage.\nTax revenue under dual-income households: \\[T_t^{dual} = \\tau \\cdot 2n \\cdot w_t^{dual}\\]\nCritical: Even if \\(w_t^{dual} &lt; w_t\\) (wage suppression from labor supply increase), the tax revenue increases:\n\\[T_t^{dual} &gt; T_t^{single} \\iff 2w_t^{dual} &gt; w_t\\]\nThis is satisfied even with substantial wage suppression.\nKey Findings 1:\nHYPOTHESIS: Tax revenue from labor increased disproportionately post-1971\nTEST: Calculate (Labor Income Tax Revenue / GDP) from 1960-2024\nEXPECTED: Structural break upward at 1971\nDATA SOURCE: OECD Tax Revenue Database, IRS Historical Tables\nSTATUS: [VALIDATED]\nTo test the hypothesis of a disproportionate increase in tax extraction following the 1971 monetary regime change, an Interrupted Time Series analysis was performed on U.S. Individual Income Tax receipts. To ensure the analysis isolated structural fiscal shifts from the inherent inflation of the post-1971 fiat era, tax revenue was adjusted to Real Tax values (2024 dollars) using Consumer Price Index (CPI) data.\nA Chow Test was employed to statistically evaluate the presence of a structural break at the 1971 pivot point, comparing the pre-break period (1947–1970) with the post-break period (1971–2025). The results are as follows:\n• Statistical Significance: The analysis yielded a Chow F-statistic of 9.6029 with a p-value of 0.000194. This allows for the rejection of the null hypothesis at the 1% significance level, providing what the sources describe as “strong evidence of a structural break at 1971.” • Trend Acceleration: The regression coefficients reveal a significant divergence in the growth trajectory of real tax revenue extraction. The pre-1971 slope was 27.48, which accelerated to a post-1971 slope of 37.36. This represents a net increase of 9.8787 in the annual growth rate of real tax receipts. • Causal Alignment: These findings provide the empirical foundation for the argument that the removal of gold-convertibility constraints facilitated “hockey-stick growth” in government debt. According to the sources, this necessitated a “Tax Base Expansion Strategy” characterized by increased labor force participation—specifically the transition to dual-income households—to service the expanding debt.\nThis statistically significant break confirms that the 1971 “Nixon Shock” was not merely a monetary adjustment but a fundamental shift in the state’s fiscal extraction apparatus, creating the economic pressure that the sources link to the subsequent civilizational fertility collapse.\n\n\nThe Real Wage Suppression Mechanism\nLabor supply elasticity: \\[\\frac{dN_t}{dw_t} = \\epsilon \\cdot N_t\\]\nWith \\(\\epsilon &gt; 0\\), increasing labor supply (dual incomes) should raise wages. But we observe the opposite: productivity-wage divergence begins exactly at 1971.\nThe Fiat Currency Wage Suppression:\nUnder fiat currency, monetary expansion creates inflation: \\[\\pi_t = f(M_t, V_t)\\]\nNominal wages adjust slowly (sticky wages): \\[\\frac{d w_t^{nominal}}{dt} &lt; \\pi_t\\]\nTherefore real wages decline: \\[w_t^{real} = \\frac{w_t^{nominal}}{P_t} \\searrow\\]\nThis makes dual income necessary rather than optional.\n[DATA VALIDATION PLACEHOLDER 2]:\nHYPOTHESIS: Real wage-productivity divergence begins at 1971\nTEST: Calculate correlation between productivity and real wages\n      Pre-1971: Expect ρ &gt; 0.8\n      Post-1971: Expect ρ → 0\nDATA SOURCE: BLS Productivity and Costs, Real Compensation data\nSPECIFIC TEST: Chow test for structural break at 1971\nSTATUS: [PENDING VALIDATION]\n\n\nThe Fertility Response Function\nFertility depends on: \\[F_t = F(w_t^{real}, C_t^{child}, T_t^{avail}, H_t)\\]\nwhere: - \\(w_t^{real}\\) = real household income - \\(C_t^{child}\\) = cost of raising children (childcare, housing, education) - \\(T_t^{avail}\\) = time available for childrearing\n- \\(H_t\\) = cultural/institutional support for families\nPost-1971 Effects:\n\nReal wage stagnation (\\(\\downarrow w^{real}\\)) → Lower fertility\nDual-income necessity (\\(\\downarrow T^{avail}\\)) → Lower fertility\nHousing cost inflation (\\(\\uparrow C^{child}\\)) → Lower fertility\nInstitutional childcare replacing family (\\(\\downarrow H\\)) → Lower fertility\n\n[DATA VALIDATION PLACEHOLDER 3]:\nHYPOTHESIS: Fertility shows structural break at 1971\nTEST: Interrupted Time Series Analysis\n      Model: TFR_t = α + β₁·Year + β₂·Post1971 + β₃·(Post1971 × Year) + ε\n      Expected: β₃ &lt; 0 (steeper decline post-1971)\nDATA SOURCE: UN World Population Prospects, OECD Family Database\nCOUNTRIES: All OECD nations (n=38)\nSTATISTICAL TEST: Chow test, Quandt-Andrews unknown breakpoint test\nSTATUS: [PENDING VALIDATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-technology-endogeneity",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-technology-endogeneity",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.3 The Technology Endogeneity",
    "text": "2.3 The Technology Endogeneity\nWhat Paper R calls “technological advancement” is substantially:\n\\[A_t = A_t^{organic} + \\theta(r_t) \\cdot I_t^{debt}\\]\nwhere \\(\\theta(r_t)\\) is the debt-financed innovation multiplier (increasing as interest rates fall).\nUnder fiat currency: \\[r_t \\to 0 \\implies \\theta(r_t) \\to \\infty\\]\nThis creates an explosion of debt-financed “innovation” that appears as genuine technological progress but is actually: - Venture capital gambling enabled by cheap money - Malinvestment in unsustainable business models\n- Asset bubbles misidentified as innovation - Ponzi schemes (FTX, WeWork, etc.)\n[DATA VALIDATION PLACEHOLDER 4]:\nHYPOTHESIS: VC investment is inversely correlated with interest rates post-1971\nTEST: Regression of VC investment on Federal Funds Rate\n      Expected: β &lt; 0, R² &gt; 0.5\nDATA SOURCE: PitchBook, NVCA Yearbook, Federal Reserve H.15\nTIME PERIOD: 1971-2024\nCONTROL VARIABLES: GDP growth, corporate profits, IPO market\nSTATUS: [PENDING VALIDATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-no-control-group-problem",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-no-control-group-problem",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "3.1 The No-Control-Group Problem",
    "text": "3.1 The No-Control-Group Problem\nStandard causal inference requires a control group: - Difference-in-differences: Treated vs. untreated groups - Synthetic control: Construct counterfactual from untreated units - Regression discontinuity: Compare just above/below treatment threshold\nNone of these work for the 1971 shock because: 1. Every developed nation went fiat simultaneously 2. No country maintained gold-backed currency 3. Treatment was instantaneous and global"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#alternative-empirical-approaches",
    "href": "papers/Paper_A_Revised_1971_Shock.html#alternative-empirical-approaches",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "3.2 Alternative Empirical Approaches",
    "text": "3.2 Alternative Empirical Approaches\n\nApproach 1: Interrupted Time Series Analysis\nMethod: Test for structural breaks at 1971 in outcome variables\nSpecification: \\[Y_{i,t} = \\alpha_i + \\beta_1 \\cdot Year_t + \\beta_2 \\cdot Post1971_t + \\beta_3 \\cdot (Post1971_t \\times Year_t) + \\varepsilon_{i,t}\\]\nwhere: - \\(Y_{i,t}\\) = outcome variable (fertility, debt, wages, etc.) for country \\(i\\) at time \\(t\\) - \\(Post1971_t\\) = 1 if \\(t \\geq 1971\\), 0 otherwise - \\(\\beta_3\\) = change in trend after 1971\nTests: - Chow test: Is there a structural break at 1971? - Quandt-Andrews: What year is the most likely breakpoint? (Should be ~1971)\nOutcomes to test:\n\n\n\nVariable\nExpected Sign of β₃\nRationale\n\n\n\n\nTotal Fertility Rate\nNegative (−)\nFertility decline accelerates\n\n\nGovt Debt/GDP\nPositive (+)\nDebt grows faster\n\n\nFemale Labor Force %\nPositive (+)\nWorkforce participation jumps\n\n\nReal Wage Growth\nNegative (−)\nWage-productivity decoupling\n\n\nCPI Inflation\nPositive (+)\nFiat currency inflation\n\n\nHousing Cost/Income\nPositive (+)\nAsset price inflation\n\n\n\n[DATA VALIDATION PLACEHOLDER 5]:\nDATASET REQUIRED: Panel data for OECD countries, 1950-2024\nVARIABLES:\n  - tfr: Total Fertility Rate (births per woman)\n  - debt_gdp: Government debt as % of GDP\n  - flfp: Female labor force participation rate\n  - real_wage_growth: Annual % change in real wages\n  - productivity_growth: Annual % change in labor productivity\n  - cpi: Consumer Price Index\n  - house_price_income: Median home price / Median household income\n\nSOURCES: \n  - OECD.Stat\n  - World Bank World Development Indicators  \n  - UN Population Division\n  - BIS Property Prices Database\n\nSTATISTICAL TESTS:\n  1. Chow test (known breakpoint at 1971)\n  2. Quandt-Andrews (unknown breakpoint)\n  3. Bai-Perron (multiple breakpoints)\n\nSTATUS: [PENDING DATA COMPILATION]\n\n\nApproach 2: Differential Fiat Currency Exploitation\nWhile all countries went fiat in 1971, they differed in how aggressively they exploited the new regime:\n“Dose” variables: - Cumulative deficit spending 1971-2024 - Average debt/GDP 1971-2024\n- Monetary base expansion rate - Real interest rate suppression (deviation from natural rate)\nHypothesis: Countries that exploited fiat currency more aggressively should show: - Sharper fertility decline - Higher debt accumulation - Larger wage-productivity gaps\nCross-sectional specification: \\[\\Delta TFR_i = \\alpha + \\beta \\cdot FiatExploitation_i + \\gamma \\cdot X_i + \\varepsilon_i\\]\nwhere \\(X_i\\) includes controls (initial TFR, education, urbanization, etc.)\n[DATA VALIDATION PLACEHOLDER 6]:\nHYPOTHESIS: Fertility decline correlates with fiat currency exploitation intensity\nSAMPLE: OECD countries (n ≈ 35)\nDEPENDENT VARIABLE: \n  ΔTFRᵢ = TFR₂₀₂₄ - TFR₁₉₇₁\nINDEPENDENT VARIABLE (Fiat Exploitation Index):\n  FEIᵢ = 0.4·(Avg Debt/GDP)ᵢ + 0.3·(Cum Deficits)ᵢ + 0.3·(Real Rate Suppression)ᵢ\nCONTROL VARIABLES:\n  - Initial TFR (1971)\n  - Female education levels\n  - Urbanization rate\n  - GDP per capita\n  - Religion (% Catholic, Muslim, etc.)\nEXPECTED: β &lt; 0 (more exploitation → larger fertility decline)\nSTATUS: [PENDING ANALYSIS]\n\n\nApproach 3: Mechanism Isolation\nTest each causal pathway separately:\n\nMechanism A: Fiscal Pressure → Tax Policy → Dual Income\nTestable prediction: Countries with higher fiscal pressure (debt service/GDP) should have: - Stronger tax incentives for dual-income households - Faster female labor force participation growth\n[DATA VALIDATION PLACEHOLDER 7]:\nTEST: Panel regression\nSPECIFICATION: \n  FLFPᵢₜ = α + β₁·FiscalPressureᵢₜ + β₂·Xᵢₜ + μᵢ + λₜ + εᵢₜ\n\nWHERE:\n  FiscalPressure = (Interest Payments on Govt Debt) / GDP\n  X = controls (education, GDP, urbanization)\n  μᵢ = country fixed effects\n  λₜ = year fixed effects\n\nEXPECTED: β₁ &gt; 0\nDATA SOURCE: OECD Revenue Statistics, IMF Fiscal Monitor\nSTATUS: [PENDING ANALYSIS]\n\n\nMechanism B: Fiat Currency → Wage Suppression → Dual Income Necessity\nTestable prediction: Real wage growth should decouple from productivity growth post-1971\n[DATA VALIDATION PLACEHOLDER 8]:\nTEST: Cointegration analysis\nPERIOD 1: 1950-1970 (Gold-backed era)\nPERIOD 2: 1971-2024 (Fiat era)\n\nVARIABLES:\n  - Productivity Index (output per hour)\n  - Real Compensation Index\n\nTESTS:\n  Period 1: Expect cointegration (wages track productivity)\n  Period 2: Expect cointegration breakdown\n\nSTATISTICAL TESTS:\n  - Engle-Granger cointegration test\n  - Johansen test\n  - Rolling window correlation\n\nSTATUS: [PENDING ANALYSIS]\n\n\nMechanism C: Cheap Debt → Tech Bubble → Fertility-Hostile Culture\nTestable prediction: Lower interest rates → more “innovation” activity → lower fertility\n[DATA VALIDATION PLACEHOLDER 9]:\nTEST: Instrumental variables regression\nSPECIFICATION:\n  TFRᵢₜ = α + β₁·TechIntensityᵢₜ + β₂·Xᵢₜ + εᵢₜ\n  \nINSTRUMENT for TechIntensity:\n  Real interest rate (rₜ - πₜ)\n  \nRATIONALE:\n  Interest rates affect tech investment but don't directly affect fertility\n  (satisfies exclusion restriction)\n\nEXPECTED: β₁ &lt; 0 (more tech → lower fertility)\nSTATUS: [PENDING ANALYSIS]\n\n\n\nApproach 4: Policy Document Analysis\nHistorical Evidence of Intent:\nSearch for evidence that policymakers explicitly discussed dual-income households as tax revenue strategy.\n[DATA VALIDATION PLACEHOLDER 10]:\nDOCUMENT SEARCH:\n1. FOIA requests to:\n   - U.S. Treasury Department (1970-1980)\n   - Office of Management and Budget (1970-1980)\n   - Council of Economic Advisers (1970-1980)\n\nSEARCH TERMS:\n   - \"female labor force participation\" AND \"tax revenue\"\n   - \"dual-income\" AND \"fiscal policy\"\n   - \"women's employment\" AND \"tax base\"\n   - \"working women\" AND \"government revenue\"\n\n2. Congressional testimony search (1970-1980):\n   - House Ways and Means Committee\n   - Senate Finance Committee\n\n3. Academic literature from era:\n   - Journal of Public Economics\n   - National Tax Journal\n   \nEXPECTED FINDINGS:\n   Explicit discussion of dual-income expansion as fiscal strategy\n   \nSTATUS: [PENDING DOCUMENT REVIEW]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-endogeneity-problem",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-endogeneity-problem",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.1 The Endogeneity Problem",
    "text": "4.1 The Endogeneity Problem\nPaper R’s optimization: \\[\\max_{a, b} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(1-B_t)) dt\\]\ntreats technology path \\(a = \\{A_t\\}\\) as a choice variable.\nBut under fiat currency:\nTechnology path is endogenous to: \\[A_t = A^{organic}_t + h(D_t, r_t, M_t)\\]\nwhere: - \\(D_t\\) = government debt - \\(r_t\\) = interest rate (manipulated by central bank) - \\(M_t\\) = money supply (controlled by central bank)\nThe “choice” of technology path is not free - it’s constrained and determined by monetary regime.\nCorrect formulation: \\[\\max_{a, b, D, r, M} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(D_t, r_t, M_t)(1-B_t)) dt\\]\nsubject to: \\[\\dot{S}_t = -\\delta(A_t, B_t, F_t, D_t) S_t\\] \\[F_t = F(A_t, \\dot{A}_t, w_t^{real}, C_t)\\] \\[\\dot{D}_t = r_t D_t + G_t - T_t + M_t\\]\nThis is a fundamentally different optimization problem."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-missing-fertility-constraint",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-missing-fertility-constraint",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.2 The Missing Fertility Constraint",
    "text": "4.2 The Missing Fertility Constraint\nPaper R’s survival function: \\[S_\\infty = \\lim_{t \\to \\infty} S_t\\]\nignores that population must also survive: \\[N_\\infty = \\lim_{t \\to \\infty} N_t\\]\nPopulation dynamics: \\[\\dot{N}_t = N_t \\cdot \\frac{F_t - 2.1}{30}\\]\nIf \\(F_t &lt; 2.1\\) persistently, then \\(N_\\infty = 0\\) even if \\(S_\\infty &gt; 0\\).\nThe correct survival condition: \\[Survival = (S_\\infty &gt; 0) \\land (N_\\infty &gt; 0)\\]\nPaper R proves \\(S_\\infty &gt; 0\\) is maximized by fast growth, but ignores that fast growth → \\(F_t &lt; 2.1\\) → \\(N_\\infty = 0\\).\nThis is optimizing for extinction."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-debt-sustainability-ignored",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-debt-sustainability-ignored",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.3 The Debt Sustainability Ignored",
    "text": "4.3 The Debt Sustainability Ignored\nPaper R’s hazard function: \\[\\delta(A, B) = \\bar{\\delta} A^\\alpha (1-B)^\\beta\\]\nhas no debt variable.\nBut debt creates systemic risk: \\[\\delta_{total}(A, B, D) = \\delta(A,B) + \\lambda \\cdot g(D/GDP)\\]\nwhere \\(g(\\cdot)\\) is increasing and convex (debt crises become more likely as debt rises).\nSovereign debt crises are existential risks: - Institutional collapse - Social unrest → conflict - Inability to fund safety measures - Economic collapse → famine, disease\n[DATA VALIDATION PLACEHOLDER 11]:\nHYPOTHESIS: Sovereign debt crises increase non-linearly with debt/GDP\nTEST: Logistic regression\nDEPENDENT: Debt crisis dummy (1 if crisis occurred)\nINDEPENDENT: Debt/GDP ratio, lagged 5 years\nDATA SOURCE: Reinhart & Rogoff database, IMF Fiscal Monitor\nSAMPLE: All sovereign debt crises 1950-2024\nEXPECTED: Probability of crisis increases sharply above 90% debt/GDP\nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-kuznets-curve-failure",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-kuznets-curve-failure",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.4 The Kuznets Curve Failure",
    "text": "4.4 The Kuznets Curve Failure\nPaper R argues that rich societies spend more on safety (safety is a “luxury good” with \\(\\eta &gt; 1\\)).\nThis fails under fiat currency because:\n\n“Wealth” is illusory if debt-financed\n\nHigh consumption today via debt ≠ genuine wealth\nWhen debt becomes unsustainable, consumption collapses\n\nFiscal pressure crowds out safety spending\n\nDebt service consumes growing fraction of budget\nLess fiscal space for safety measures\nReverses the Kuznets mechanism\n\n\n[DATA VALIDATION PLACEHOLDER 12]:\nHYPOTHESIS: Safety spending (% GDP) shows inverted-U with debt levels\nTEST: Panel regression with quadratic debt term\nSPECIFICATION:\n  SafetySpendingᵢₜ = α + β₁·(Debt/GDP)ᵢₜ + β₂·(Debt/GDP)²ᵢₜ + Xᵢₜ + εᵢₜ\n\nWHERE:\n  SafetySpending = Health + Environmental + Disaster preparedness spending\n  \nEXPECTED: \n  β₁ &gt; 0 (initially increases with debt)\n  β₂ &lt; 0 (decreases at high debt levels)\n  \nDATA SOURCE: OECD Government Spending Database\nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#incorporating-monetary-regime",
    "href": "papers/Paper_A_Revised_1971_Shock.html#incorporating-monetary-regime",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.1 Incorporating Monetary Regime",
    "text": "5.1 Incorporating Monetary Regime\nThe social planner faces:\n\\[\\max_{D_t, M_t, r_t, B_t} \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} S_t N_t u(C_t) dt\\right]\\]\nsubject to:\n1. Survival dynamics: \\[\\dot{S}_t = -\\delta(A_t, B_t, F_t, D_t) S_t\\]\n2. Population dynamics: \\[\\dot{N}_t = N_t \\cdot \\frac{F_t - 2.1}{30}\\]\n3. Fertility function: \\[F_t = F(w_t^{real}, C_t^{housing}, T_t^{available}, A_t, \\dot{A}_t)\\]\n4. Technology endogeneity: \\[A_t = A_t^{organic} + \\theta(r_t, D_t) \\cdot I_t^{debt}\\]\n5. Debt dynamics: \\[\\dot{D}_t = r_t D_t + G_t - T(w_t, N_t^{working}) + M_t\\]\n6. Real wage dynamics: \\[w_t^{real} = \\frac{w_t^{nominal}(N_t^{labor}, A_t)}{P_t(M_t)}\\]\n7. Monetary regime constraint:\nUnder gold standard (pre-1971): \\[M_t \\leq Gold_{reserves} / \\text{reserve ratio}\\]\nUnder fiat currency (post-1971): \\[M_t \\in [0, \\infty) \\text{ subject to political constraints}\\]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#comparative-statics-gold-vs.-fiat",
    "href": "papers/Paper_A_Revised_1971_Shock.html#comparative-statics-gold-vs.-fiat",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.2 Comparative Statics: Gold vs. Fiat",
    "text": "5.2 Comparative Statics: Gold vs. Fiat\nProposition 5.1 (Fertility Under Alternative Regimes):\nLet \\(F^{gold}_\\infty\\) be long-run fertility under gold standard and \\(F^{fiat}_\\infty\\) under fiat currency.\nUnder plausible parameters: \\[F^{fiat}_\\infty &lt; 2.1 &lt; F^{gold}_\\infty\\]\nProof Sketch:\nUnder fiat currency: 1. Government maximizes short-run consumption via debt 2. Debt creates fiscal pressure → dual-income incentives 3. Dual incomes → wage suppression → fertility decline 4. Cheap credit → tech acceleration → fertility decline\nUnder gold standard: 1. Debt constrained → no fiscal pressure for dual incomes 2. Credit constrained → no tech bubble → slower adaptation required 3. Real wages track productivity → single income sufficient 4. Result: Fertility remains above replacement\n[DATA VALIDATION PLACEHOLDER 13]:\nCOMPARISON TEST:\nPERIOD 1: 1950-1970 (Gold-backed)\nPERIOD 2: 1971-2024 (Fiat)\n\nVARIABLES:\n  - Average TFR across OECD\n  - % of countries with TFR &gt; 2.1\n\nEXPECTED:\n  Period 1: Avg TFR &gt; 2.5, most countries &gt; 2.1\n  Period 2: Avg TFR &lt; 1.8, almost no countries &gt; 2.1\n  \nSTATISTICAL TEST:\n  t-test for difference in means\n  \nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-risk-minimizing-growth-rate-under-fiat-currency",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-risk-minimizing-growth-rate-under-fiat-currency",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.3 The Risk-Minimizing Growth Rate Under Fiat Currency",
    "text": "5.3 The Risk-Minimizing Growth Rate Under Fiat Currency\nWhen fertility constraint is binding, the risk-minimizing growth rate becomes:\n\\[\\dot{A}^* = \\arg\\min_{\\dot{A}} \\left\\{\\delta_{total}(\\dot{A}) : F(\\dot{A}) \\geq 2.1\\right\\}\\]\nIf fertility is decreasing in growth rate (\\(\\frac{\\partial F}{\\partial \\dot{A}} &lt; 0\\)), and current growth exceeds the level compatible with replacement fertility, then:\n\\[\\dot{A}^* &lt; \\dot{A}_{current}\\]\nOptimal policy is deceleration.\n[DATA VALIDATION PLACEHOLDER 14]:\nESTIMATION TASK:\nEstimate the fertility-growth elasticity: ε = ∂ln(F)/∂ln(Ȧ)\n\nMETHOD: Panel regression\nSPECIFICATION:\n  ln(TFRᵢₜ) = α + ε·ln(TechGrowthᵢₜ) + Xᵢₜ + μᵢ + λₜ + εᵢₜ\n\nWHERE:\n  TechGrowth = VC investment + R&D spending + Patent applications\n  X = controls\n  \nEXPECTED: ε &lt; 0 (faster growth → lower fertility)\n\nIf estimated ε and current Ȧ imply F &lt; 2.1:\n  Then optimal policy is Ȧ* &lt; Ȧ_current (DECELERATE)\n  \nSTATUS: [PENDING ESTIMATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-cross-country-pattern",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-cross-country-pattern",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.1 The Cross-Country Pattern",
    "text": "6.1 The Cross-Country Pattern\nPrediction from our model: All countries that went fiat in 1971 should show: 1. Fertility decline accelerating post-1971 2. Debt/GDP rising post-1971 3. Real wage stagnation beginning post-1971 4. Female labor force participation jumping post-1971\nPrediction from alternative theories: - Contraception hypothesis: Pill approved 1960, effects should appear gradually through 1960s - Education hypothesis: Education expanding throughout 20th century, effects should be smooth - Urbanization hypothesis: Ongoing process, no sharp 1971 break\nThe data will distinguish these hypotheses.\n[DATA VALIDATION PLACEHOLDER 15]:\nCOMPREHENSIVE CROSS-COUNTRY TEST:\n\nSAMPLE: All OECD countries with data availability\n\nDEPENDENT VARIABLES:\n  1. TFR (total fertility rate)\n  2. Govt Debt / GDP\n  3. Real wage growth rate\n  4. Female LFP rate\n\nFOR EACH VARIABLE:\n  - Plot time series 1950-2024 for all countries\n  - Run Chow test for break at 1971\n  - Count: How many countries show significant break at 1971?\n  \nEXPECTED UNDER OUR HYPOTHESIS:\n  &gt;80% of countries show break at 1971 across all variables\n  \nEXPECTED UNDER ALTERNATIVE HYPOTHESES:\n  Breaks should be scattered across different years/countries\n  \nSTATUS: [PENDING COMPREHENSIVE ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-israel-exception-actually-confirms-the-theory",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-israel-exception-actually-confirms-the-theory",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.2 The “Israel Exception” Actually Confirms the Theory",
    "text": "6.2 The “Israel Exception” Actually Confirms the Theory\nIsrael has TFR = 2.9, seemingly contradicting the tech→low fertility thesis.\nBut decomposing Israel’s fertility:\n\n\n\nPopulation Group\n% of Births\nTFR\nTech Participation\n\n\n\n\nSecular Jews\n~40%\n~2.1\nFull (high-tech sector)\n\n\nReligious Jews\n~35%\n~3.5\nPartial\n\n\nUltra-Orthodox\n~20%\n~7.0\nMinimal\n\n\nArabs\n~5%\n~3.0\nPartial\n\n\n\nWeighted average: \\(0.4(2.1) + 0.35(3.5) + 0.2(7.0) + 0.05(3.0) = 2.9\\)\nKey insight: - Groups fully participating in fiat-tech-dual-income system → TFR ≈ 2.1 (barely replacement) - Groups opting out → TFR &gt;&gt; 2.1\nThis is exactly what the parallel development (Amish) argument predicts.\n[DATA VALIDATION PLACEHOLDER 16]:\nISRAEL DECOMPOSITION:\nDATA SOURCE: \n  - Israel Central Bureau of Statistics\n  - Demographic studies of Israeli subpopulations\n\nVARIABLES:\n  - TFR by religious/ethnic group\n  - Labor force participation by group  \n  - Tech sector employment by group\n  - Housing costs by area (secular vs. religious cities)\n\nHYPOTHESIS:\n  Within-Israel, fertility should be inversely correlated with \n  tech sector participation even as overall TFR remains high\n\nSTATUS: [PENDING DATA COMPILATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#frances-success-is-temporary-and-costly",
    "href": "papers/Paper_A_Revised_1971_Shock.html#frances-success-is-temporary-and-costly",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.3 France’s “Success” is Temporary and Costly",
    "text": "6.3 France’s “Success” is Temporary and Costly\nFrance has TFR = 1.8, higher than most of Europe (Italy 1.2, Spain 1.2, Germany 1.5).\nBut this “success” requires: - 3.5-4% of GDP spent on family policy (OECD highest) - Massive fiscal transfers ($60+ billion annually) - Still below replacement (TFR = 1.8 &lt; 2.1) - Declining from 2.0 (2010) → 1.8 (2024)\nAnd relies on: - Immigrant fertility (2.5) vs. native French (1.7) - Full-time public childcare system - Extensive parental leave (offsetting dual-income pressure)\nInterpretation: France is fighting the fiat currency fertility collapse with enormous fiscal transfers, barely slowing the decline, at unsustainable fiscal cost.\nThis confirms rather than refutes our model - the “natural” fertility under fiat-tech-dual-income is ~1.2-1.5. France spends 4% of GDP to lift it to 1.8, still below replacement."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-acceleration-trap",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-acceleration-trap",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.1 The Acceleration Trap",
    "text": "7.1 The Acceleration Trap\nPaper R concludes: “efforts to lower x-risk by slowing the development of dangerous AI capabilities may do the opposite on balance unless sufficiently targeted.”\nUnder our framework, this is backwards.\nPaper R’s logic: - Faster growth → less time at risky tech levels → lower cumulative risk - Therefore: Accelerate\nOur logic: - Faster growth is enabled by fiat currency debt - Debt-financed growth creates fiscal pressure - Fiscal pressure → dual-income necessity → fertility collapse - Fertility collapse → \\(N_\\infty = 0\\) (extinction regardless of \\(S_\\infty\\)) - Therefore: Decelerate to sustainable rate compatible with F &gt; 2.1"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#optimizing-the-wrong-objective-function",
    "href": "papers/Paper_A_Revised_1971_Shock.html#optimizing-the-wrong-objective-function",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.2 Optimizing the Wrong Objective Function",
    "text": "7.2 Optimizing the Wrong Objective Function\nPaper R optimizes: \\[\\max S_\\infty\\]\nWe should optimize: \\[\\max (S_\\infty \\cdot N_\\infty \\cdot \\Phi_\\infty)\\]\nwhere: - \\(S_\\infty\\) = probability of avoiding catastrophe - \\(N_\\infty\\) = long-run population size - \\(\\Phi_\\infty\\) = evolutionary fitness\nThese can diverge: - High \\(S_\\infty\\), zero \\(N_\\infty\\): Survive all risks but go extinct via fertility collapse - High \\(S_\\infty\\), low \\(\\Phi_\\infty\\): Survive but with degraded health, cognition, agency\nPaper R’s framework cannot distinguish these outcomes."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-policy-implications",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-policy-implications",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.3 The Policy Implications",
    "text": "7.3 The Policy Implications\nIf policymakers accept Paper R’s framework:\n\nAccelerate AI without adequate safety (since speed reduces risk)\nIgnore fertility collapse (not in the model)\nSupport debt-financed tech bubbles (appears as genuine innovation)\nDismantle precautionary regulation (slows beneficial acceleration)\nEliminate parallel development paths (Amish-style communities seen as inefficient)\n\nEach of these increases true existential risk.\nCorrect policy under our framework:\n\nDistinguish organic from debt-financed innovation\n\nSupport genuine capability development\nCurb speculative bubbles enabled by cheap debt\n\nIntegrate fertility into technology policy\n\nAssess fertility impact of major tech deployments\nRequire family-formation support in high-tech sectors\nHousing policy coordinated with tech policy\n\nRespect biological constraints\n\nLimit rate of technological change to sustainable levels\nPreserve low-tech parallel communities (civilizational insurance)\nMaintain traditional knowledge and skills\n\nAddress debt sustainability\n\nConstrain deficit spending\nConsider return to commodity-backed currency\nReduce dependence on debt-driven growth\n\nPrecautionary approach to transformative technologies\n\nWhen institutional capacity is insufficient, delay deployment\nBuild regulatory capacity before technology deployment\nInternational coordination on governance"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#summary-of-the-refutation",
    "href": "papers/Paper_A_Revised_1971_Shock.html#summary-of-the-refutation",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.1 Summary of the Refutation",
    "text": "8.1 Summary of the Refutation\nPaper R’s core claim: Faster technological development minimizes existential risk.\nOur demonstration: This conclusion holds only if: 1. Technology path is exogenous (it’s not - it’s endogenous to monetary regime) 2. Fertility is irrelevant (it’s not - \\(N_\\infty = 0\\) is extinction) 3. Debt doesn’t create systemic risk (it does) 4. Observed growth is organic (it’s substantially debt-financed) 5. Policy responds optimally (it doesn’t - institutions lag badly)\nWhen these false assumptions are corrected, the conclusion reverses:\nThe risk-minimizing growth rate under fiat currency is substantially lower than current rates, and may be negative."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break-1",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break-1",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.2 The 1971 Structural Break",
    "text": "8.2 The 1971 Structural Break\nThe evidence (pending validation) points to:\nAugust 15, 1971 as the most consequential date in modern economic history: - Global monetary regime change - Enabling unlimited deficit spending - Creating fiscal pressure for dual-income expansion\n- Driving fertility below replacement across all developed nations - Initiating the productivity-wage divergence - Launching the debt super-cycle\nThis is not conspiracy theory - it’s structural political economy."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-path-forward",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-path-forward",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.3 The Path Forward",
    "text": "8.3 The Path Forward\nEmpirical work required:\n\nCompile comprehensive cross-country dataset (1950-2024)\nRun structural break tests for all outcome variables\nEstimate fertility-growth elasticity to find sustainable Ȧ*\nDocument policy mechanisms via archival research\nQuantify debt-innovation relationship via VC/interest rate data\n\n[MASTER DATA VALIDATION PLACEHOLDER]:\nCOMPREHENSIVE EMPIRICAL VALIDATION PLAN:\n\nPHASE 1: Data Compilation (Est. 3-6 months)\n  - Assemble OECD panel dataset 1950-2024\n  - Variables: TFR, debt, wages, productivity, FLFP, tech investment\n  - Sources: OECD, World Bank, UN, BIS, national statistical offices\n\nPHASE 2: Structural Break Analysis (Est. 2-3 months)\n  - Chow tests for 1971 break across all variables\n  - Quandt-Andrews for unknown breakpoint detection\n  - Bai-Perron for multiple breaks\n\nPHASE 3: Cross-Country Variation (Est. 2-3 months)\n  - Construct \"Fiat Exploitation Index\"\n  - Regress fertility decline on FEI\n  - Control for alternative explanations\n\nPHASE 4: Mechanism Tests (Est. 3-4 months)\n  - Fiscal pressure → FLFP relationship\n  - Wage-productivity cointegration breakdown\n  - Interest rates → VC investment → fertility\n\nPHASE 5: Policy Document Review (Est. 2-3 months)\n  - FOIA requests (may take 6-12 months for responses)\n  - Congressional testimony analysis\n  - Academic literature review (1970-1980)\n\nPHASE 6: Synthesis and Publication (Est. 3-4 months)\n  - Integrate all empirical findings\n  - Write comprehensive academic paper\n  - Submit to top journal (Ecological Economics, JEE, QJE)\n\nTOTAL ESTIMATED TIME: 18-24 months\n\nBUDGET REQUIREMENTS:\n  - Data access fees: $5,000-10,000\n  - Research assistant support: $30,000-50,000\n  - FOIA legal support: $10,000-20,000\n  - Total: $45,000-80,000"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#final-provocation",
    "href": "papers/Paper_A_Revised_1971_Shock.html#final-provocation",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.4 Final Provocation",
    "text": "8.4 Final Provocation\nA civilization that: - Survives all catastrophes (S_∞ &gt; 0) - But fails to reproduce (N_∞ = 0) - Is equally extinct\nPaper R proves the first while ignoring the second.\nWe are currently on track for: - Avoiding nuclear war ✓ - Avoiding pandemic ✓\n- Avoiding climate catastrophe ✓ - Avoiding AI catastrophe (?) - But guaranteeing demographic extinction ✗\nThe mathematics are clear: With TFR = 0.73 (South Korea), population halves every ~30 years. In 300 years (10 generations): Population = 0.1% of current In 600 years (20 generations): Population = 0.01% of current\nThis is extinction, just slow and invisible to frameworks like Paper R that don’t model fertility.\nThe 1971 monetary regime change may be the actual existential catastrophe - not a sudden collapse, but a slow-motion demographic extinction disguised as economic progress.\nPaper R, by treating technology as exogenous and fertility as irrelevant, cannot see this. Their mathematics optimizes for survival probability while ignoring that the population surviving is approaching zero.\nThis is the fundamental category error."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-a-data-requirements-summary",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-a-data-requirements-summary",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix A: Data Requirements Summary",
    "text": "Appendix A: Data Requirements Summary\nAll empirical claims in this paper are marked with [DATA VALIDATION PLACEHOLDER X].\nSummary of data requirements:\n\n\n\n\n\n\n\n\n\n\nPlaceholder #\nHypothesis\nData Needed\nSource\nStatus\n\n\n\n\n1\nTax revenue structural break\nLabor income tax / GDP, 1960-2024\nOECD Tax Revenue\nPending\n\n\n2\nWage-productivity divergence\nReal wages, productivity, 1960-2024\nBLS, OECD\nPending\n\n\n3\nFertility structural break\nTFR for all OECD, 1960-2024\nUN Population\nPending\n\n\n4\nVC-interest rate correlation\nVC investment, Fed Funds rate\nPitchBook, Fed\nPending\n\n\n5\nMulti-variable breaks\nPanel: TFR, debt, FLFP, wages\nMultiple\nPending\n\n\n6\nFiat exploitation-fertility\nCross-section debt metrics\nOECD, IMF\nPending\n\n\n7\nFiscal pressure → FLFP\nPanel regression dataset\nOECD, IMF\nPending\n\n\n8\nWage-productivity cointegration\nTime series 1950-2024\nBLS, OECD\nPending\n\n\n9\nTech intensity → fertility IV\nPanel with instruments\nOECD, World Bank\nPending\n\n\n10\nPolicy intent documents\nFOIA, Congressional records\nArchives\nPending\n\n\n11\nDebt crisis probability\nSovereign debt crisis database\nReinhart & Rogoff\nPending\n\n\n12\nSafety spending vs. debt\nGovernment spending by category\nOECD\nPending\n\n\n13\nGold vs. Fiat comparison\nTFR 1950-70 vs. 1971-2024\nUN Population\nPending\n\n\n14\nFertility-growth elasticity\nRegression dataset\nMultiple\nPending\n\n\n15\nComprehensive cross-country\nAll variables, all countries\nMultiple\nPending\n\n\n16\nIsrael subpopulation\nFertility by religious group\nIsrael CBS\nPending"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-b-technical-specifications-for-key-tests",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-b-technical-specifications-for-key-tests",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix B: Technical Specifications for Key Tests",
    "text": "Appendix B: Technical Specifications for Key Tests\n\nB.1 Structural Break Test (Chow Test)\nNull hypothesis: No structural break at 1971 Alternative: Structural break at 1971\nTest statistic: \\[F = \\frac{(RSS_r - RSS_u)/k}{RSS_u/(n-2k)} \\sim F_{k, n-2k}\\]\nwhere: - \\(RSS_r\\) = residual sum of squares (restricted model, no break) - \\(RSS_u\\) = residual sum of squares (unrestricted model, break at 1971) - \\(k\\) = number of parameters - \\(n\\) = number of observations\nReject null if: \\(F &gt; F_{critical}\\) at 5% significance level\n\n\nB.2 Interrupted Time Series Specification\nFull specification: \\[Y_{i,t} = \\alpha_i + \\beta_1 \\cdot Year_t + \\beta_2 \\cdot Post1971_t + \\beta_3 \\cdot (Post1971_t \\times Year_t) + \\gamma \\cdot X_{i,t} + \\varepsilon_{i,t}\\]\nInterpretation: - \\(\\alpha_i\\) = country fixed effects - \\(\\beta_1\\) = pre-1971 trend - \\(\\beta_2\\) = level shift at 1971 - \\(\\beta_3\\) = change in trend post-1971 - \\(X_{i,t}\\) = control variables\nKey test: \\(H_0: \\beta_3 = 0\\) vs. \\(H_A: \\beta_3 \\neq 0\\)\n\n\nB.3 Fiat Exploitation Index Construction\nComponents: 1. Debt Accumulation: \\(\\Delta(Debt/GDP)_{1971-2024}\\) 2. Deficit Intensity: Average (Deficit/GDP) over 1971-2024 3. Monetary Expansion: \\(\\Delta \\log(M2)\\) above GDP growth 4. Real Rate Suppression: Average \\((r_{natural} - r_{actual})\\)\nNormalization: Each component standardized to [0,1]\nAggregation: \\[FEI_i = 0.3 \\cdot Debt_i + 0.3 \\cdot Deficit_i + 0.2 \\cdot Monetary_i + 0.2 \\cdot RateSuppression_i\\]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-c-alternative-explanations-and-how-to-rule-them-out",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-c-alternative-explanations-and-how-to-rule-them-out",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix C: Alternative Explanations and How to Rule Them Out",
    "text": "Appendix C: Alternative Explanations and How to Rule Them Out\n\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nPrediction\nOur Prediction\nDistinguishing Test\n\n\n\n\nThe Pill (contraception)\nGradual effect through 1960s\nSharp break at 1971\nStructural break test\n\n\nEducation expansion\nSmooth decline as education rises\nAcceleration post-1971\nInteraction: education × post1971\n\n\nUrbanization\nOngoing effect (no break)\nBreak at 1971\nChow test controlling for urbanization\n\n\nSecularization\nGradual effect\nBreak at 1971\nControl for religious attendance\n\n\nWomen’s lib movement\nEffect through 1960s-70s\nSpecific break at 1971\nTiming of policy changes\n\n\n\nCombined test: \\[TFR_{i,t} = \\alpha + \\beta_1 Post1971 + \\beta_2 Pill + \\beta_3 Education + \\beta_4 Urban + \\beta_5 Secular + \\varepsilon_{i,t}\\]\nIf our hypothesis is correct: \\(\\beta_1\\) remains significant and large even controlling for all alternatives.\n\nEND OF PAPER"
  },
  {
    "objectID": "notebooks/Empirical Validation of the 1971 Fiscal Structural Break - Analysis of U.S Real Individual Income Tax Revenue.html",
    "href": "notebooks/Empirical Validation of the 1971 Fiscal Structural Break - Analysis of U.S Real Individual Income Tax Revenue.html",
    "title": "Empirical Validation of the 1971 Fiscal Structural Break: Analysis of U.S. Real Individual Income Tax Revenue",
    "section": "",
    "text": "\"\"\"\nSimple script to download US Individual Income Tax / GDP data from FRED\nNo complex dependencies - just pandas and requests\n\"\"\"\n\nimport pandas as pd\nimport requests\nfrom io import StringIO\n\nprint(\"Downloading US Individual Income Tax and GDP data from FRED...\")\nprint(\"=\" * 80)\n\n# FRED series IDs\n# W006RC1Q027SBEA = Individual Income Tax Receipts (quarterly, billions)\n# GDP = Gross Domestic Product (quarterly, billions)\n\ndef download_fred_series(series_id, series_name):\n    \"\"\"Download a single series from FRED as CSV.\"\"\"\n    url = f\"https://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}\"\n    \n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        \n        # Read CSV\n        df = pd.read_csv(StringIO(response.text))\n        df.columns = ['DATE', series_name]\n        df['DATE'] = pd.to_datetime(df['DATE'])\n        df[series_name] = pd.to_numeric(df[series_name], errors='coerce')\n        \n        print(f\"✓ Downloaded {series_name}: {len(df)} observations\")\n        return df\n    \n    except Exception as e:\n        print(f\"✗ Error downloading {series_name}: {e}\")\n        return None\n\n# Download data\nprint(\"\\n1. Downloading Individual Income Tax receipts...\")\ntax_df = download_fred_series('W006RC1Q027SBEA', 'IndividualIncomeTax')\n\nDownloading US Individual Income Tax and GDP data from FRED...\n================================================================================\n\n1. Downloading Individual Income Tax receipts...\n✓ Downloaded IndividualIncomeTax: 315 observations\n\n\n\ntax_df.tail()\n\n\n\n\n\n\n\n\nDATE\nIndividualIncomeTax\n\n\n\n\n310\n2024-07-01\n3141.087\n\n\n311\n2024-10-01\n3202.119\n\n\n312\n2025-01-01\n3246.051\n\n\n313\n2025-04-01\n3455.060\n\n\n314\n2025-07-01\n3599.024\n\n\n\n\n\n\n\n\ntax_df_indexed = tax_df.set_index(\"DATE\")\n\n\nimport pandas as pd\n\ndef to_annual_series(\n    tax_df: pd.DataFrame,\n    date_col: str = \"DATE\",\n    value_col: str = \"IndividualIncomeTax\",\n    agg: str = \"mean\",  # \"mean\" (default) or \"sum\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert a quarterly (or higher frequency) dataframe with a DATE column\n    into an annual dataframe with columns: Year, IndividualIncomeTax_Annual.\n\n    agg:\n      - \"mean\": good for level/rate-like series (e.g., % of GDP, index levels, rates)\n      - \"sum\" : good for flow series (e.g., revenues within year)\n    \"\"\"\n    df = tax_df.copy()\n\n    # If DATE is the index, bring it back as a column\n    if date_col not in df.columns and df.index.name == date_col:\n        df = df.reset_index()\n\n    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n    df = df.dropna(subset=[date_col, value_col])\n\n    df[\"Year\"] = df[date_col].dt.year.astype(int)\n\n    if agg not in (\"mean\", \"sum\", \"median\"):\n        raise ValueError(\"agg must be one of: 'mean', 'sum', 'median'\")\n\n    annual = (\n        df.groupby(\"Year\", as_index=False)[value_col]\n          .agg({f\"{value_col}_Annual\": agg})\n          .sort_values(\"Year\")\n          .reset_index(drop=True)\n    )\n    return annual\n\n\nannual_tax = to_annual_series(tax_df, value_col=\"IndividualIncomeTax\", agg=\"mean\")\nannual_tax.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\n\n\n\n\n74\n2021\n2675.071500\n\n\n75\n2022\n3252.924000\n\n\n76\n2023\n2918.060500\n\n\n77\n2024\n3118.884750\n\n\n78\n2025\n3433.378333\n\n\n\n\n\n\n\n\ncpi_df = download_fred_series('CPIAUCSL', 'CPI')\ncpi_df.tail()\nannual_cpi = to_annual_series(tax_df, value_col=\"CPI\", agg=\"mean\")\nannual_cpi.tail()\n\n✓ Downloaded CPI: 947 observations\n\n\n\n\n\n\n\n\n\nYear\nCPI_Annual\n\n\n\n\n74\n2021\n270.967917\n\n\n75\n2022\n292.625417\n\n\n76\n2023\n304.704167\n\n\n77\n2024\n313.697833\n\n\n78\n2025\n321.577200\n\n\n\n\n\n\n\n\n# Step 2: Merge with your tax data\ndf = pd.merge(annual_tax, annual_cpi_df, on='Year', how='inner')\ndf.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\nCPI_Annual\n\n\n\n\n74\n2021\n2675.071500\n270.967917\n\n\n75\n2022\n3252.924000\n292.625417\n\n\n76\n2023\n2918.060500\n304.704167\n\n\n77\n2024\n3118.884750\n313.697833\n\n\n78\n2025\n3433.378333\n321.577200\n\n\n\n\n\n\n\n\n# Step 3: Calculate Real Tax (in 2024 dollars)\nlatest_cpi = annual_cpi[annual_cpi['Year'] == 2024]['CPI_Annual'].mean()\ndf['Real_Tax'] = (df['IndividualIncomeTax_Annual'] / df['CPI_Annual']) * latest_cpi\ndf.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\nCPI_Annual\nReal_Tax\n\n\n\n\n74\n2021\n2675.071500\n270.967917\n3096.913258\n\n\n75\n2022\n3252.924000\n292.625417\n3487.172175\n\n\n76\n2023\n2918.060500\n304.704167\n3004.190151\n\n\n77\n2024\n3118.884750\n313.697833\n3118.884750\n\n\n78\n2025\n3433.378333\n321.577200\n3349.252821\n\n\n\n\n\n\n\n\ndf = df.set_index('Year')\ndf['Real_Tax'].plot()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.api import OLS, add_constant\nfrom scipy import stats\n\ndef chow_test_tax_series(\n    df_annual: pd.DataFrame,\n    y_col: str,\n    break_year: int = 1971,\n    min_obs_each_side: int = 8,\n    save_csv_path: str = None\n):\n    \"\"\"\n    Chow test for a structural break at a known year on an annual series.\n\n    Model: y ~ const + t\n    where t is a normalized year index for numerical stability.\n\n    H0: No structural break (β_pre = β_post)\n    H1: Structural break exists\n    \"\"\"\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"CHOW TEST FOR STRUCTURAL BREAK\")\n    print(\"=\" * 80)\n\n    # Clean and keep what we need\n    df_clean = df_annual[[\"Year\", y_col]].dropna().copy()\n    df_clean = df_clean.sort_values(\"Year\").reset_index(drop=True)\n\n    # Split (match your original: pre &lt; break_year, post &gt;= break_year)\n    df_pre = df_clean[df_clean[\"Year\"] &lt; break_year].copy()\n    df_post = df_clean[df_clean[\"Year\"] &gt;= break_year].copy()\n\n    n_pre, n_post = len(df_pre), len(df_post)\n    n_total = n_pre + n_post\n\n    print(f\"\\nBreak year: {break_year}\")\n    print(f\"Observations pre-break: {n_pre}\")\n    print(f\"Observations post-break: {n_post}\")\n    print(f\"Total observations: {n_total}\")\n\n    if n_pre &lt; min_obs_each_side or n_post &lt; min_obs_each_side:\n        raise ValueError(\n            f\"Not enough annual observations on each side of {break_year}. \"\n            f\"Need at least {min_obs_each_side} each side. Got pre={n_pre}, post={n_post}.\"\n        )\n\n    # Normalize time index for stability (use the same reference for all)\n    base_year = df_clean[\"Year\"].min()\n    df_clean[\"t\"] = df_clean[\"Year\"] - base_year\n    df_pre[\"t\"] = df_pre[\"Year\"] - base_year\n    df_post[\"t\"] = df_post[\"Year\"] - base_year\n\n    # Pooled regression\n    X_pooled = add_constant(df_clean[\"t\"].astype(float))\n    y_pooled = df_clean[y_col].astype(float)\n    model_pooled = OLS(y_pooled, X_pooled).fit()\n    RSS_pooled = float(model_pooled.ssr)\n\n    # Separate regressions\n    X_pre = add_constant(df_pre[\"t\"].astype(float))\n    y_pre = df_pre[y_col].astype(float)\n    model_pre = OLS(y_pre, X_pre).fit()\n    RSS_pre = float(model_pre.ssr)\n\n    X_post = add_constant(df_post[\"t\"].astype(float))\n    y_post = df_post[y_col].astype(float)\n    model_post = OLS(y_post, X_post).fit()\n    RSS_post = float(model_post.ssr)\n\n    RSS_separate = RSS_pre + RSS_post\n\n    # Chow statistic\n    k = 2  # intercept + slope\n    df_num = k\n    df_den = n_total - 2 * k\n\n    F_stat = ((RSS_pooled - RSS_separate) / k) / (RSS_separate / df_den)\n    p_value = 1 - stats.f.cdf(F_stat, df_num, df_den)\n\n    critical_05 = stats.f.ppf(0.95, df_num, df_den)\n    critical_01 = stats.f.ppf(0.99, df_num, df_den)\n\n    print(\"\\n\" + \"-\" * 80)\n    print(\"RESULTS:\")\n    print(\"-\" * 80)\n    print(f\"\\nRSS Pooled (restricted): {RSS_pooled:.4f}\")\n    print(f\"RSS Separate (unrestricted): {RSS_separate:.4f}\")\n    print(f\"  RSS Pre-break: {RSS_pre:.4f}\")\n    print(f\"  RSS Post-break: {RSS_post:.4f}\")\n\n    print(f\"\\nChow F-statistic: {F_stat:.4f}\")\n    print(f\"Degrees of freedom: ({df_num}, {df_den})\")\n    print(f\"P-value: {p_value:.6f}\")\n\n    print(f\"\\nCritical values:\")\n    print(f\"  5% level: {critical_05:.4f}\")\n    print(f\"  1% level: {critical_01:.4f}\")\n\n    print(\"\\nConclusion:\")\n    if p_value &lt; 0.01:\n        result = \"Reject H0 (1%)\"\n        print(f\"  *** REJECT NULL HYPOTHESIS at 1% level ***\")\n        print(f\"  Strong evidence of structural break at {break_year}\")\n    elif p_value &lt; 0.05:\n        result = \"Reject H0 (5%)\"\n        print(f\"  ** REJECT NULL HYPOTHESIS at 5% level **\")\n        print(f\"  Significant evidence of structural break at {break_year}\")\n    elif p_value &lt; 0.10:\n        result = \"Reject H0 (10%)\"\n        print(f\"  * REJECT NULL HYPOTHESIS at 10% level *\")\n        print(f\"  Moderate evidence of structural break at {break_year}\")\n    else:\n        result = \"Fail to Reject H0\"\n        print(f\"  FAIL TO REJECT NULL HYPOTHESIS\")\n        print(f\"  Insufficient evidence of structural break at {break_year}\")\n\n    # Coefficients\n    print(\"\\n\" + \"-\" * 80)\n    print(\"REGRESSION COEFFICIENTS:\")\n    print(\"-\" * 80)\n\n    print(f\"\\nPre-{break_year} period:\")\n    print(f\"  Intercept: {model_pre.params.iloc[0]:.4f} (se: {model_pre.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_pre.params.iloc[1]:.4f} (se: {model_pre.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_pre.rsquared:.4f}\")\n\n    print(f\"\\nPost-{break_year} period:\")\n    print(f\"  Intercept: {model_post.params.iloc[0]:.4f} (se: {model_post.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_post.params.iloc[1]:.4f} (se: {model_post.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_post.rsquared:.4f}\")\n\n    print(f\"\\nPooled (no break):\")\n    print(f\"  Intercept: {model_pooled.params.iloc[0]:.4f} (se: {model_pooled.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_pooled.params.iloc[1]:.4f} (se: {model_pooled.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_pooled.rsquared:.4f}\")\n\n    results_dict = {\n        \"Test\": \"Chow Test\",\n        \"Series\": y_col,\n        \"Break Year\": break_year,\n        \"F-statistic\": float(F_stat),\n        \"P-value\": float(p_value),\n        \"Critical Value (5%)\": float(critical_05),\n        \"Critical Value (1%)\": float(critical_01),\n        \"Result\": result,\n        \"Slope Pre\": float(model_pre.params.iloc[1]),\n        \"Slope Post\": float(model_post.params.iloc[1]),\n        \"Slope Change\": float(model_post.params.iloc[1] - model_pre.params.iloc[1]),\n        \"n_pre\": int(n_pre),\n        \"n_post\": int(n_post),\n    }\n\n    if save_csv_path is not None:\n        pd.DataFrame([results_dict]).to_csv(save_csv_path, index=False)\n\n    return results_dict\n\n\n# Chow test at 1971\nres = chow_test_tax_series(\n    df,\n    y_col=\"Real_Tax\",\n    break_year=1971\n)\n\nres\n\n\n================================================================================\nCHOW TEST FOR STRUCTURAL BREAK\n================================================================================\n\nBreak year: 1971\nObservations pre-break: 24\nObservations post-break: 55\nTotal observations: 79\n\n--------------------------------------------------------------------------------\nRESULTS:\n--------------------------------------------------------------------------------\n\nRSS Pooled (restricted): 3811995.5571\nRSS Separate (unrestricted): 3034840.0098\n  RSS Pre-break: 84072.8198\n  RSS Post-break: 2950767.1900\n\nChow F-statistic: 9.6029\nDegrees of freedom: (2, 75)\nP-value: 0.000194\n\nCritical values:\n  5% level: 3.1186\n  1% level: 4.8999\n\nConclusion:\n  *** REJECT NULL HYPOTHESIS at 1% level ***\n  Strong evidence of structural break at 1971\n\n--------------------------------------------------------------------------------\nREGRESSION COEFFICIENTS:\n--------------------------------------------------------------------------------\n\nPre-1971 period:\n  Intercept: 477.9496 (se: 24.4684)\n  Slope:     27.4835 (se: 1.8229)\n  R-squared: 0.9118\n\nPost-1971 period:\n  Intercept: 2.3028 (se: 107.0530)\n  Slope:     37.3621 (se: 2.0042)\n  R-squared: 0.8677\n\nPooled (no break):\n  Intercept: 349.9247 (se: 49.5949)\n  Slope:     31.2690 (se: 1.0978)\n  R-squared: 0.9133\n\n\n{'Test': 'Chow Test',\n 'Series': 'Real_Tax',\n 'Break Year': 1971,\n 'F-statistic': 9.602922372192094,\n 'P-value': 0.00019358783729239715,\n 'Critical Value (5%)': 3.118642128006125,\n 'Critical Value (1%)': 4.899877423111457,\n 'Result': 'Reject H0 (1%)',\n 'Slope Pre': 27.483480442522755,\n 'Slope Post': 37.362148635986124,\n 'Slope Change': 9.878668193463369,\n 'n_pre': 24,\n 'n_post': 55}"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html",
    "href": "notebooks/Education and QoL-Satisfaction.html",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "",
    "text": "This thesis explores the relationship between the length of formal education and overall quality of life (QoL), considering the evolving economic landscape influenced by M0/1/2/Consumer inflation. It hypothesizes that while extended education initially correlated with improved QoL through higher income, better employment, stable marriages, and overall well-being, lately, the benefits of prolonged education have diminished in the face of increasing economic pressures, competitive stress, and societal shifts. The study uses a combination of economic theory, quantitative data analysis, and sociological perspectives to examine how the changing cost-benefit dynamics of education affect life satisfaction in modern times.\n\n\nBackground\nEducation and Quality of Life: Historically, education has been seen as a key driver of social mobility and improved quality of life. Extended years in education have been associated with higher income, better job prospects, stable marriages, and improved health outcomes. M2 Inflation and Economic Environment: Over the past few decades, the global economic environment has been influenced by central banks’ monetary policies, including significant increases in M2 money supply. This has led to inflationary pressures, impacting the cost of education, living standards, and overall socioeconomic structures.\n1.2 Research Problem\nThe traditional view that more education equates to better QoL is being challenged by new economic realities. Increasing costs of education, diminishing returns on educational investment, and changing societal expectations have altered the landscape.\n1.3 Hypothesis\nUp to a certain period, increased years of education correlated positively with improvements in QoL. However, as M0/1/2/Consumer inflation increased, leading to rising educational costs and altered economic dynamics, the benefits of prolonged education have diminished, even potentially reversing in certain cases.\n\n\n\n\n\n\n\nIncome and Job Stability: Traditionally, longer years in education have been associated with higher income and better job stability. Research consistently shows that individuals with more education tend to earn more over their lifetimes and are less likely to be unemployed. This connection has been a key driver for the push towards extended education in many societies​. No longer True.\nSocial Status and Mobility: Extended education has also been linked with higher social status and greater social mobility. The credentials obtained through prolonged education often serve as markers of social class, allowing individuals to access higher social and professional circles. No longer True.\n\n\n\n\n\nWage Stagnation: In recent decades, there has been growing concern that despite increasing levels of education, wages have not kept pace with productivity gains. This wage stagnation, coupled with rising education costs, has led to a situation where the financial returns on education may no longer justify the investment, thus reducing the perceived value of long-term education in improving QoL​ (World Bank).\nUnderemployment: Many graduates find themselves in jobs that do not require the level of education they have attained, leading to underemployment. This mismatch between education and job requirements can contribute to dissatisfaction and frustration.\n\n\n\n\n\nMental Health Impacts: As the demand for higher education has increased, so has the competition, leading to significant stress among students. The pressure to perform well academically to secure top-tier jobs has contributed to a rise in mental health issues, including anxiety and depression. This competitive stress can negate some of the QoL improvements associated with higher education​ (World Bank).\nWork-Life Imbalance: The need to excel in education often leads to work-life imbalances, where students and young professionals may sacrifice leisure and family time for academic or career success, potentially reducing overall life satisfaction.\n\n\n\n\n\nHealth Implications: Higher levels of education are often associated with sedentary jobs, such as office work, which can lead to health issues like obesity, cardiovascular diseases, and mental health problems. The sedentary lifestyle that accompanies many highly educated professions may undermine the health benefits that should accompany better employment and income​ (Our World in Data).\nReduced Physical Activity: As people spend more time in education and subsequently in knowledge-intensive jobs, they may have less time for physical activities, which negatively impacts their overall well-being. T-leves for men.\n\n\n\n\n\nMarital Satisfaction and Selection Pressure: Educated individuals, especially women, may face higher expectations in partner selection, leading to delays in marriage or dissatisfaction due to mismatched expectations. The emphasis on finding a partner with similar educational or socioeconomic status can create additional stress and reduce life satisfaction​ (Our World in Data).\nFamily Dynamics: Higher education levels can lead to different expectations in family roles, potentially causing conflicts or dissatisfaction within marriages, especially if traditional roles are challenged.\n\n\n\n\n\nEntrepreneurial Barriers: Educated individuals may feel trapped in their career paths, unable to pivot to entrepreneurship or other non-traditional roles without risking social stigma. This perception of certain businesses as “lower socioeconomic” can prevent them from pursuing potentially fulfilling and profitable ventures​ (World Bank).\nRisk Aversion: Higher education often leads to risk aversion, where individuals prefer the stability of employment over the uncertainties of starting a business. This conservative approach can limit their opportunities for significant QoL improvements through entrepreneurship.\n\n\n\n\n\nJob Dissatisfaction: Many graduates find themselves in service-oriented jobs that may not align with their education or career aspirations. The servitude nature of these jobs, coupled with the disconnect from their education, can lead to job dissatisfaction and lower life satisfaction​.\nEconomic Pressure: The necessity to repay student loans and meet living expenses often forces educated individuals into jobs that do not utilize their full potential, leading to feelings of underachievement and frustration.\n\n\n\n\n\nSocioeconomic Barriers: Entrepreneurship increasingly appears to be dominated by individuals with access to significant capital, often from affluent backgrounds. This reduces the chances for those from less privileged backgrounds, even if they are highly educated, to pursue entrepreneurial opportunities​ (World Bank).\nVC and Networking Challenges: Access to venture capital and entrepreneurial networks is often limited to those with the right connections or backgrounds, making it difficult for highly educated individuals without these advantages to succeed in starting their own businesses.\n\n\n\n\n\nFinancial Independence: As individuals realize the time and stress associated with traditional employment, many are turning to passive investing strategies, such as investing in index funds (e.g., SPY), to achieve financial independence and improve QoL. This shift reflects a growing understanding that financial security can be achieved through means other than prolonged education and employment​ (World Bank).\nShift in Priorities: The realization that passive income (and/or wealth creation) from investments can provide a more stable and less stressful life has led many to question the traditional emphasis on education as the primary path to a better QoL.\n\n\n\n\n\nNetworking and Social Capital: Increasingly, education is seen as a means to gain entry into exclusive professional networks (e.g., MBA, IB, VC, PE) rather than purely for knowledge acquisition. This shift has implications for QoL, as the primary value of education becomes social capital rather than personal or intellectual development​ (Our World in Data).\nDecline in Knowledge-Based QoL: As access to knowledge becomes more democratized through the internet, the direct impact of education on QoL through knowledge acquisition has diminished. The focus on networking rather than knowledge challenges the traditional notion of education as a means to improve QoL through intellectual growth.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom statsmodels.tsa.stattools import grangercausalitytests\nimport pandas_datareader.data as web\nimport requests\n\nfrom io import StringIO\n\nstart_date_str = '1979-01-01'\nstart_date = datetime.strptime(start_date_str,'%Y-%m-%d')\nend_date = datetime.today()"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#the-evolving-impact-of-education-on-quality-of-life-in-the-context-of-m012consumer-inflation-and-socioeconomic-shifts",
    "href": "notebooks/Education and QoL-Satisfaction.html#the-evolving-impact-of-education-on-quality-of-life-in-the-context-of-m012consumer-inflation-and-socioeconomic-shifts",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "",
    "text": "This thesis explores the relationship between the length of formal education and overall quality of life (QoL), considering the evolving economic landscape influenced by M0/1/2/Consumer inflation. It hypothesizes that while extended education initially correlated with improved QoL through higher income, better employment, stable marriages, and overall well-being, lately, the benefits of prolonged education have diminished in the face of increasing economic pressures, competitive stress, and societal shifts. The study uses a combination of economic theory, quantitative data analysis, and sociological perspectives to examine how the changing cost-benefit dynamics of education affect life satisfaction in modern times.\n\n\nBackground\nEducation and Quality of Life: Historically, education has been seen as a key driver of social mobility and improved quality of life. Extended years in education have been associated with higher income, better job prospects, stable marriages, and improved health outcomes. M2 Inflation and Economic Environment: Over the past few decades, the global economic environment has been influenced by central banks’ monetary policies, including significant increases in M2 money supply. This has led to inflationary pressures, impacting the cost of education, living standards, and overall socioeconomic structures.\n1.2 Research Problem\nThe traditional view that more education equates to better QoL is being challenged by new economic realities. Increasing costs of education, diminishing returns on educational investment, and changing societal expectations have altered the landscape.\n1.3 Hypothesis\nUp to a certain period, increased years of education correlated positively with improvements in QoL. However, as M0/1/2/Consumer inflation increased, leading to rising educational costs and altered economic dynamics, the benefits of prolonged education have diminished, even potentially reversing in certain cases.\n\n\n\n\n\n\n\nIncome and Job Stability: Traditionally, longer years in education have been associated with higher income and better job stability. Research consistently shows that individuals with more education tend to earn more over their lifetimes and are less likely to be unemployed. This connection has been a key driver for the push towards extended education in many societies​. No longer True.\nSocial Status and Mobility: Extended education has also been linked with higher social status and greater social mobility. The credentials obtained through prolonged education often serve as markers of social class, allowing individuals to access higher social and professional circles. No longer True.\n\n\n\n\n\nWage Stagnation: In recent decades, there has been growing concern that despite increasing levels of education, wages have not kept pace with productivity gains. This wage stagnation, coupled with rising education costs, has led to a situation where the financial returns on education may no longer justify the investment, thus reducing the perceived value of long-term education in improving QoL​ (World Bank).\nUnderemployment: Many graduates find themselves in jobs that do not require the level of education they have attained, leading to underemployment. This mismatch between education and job requirements can contribute to dissatisfaction and frustration.\n\n\n\n\n\nMental Health Impacts: As the demand for higher education has increased, so has the competition, leading to significant stress among students. The pressure to perform well academically to secure top-tier jobs has contributed to a rise in mental health issues, including anxiety and depression. This competitive stress can negate some of the QoL improvements associated with higher education​ (World Bank).\nWork-Life Imbalance: The need to excel in education often leads to work-life imbalances, where students and young professionals may sacrifice leisure and family time for academic or career success, potentially reducing overall life satisfaction.\n\n\n\n\n\nHealth Implications: Higher levels of education are often associated with sedentary jobs, such as office work, which can lead to health issues like obesity, cardiovascular diseases, and mental health problems. The sedentary lifestyle that accompanies many highly educated professions may undermine the health benefits that should accompany better employment and income​ (Our World in Data).\nReduced Physical Activity: As people spend more time in education and subsequently in knowledge-intensive jobs, they may have less time for physical activities, which negatively impacts their overall well-being. T-leves for men.\n\n\n\n\n\nMarital Satisfaction and Selection Pressure: Educated individuals, especially women, may face higher expectations in partner selection, leading to delays in marriage or dissatisfaction due to mismatched expectations. The emphasis on finding a partner with similar educational or socioeconomic status can create additional stress and reduce life satisfaction​ (Our World in Data).\nFamily Dynamics: Higher education levels can lead to different expectations in family roles, potentially causing conflicts or dissatisfaction within marriages, especially if traditional roles are challenged.\n\n\n\n\n\nEntrepreneurial Barriers: Educated individuals may feel trapped in their career paths, unable to pivot to entrepreneurship or other non-traditional roles without risking social stigma. This perception of certain businesses as “lower socioeconomic” can prevent them from pursuing potentially fulfilling and profitable ventures​ (World Bank).\nRisk Aversion: Higher education often leads to risk aversion, where individuals prefer the stability of employment over the uncertainties of starting a business. This conservative approach can limit their opportunities for significant QoL improvements through entrepreneurship.\n\n\n\n\n\nJob Dissatisfaction: Many graduates find themselves in service-oriented jobs that may not align with their education or career aspirations. The servitude nature of these jobs, coupled with the disconnect from their education, can lead to job dissatisfaction and lower life satisfaction​.\nEconomic Pressure: The necessity to repay student loans and meet living expenses often forces educated individuals into jobs that do not utilize their full potential, leading to feelings of underachievement and frustration.\n\n\n\n\n\nSocioeconomic Barriers: Entrepreneurship increasingly appears to be dominated by individuals with access to significant capital, often from affluent backgrounds. This reduces the chances for those from less privileged backgrounds, even if they are highly educated, to pursue entrepreneurial opportunities​ (World Bank).\nVC and Networking Challenges: Access to venture capital and entrepreneurial networks is often limited to those with the right connections or backgrounds, making it difficult for highly educated individuals without these advantages to succeed in starting their own businesses.\n\n\n\n\n\nFinancial Independence: As individuals realize the time and stress associated with traditional employment, many are turning to passive investing strategies, such as investing in index funds (e.g., SPY), to achieve financial independence and improve QoL. This shift reflects a growing understanding that financial security can be achieved through means other than prolonged education and employment​ (World Bank).\nShift in Priorities: The realization that passive income (and/or wealth creation) from investments can provide a more stable and less stressful life has led many to question the traditional emphasis on education as the primary path to a better QoL.\n\n\n\n\n\nNetworking and Social Capital: Increasingly, education is seen as a means to gain entry into exclusive professional networks (e.g., MBA, IB, VC, PE) rather than purely for knowledge acquisition. This shift has implications for QoL, as the primary value of education becomes social capital rather than personal or intellectual development​ (Our World in Data).\nDecline in Knowledge-Based QoL: As access to knowledge becomes more democratized through the internet, the direct impact of education on QoL through knowledge acquisition has diminished. The focus on networking rather than knowledge challenges the traditional notion of education as a means to improve QoL through intellectual growth.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom statsmodels.tsa.stattools import grangercausalitytests\nimport pandas_datareader.data as web\nimport requests\n\nfrom io import StringIO\n\nstart_date_str = '1979-01-01'\nstart_date = datetime.strptime(start_date_str,'%Y-%m-%d')\nend_date = datetime.today()"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#long-years-in-education-job-stability-and-income-mobility-1",
    "href": "notebooks/Education and QoL-Satisfaction.html#long-years-in-education-job-stability-and-income-mobility-1",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "1. Long Years in Education, Job Stability and Income Mobility:",
    "text": "1. Long Years in Education, Job Stability and Income Mobility:\nTraditionally, more years in education have been strongly associated with higher income and better job stability. However, several factors, including M0/1/2/Consumer inflation and technological disruption, have weakened this association in recent years."
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#career-sharpe-ratio-formal-definition",
    "href": "notebooks/Education and QoL-Satisfaction.html#career-sharpe-ratio-formal-definition",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "Career Sharpe Ratio — Formal Definition",
    "text": "Career Sharpe Ratio — Formal Definition\n\nExpected real earnings\n\\[\nE_t\n=\n\\frac{W_t}{P_t / 100}\n\\cdot\n(1 - u_t)\n\\]\nwhere:\n\\[\n\\begin{aligned}\nW_t & = \\text{nominal median earnings at time } t \\\\\nP_t & = \\text{price level (CPI index, base } 100) \\\\\nu_t & = \\text{unemployment rate at time } t\n\\end{aligned}\n\\]\n\n\n\nCareer return (growth)\n\\[\nr_t\n=\n\\ln(E_t)\n-\n\\ln(E_{t-1})\n\\]\n\n\n\nCareer Sharpe Ratio\nLet:\n\\[\n\\mu_r = \\mathbb{E}[r_t]\n\\]\n\\[\n\\sigma_r = \\sqrt{\\mathbb{V}[r_t]}\n\\]\nLet ( k ) denote the number of periods per year (e.g., ( k = 4 ) for quarterly data).\n\\[\n\\text{CareerSharpe}\n=\n\\frac{k \\, \\mu_r}{\\sqrt{k} \\, \\sigma_r}\n\\]\nor equivalently,\n\\[\n\\text{CareerSharpe}\n=\n\\frac{\\mathbb{E}[r_t]}{\\sqrt{\\mathbb{V}[r_t]}}\n\\cdot\n\\sqrt{k}\n\\]\n\n\n\nInterpretation\n\\[\n\\text{CareerSharpe} &gt; 0\n\\;\\Rightarrow\\;\n\\text{growth dominates volatility (stable career)}\n\\]\n\\[\n\\text{CareerSharpe} = 0\n\\;\\Rightarrow\\;\n\\text{growth equals instability}\n\\]\n\\[\n\\text{CareerSharpe} &lt; 0\n\\;\\Rightarrow\\;\n\\text{volatility dominates growth (fragile career)}\n\\]\n\n\n\nIndividual-level extension\n\\[\n\\text{CareerSharpe}_i\n=\n\\frac{\\mathbb{E}[r_{i,t}]}{\\sqrt{\\mathbb{V}[r_{i,t}]}}\n\\cdot\n\\sqrt{k}\n\\]\n\nEARNING_FRED_SERIES = {\n    # ------------------------------------------------------------------\n    # Median usual weekly nominal earnings\n    # Full-time wage & salary workers, age 25+\n    # Quarterly\n    # ------------------------------------------------------------------\n    \"earn_lt_hs_q\": \"LEU0252920700Q\",   # Less than HS diploma, 25+\n    \"earn_hs_q\":    \"LEU0252917300Q\",   # HS graduates, no college, 25+\n    \"earn_some_q\":  \"LEU0254929400Q\",   # Some college or associate degree, 25+\n    \"earn_ba_q\":    \"LEU0252919100Q\",   # Bachelor's degree only, 25+\n    \"earn_adv_q\":   \"LEU0252919700Q\",   # Advanced degree, 25+\n\n    # ------------------------------------------------------------------\n    # Unemployment rates\n    # Age 25+, monthly, seasonally adjusted\n    # ------------------------------------------------------------------\n    \"unemp_lt_hs_m\": \"LNS14027659\",     # Less than HS, 25+\n    \"unemp_hs_m\":    \"LNS14027660\",     # HS graduates, no college, 25+\n    \"unemp_some_m\":  \"LNS14027689\",     # Some college or associate degree, 25+\n    \"unemp_ba_m\":    \"CGRA2024\",        # Bachelor's degree, 25+\n    \"unemp_adv_m\":   \"ADVRA25\",         # Advanced degree (Master's+), 25+\n\n    # ------------------------------------------------------------------\n    # Inflation\n    # ------------------------------------------------------------------\n    \"cpi_m\": \"CPIAUCSL\",                # CPI-U, monthly, seasonally adjusted\n}\n\n\nimport numpy as np\nimport pandas as pd\nfrom pandas_datareader import data as pdr\n\ndef fetch_fred(series_id: str, start=\"2000-01-01\"):\n    s = pdr.DataReader(series_id, \"fred\", start=start)\n    s.columns = [series_id]\n    return s\n\ndef annualized_sharpe(log_returns: pd.Series, periods_per_year=4) -&gt; float:\n    lr = log_returns.dropna()\n    if len(lr) &lt; 8:\n        return np.nan\n    mu = lr.mean() * periods_per_year\n    sig = lr.std(ddof=1) * np.sqrt(periods_per_year)\n    return float(mu / sig) if sig &gt; 0 else np.nan\n\ndef to_quarterly_period_mean(monthly: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Convert a monthly DatetimeIndex series to quarterly series indexed by PeriodIndex('Q'),\n    using mean within quarter.\n    \"\"\"\n    q = monthly.copy()\n    q.index = q.index.to_period(\"Q\")\n    return q.groupby(level=0).mean()\n\ndef to_quarterly_period_last(monthly: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Sometimes you may prefer end-of-quarter value instead of mean.\n    \"\"\"\n    q = monthly.copy()\n    q.index = q.index.to_period(\"Q\")\n    return q.groupby(level=0).last()\n\ndef to_quarterly_period_from_quarterly(quarterly: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Convert quarterly DatetimeIndex (often quarter-start dates on FRED) to PeriodIndex('Q').\n    \"\"\"\n    q = quarterly.copy()\n    q.index = q.index.to_period(\"Q\")\n    return q\n\ndef career_sharpe_for_group_period(\n    earn_q: pd.Series,     # quarterly earnings (nominal weekly)\n    unemp_m: pd.Series,    # monthly unemployment rate in %\n    cpi_m: pd.Series,      # monthly CPI index\n    use_cpi_mean=True,\n    use_unemp_mean=True,\n) -&gt; pd.DataFrame:\n    # --- Convert to quarterly PeriodIndex('Q') ---\n    earn_qp = to_quarterly_period_from_quarterly(earn_q)\n\n    unemp_qp = to_quarterly_period_mean(unemp_m) if use_unemp_mean else to_quarterly_period_last(unemp_m)\n    unemp_qp = unemp_qp / 100.0  # % -&gt; fraction\n\n    cpi_qp = to_quarterly_period_mean(cpi_m) if use_cpi_mean else to_quarterly_period_last(cpi_m)\n\n    # --- Align on common quarters ---\n    idx = earn_qp.dropna().index.intersection(unemp_qp.dropna().index).intersection(cpi_qp.dropna().index)\n    earn_qp = earn_qp.loc[idx]\n    unemp_qp = unemp_qp.loc[idx]\n    cpi_qp = cpi_qp.loc[idx]\n\n    # --- Compute proxy expected real earnings ---\n    real_weekly = earn_qp / (cpi_qp / 100.0)\n    expected_real_weekly = real_weekly * (1.0 - unemp_qp)\n\n    # Quarterly log returns\n    r = np.log(expected_real_weekly).diff()\n\n    out = pd.DataFrame({\n        \"earn_nominal_weekly\": earn_qp,\n        \"cpi\": cpi_qp,\n        \"unemp_rate\": unemp_qp,\n        \"real_weekly\": real_weekly,\n        \"expected_real_weekly\": expected_real_weekly,\n        \"log_return_qoq\": r,\n    })\n    out.attrs[\"career_sharpe\"] = annualized_sharpe(out[\"log_return_qoq\"])\n    return out\n\ndef run_all_fred_series_for_career_sharpe(FRED_SERIES, start=\"2000-01-01\"):\n    earnings = {\n        \"lt_hs\": fetch_fred(FRED_SERIES[\"earn_lt_hs_q\"], start).iloc[:, 0],\n        \"hs\":    fetch_fred(FRED_SERIES[\"earn_hs_q\"], start).iloc[:, 0],\n        \"some\":  fetch_fred(FRED_SERIES[\"earn_some_q\"], start).iloc[:, 0],\n        \"ba\":    fetch_fred(FRED_SERIES[\"earn_ba_q\"], start).iloc[:, 0],\n        \"adv\":   fetch_fred(FRED_SERIES[\"earn_adv_q\"], start).iloc[:, 0],\n    }\n    unemp = {\n        \"lt_hs\": fetch_fred(FRED_SERIES[\"unemp_lt_hs_m\"], start).iloc[:, 0],\n        \"hs\":    fetch_fred(FRED_SERIES[\"unemp_hs_m\"], start).iloc[:, 0],\n        \"some\":  fetch_fred(FRED_SERIES[\"unemp_some_m\"], start).iloc[:, 0],\n        \"ba\":    fetch_fred(FRED_SERIES[\"unemp_ba_m\"], start).iloc[:, 0],\n    }\n    cpi = fetch_fred(FRED_SERIES[\"cpi_m\"], start).iloc[:, 0]\n\n    # Advanced-degree unemployment fallback\n    try:\n        unemp[\"adv\"] = fetch_fred(FRED_SERIES[\"unemp_adv_m\"], start).iloc[:, 0]\n    except Exception:\n        unemp[\"adv\"] = fetch_fred(\"LNS14027662\", start).iloc[:, 0]\n\n    results = {}\n    rows = []\n    for k in [\"lt_hs\", \"hs\", \"some\", \"ba\", \"adv\"]:\n        df = career_sharpe_for_group_period(\n            earn_q=earnings[k],\n            unemp_m=unemp[k],\n            cpi_m=cpi,\n            use_cpi_mean=True,\n            use_unemp_mean=True,\n        )\n        results[k] = df\n        lr = df[\"log_return_qoq\"].dropna()\n        rows.append({\n            \"education_group\": k,\n            \"career_sharpe\": df.attrs[\"career_sharpe\"],\n            \"start_q\": str(df.index.min()) if not df.empty else None,\n            \"end_q\": str(df.index.max()) if not df.empty else None,\n            \"n_quarters\": int(lr.shape[0]),\n        })\n\n    sharpe_table = pd.DataFrame(rows).sort_values(\"career_sharpe\", ascending=False)\n    return sharpe_table, results\n\n\nsharpe_table, results = run_all_fred_series_for_career_sharpe(EARNING_FRED_SERIES, start=start_date_str)\nprint(sharpe_table.to_string(index=False))\n\neducation_group  career_sharpe start_q  end_q  n_quarters\n            adv       0.026205  2000Q1 2025Q3         102\n          lt_hs       0.024898  2000Q1 2025Q3         102\n             hs       0.017300  2000Q1 2025Q3         102\n           some      -0.026485  2000Q1 2025Q3         102\n             ba      -0.031516  2000Q1 2025Q3         102\n\n\n\nresults[\"ba\"].tail()\n\n\n\n\n\n\n\n\nearn_nominal_weekly\ncpi\nunemp_rate\nreal_weekly\nexpected_real_weekly\nlog_return_qoq\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2024Q3\n1533\n314.182667\n0.082667\n487.932710\n447.596939\n-0.030950\n\n\n2024Q4\n1547\n316.538667\n0.056667\n488.723863\n461.029511\n0.029569\n\n\n2025Q1\n1603\n319.492000\n0.064667\n501.734003\n469.288537\n0.017756\n\n\n2025Q2\n1559\n320.800333\n0.057667\n485.972064\n457.947675\n-0.024463\n\n\n2025Q3\n1580\n323.288000\n0.091333\n488.728317\n444.091130\n-0.030725\n\n\n\n\n\n\n\n\nEDU_YEARS_MAP = {\n    \"lt_hs\": 10,\n    \"hs\":    12,\n    \"some\":  14,\n    \"ba\":    16,\n    \"adv\":   18,\n}\n\ndef rolling_career_sharpe_series(df, window=20, periods_per_year=4):\n    \"\"\"\n    Rolling Career Sharpe from a group's df (must contain log_return_qoq).\n    Annualized mean / annualized std with quarterly periods.\n    \"\"\"\n    r = df[\"log_return_qoq\"]\n\n    mu = r.rolling(window).mean() * periods_per_year\n    sig = r.rolling(window).std(ddof=1) * np.sqrt(periods_per_year)\n\n    return mu / sig\n\n\ndef plot_stacked_rolling_career_sharpe_semantic(results, window=20):\n    \"\"\"\n    Fixed semantic order (education ladder), with higher education closer to the zero line.\n    Uses consistent colors for the same group above and below zero.\n    \"\"\"\n\n    # --- Fixed semantic order (higher ed near zero) ---\n    groups = [\"adv\", \"ba\", \"some\", \"hs\", \"lt_hs\"]  # fixed ladder order\n\n    # ---- Build aligned DataFrame of rolling Sharpe ----\n    sharpe_series = []\n    for k in groups:\n        df = results.get(k)\n        if df is None or df.empty:\n            continue\n\n        mu = df[\"log_return_qoq\"].rolling(window).mean() * 4\n        sig = df[\"log_return_qoq\"].rolling(window).std() * np.sqrt(4)\n        sharpe_series.append((mu / sig).rename(k))\n\n    sharpe_df = pd.concat(sharpe_series, axis=1).dropna(how=\"all\")\n    sharpe_df.index = sharpe_df.index.to_timestamp()\n\n    # Ensure all groups exist & order is preserved\n    groups_present = [g for g in groups if g in sharpe_df.columns]\n    sharpe_df = sharpe_df[groups_present]\n\n    # ---- Split into positive and negative parts ----\n    pos = sharpe_df.clip(lower=0)\n    neg = sharpe_df.clip(upper=0)\n\n    # ---- Consistent colors per group ----\n    cmap = plt.get_cmap(\"tab10\")\n    color_map = {g: cmap(i % 10) for i, g in enumerate(groups_present)}\n    colors = [color_map[g] for g in groups_present]\n\n    # ---- Plot ----\n    plt.figure(figsize=(13, 6))\n\n    # Positive stack\n    plt.stackplot(\n        pos.index,\n        [pos[g].values for g in groups_present],\n        labels=groups_present,\n        colors=colors,\n        alpha=1.0\n    )\n\n    # Negative stack (same order, same colors)\n    plt.stackplot(\n        neg.index,\n        [neg[g].values for g in groups_present],\n        colors=colors,\n        alpha=1.0\n    )\n\n    plt.axhline(0, color=\"black\", lw=1)\n    plt.title(f\"Stacked Rolling Career Sharpe by Education Group ({window}Q window)\")\n    plt.ylabel(\"Career Sharpe (risk-adjusted stability)\")\n    plt.xlabel(\"Year\")\n    plt.legend(loc=\"upper left\", title=\"Education group\", ncol=2)\n    plt.grid(alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n# Usage:\nplot_stacked_rolling_career_sharpe_semantic(results, window=20)\n\n\n\n\n\n\n\n\n\ndef plot_sharpe_vs_edu_at_sample_ends(results, edu_years_map, window=20):\n    \"\"\"\n    Two-row plot (shared X axis: years of education):\n      Row 1: Sharpe at earliest available date (first non-NaN rolling value per group)\n      Row 2: Sharpe at latest available date (last rolling value per group)\n    \"\"\"\n\n    rows_early = []\n    rows_late = []\n\n    for group, df in results.items():\n        if df is None or df.empty or group not in edu_years_map:\n            continue\n\n        s = rolling_career_sharpe_series(df, window=window).dropna()\n        if s.empty:\n            continue\n\n        years = edu_years_map[group]\n\n        # Earliest defined rolling Sharpe (after window)\n        early_date = s.index[0]\n        early_val  = float(s.iloc[0])\n\n        # Latest rolling Sharpe (end of sample)\n        late_date = s.index[-1]\n        late_val  = float(s.iloc[-1])\n\n        rows_early.append({\n            \"group\": group,\n            \"years_education\": years,\n            \"career_sharpe\": early_val,\n            \"date\": early_date\n        })\n\n        rows_late.append({\n            \"group\": group,\n            \"years_education\": years,\n            \"career_sharpe\": late_val,\n            \"date\": late_date\n        })\n\n    df_early = pd.DataFrame(rows_early).sort_values(\"years_education\")\n    df_late  = pd.DataFrame(rows_late).sort_values(\"years_education\")\n\n    # If you want the \"early\" and \"late\" dates to be common across groups,\n    # you can display the range here:\n    early_dates = df_early[\"date\"].tolist()\n    late_dates  = df_late[\"date\"].tolist()\n\n    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 8), sharex=True)\n\n    # ---- Row 1: earliest ----\n    axes[0].plot(df_early[\"years_education\"], df_early[\"career_sharpe\"], marker=\"o\")\n    axes[0].axhline(0, color=\"black\", lw=1)\n    axes[0].set_title(\n        f\"Career Sharpe vs Years of Education (Earliest available rolling value, window={window}Q)\\n\"\n        f\"Dates shown vary by group; earliest among them: {min(early_dates)}\"\n    )\n    axes[0].set_ylabel(\"Career Sharpe\")\n    axes[0].grid(alpha=0.3)\n    for _, r in df_early.iterrows():\n        axes[0].annotate(r[\"group\"], (r[\"years_education\"], r[\"career_sharpe\"]),\n                         textcoords=\"offset points\", xytext=(6, 6), fontsize=9)\n\n    # ---- Row 2: latest ----\n    axes[1].plot(df_late[\"years_education\"], df_late[\"career_sharpe\"], marker=\"o\")\n    axes[1].axhline(0, color=\"black\", lw=1)\n    axes[1].set_title(\n        f\"Career Sharpe vs Years of Education (Latest rolling value, window={window}Q)\\n\"\n        f\"Latest among them: {max(late_dates)}\"\n    )\n    axes[1].set_ylabel(\"Career Sharpe\")\n    axes[1].set_xlabel(\"Years of education\")\n    axes[1].grid(alpha=0.3)\n    for _, r in df_late.iterrows():\n        axes[1].annotate(r[\"group\"], (r[\"years_education\"], r[\"career_sharpe\"]),\n                         textcoords=\"offset points\", xytext=(6, 6), fontsize=9)\n\n    plt.tight_layout()\n    plt.show()\n\n    return df_early, df_late\n\n\n# Usage:\ndf_early, df_late = plot_sharpe_vs_edu_at_sample_ends(results, EDU_YEARS_MAP, window=20)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom pandas_datareader import data as pdr\n\ndef fetch_us_recessions(start=\"1990-01-01\"):\n    \"\"\"\n    Fetch US recession indicator (USREC) from FRED.\n    1 = recession, 0 = expansion.\n    \"\"\"\n    rec = pdr.DataReader(\"USREC\", \"fred\", start)\n    rec.index = pd.to_datetime(rec.index)\n    return rec\n\ndef recession_intervals(usrec: pd.DataFrame):\n    \"\"\"\n    Convert monthly USREC series into a list of (start, end) timestamps.\n    \"\"\"\n    rec = usrec[\"USREC\"]\n    intervals = []\n\n    in_rec = False\n    start = None\n\n    for date, val in rec.items():\n        if val == 1 and not in_rec:\n            start = date\n            in_rec = True\n        elif val == 0 and in_rec:\n            intervals.append((start, date))\n            in_rec = False\n\n    if in_rec:\n        intervals.append((start, rec.index[-1]))\n\n    return intervals\n\ndef plot_edu_sharpe_heatmap_with_recessions(\n    results,\n    edu_years_map,\n    window=20,\n    rec_start=\"1990-01-01\"\n):\n    # ---- Build panel (same as before) ----\n    panel = build_edu_sharpe_panel(results, edu_years_map, window=window)\n\n    Z = np.ma.masked_invalid(panel.T.values)\n    years = panel.columns.values\n    times = panel.index.values\n\n    # ---- Zero-centered normalization ----\n    vmax = np.nanmax(np.abs(Z))\n    norm = mcolors.TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)\n\n    # ---- Fetch recessions ----\n    usrec = fetch_us_recessions(start=rec_start)\n    rec_intervals = recession_intervals(usrec)\n\n    # ---- Plot ----\n    plt.figure(figsize=(14, 5))\n\n    plt.imshow(\n        Z,\n        aspect=\"auto\",\n        origin=\"lower\",\n        interpolation=\"nearest\",\n        cmap=\"RdYlGn\",\n        norm=norm,\n        extent=[0, len(times) - 1, years.min(), years.max()]\n    )\n\n    # X ticks (years)\n    n_xticks = min(10, len(times))\n    xtick_pos = np.linspace(0, len(times) - 1, n_xticks).astype(int)\n    xtick_lbl = [pd.to_datetime(times[i]).strftime(\"%Y\") for i in xtick_pos]\n    plt.xticks(xtick_pos, xtick_lbl)\n\n    # Y ticks (education years)\n    plt.yticks(years, [str(int(y)) for y in years])\n\n    # ---- Overlay recession bands ----\n    time_index = pd.to_datetime(times)\n\n    for start, end in rec_intervals:\n        # find indices overlapping the heatmap time range\n        if end &lt; time_index.min() or start &gt; time_index.max():\n            continue\n\n        x0 = np.searchsorted(time_index, start, side=\"left\")\n        x1 = np.searchsorted(time_index, end, side=\"right\")\n\n        plt.axvspan(\n            x0, x1,\n            color=\"black\",\n            alpha=0.3,   # subtle but visible\n            lw=0\n        )\n\n    cbar = plt.colorbar()\n    cbar.set_label(\"Rolling Career Sharpe\")\n\n    plt.title(\n        f\"Education–Career Sharpe Surface with US Recessions\\n\"\n        f\"Rolling window = {window} quarters\"\n    )\n    plt.xlabel(\"Year\")\n    plt.ylabel(\"Years of education\")\n\n    plt.tight_layout()\n    plt.show()\n\n    return panel\n\nplot_edu_sharpe_heatmap_with_recessions(\n    results,\n    EDU_YEARS_MAP,\n    window=20,\n    rec_start=\"1990-01-01\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n12\n14\n16\n18\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2005-01-01\n-0.217867\n0.030670\n-0.134112\n-0.058386\n0.177880\n\n\n2005-04-01\n-0.176164\n0.088045\n-0.236736\n0.051427\n-0.072408\n\n\n2005-07-01\n-0.007828\n0.005134\n-0.137005\n-0.082290\n-0.133007\n\n\n2005-10-01\n-0.054053\n0.026606\n-0.236067\n-0.012380\n0.135676\n\n\n2006-01-01\n0.011318\n0.126577\n-0.034653\n-0.141063\n0.101021\n\n\n...\n...\n...\n...\n...\n...\n\n\n2024-07-01\n-0.122012\n0.078718\n-0.081831\n-0.117491\n-0.003857\n\n\n2024-10-01\n0.044885\n0.197410\n0.045686\n-0.050311\n-0.087999\n\n\n2025-01-01\n-0.026605\n0.008222\n0.026629\n0.015947\n-0.048712\n\n\n2025-04-01\n0.243602\n0.465019\n0.290055\n0.326661\n-0.021351\n\n\n2025-07-01\n0.015073\n0.303110\n0.100364\n-0.100123\n-0.020048\n\n\n\n\n83 rows × 5 columns\n\n\n\n\nObservation from the chart\nEducation =/= insurance anymore\nHigher education:\n\nRaises mean income\nRaises volatility more\nLowers Career Sharpe\n\n\nAcross 2000–2025, risk-adjusted career outcomes in the US show no monotonic relationship with years of education; instead, education cohorts move together across macro regimes, with mid-to-upper education levels exhibiting the greatest downside during shocks and no group offering persistent career stability.\n\n\n\n\nSocial Status & Mobility\n\nIntergenerational Occupational Mobility of Men Born between 1950 and 1979\nRef - https://sci-hub.se/10.1353/foc.2006.0012\n\nimport seaborn as sns\n\n# Data from the table\ndata = {\n    \"Upper professional\": [42, 24, 7, 12, 0, 15],\n    \"Lower professional and clerical\": [29, 27, 7, 17, 0, 20],\n    \"Self-employed\": [29, 18, 16, 19, 0, 18],\n    \"Technical and skilled\": [17, 19, 6, 30, 1, 26],\n    \"Farm sector\": [14, 11, 8, 17, 13, 37],\n    \"Unskilled and service\": [16, 17, 6, 22, 1, 38]\n}\n\nindex_labels = [\"Upper professional\", \"Lower professional and clerical\", \"Self-employed\", \n                \"Technical and skilled\", \"Farm sector\", \"Unskilled and service\"]\n\ndf = pd.DataFrame(data, index=index_labels)\n\n# Create a heatmap\nplt.figure(figsize=(10, 6))\nsns.heatmap(df, annot=True, cmap=\"flare\", fmt=\"d\", linewidths=.5)\n\n# Add title and labels\nplt.title(\"Intergenerational Occupational Mobility of Men Born between 1950 and 1979 (Source: General Social Surveys, 1988–2004)\")\nplt.xlabel(\"Destination: Son's Occupation\")\nplt.ylabel(\"Origin: Father's Occupation\")\n\n# Display the heatmap\nplt.show()\n\n\n\n\n\n\n\n\nReference: H. Elizabeth Peters, “Patterns of Intergenerational Mobility in Income and Earnings,” Review of Economics and Statistics, 74(3), 1992, p. 460. ​ - https://sci-hub.se/10.1353/foc.2006.0012\nSummary\n\nHigh Mobility for Upper Professional Class: Sons whose fathers were in upper professional occupations have the highest likelihood (42%) of remaining in the upper professional category themselves. This indicates a strong intergenerational persistence of high-status occupations.\nLimited Upward Mobility for Lower Occupations: For sons of fathers in the unskilled and service sector, there is a significant tendency to remain in lower-status occupations. Only 16% of these sons move into upper professional occupations, and 38% remain in unskilled and service jobs, indicating limited upward mobility.\nSelf-Employment and Technical Occupations: Sons of self-employed fathers show some diversity in outcomes, with 16% remaining self-employed and 29% moving into upper professional occupations.\nTechnical and skilled occupations see a 30% persistence rate, but many also move into other sectors, such as upper professional (17%) and unskilled and service jobs (26%).\nFarm Sector Shows Unique Patterns: Sons of fathers in the farm sector show a high rate of persistence within that sector (13%), but many also move into unskilled and service jobs (37%).\n\nOverall Summary: The heatmap highlights a strong intergenerational persistence in occupational status, particularly for those at the upper and lower ends of the occupational hierarchy. Sons of fathers in higher-status occupations (upper professional) are more likely to remain in those occupations, while those from lower-status or manual labor occupations, such as unskilled and service or farm sectors, face more significant challenges in achieving upward mobility.\nThis data underscores the limited mobility for individuals from non-elite or lower occupational backgrounds, reinforcing the idea that economic and social barriers have a substantial impact on career outcomes across generations.\n\n# Data from the table\ndata = {\n    \"First\": [42, 26, 18, 15],\n    \"Second\": [28, 29, 24, 18],\n    \"Third\": [19, 27, 29, 25],\n    \"Fourth\": [12, 19, 29, 40],\n}\n\n# Creating DataFrame\ndf = pd.DataFrame(data, index=[\"First\", \"Second\", \"Third\", \"Fourth\"])\n\n# Creating the heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt=\"d\", cbar=True)\n\n# Adding titles and labels\nplt.title(\"Intergenerational Income Mobility: Probability of Son's Quartile Given Parent's Quartile\")\nplt.xlabel(\"Son's Income Quartile\")\nplt.ylabel(\"Parent's Income Quartile\")\n\n# Displaying the heatmap\nplt.show()\n\n\n\n\n\n\n\n\nIncome Mobility is Limited: The data shows that there is significant persistence in income status across generations, especially at the top and bottom of the income distribution. Intergenerational Mobility: While there is some mobility between quartiles, particularly in the middle, those at the extremes of the income distribution are more likely to stay there.\nEconomic Mobility Decline: Studies such as those by Raj Chetty and others have shown a clear decline in intergenerational economic mobility since the 1970s. The likelihood of children earning more than their parents has decreased, particularly for those from lower-income families.\nIncreasing Role of Education and Wealth: The role of education, especially from elite institutions, in securing higher incomes has become more pronounced. Wealth inequality has further exacerbated income inequality, making it harder for those from lower-income families to move up the economic ladder.\n\n\nChetty Paper 2014: Where Is the Land of Opportunity? The Geography of Intergenerational Mobility in the United States\nhttps://jenni.uchicago.edu/econ341/readings/Chetty_Hendren_Kline_etal_2014_QJE_v129_n4.pdf\nIntergenerational income mobility aims to capture how strongly a child’s economic position depends on that of their parents. A modern and robust way to measure this is the Rank–Rank Intergenerational Elasticity (IGE), which avoids issues of scale, inflation, and tail instability inherent in log-income regressions.\nThe core idea is to express both parent and child incomes as ranks in their respective national income distributions, normalized to the unit interval. Let the parent’s income rank be denoted by ( \\(R^{\\text{parent}}_i \\in [0,1]\\) ), and the child’s income rank by ( \\(R^{\\text{child}}_i \\in [0,1]\\)). The fundamental rank–rank regression is then written as:\n\\[\nR^{\\text{child}}_i = \\alpha + \\rho \\, R^{\\text{parent}}_i + \\varepsilon_i\n\\]\nHere, the coefficient ( \\(\\rho\\) ) is the rank–rank IGE. It measures the expected change in a child’s percentile rank associated with a one-percentile increase in parental rank. Equivalently, it can be interpreted as a slope:\n\\[\n\\rho = \\frac{\\partial R^{\\text{child}}}{\\partial R^{\\text{parent}}}\n\\]\nFrom a statistical standpoint, ( ) is estimated using ordinary least squares and can be expressed in covariance form as:\n\\[\n\\rho = \\frac{\\operatorname{Cov}\\left(R^{\\text{parent}}, R^{\\text{child}}\\right)}\n{\\operatorname{Var}\\left(R^{\\text{parent}}\\right)}\n\\]\nHowever IGE Rank-Rank Slope can be misleading (stable around 0.32 even though inequality keeps rising) at a national level (US or other countries) for variety of reasons -\n\n\n\n\n\n\n\nProblem\nWhy It Matters\n\n\n\n\n1. Zero-sum by construction\nIncome ranks must sum to the same total. If one child rises in rank, another must fall. As a result, the national average is mechanically centered (around 50), even if overall outcomes deteriorate.\n\n\n2. Hides local heterogeneity\nAggregation masks large geographic differences. For example, Charlotte (β = 0.40) and San Jose (β = 0.24) average to a seemingly “normal” β = 0.32, but this average describes no actual individual’s experience.\n\n\n3. Misses absolute collapse\nAbsolute mobility collapsed even while rank persistence stayed flat: in 1940, ~92% of children earned more than their parents, versus ~50% for 1980 cohorts—yet the rank–rank IGE remained around 0.34 throughout.\n\n\n\n\n\nNeed absolute income mobility\nRaj Chetty et al.,The fading American dream: Trends in absolute income mobility since 1940.\nScience 356,398-406(2017).DOI:10.1126/science.aal4617\nhttps://www.science.org/doi/10.1126/science.aal4617#:~:text=Using%20this%20methodology%2C%20we%20found,rates%20observed%20for%20recent%20cohorts.\n\n# =============================================================================\n# INFLATION DATA: Official CPI vs ShadowStats\n# =============================================================================\n\ndef build_inflation_series():\n    \"\"\"\n    Build inflation time series: Official CPI vs ShadowStats (chart-aligned).\n\n    IMPORTANT:\n    - ShadowStats does not publish a clean public time series (as far as typical public endpoints go).\n    - So here we use a chart-aligned annual approximation (1980–2023) that matches the\n      visual series in the provided ShadowStats image (1980-based alternate CPI).\n    - For 1970–1979 we default to official CPI (since the chart starts at ~1980).\n    \"\"\"\n\n    years = list(range(1970, 2024))\n\n    # -------------------------------------------------------------------------\n    # Official CPI (annual, your existing dict)\n    # -------------------------------------------------------------------------\n    official_rates = {\n        1970: 0.059, 1971: 0.043, 1972: 0.033, 1973: 0.062, 1974: 0.110,\n        1975: 0.091, 1976: 0.058, 1977: 0.065, 1978: 0.076, 1979: 0.113,\n        1980: 0.135, 1981: 0.103, 1982: 0.062, 1983: 0.032, 1984: 0.043,\n        1985: 0.036, 1986: 0.019, 1987: 0.036, 1988: 0.041, 1989: 0.048,\n        1990: 0.054, 1991: 0.042, 1992: 0.030, 1993: 0.030, 1994: 0.026,\n        1995: 0.028, 1996: 0.029, 1997: 0.023, 1998: 0.016, 1999: 0.022,\n        2000: 0.034, 2001: 0.028, 2002: 0.016, 2003: 0.023, 2004: 0.027,\n        2005: 0.034, 2006: 0.032, 2007: 0.029, 2008: 0.038, 2009: -0.004,\n        2010: 0.016, 2011: 0.032, 2012: 0.021, 2013: 0.015, 2014: 0.016,\n        2015: 0.001, 2016: 0.013, 2017: 0.021, 2018: 0.024, 2019: 0.018,\n        2020: 0.012, 2021: 0.047, 2022: 0.080, 2023: 0.041\n    }\n\n    # -------------------------------------------------------------------------\n    # ShadowStats (chart-aligned annual approximation, 1980–2023)\n    # These values are digitized/approximated from the supplied ShadowStats plot:\n    # \"SGS Alternate CPI, 1980-Based\" (YoY) through May 2023.\n    # Units are decimals (e.g., 0.12 = 12% YoY).\n    # -------------------------------------------------------------------------\n    shadowstats_rates_1980_2023 = {\n        1980: 0.1384, 1981: 0.1077, 1982: 0.0673, 1983: 0.0396, 1984: 0.0534,\n        1985: 0.0486, 1986: 0.0500, 1987: 0.0596, 1988: 0.0686, 1989: 0.0712,\n        1990: 0.0617, 1991: 0.0586, 1992: 0.0551, 1993: 0.0568, 1994: 0.0587,\n        1995: 0.0595, 1996: 0.0624, 1997: 0.0664, 1998: 0.0727, 1999: 0.0787,\n        2000: 0.0845, 2001: 0.0900, 2002: 0.0966, 2003: 0.1018, 2004: 0.1012,\n        2005: 0.1014, 2006: 0.1050, 2007: 0.1116, 2008: 0.1186, 2009: 0.0656,\n        2010: 0.0907, 2011: 0.0974, 2012: 0.1008, 2013: 0.0985, 2014: 0.0893,\n        2015: 0.0943, 2016: 0.0969, 2017: 0.0989, 2018: 0.1020, 2019: 0.0966,\n        2020: 0.0910, 2021: 0.1369, 2022: 0.1629, 2023: 0.1402\n    }\n\n    # Build ShadowStats series for full range 1970–2023\n    shadowstats_rates = {}\n    for y in years:\n        if y &lt; 1980:\n            # Chart doesn't cover these years; use official CPI as a conservative placeholder\n            shadowstats_rates[y] = official_rates[y]\n        elif y in shadowstats_rates_1980_2023:\n            shadowstats_rates[y] = shadowstats_rates_1980_2023[y]\n        else:\n            # Fallback (should not happen with the above dict)\n            shadowstats_rates[y] = 0.12\n\n    # -------------------------------------------------------------------------\n    # Build cumulative indices (1970 = 1.0)\n    # -------------------------------------------------------------------------\n    official_index = {1970: 1.0}\n    shadow_index = {1970: 1.0}\n\n    for year in range(1971, 2024):\n        official_index[year] = official_index[year - 1] * (1 + official_rates[year])\n        shadow_index[year] = shadow_index[year - 1] * (1 + shadowstats_rates[year])\n\n    df = pd.DataFrame({\n        \"year\": years,\n        \"official_cpi_rate\": [official_rates[y] for y in years],\n        \"shadowstats_rate\": [shadowstats_rates[y] for y in years],\n        \"official_cpi_index\": [official_index[y] for y in years],\n        \"shadowstats_index\": [shadow_index[y] for y in years],\n    })\n\n    df[\"shadow_vs_official_ratio\"] = df[\"shadowstats_index\"] / df[\"official_cpi_index\"]\n    return df\n\n\n\n# =============================================================================\n# ABSOLUTE MOBILITY DATA from Chetty et al.\n# =============================================================================\n\ndef build_chetty_absolute_mobility():\n    \"\"\"\n    Build absolute mobility series from Chetty et al. (2017) \"Fading American Dream\"\n    \n    Definition: % of children earning more than their parents at age 30\n    (in constant dollars using official CPI)\n    \n    Data from Table 1 of the paper + extensions.\n    \"\"\"\n    \n    # Published data from Chetty et al. (2017)\n    mobility = pd.DataFrame({\n        'birth_cohort': [1940, 1945, 1950, 1955, 1960, 1965, 1970, 1975, 1980, 1985, 1990, 1993],\n        'child_income_year': [1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015, 2020, 2023],\n        'parent_income_year': [1940, 1945, 1950, 1955, 1960, 1965, 1970, 1975, 1980, 1985, 1990, 1993],\n        'absolute_mobility_official': [\n            0.92,  # 1940 cohort\n            0.88,  # 1945 cohort  \n            0.79,  # 1950 cohort\n            0.70,  # 1955 cohort\n            0.62,  # 1960 cohort\n            0.56,  # 1965 cohort\n            0.52,  # 1970 cohort\n            0.50,  # 1975 cohort\n            0.50,  # 1980 cohort (from paper)\n            0.48,  # 1985 cohort (extrapolated)\n            0.46,  # 1990 cohort (extrapolated)\n            0.45,  # 1993 cohort (extrapolated to 2023)\n        ]\n    })\n    \n    return mobility\n\n\n# =============================================================================\n# SHADOWSTATS ADJUSTMENT\n# =============================================================================\n\ndef adjust_mobility_for_shadowstats(mobility_df, inflation_df):\n    \"\"\"\n    Adjust absolute mobility for ShadowStats inflation.\n    \n    Logic:\n    ------\n    If TRUE inflation &gt; official CPI, then:\n    1. Parent's purchasing power was HIGHER than official stats suggest\n    2. The \"real\" bar to beat parents is HIGHER\n    3. Fewer children truly beat their parents\n    \n    Method:\n    -------\n    For each cohort, compute the cumulative inflation gap between\n    parent's income year and child's income year under both measures.\n    \n    If ShadowStats shows 3x the price increase vs official CPI,\n    then roughly 1/3 as many children truly beat their parents\n    (assuming nominal income grew at similar rates).\n    \"\"\"\n    \n    adjusted = []\n    \n    for _, row in mobility_df.iterrows():\n        cohort = row['birth_cohort']\n        parent_year = int(row['parent_income_year'])\n        child_year = int(row['child_income_year'])\n        official_mobility = row['absolute_mobility_official']\n        \n        # Get inflation indices\n        parent_official = inflation_df[inflation_df['year'] == parent_year]['official_cpi_index'].values\n        child_official = inflation_df[inflation_df['year'] == child_year]['official_cpi_index'].values\n        parent_shadow = inflation_df[inflation_df['year'] == parent_year]['shadowstats_index'].values\n        child_shadow = inflation_df[inflation_df['year'] == child_year]['shadowstats_index'].values\n        \n        if len(parent_official) &gt; 0 and len(child_official) &gt; 0:\n            # Inflation from parent year to child year\n            official_inflation = child_official[0] / parent_official[0]\n            shadow_inflation = child_shadow[0] / parent_shadow[0]\n            \n            # Gap ratio: how much higher is ShadowStats inflation\n            inflation_gap = shadow_inflation / official_inflation\n            \n            # Adjustment: if prices rose 2x more under ShadowStats,\n            # nominal wages would need to be 2x higher to maintain same\n            # purchasing power. Assuming wage growth was similar under both,\n            # true mobility is roughly 1/sqrt(gap) of official estimate.\n            # (sqrt because the distribution matters, not just the mean)\n            \n            adjustment_factor = 1 / np.sqrt(inflation_gap)\n            adjusted_mobility = official_mobility * adjustment_factor\n            adjusted_mobility = np.clip(adjusted_mobility, 0.05, 0.95)\n        else:\n            inflation_gap = np.nan\n            adjusted_mobility = official_mobility * 0.63  # Default: ~1/sqrt(2.5)\n        \n        adjusted.append({\n            'birth_cohort': cohort,\n            'child_income_year': child_year,\n            'parent_income_year': parent_year,\n            'official_mobility': official_mobility,\n            'shadowstats_mobility': adjusted_mobility,\n            'inflation_gap_ratio': inflation_gap,\n            'mobility_gap_pp': (official_mobility - adjusted_mobility) * 100\n        })\n    \n    return pd.DataFrame(adjusted)\n\n\n# =============================================================================\n# VISUALIZATION\n# =============================================================================\n\nimport matplotlib.pyplot as plt\n\ndef visualize_results(mobility_df):\n    \"\"\"Create visualization: Absolute mobility (Official CPI vs ShadowStats).\"\"\"\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n    fig.suptitle(\n        \"Absolute Mobility: Official CPI vs ShadowStats\",\n        fontsize=14,\n        fontweight=\"bold\",\n        y=0.98,\n    )\n\n    x = mobility_df[\"birth_cohort\"]\n    y_off = mobility_df[\"official_mobility\"] * 100\n    y_shd = mobility_df[\"shadowstats_mobility\"] * 100\n\n    # Lines\n    ax.plot(\n        x, y_off,\n        \"b-o\",\n        linewidth=2.5,\n        markersize=8,\n        label=\"Official CPI\",\n        zorder=3,\n    )\n    ax.plot(\n        x, y_shd,\n        \"r-s\",\n        linewidth=2.5,\n        markersize=8,\n        label=\"ShadowStats Adjusted\",\n        zorder=3,\n    )\n\n    # Shaded gap\n    ax.fill_between(\n        x,\n        y_off,\n        y_shd,\n        alpha=0.30,\n        color=\"red\",\n        label=\"Hidden decline\",\n        zorder=2,\n    )\n\n    # Reference line\n    ax.axhline(50, color=\"gray\", linestyle=\"--\", linewidth=1.5, alpha=0.7)\n\n    # Labels, styling\n    ax.set_xlabel(\"Birth Cohort\", fontsize=11)\n    ax.set_ylabel(\"% Earning More Than Parents\", fontsize=11)\n    ax.set_title(\"Absolute Mobility Over Time\", fontsize=12, fontweight=\"bold\")\n\n    ax.set_ylim(0, 100)\n    ax.set_xlim(1938, 1995)\n\n    ax.legend(loc=\"upper right\", fontsize=9)\n    ax.grid(True, alpha=0.3)\n\n    fig.tight_layout(rect=[0, 0, 1, 0.95])\n    plt.show()\n\n    return fig, ax\n\n# =============================================================================\n# MAIN\n# =============================================================================\n\nprint(\"=\" * 70)\nprint(\"ABSOLUTE MOBILITY: Official CPI vs ShadowStats Inflation\")\nprint(\"=\" * 70)\n\n# 1. Build inflation series\ninflation_df = build_inflation_series()\n\n# 2. Build Chetty mobility data\nmobility_df = build_chetty_absolute_mobility()\n\n# 3. Adjust for ShadowStats\nadjusted_df = adjust_mobility_for_shadowstats(mobility_df, inflation_df)\n\n# 4. Visualize\nfig = visualize_results(adjusted_df)\n\n======================================================================\nABSOLUTE MOBILITY: Official CPI vs ShadowStats Inflation\n======================================================================\n\n\n\n\n\n\n\n\n\nChetty et. al. (2017) has considered counterfactual analysis to understand reasons behind collapse of income mobility:\n\n“Why have rates of absolute income mobility fallen so sharply over the last half century, and what policies can restore absolute mobility to earlier levels? We used simulations to evaluate the effects of two key trends over the past half century: declining rates of GDP growth and greater inequality in the distribution of GDP (17, 35).”\n\n\n\n\nimage.png\n\n\n\n\n\nIt’s Inflation !"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#reducing-compensation-given-a-productivity-level-1",
    "href": "notebooks/Education and QoL-Satisfaction.html#reducing-compensation-given-a-productivity-level-1",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "2. Reducing Compensation Given a Productivity Level",
    "text": "2. Reducing Compensation Given a Productivity Level\n\nM0/1/2 Inflation and Real Wages: The increase in M2 money supply has contributed to inflation, which in turn has eroded the purchasing power of wages. While nominal wages might rise, real wages—adjusted for inflation—have stagnated or even declined for many workers, particularly those in jobs traditionally associated with higher education.\nData Evidence: Studies such as those from the Federal Reserve Bank of St. Louis have shown that while M2 has increased significantly, the growth in real wages has not kept pace, particularly after the 2008 financial crisis​ (St. Louis Fed).\n\nExample: The Economic Policy Institute found that between 1979 and 2020, the median worker’s wages grew by only 15.1% when adjusted for inflation, while productivity increased by 61.8%. This decoupling suggests that higher education does not necessarily lead to proportionate income growth in an inflationary environment.\nDownload Productivity data\n\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"\n}\nproductivity_url = \"https://download.bls.gov/pub/time.series/pr/pr.data.1.AllData\"\n\ndata = requests.get(productivity_url, headers=headers).text\n\nproductivity_data = pd.read_csv(StringIO(data), sep=\"\\t\")\nproductivity_data.columns = ['series_id', 'year', 'period', 'value','footnote_codes']\nproductivity_data['series_id'] = productivity_data['series_id'].str.strip()\nproductivity_data['period'] = productivity_data['period'].str.strip()\n#productivity_data.info()\n\n# Step 2: Process the productivity data\n# Filtering for relevant data (example uses a hypothetical series code PRS85006093 for nonfarm business productivity)\nproductivity_data = productivity_data[(productivity_data['series_id'] == 'PRS85006093') & (productivity_data['year'] &gt;= start_date.year)]\nproductivity_data = productivity_data[['year', 'value']].rename(columns={'value': 'Productivity_Index'})\n\nproductivity_data.head()\n\n\n\n\n\n\n\n\nyear\nProductivity_Index\n\n\n\n\n45473\n1979\n49.615\n\n\n45474\n1979\n49.523\n\n\n45475\n1979\n49.463\n\n\n45476\n1979\n49.405\n\n\n45477\n1979\n49.360\n\n\n\n\n\n\n\nDownload Median Wage Data\n\nimport pandas_datareader as pdr\nfrom datetime import datetime\n\n# LEU0252881600Q: Median usual weekly real earnings: Wage and salary workers: 16 years and over\nwage_data = pdr.get_data_fred('LEU0252881600Q', start=datetime(1979, 1, 1), end=datetime(2020, 12, 31))\n\n# Preview the data\nwage_data = wage_data.groupby(wage_data.index.year)[\"LEU0252881600Q\"].median()\n\nwage_data = wage_data.reset_index()\n\n# Step 3: Process the wage data\n# Assuming the data is already in the right format\nwage_data = wage_data[wage_data['DATE'] &gt;= 1979]\nwage_data = wage_data[['DATE', 'LEU0252881600Q']].rename(columns={'DATE':'year', 'LEU0252881600Q': 'Weekly_Median_Wage'})\nwage_data.head()\n\n\n\n\n\n\n\n\nyear\nWeekly_Median_Wage\n\n\n\n\n0\n1979\n331.0\n\n\n1\n1980\n316.0\n\n\n2\n1981\n312.5\n\n\n3\n1982\n314.5\n\n\n4\n1983\n314.0\n\n\n\n\n\n\n\n\n# Step 4: Merge the datasets on the year\nmerged_data = pd.merge(productivity_data, wage_data, left_on='year', right_on='year')\nmerged_data.head()\n\n\n\n\n\n\n\n\nyear\nProductivity_Index\nWeekly_Median_Wage\n\n\n\n\n0\n1979\n49.615\n331.0\n\n\n1\n1979\n49.523\n331.0\n\n\n2\n1979\n49.463\n331.0\n\n\n3\n1979\n49.405\n331.0\n\n\n4\n1979\n49.360\n331.0\n\n\n\n\n\n\n\n\n# Step 5: Calculate cumulative growth\nbase_productivity = merged_data['Productivity_Index'].iloc[0]\nbase_wage = merged_data['Weekly_Median_Wage'].iloc[0]\n\nmerged_data['Productivity_Growth'] = ((merged_data['Productivity_Index'] - base_productivity) / base_productivity) * 100\nmerged_data['Weekly_Median_Wage_Growth'] = ((merged_data['Weekly_Median_Wage'] - base_wage) / base_wage) * 100\n\n# Step 6: Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(merged_data['year'], merged_data['Productivity_Growth'], label='Productivity Growth')\nplt.plot(merged_data['year'], merged_data['Weekly_Median_Wage_Growth'], label='Weekly Median Wage Growth')\nplt.title('Productivity Growth vs. Weekly Median Wage Growth (1979-2020)')\nplt.xlabel('Year')\nplt.ylabel('Cumulative Growth (%)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Display the final growth rates\nprint(f\"Productivity Growth (1979-2020): {merged_data['Productivity_Growth'].iloc[-1]:.2f}%\")\nprint(f\"Weekly Median Wage Growth (1979-2020): {merged_data['Weekly_Median_Wage_Growth'].iloc[-1]:.2f}%\")\n\n\n\n\n\n\n\n\nProductivity Growth (1979-2020): 120.07%\nWeekly Median Wage Growth (1979-2020): 14.95%\n\n\n\nImpact of M2 Inflation: Rising Costs of Living and Education\n\nEducation Costs: The cost of education has risen significantly, outpacing inflation and wage growth. As M2 inflation contributes to the overall increase in the cost of living, students graduate with higher levels of debt, which diminishes the net financial returns of their education.\nCase Study: According to the College Board, the average cost of tuition and fees at private four-year institutions in the U.S. has more than doubled since 2000, while wages have not kept pace with these increases​\n\nData Source: https://research.collegeboard.org/media/xlsx/trends-college-pricing-excel-data-2023.xlsx\n\ntution_data = pd.read_csv(\"data/tuition_private_4yr_current_dollars_final_cleaned.csv\")\ntution_data.columns = ['year', 'Private_Nonprofit_Four_Year']\ntution_data.head()\n\n\n\n\n\n\n\n\nyear\nPrivate_Nonprofit_Four_Year\n\n\n\n\n0\n1971\n1830.0\n\n\n1\n1972\n1950.0\n\n\n2\n1973\n2050.0\n\n\n3\n1974\n2130.0\n\n\n4\n1975\n2290.0\n\n\n\n\n\n\n\n\n# Step 4: Merge the datasets on the year\nmerged_data = pd.merge(tution_data, wage_data, left_on='year', right_on='year')\nmerged_data.head()\n\n\n\n\n\n\n\n\nyear\nPrivate_Nonprofit_Four_Year\nWeekly_Median_Wage\n\n\n\n\n0\n1979\n3230.0\n331.0\n\n\n1\n1980\n3620.0\n316.0\n\n\n2\n1981\n4110.0\n312.5\n\n\n3\n1982\n4640.0\n314.5\n\n\n4\n1983\n5090.0\n314.0\n\n\n\n\n\n\n\n\n# Step 5: Calculate cumulative growth\nbase_tution = merged_data['Private_Nonprofit_Four_Year'].iloc[0]\nbase_wage = merged_data['Weekly_Median_Wage'].iloc[0]\n\nmerged_data['Tution_Growth'] = ((merged_data['Private_Nonprofit_Four_Year'] - base_tution) / base_tution) * 100\nmerged_data['Weekly_Median_Wage_Growth'] = ((merged_data['Weekly_Median_Wage'] - base_wage) / base_wage) * 100\n\n# Step 6: Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(merged_data['year'], merged_data['Tution_Growth'], label='Tution Growth (Private Nonprofit Four Year)')\nplt.plot(merged_data['year'], merged_data['Weekly_Median_Wage_Growth'], label='Weekly Median Wage Growth')\nplt.title('Tution Growth (Private Nonprofit Four Year) vs. Weekly Median Wage Growth (1979-2020)')\nplt.xlabel('Year')\nplt.ylabel('Cumulative Growth (%)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Display the final growth rates\nprint(f\"Tution Growth (Private Nonprofit Four Year) (1979-): {merged_data['Tution_Growth'].iloc[-1]:.2f}%\")\nprint(f\"Weekly Median Wage Growth (1979-): {merged_data['Weekly_Median_Wage_Growth'].iloc[-1]:.2f}%\")\n\n\n\n\n\n\n\n\nTution Growth (Private Nonprofit Four Year) (1979-): 1053.87%\nWeekly Median Wage Growth (1979-): 14.95%\n\n\nNow let’s explore how Tution Growth and M2 Inflation are related.\n\n\nTution Growth and M2 Inflation\n\n# Fetch M2 Money Stock data\nm2_supply = web.DataReader('M2SL', 'fred', start_date, end_date)\n\nm2_supply = m2_supply.groupby(m2_supply.index.year)[\"M2SL\"].mean()\nm2_supply.dropna()\n\nm2_supply = pd.DataFrame(data={\n    'year': m2_supply.index,\n    'm2sl': m2_supply.values\n})\nm2_supply.reset_index()\nm2_supply.head(5)\n\n\n\n\n\n\n\n\nyear\nm2sl\n\n\n\n\n0\n1979\n1425.666667\n\n\n1\n1980\n1540.183333\n\n\n2\n1981\n1679.291667\n\n\n3\n1982\n1830.925000\n\n\n4\n1983\n2054.466667\n\n\n\n\n\n\n\n\ntution_m2sl_merged_data = pd.merge(tution_data, m2_supply, left_on='year', right_on='year')\ntution_m2sl_merged_data.head()\n\n\n\n\n\n\n\n\nyear\nPrivate_Nonprofit_Four_Year\nm2sl\n\n\n\n\n0\n1979\n3230.0\n1425.666667\n\n\n1\n1980\n3620.0\n1540.183333\n\n\n2\n1981\n4110.0\n1679.291667\n\n\n3\n1982\n4640.0\n1830.925000\n\n\n4\n1983\n5090.0\n2054.466667\n\n\n\n\n\n\n\n\n# Perform Granger Causality Test\ngranger_test = grangercausalitytests(tution_m2sl_merged_data[['Private_Nonprofit_Four_Year', 'm2sl']], \n                                     maxlag=12, \n                                     verbose=True)\n\n\nGranger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=0.0591  , p=0.8092  , df_denom=41, df_num=1\nssr based chi2 test:   chi2=0.0634  , p=0.8012  , df=1\nlikelihood ratio test: chi2=0.0634  , p=0.8013  , df=1\nparameter F test:         F=0.0591  , p=0.8092  , df_denom=41, df_num=1\n\nGranger Causality\nnumber of lags (no zero) 2\nssr based F test:         F=0.2816  , p=0.7561  , df_denom=38, df_num=2\nssr based chi2 test:   chi2=0.6373  , p=0.7271  , df=2\nlikelihood ratio test: chi2=0.6327  , p=0.7288  , df=2\nparameter F test:         F=0.2816  , p=0.7561  , df_denom=38, df_num=2\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=11.8657 , p=0.0000  , df_denom=35, df_num=3\nssr based chi2 test:   chi2=42.7165 , p=0.0000  , df=3\nlikelihood ratio test: chi2=29.4689 , p=0.0000  , df=3\nparameter F test:         F=11.8657 , p=0.0000  , df_denom=35, df_num=3\n\nGranger Causality\nnumber of lags (no zero) 4\nssr based F test:         F=8.6750  , p=0.0001  , df_denom=32, df_num=4\nssr based chi2 test:   chi2=44.4595 , p=0.0000  , df=4\nlikelihood ratio test: chi2=30.1133 , p=0.0000  , df=4\nparameter F test:         F=8.6750  , p=0.0001  , df_denom=32, df_num=4\n\nGranger Causality\nnumber of lags (no zero) 5\nssr based F test:         F=6.3667  , p=0.0004  , df_denom=29, df_num=5\nssr based chi2 test:   chi2=43.9086 , p=0.0000  , df=5\nlikelihood ratio test: chi2=29.6339 , p=0.0000  , df=5\nparameter F test:         F=6.3667  , p=0.0004  , df_denom=29, df_num=5\n\nGranger Causality\nnumber of lags (no zero) 6\nssr based F test:         F=6.0540  , p=0.0005  , df_denom=26, df_num=6\nssr based chi2 test:   chi2=54.4860 , p=0.0000  , df=6\nlikelihood ratio test: chi2=34.0958 , p=0.0000  , df=6\nparameter F test:         F=6.0540  , p=0.0005  , df_denom=26, df_num=6\n\nGranger Causality\nnumber of lags (no zero) 7\nssr based F test:         F=5.2112  , p=0.0012  , df_denom=23, df_num=7\nssr based chi2 test:   chi2=60.2682 , p=0.0000  , df=7\nlikelihood ratio test: chi2=36.1043 , p=0.0000  , df=7\nparameter F test:         F=5.2112  , p=0.0012  , df_denom=23, df_num=7\n\nGranger Causality\nnumber of lags (no zero) 8\nssr based F test:         F=4.1960  , p=0.0044  , df_denom=20, df_num=8\nssr based chi2 test:   chi2=62.1003 , p=0.0000  , df=8\nlikelihood ratio test: chi2=36.4529 , p=0.0000  , df=8\nparameter F test:         F=4.1960  , p=0.0044  , df_denom=20, df_num=8\n\nGranger Causality\nnumber of lags (no zero) 9\nssr based F test:         F=4.0155  , p=0.0066  , df_denom=17, df_num=9\nssr based chi2 test:   chi2=76.5310 , p=0.0000  , df=9\nlikelihood ratio test: chi2=41.0296 , p=0.0000  , df=9\nparameter F test:         F=4.0155  , p=0.0066  , df_denom=17, df_num=9\n\nGranger Causality\nnumber of lags (no zero) 10\nssr based F test:         F=3.1079  , p=0.0262  , df_denom=14, df_num=10\nssr based chi2 test:   chi2=77.6984 , p=0.0000  , df=10\nlikelihood ratio test: chi2=40.9278 , p=0.0000  , df=10\nparameter F test:         F=3.1079  , p=0.0262  , df_denom=14, df_num=10\n\nGranger Causality\nnumber of lags (no zero) 11\nssr based F test:         F=4.4091  , p=0.0105  , df_denom=11, df_num=11\nssr based chi2 test:   chi2=149.9104, p=0.0000  , df=11\nlikelihood ratio test: chi2=57.3950 , p=0.0000  , df=11\nparameter F test:         F=4.4091  , p=0.0105  , df_denom=11, df_num=11\n\nGranger Causality\nnumber of lags (no zero) 12\nssr based F test:         F=13.2302 , p=0.0005  , df_denom=8, df_num=12\nssr based chi2 test:   chi2=654.8936, p=0.0000  , df=12\nlikelihood ratio test: chi2=100.2252, p=0.0000  , df=12\nparameter F test:         F=13.2302 , p=0.0005  , df_denom=8, df_num=12\n\n\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:1545: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n\n\nThe Granger causality tests provide strong evidence that changes in the M2 money supply (M2SL) do indeed cause changes in the cost of tuition at Private Nonprofit Four-Year institutions, particularly when considering lags 3 through 12. The most significant causal effects are observed around lags 3-6, with some weakening around lags 7-10, and a resurgence at higher lags (lag 12). This analysis suggests that M2SL is a significant predictor of tuition costs over various lag periods, implying that changes in the money supply could have a delayed impact on educational costs.\nStudent Debt:\nFederal Reserve data shows that student debt in the U.S. has skyrocketed, surpassing $1.7 trillion in 2021. This growing debt burden makes it harder for graduates to achieve financial stability, let alone upward mobility.\n\n\nImpact of Technological Disruption: Automation and Job Displacement:\n\nTech Disruption: Technological advancements, particularly in automation and AI, have disrupted industries that traditionally offered stable employment to highly educated individuals. Jobs in fields like accounting (A), legal services (L), and even medicine (M) (LAM in acronym) are increasingly being automated, reducing the demand for highly educated workers in these areas.\nResearch Findings: A 2020 report by the World Economic Forum predicts that by 2025, automation will displace about 85 million jobs globally, many of which are held by individuals with higher education. This disruption challenges the stability that higher education once promised​ (World Bank).\nGlobal Displacement Estimates: 400 to 800 million individuals globally could be displaced by automation and need to find new jobs by 2030. This estimate reflects the potential impact under various scenarios of automation adoption.\nOccupation Shifts: 75 to 375 million workers may need to switch occupational categories and learn new skills to remain employed, depending on the speed of automation adoption. This transition represents 3% to 14% of the global workforce.\n300 to 365 million new jobs could be created globally by 2030 from rising incomes and consumption, especially in emerging economies. This implies 100-435 million jobs get destroyed. This wave of automation, at least, is not a net job creator without even quantifying stress-level or level of satisfaction from those that will exist by 2030.\n\n\nExample: The rise of AI tools like GPT (Generative Pre-trained Transformer) has started to replace certain tasks performed by professionals in law and finance, sectors that were once considered safe for those with advanced degrees.\nDisplaced workers are often forced into lower-paying jobs or must invest in learning new technologies (SaaS tools, AI/ML etc.) to remain employable, perpetuating a cycle of continual upskilling without significant wage growth.\nSources - JOBS LOST, JOBS GAINED: WORKFORCE TRANSITIONS IN A TIME OF AUTOMATION https://www.mckinsey.com/~/media/mckinsey/industries/public%20and%20social%20sector/our%20insights/what%20the%20future%20of%20work%20will%20mean%20for%20jobs%20skills%20and%20wages/mgi-jobs-lost-jobs-gained-executive-summary-december-6-2017.pdf\n## Hypothesis: &gt; Benefits of automation mostly accrue to Venture Capitalists (VCs), private equity (PEs) firms, investment funds, governments (via tax collection) and enterprises, while workers face job displacement or wage stagnation\nThe labor share — the fraction of economic output that accrues to workers as compensation in exchange for their labor, could be a way to measure whether the benefits of automation is accruing to labour\n\nSource: - https://www.bls.gov/opub/mlr/2017/article/estimating-the-us-labor-share.htm\n\nSource: - A new look at the declining labor share of income in the United States https://www.mckinsey.com/~/media/McKinsey/Featured%20Insights/Employment%20and%20Growth/A%20new%20look%20at%20the%20declining%20labor%20share%20of%20income%20in%20the%20United%20States/MGI-A-new-look-at-the-declining-labor-share-of-income-in-the-United-States.pdf\n\n\nSummary of Five Main Reasons for Declining Labor Share:\n\n\nCapital Deepening, Substitution, and Automation:  Technological advancements, such as powerful computers and robots, reduce the cost of investment in capital, incentivizing companies to substitute labor with capital. This shift can lead to a decrease in the labor share of income as less labor is needed in production. However, the relationship is complex because technology can also complement labor, raising productivity and potentially leading to wage increases. The decline in labor’s share may occur if capital becomes more prominent in production, increasing returns on capital relative to labor.\n\n\n“Superstar” Effects and Consolidation:  The rise of “superstar” firms that dominate profits and value added in their industries, especially in knowledge-intensive sectors, has led to a larger share of economic value going to capital owners rather than labor. This phenomenon is often associated with consolidation and reduced competition, which can occur in regulated sectors or those protected by strong intellectual property rights. These superstar firms often deploy more capital or achieve higher returns, further reducing the labor share.\n\n\nGlobalization and Labor Bargaining Power:  Increased global trade and competition from countries with lower labor costs, along with the threat of offshoring jobs, have put downward pressure on wages and employment. The weakening of labor market institutions, such as unions, has further diminished workers’ bargaining power, contributing to a declining labor share. While stronger bargaining power or higher minimum wages can temporarily increase the labor share, these measures may also encourage greater capital substitution in the long term.\n\n\nHigher Depreciation (Due to Shift to Intangible Capital):  The shift toward greater use of intangible assets, like intellectual property (IP) and software, which have faster depreciation cycles than traditional physical assets, has increased the depreciation share of income. This reduces the overall amount available to labor and capital. Additionally, the economy has been working through a capital overhang from the investment boom before the financial crisis, further increasing depreciation and reducing the labor share.\n\n\nSupercycles and Boom-Bust:  Certain sectors, particularly those in energy and minerals, are subject to price supercycles, where rapidly rising commodity prices increase profits and reduce labor’s share of income. Other sectors, such as real estate and construction, have experienced boom-bust cycles that shift the capital and labor share of income. For example, tech services have shown industry-specific contractions followed by recovery, affecting labor’s share over time.\n\n\nOut of these causative factors (i), (ii) and (iv) are directly or indirectly related to Tech & Automation"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#impact-of-technological-disruption-underemployment-of-graduates",
    "href": "notebooks/Education and QoL-Satisfaction.html#impact-of-technological-disruption-underemployment-of-graduates",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "Impact of Technological Disruption: Underemployment of Graduates:",
    "text": "Impact of Technological Disruption: Underemployment of Graduates:\n\nOverqualification: As more people pursue higher education, the labor market becomes saturated with degree holders, leading to underemployment. Graduates often find themselves in jobs that do not require their level of education, resulting in lower job satisfaction and income.\nStatistical Evidence: The Federal Reserve Bank of New York reported that as of 2021, about 41% of recent college graduates in the U.S. were underemployed, meaning they were working in jobs that typically do not require a bachelor’s degree\n52% of Graduates Underemployed: One year after graduation, 52% of college graduates with a terminal bachelor’s degree are underemployed. This rate decreases only slightly to 45% after 10 years​(Talent-Disrupted-2).\n\n\nSTEM is not a silver bullet. While policymakers typically think of STEM (science, technology, engineering, and mathematics) programs as a sure pathway to college-level employment and high wages, the reality is more nuanced. Graduates with a bachelor’s degree in computer science, engineering, or mathematics tend to experience very low underemployment, while those with a degree in a life sciences field (e.g., biology) tend to face higher underemployment rates.\n\n\nPersistence of Underemployment: 73% Remain Underemployed: A staggering 73% of those who start out underemployed remain in such positions 10 years after graduation\nUnderemployment / Unemployment, beyond skill-gap, can also indicate over-supply in labour market. One way to measure that would be to capture data about Avg Applications per Job by Sectors or ratio of Job Openings to Unemployment\nOvereducation and depressive symptoms: diminishing mental health returns to education (https://sci-hub.se/10.1111/1467-9566.12039) &gt; On the supply side, the labour market value of educational credentials inflated (Hannum and Buchmann 2005), whereas on the demand side, employers started to compete for employees with the highest credentials in order to reduce the costs of job training (Hirsch 1977, Thurow 1976). As a result, the fact that the supply of highly educated people outnumbered the demand for educated labour (Freeman 1976) led to some highly educated people ending up in jobs that actually required lower qualifications (Duncan and Hoffman 1981). This phenomenon of overeducation thus became a permanent condition for a substantial number of employees (Pritchett 2001, Rubb 2003, Vaisey 2006). At the population level the presence of overeducation is inferred from the observation of diminishing returns to tertiary education. For instance, Freeman (1976) defines overeducation as ‘a falling private rate of return to college education’ (Psacharopoulos, 1994: 1334). At the individual level it is defined as job–education mismatch, that is, when ‘the level of education acquired exceeds the level of education required to adequately perform the job’ (Wolbers 2003: 251).\n\nSources:\n\nTALENT DISRUPTED - College Graduates, Underemployment, and the Way Forward\ncollege-labor-market\n\n\nTrend in Job openings to unemployment\n\n# BLS API Key\napi_key = '36aea4409aef4dd787a9ab7107c9d232'\n\n# Define the series IDs for job openings (JOLTS) and unemployment (UNRATE)\nseries_ids = {\n    \"Job Openings\": \"JTS000000000000000JOL\", \n    \"Unemployment\": \"LNS13000000\"\n}\n\n# Define the endpoint and parameters for the API request\nendpoint = \"https://api.bls.gov/publicAPI/v2/timeseries/data/\"\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\n\n# Request data for both series\ndata = {\n    \"seriesid\": list(series_ids.values()),\n\n    # \n    # NOTE:max data range can be of 20 years\n    #\n    \"startyear\": '2004',\n    \"endyear\": '2024',\n    \"registrationkey\": api_key\n}\n\nresponse = requests.post(endpoint, json=data, headers=headers)\njson_data = response.json()\n\n\n# Extract data and create a DataFrame\nseries_data = {}\nfor series in json_data['Results']['series']:\n    series_id = series['seriesID']\n    series_name = [key for key, value in series_ids.items() if value == series_id][0]\n    data_points = [(item['year'], item['value']) for item in series['data']]\n    df = pd.DataFrame(data_points, columns=['Year', series_name])\n    df[series_name] = df[series_name].astype(float)\n    series_data[series_name] = df.set_index('Year')\n\n# Merge the two dataframes\ndf_job_openings_unemployment_merged = pd.merge(series_data['Job Openings'], \n                                               series_data['Unemployment'], \n                                               left_index=True, \n                                               right_index=True)\n\n# Calculate the ratio of Job Openings to Unemployment\ndf_job_openings_unemployment_merged['Job Openings to Unemployment Ratio'] = df_job_openings_unemployment_merged['Job Openings'] / df_job_openings_unemployment_merged['Unemployment']\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.plot(df_job_openings_unemployment_merged.index, \n         df_job_openings_unemployment_merged['Job Openings to Unemployment Ratio'], \n         marker='o')\nplt.title('Job Openings to Unemployment Ratio (2004-2024)')\nplt.xlabel('Year')\nplt.ylabel('Ratio of Job Openings to Unemployment')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nWhat’s interesting is that the ratio remained &lt;1 (over-supply/skill-gap) till 2018, corrected during COVID and rebounded stronger. No we don’t know how much of that rebound is caused by transitioning to the “new world order”, remote/WFH tech jobs or monetary stimulus provided by government.\n\nTrend in Job openings to unemployment - By Sectors\n\n# Define the series IDs for job openings (JOLTS) and unemployment for various sectors\n\n# https://www.bls.gov/help/hlpforma.htm#jt\n#\nseries_ids = {\n    \"Tech Job Openings\": \"JTS540099000000000JOL\",        # Professional and business services (often used as a proxy for tech)\n    \"Manufacturing Job Openings\": \"JTS300000000000000JOL\",\n    \"Retail Job Openings\": \"JTS440000000000000JOL\",\n    # \"Food Services Job Openings\": \"JTS720000000000000JOL\",\n\n    # https://data.bls.gov/timeseries/LNU04032215\n    #\n    # 04 = rate, 03 = level (numbers)\n    #\n    \"Tech Unemployment\": \"LNU03032239\",           # Unemployment for professional and technical services\n    \"Manufacturing Unemployment\": \"LNU03032232\",\n    \"Retail Unemployment\": \"LNU03032235\"\n    # \"Food Services Unemployment\": \"LNS14000032\"\n}\n\n# Define the endpoint and parameters for the API request\nendpoint = \"https://api.bls.gov/publicAPI/v2/timeseries/data/\"\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\n\n# Request data for each series\ndata = {\n    \"seriesid\": list(series_ids.values()),\n    \"startyear\": \"2004\",\n    \"endyear\": \"2024\",\n    \"registrationkey\": api_key\n}\n\nresponse = requests.post(endpoint, json=data, headers=headers)\njson_data = response.json()\n\n\n# Extract data and create DataFrames for each sector\nsector_data = {}\nfor series in json_data['Results']['series']:\n    series_id = series['seriesID']\n    series_name = [key for key, value in series_ids.items() if value == series_id][0]\n    data_points = [(item['year'], item['value']) for item in series['data']]\n    df = pd.DataFrame(data_points, columns=['Year', series_name])\n    df[series_name] = df[series_name].astype(float)\n    sector_data[series_name] = df.set_index('Year')\n\n\n# Merge job openings and unemployment data for each sector\nsectors = [\"Tech\", \"Manufacturing\", \"Retail\"]\ndf_ratios = pd.DataFrame()\n\nfor sector in sectors:\n    job_openings_col = f\"{sector} Job Openings\"\n    unemployment_col = f\"{sector} Unemployment\"\n    df_merged = pd.merge(sector_data[job_openings_col], sector_data[unemployment_col], left_index=True, right_index=True)\n    df_merged[f\"{sector} Job Openings to Unemployment Ratio\"] = df_merged[job_openings_col] / df_merged[unemployment_col]\n    if df_ratios.empty:\n        df_ratios = df_merged[[f\"{sector} Job Openings to Unemployment Ratio\"]]\n    else:\n        df_ratios = df_ratios.join(df_merged[[f\"{sector} Job Openings to Unemployment Ratio\"]], how='outer')\n\n\ndf_job_to_unemp_ratios = df_ratios.groupby(df_ratios.index)[[\"Tech Job Openings to Unemployment Ratio\", \"Manufacturing Job Openings to Unemployment Ratio\", \"Retail Job Openings to Unemployment Ratio\"]].mean()\ndf_job_to_unemp_ratios.head()\n\n\n\n\n\n\n\n\nTech Job Openings to Unemployment Ratio\nManufacturing Job Openings to Unemployment Ratio\nRetail Job Openings to Unemployment Ratio\n\n\nYear\n\n\n\n\n\n\n\n2004\n0.780469\n0.273507\n0.349046\n\n\n2005\n0.958135\n0.363483\n0.430798\n\n\n2006\n1.167889\n0.479952\n0.483758\n\n\n2007\n1.263115\n0.479159\n0.489625\n\n\n2008\n0.815361\n0.260251\n0.345037\n\n\n\n\n\n\n\n\n# Plot the data\nplt.figure(figsize=(14, 8))\nplt.plot(df_job_to_unemp_ratios.index, df_job_to_unemp_ratios[\"Tech Job Openings to Unemployment Ratio\"], marker='o', label=\"Tech Job Openings to Unemployment Ratio\")\nplt.plot(df_job_to_unemp_ratios.index, df_job_to_unemp_ratios[\"Manufacturing Job Openings to Unemployment Ratio\"], marker='o', label=\"Manufacturing Job Openings to Unemployment Ratio\")\nplt.plot(df_job_to_unemp_ratios.index, df_job_to_unemp_ratios[\"Retail Job Openings to Unemployment Ratio\"], marker='o', label=\"Retail Job Openings to Unemployment Ratio\")\nplt.title('Job Openings to Unemployment Ratio by Sector (2004-2024)')\nplt.xlabel('Year')\nplt.ylabel('Ratio of Job Openings to Unemployment')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe above chart illustrates the ratio of job openings to unemployment across three sectors: Tech, Manufacturing, and Retail, over the period from 2004 to 2024. The Tech sector consistently shows a higher ratio compared to Manufacturing and Retail, particularly after 2014, where it surpasses a ratio of 1.0, indicating more job openings than unemployed individuals in that sector. The ratio peaks around 2022 at approximately 3.0 before slightly declining in 2023. The Manufacturing sector shows a steady increase in the ratio from around 0.2 in 2009 to about 1.0 in 2022, indicating a tightening labor market. Retail also shows a similar trend, with the ratio increasing from around 0.2 in 2009 to about 1.0 in 2022. However, both Manufacturing and Retail sectors exhibit more fluctuation compared to the Tech sector, particularly noticeable during the 2020-2021 period, reflecting the impact of economic disruptions during that time.\nReflections\nAlthough tech is creating more openings than official unemployment numbers, there are two caveats.\n\nTech itself changes fast and anyone in tech needs constant re-skilling/up-skilling to cope up with the change.\nThe definition of “Unemplyment” is tricky. &gt; Unemployment rate The unemployment rate represents the number of unemployed people as a percentage of the labor force (the labor force is the sum of the employed and unemployed). The unemployment rate is calculated as: (Unemployed ÷ Labor Force) x 100.\n\nNot in the labor force: In the Current Population Survey, people are classified as not in the labor force if: \n\n\nthey were not employed during the survey reference week and \nthey had not actively looked for work (or been on temporary layoff) in the last 4 weeks  In other words, people not in the labor force are those who do not meet the criteria to be classified as either employed or unemployed, as defined above. People not in the labor force are asked whether they want a job and if they were available to take a job during the survey reference week. They also are asked about their job search activity in the last 12 months (or since the end of their last job, if they held one in the last 12 months) and their reason for not having looked for work in the most recent 4 weeks.\n\n\nThe value of degrees in fields like business, computer science, and engineering has significantly diminished compared to 30 years ago. While these degrees once paved a reliable path to stable and lucrative careers, today’s landscape is far more competitive, with qualified professionals and offshore workers willing to work for less. As a result, merely obtaining a degree is no longer a guaranteed ticket to success; one must be exceptionally skilled, and this often needs to start before even pursuing the degree. For many, it may be more advantageous to take the risk of business ownership and work for themselves, where they have greater control over their income and career. From the perspective of a seasoned software engineer with over a decade of experience, including leadership roles, the field has become increasingly challenging and less rewarding. If given the chance to start over, they would prioritize gaining experience quickly through startups, learning the intricacies of business, and eventually pursuing entrepreneurship to have more control over their destiny and financial rewards. In essence, a degree alone is not enough anymore; individuals must take proactive control of their careers and explore alternative paths like entrepreneurship to secure their futures.\n\nStudent Debt: Federal Reserve data shows that student debt in the U.S. has skyrocketed, surpassing $1.7 trillion in 2021. This growing debt burden makes it harder for graduates to achieve financial stability, let alone upward mobility."
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#jor",
    "href": "notebooks/Education and QoL-Satisfaction.html#jor",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "JOR",
    "text": "JOR\nThe Job Openings Rate (JOR) is defined as the number of job openings on the last business day of the month as a percentage of total employment plus job openings. Mathematically:\n$\n= ( ) $\n\nKey Components:\n\nJob Openings: The number of available positions employers are actively recruiting to fill.\nTotal Employment: The total number of individuals currently employed in the workforce.\nDenominator: The sum of total employment and job openings represents the total labor market capacity.\n\n\n\nPurpose:\n\nThe JOR serves as an indicator of labor demand and provides insights into economic health.\nHigher rates may signal strong demand for workers, while lower rates can indicate reduced hiring activity.\n\nThis definition is relevant to the plotted data in your script, where JOR trends are analyzed across various industry sectors.\n\nimport requests\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Define your BLS API key (replace with your actual API key)\nAPI_KEY = \"36aea4409aef4dd787a9ab7107c9d232\"\n\n# Define the series IDs for different industries (replace with actual series IDs for JOR)\nseries_ids = {\n    \"Construction\": \"JTU000000000000000JOR\",\n    \"Manufacturing\": \"JTU300000000000000JOR\",\n    \"Retail Trade\": \"JTU440000000000000JOR\",\n    \"Professional Services\": \"JTU600000000000000JOR\",\n    \"Leisure and Hospitality\": \"JTU700000000000000JOR\",\n}\n\n# Define the API URL\nBASE_URL = \"https://api.bls.gov/publicAPI/v2/timeseries/data/\"\n\n# Fetch data from BLS API\ndef fetch_bls_data(series_id):\n    payload = {\n        \"seriesid\": [series_id],\n        \"startyear\": \"2000\",\n        \"endyear\": \"2024\",\n        \"registrationkey\": API_KEY,\n    }\n    response = requests.post(BASE_URL, json=payload)\n    if response.status_code == 200:\n        data = response.json()\n        return data[\"Results\"][\"series\"][0][\"data\"]\n    else:\n        print(f\"Error fetching data for {series_id}: {response.status_code}\")\n        return []\n\n# Process the fetched data\ndef process_data(data):\n    years = []\n    values = []\n    for entry in data:\n        years.append(int(entry[\"year\"]))\n        values.append(float(entry[\"value\"]))\n    return pd.Series(values, index=years)\n\n# Fetch and process data for all series\njor_data = {}\nfor sector, series_id in series_ids.items():\n    raw_data = fetch_bls_data(series_id)\n    jor_data[sector] = process_data(raw_data)\n\n# Combine all data into a single DataFrame\ndf = pd.DataFrame(jor_data)\n\n# Plot the data\nplt.figure(figsize=(12, 6))\nfor column in df.columns:\n    plt.plot(df.index, df[column], marker=\"o\", label=column)\n\n# Add chart details\nplt.title(\"Job Openings Rate (JOR) by Industry Sector (US)\", fontsize=14)\nplt.xlabel(\"Year\", fontsize=12)\nplt.ylabel(\"Job Openings Rate (%)\", fontsize=12)\nplt.legend(title=\"Industry Sector\", fontsize=10)\nplt.grid(True, linestyle=\"--\", alpha=0.7)\n\n# Show the plot\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#b.-social-status-and-mobility",
    "href": "notebooks/Education and QoL-Satisfaction.html#b.-social-status-and-mobility",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "1b. Social Status and Mobility",
    "text": "1b. Social Status and Mobility\n\nLabor Market Saturation and Reduced Returns on Education:\nThe saturation of the labor market with degree holders has diminished the economic returns of education for many individuals. With more people obtaining degrees, the competition for top-tier jobs has intensified, leading to a situation where a degree alone is no longer sufficient to guarantee upward mobility. This has led to a “credential inflation,” where higher qualifications are needed to stand out, further pushing individuals towards costly graduate education, such as MBAs, which again are becoming increasingly expensive due to M2 inflation.\nStrategies to validate:\n\nUse LinkedIn data scraping for Application-to-job ratio\n% of MBA degrees confered over years (National Center for Education Statistics (NCES) data)\n\n\n\nRise of Elite Education as a Gateway\n\nBusiness Leadership: Frank and Cook discuss how elite business schools (e.g., Harvard Business School, Stanford Graduate School of Business) dominate the pathways to top executive positions in Fortune 500 companies. The networks and brand recognition of these institutions give their graduates a significant edge in the competition for leadership roles.\nLegal Profession: The book highlights how top law firms overwhelmingly recruit from a handful of elite law schools (e.g., Harvard, Yale, Stanford). Graduates from these schools have a much higher chance of securing high-paying positions, regardless of their actual performance in law school compared to graduates from less prestigious institutions.\nEducational background plays a crucial role in determining career trajectories. Those from elite schools continue to dominate leadership positions, whereas those from non-elite schools find it harder to break into these roles, even if they start at the same level.\nQuantitative Data: The study provides data indicating that over the last few decades, the proportion of executives coming from elite schools has increased, while the proportion from non-elite schools has decreased.\nReference: Frank, R. H., & Cook, P. J. (1995). The Winner-Take-All Society. This book discusses how certain elite institutions have monopolized access to top jobs.\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\nReference: - Diversity, Hierarchy, and Fit in Legal Careers: Insights from Fifteen Years of Qualitative Interviews https://www.law.georgetown.edu/legal-ethics-journal/wp-content/uploads/sites/24/2019/01/GT-GJLE180004.pdf\n\n\nData on Educational Attainment and Professional Success\nTo create a chart showing the proportion of executives from elite versus non-elite schools from 1980 to 2020, data from various studies indicate the following trends:\n1980s: In the early 1980s, around 50% of executives in top U.S. firms had graduated from elite schools, with this figure remaining fairly stable through the decade. Elite schools are often defined as Ivy League institutions and other top-tier universities like Stanford and MIT​( Oxford Academic ).\n1990s: The 1990s saw a slight increase in the proportion of executives from elite schools, reaching around 55%. This was driven by the increasing value placed on prestigious MBA programs from elite institutions as a key qualification for senior management roles​( Oxford Academic ).\n2000s: During the 2000s, the proportion of executives from elite schools continued to rise, peaking at around 60-65% by the late 2000s. This trend was supported by the globalization of business and the preference for executives with international educational experiences, often obtained at elite institutions​( SpringerLink ).\n2010-2020: The proportion of executives from elite schools has stabilized, fluctuating between 60-70%. This period also saw an increase in the importance of non-traditional elite schools, particularly for tech and innovative companies, where elite institutions like Stanford and MIT played a major role​( SpringerLink , Oxford Academic ).\nRef - Steven Brint, Sarah R K Yoshikawa, The Educational Backgrounds of American Business and Government Leaders: Inter-Industry Variation in Recruitment from Elite Colleges and Graduate Programs, Social Forces, Volume 96, Issue 2, December 2017, Pages 561–590, https://doi.org/10.1093/sf/sox059 https://academic.oup.com/sf/article-abstract/96/2/561/4622952?login=false - https://link.springer.com/chapter/10.1007/978-3-319-59966-3_5\nEducational Backgrounds of American Business and Government Leaders\nSteven Brint, Sarah R K Yoshikawa, The Educational Backgrounds of American Business and Government Leaders: Inter-Industry Variation in Recruitment from Elite Colleges and Graduate Programs, Social Forces, Volume 96, Issue 2, December 2017, Pages 561–590, https://doi.org/10.1093/sf/sox059\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data from the table\ngroups = [\n    \"A. Symbol production/Knowledge sector\", \"A. Symbol production/Knowledge sector\", \n    \"A. Symbol production/Knowledge sector\", \"A. Symbol production/Knowledge sector\", \n    \"A. Symbol production/Knowledge sector\",\n    \"B. Material production/Knowledge sector\", \"B. Material production/Knowledge sector\", \n    \"B. Material production/Knowledge sector\", \"B. Material production/Knowledge sector\", \n    \"B. Material production/Knowledge sector\",\n    \"C. Material production/Outside knowledge sector\", \"C. Material production/Outside knowledge sector\", \n    \"C. Material production/Outside knowledge sector\", \"C. Material production/Outside knowledge sector\", \n    \"C. Material production/Outside knowledge sector\"\n]\n\nindustries = [\n    \"Internet services\", \"Entertainment/Media\", \"Finance\", \"Computer Software\", \"Government\",\n    \"Pharmaceuticals\", \"Telecommunications\", \"Aerospace/Security\", \"Health care\", \"Energy\",\n    \"Apparel\", \"Chemicals\", \"Construction\", \"Food products\", \"Motor vehicles\"\n]\n\n# Steven Brint, Sarah R K Yoshikawa, The Educational Backgrounds of American Business and Government Leaders: Inter-Industry Variation in Recruitment from Elite Colleges and Graduate Programs, Social Forces, Volume 96, Issue 2, December 2017, Pages 561–590, https://doi.org/10.1093/sf/sox059 https://academic.oup.com/sf/article-abstract/96/2/561/4622952?login=false\n\nbachelors_degrees = [32, 28, 28, 22, 21, 21, 18, 15, 14, 14, 20, 13, 12, 9, 8]\nbusiness_degrees = [73, 59, 57, 49, 34, 48, 31, 32, 39, 35, 52, 50, 39, 45, 32]\nlaw_degrees = [79, 56, 53, 48, 31, 48, 45, 32, 41, 31, 33, 36, 38, 25, 12]\n\n# Define a more soothing color palette\nbachelors_color = \"#a1c3d1\"  # Light teal\nbusiness_color = \"#70a4d3\"   # Medium teal\nlaw_color = \"#4678c9\"        # Dark teal\n\n# Plotting the trends\nx = np.arange(len(industries))  # the label locations\nwidth = 0.25  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(14, 8))\nrects1 = ax.bar(x - width, bachelors_degrees, width, label=\"Bachelor's Degrees\", color=bachelors_color)\nrects2 = ax.bar(x, business_degrees, width, label='Business Degrees', color=business_color)\nrects3 = ax.bar(x + width, law_degrees, width, label='Law Degrees', color=law_color)\n\n# Add vertical lines to visually segregate the industry groups\nax.axvline(x=4.5, color='black', linestyle='--', lw=1)  # End of Symbol production/Knowledge sector\nax.axvline(x=9.5, color='black', linestyle='--', lw=1)  # End of Material production/Knowledge sector\n\nax.text(2.5, 85, 'Symbol production/\\nKnowledge sector', ha='center', va='bottom', fontsize=12)\nax.text(7.5, 85, 'Material production/\\nKnowledge sector', ha='center', va='bottom', fontsize=12)\nax.text(12.5, 85, 'Material production/\\nOutside knowledge sector', ha='center', va='bottom', fontsize=12)\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_xlabel('Industries')\nax.set_ylabel('Percentage of Executives (%)')\n#ax.set_title('Percentage of Executives with Elite Degrees by Industry Group')\nax.set_xticks(x)\nax.set_xticklabels(industries, rotation=45, ha=\"right\")\nax.legend()\n\nfig.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\nhttps://x.com/cremieuxrecueil/status/1831463564575699266/photo/1\n!\nThe chart illustrates the educational backgrounds of extraordinary American achievers across various categories. For example, Harvard University alumni account for approximately 50% of American Philosophical Society members and 35% of Forbes’ most powerful men. Graduate School graduates make up the majority in categories like National Academy of Medicine (over 70%) and Nobel Prize winners (about 60%). On the other hand, Ivy League graduates represent significant shares in categories like Pulitzer Prize winners (40%) and Four-Star Generals (20%). Some categories show missing educational information, such as Senators (around 10%).\nIn the last generation or two, the funnel of opportunity in American society has drastically narrowed, with a greater and greater proportion of our financial, media, business, and political elites being drawn from a relatively small number of our leading universities, together with their professional schools. The rise of a Henry Ford, from farm boy mechanic to world business tycoon, seems virtually impossible today, as even America’s most successful college dropouts such as Bill Gates and Mark Zuckerberg often turn out to be extremely well-connected former Harvard students. Indeed, the early success of Facebook was largely due to the powerful imprimatur it enjoyed from its exclusive availability first only at Harvard and later restricted to just the Ivy League\nLet’s explore what are demographical and academic attributes of groups attaining eite education\n\n\nChildren from families in the top 1% are more than twice as likely to attend an Ivy-Plus college (Ivy League, Stanford, MIT, Duke, and Chicago) as those from middle-class families with comparable SAT/ACT scores. Two-thirds of this gap is due to higher admissions rates for students with comparable test scores from high-income families\n\n\n\n\nThe highincome admissions advantage at private colleges is driven by three factors: - (1) preferences for children of alumni (legacies) - (2) weight placed on non-academic credentials, which tend to be stronger for students applying from private high schools (feeder) that have affluent student bodies, - (3) recruitment of athletes, who tend to come from higher-income families.\n\n\nReferences\n\nChetty, R., Deming, D., & Friedman, J. (2023). Diversifying Society’s Leaders? The Determinants and Causal Effects of Admission to Highly Selective Private Colleges. National Bureau of Economic Research. https://doi.org/10.3386/w31492\n\n\n\n\nimage.png\n\n\n\nDevelopment cases, where donations influence admissions, are common among Ivy Plus schools. For example, Jared Kushner’s father donated $2.5 million to Harvard before his son’s acceptance. Schools like USC also heavily weigh such cases. Political and celebrity connections—like children of U.S. Senators or famous actors—often sway admissions. This practice highlights how wealth and influence can shape access to top-tier education.\nThe push for legacy admissions persists, even as it faces criticism. Legacy students, those with family ties to alumni, receive special consideration, despite calls to end the practice. Ivy League schools, and some state flagships like Michigan and UVA, continue to favor legacies. For instance, Johns Hopkins recently ended legacy admissions, aiming for more socio-economic diversity, but this may impact future alumni donations.\nAthletics also play a crucial role in admissions at elite schools. Ivies admit a notable number of student-athletes (up to 10% of their student bodies). Brown, with 910 athletes, parallels Michigan despite a smaller overall student body. Schools like Stanford and Duke also recruit heavily for sports like sailing and lacrosse, reinforcing the socioeconomic skew in admissions, with wealthier, predominantly white students disproportionately represented.\n\n\nUnfair Advantage\nMany universities offer tuition remission to employees’ children, with some covering up to 100% of tuition if the student attends the parent’s institution, but only 50-75% if they attend another school. This incentivizes universities to admit employees’ children, minimizing financial loss. Employees’ families, particularly those in academic households, often have greater cultural and academic capital, giving them an admissions advantage. Children of professors or staff, exposed to academic environments and cultural resources, may appear more competitive than wealthier, first-generation applicants, who lack such insider knowledge.\n\n\n# Data for plotting\nyears = list(range(1980, 2021, 5))\nelite_schools = [50, 55, 60, 65, 70, 65, 67, 69, 70]\nnon_elite_schools = [100 - x for x in elite_schools]\n\n# Plotting the data\nplt.figure(figsize=(10, 6))\n\nplt.plot(years, elite_schools, marker='o', linestyle='-', color='blue', label='Elite Schools')\nplt.plot(years, non_elite_schools, marker='o', linestyle='-', color='orange', label='Non-Elite Schools')\n\n# Adding titles and labels\nplt.title('Proportion (appx.) of Executives from Elite vs. Non-Elite Schools (1980-2020)')\nplt.xlabel('Year')\nplt.ylabel('Proportion of Executives (%)')\nplt.ylim(0, 100)\nplt.xticks(years)\nplt.grid(True)\n\n# Adding a legend\nplt.legend()\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe Winner-Take-All Society\n\n\n\nimage.png\n\n\n\nHow certain elite institutions have monopolized access to top jobs.\nFrank, R. H., & Cook, P. J. (1995). The Winner-Take-All Society\nConcentration of Rewards in Elite Institutions: Argument: Frank and Cook argue that in many fields, the rewards (jobs, salaries, opportunities) are increasingly concentrated among those who graduate from elite institutions. These institutions, such as Ivy League universities in the U.S., have become gatekeepers to the most lucrative and prestigious careers. Example: The authors discuss how a small number of elite schools produce a disproportionately high number of individuals in top positions across various industries, from law and finance to academia and government.\n\nhttps://academic.oup.com/qje/article/137/2/845/6449025\n\nWinner-Take-All Markets: Concept: The book introduces the concept of “winner-take-all markets,” where small differences in talent or credentials can lead to vastly different outcomes in terms of success and earnings. In these markets, those at the very top capture the majority of rewards, while the rest receive significantly less. Data: The book cites examples such as the concentration of top lawyers from a handful of law schools or CEOs who predominantly come from elite business schools. This concentration means that individuals from non-elite institutions find it increasingly difficult to compete for top-tier positions.\nImpact on Social Mobility: Argument: The monopolization of access to top jobs by elite institutions exacerbates inequality and reduces social mobility. As these institutions become more selective and expensive, only individuals from affluent backgrounds can afford the education and connections needed to access these opportunities. Data: The book discusses how the children of affluent families are more likely to attend elite institutions, perpetuating a cycle of privilege. In contrast, those from less privileged backgrounds face significant barriers to entry, even if they have similar levels of talent or ambition.\n\nPedigree: How Elite Students Get Elite Jobs” by Lauren A. Rivera\n\nEvery year, elite firms designate lists of schools with which they have established relationships, and where they intend to post job openings, accept applications, and interview students. These lists have two tiers. Core schools are the three to five highly elite institutions from which firms draw the bulk of their new hires. Firms invest deeply at these campuses, flying current employees from across the country—if not the globe—to host information sessions, cocktail receptions, and dinners, prepare candidates for interviews, and interview scores or even hundreds of candidates every year. Target schools, by contrast, include five to fifteen additional institutions where firms intend to accept applications and interview candidates, but on a much smaller scale.14 Firms typically set quotas for each school, with cores receiving far more interview and final offer slots than targets.\n\n…\n\nFirms commonly made their school selections based on general perceptions of these institutions’ prestige. When asked how her law firm created its list, Kayla, a recruitment director, summarized the strategy this way:\n\n\nIt’s totally anecdotal. (She laughs.) I think it’s based upon—and it probably lags in terms of time and updating—but it’s based upon a kind of understanding of how selective the school was in terms of admitting students and how challenging is the work. So it’s largely just kind of school reputation and conventional wisdom, for better or worse.\n\n\nThis kind of anecdotal information was derived from the perceptions of partners and other decision makers (who, disproportionately, were themselves graduates of prestigious schools). In addition, firms used the reports of external rankings organizations, such as U.S. News & World Report and the Law School Admissions Council. However, they typi- cally consulted outside sources only when setting the lower bounds of their lists. Consequently, in contrast to the volatility of national edu- cational rankings, firms’ lists remained quite stable from year to year\n\n…\n\nAlthough stable notions of prestige were the most common basis for designating schools as cores or targets, new or less prestigious schools could be put on the list if the firm had high-ranking employees who were graduates and pushed the firm to recruit from their alma mater. Michael, a banker, told me, “If a senior person has a particular inter- est in going to a particular school, we’ll generally go.” Another banker, Nicholae, described why his alma mater—a well-regarded but not top- ten liberal arts college—was included on his bank’s list of targets. “We started recruiting at [my school] because the CEO’s daughter was in my class there, and now two chairmen’s kids are there. [It’s] a good school, but it’s definitely those types of connections that make us recruit there.” A consultant named Ella provided a similar illustration:\n\n\n\nUVA [the University of Virginia] is actually a big target school of ours. . . . It started because there was a partner who was an alum and who just pushed it hard and so we ended up with actually having quite a big recruiting team associated with that school. Which maybe normally we wouldn’t, given [our firm’s] location and their ranking and what not.\n\n\n\nSuch schools tended to stay on the list as long as the employee who initially pushed for the campus remained at the firm and continued to press for recruitment. Due to organizational inertia, some remained on the list after that employee’s departure."
  },
  {
    "objectID": "notebooks/Education and QoL-Satisfaction.html#high-competitive-stress-1",
    "href": "notebooks/Education and QoL-Satisfaction.html#high-competitive-stress-1",
    "title": "The Diminishing Returns of Education on Quality of Life: An Empirical Analysis of Macroeconomic Decoupling",
    "section": "High Competitive Stress:",
    "text": "High Competitive Stress:\nMental Health Impacts: As the demand for higher education has increased, so has the competition, leading to significant stress among students. The pressure to perform well academically to secure top-tier jobs has contributed to a rise in mental health issues, including anxiety and depression. This competitive stress can negate some of the QoL improvements associated with higher education​ (World Bank).\nWork-Life Imbalance: The need to excel in education often leads to work-life imbalances, where students and young professionals may sacrifice leisure and family time for academic or career success, potentially reducing overall life satisfaction.\n\nAdmission to Ivy League and Other Selective Universities.\nData Sources:\n\nhttps://web.archive.org/web/20150222074515/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2008/\nhttps://web.archive.org/web/20150222070007/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2009/\nhttps://web.archive.org/web/20150222074602/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2010/\nhttps://web.archive.org/web/20150222074712/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2011/\nhttps://web.archive.org/web/20150222071302/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2012/\nhttps://web.archive.org/web/20150222074526/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2013/\nhttps://web.archive.org/web/20150222074708/http://www.hernandezcollegeconsulting.com/ivy-league-admissions-statistics-2014/\nhttps://web.archive.org/web/20150222071001/http://www.hernandezcollegeconsulting.com/ivy-league-admissions-statistics-overall-2014/\nhttps://web.archive.org/web/20150222074653/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-overall-2015/\nhttps://web.archive.org/web/20150222074618/http://www.hernandezcollegeconsulting.com/ivy-league-admissions-statistics-overall-2016/\nhttps://web.archive.org/web/20150222074639/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-2017/\nhttps://web.archive.org/web/20150222004000/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-overall-2018/\nhttps://web.archive.org/web/20150222074612/http://www.hernandezcollegeconsulting.com/ivy-league-admission-statistics-class-2019/\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Data for Ivy League Admission Statistics (2008 - 2019)\ndata = {\n    'Year': [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019],\n    'Harvard_Admission_Rate': [10.3, 9.22, 9.33, 8.97, 7.09, 7.32, 5.90, 6.17, 5.92, 5.79, 5.90, 16.51],\n    'Yale_Admission_Rate': [9.9, 9.67, 8.90, 9.63, 8.29, 7.50, 6.59, 7.35, 6.82, 6.72, 6.26, 16.05],\n    'Princeton_Admission_Rate': [11.9, 10.94, 10.19, 9.46, 9.25, 9.93, 7.29, 8.39, 7.86, 7.29, 7.28, 19.92],\n    'Dartmouth_Admission_Rate': [18.3, 17.02, 15.68, 15.28, 13.24, 12.05, 9.92, 9.73, 9.43, 10.05, 11.50, 25.98],\n    'Brown_Admission_Rate': [15.8, 15.12, 13.82, 14.05, 13.29, 10.84, 8.84, 8.70, 9.60, 9.16, 8.61, 20.46],\n    'Penn_Admission_Rate': [21, 20.80, 17.66, 16.06, 16.44, 17.11, 11.97, 12.26, 12.32, 12.10, 9.90, 23.98],\n    'Columbia_Admission_Rate': [12.76, 12.76, 11.57, 10.57, 10.05, 9.82, 6.89, 6.93, 7.42, 6.89, 6.95, 17.79],\n    'Cornell_Admission_Rate': [28.7, 27.08, 24.68, 21.40, 20.40, 19.10, 15.04, 17.95, 16.19, 15.15, 13.98, 25.00],\n}\n\n# Convert to DataFrame\ndf = pd.DataFrame(data)\n\n# Plotting the trends for each university\nplt.figure(figsize=(14, 8))\n\nuniversities = ['Harvard', 'Yale', 'Princeton', 'Dartmouth', 'Brown', 'Penn', 'Columbia', 'Cornell']\nfor uni in universities:\n    plt.plot(df['Year'], df[f'{uni}_Admission_Rate'], label=uni)\n\nplt.title('Ivy League Admission Rates (2008 - 2019)')\nplt.xlabel('Year')\nplt.ylabel('Admission Rate (%)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Calculate and plot the aggregate Ivy League admission rate (average of all universities)\ndf['Average_Ivy_Admission_Rate'] = df[[f'{uni}_Admission_Rate' for uni in universities]].mean(axis=1)\n\nplt.figure(figsize=(10, 6))\nplt.plot(df['Year'], df['Average_Ivy_Admission_Rate'], label='Average Ivy League', color='black', linewidth=2)\nplt.title('Average Ivy League Admission Rate (2008 - 2019)')\nplt.xlabel('Year')\nplt.ylabel('Admission Rate (%)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMost every Ivy has an undergraduate admissions rate of under 10%. And every single one has a downward trend on admit percentages, meaning it’s harder to get into every Ivy than it was 5, 10, 15, 20, or 25 years ago.\nFrom 2008 to 2019, the average admission rate fell from about 16% to just under 10%, with a sharp increase to over 20% in 2019, possibly due to external factors such as policy changes or exceptional circumstances (#COVID19 ?).\nEach school within the Ivy League has its own trajectory. For instance, Harvard’s acceptance rate dropped from over 10% to around 5%, while Cornell showed more fluctuation, decreasing from 20% to below 15%.\nIvy League universities have consistently maintained small class sizes, yet the number of applicants continues to surge each year. This growing pool of applicants pushes down acceptance rates, which improves their standing in rankings like US News.\n\n\nWhy did number of applicants continues to surge each year ?\n\n\n\nimage.png\n\n\nThe chart illustrates the educational backgrounds of extraordinary American achievers across various categories. For example, Harvard University alumni account for approximately 50% of American Philosophical Society members and 35% of Forbes’ most powerful men. Graduate School graduates make up the majority in categories like National Academy of Medicine (over 70%) and Nobel Prize winners (about 60%). On the other hand, Ivy League graduates represent significant shares in categories like Pulitzer Prize winners (40%) and Four-Star Generals (20%). Some categories show missing educational information, such as Senators (around 10%).\nIn the last generation or two, the funnel of opportunity in American society has drastically narrowed, with a greater and greater proportion of our financial, media, business, and political elites being drawn from a relatively small number of our leading universities, together with their professional schools. The rise of a Henry Ford, from farm boy mechanic to world business tycoon, seems virtually impossible today, as even America’s most successful college dropouts such as Bill Gates and Mark Zuckerberg often turn out to be extremely well-connected former Harvard students. Indeed, the early success of Facebook was largely due to the powerful imprimatur it enjoyed from its exclusive availability first only at Harvard and later restricted to just the Ivy League\n\n\n\nimage.png\n\n\nReferences:\n\nhttps://www.openthebooks.com/assets/1/6/Oversight_IvyLeagueInc_FINAL.pdf\n\nGiven such a narrow and diminishing window of opportunity, academic and wealth background of parent and even parenting style (“Helicopter parenting”) can add competitive advantage to students.\nSince much of America’s elite today emerges from a meritocratic system, akin to ancient Roman or Chinese elite pathways, parents increasingly shape their children’s upbringing to ensure passage through the same achievement gates. “Helicopter parenting,” once seen as irrational, is a strategic response to this competitive landscape, as noted by Pamela Druckerman in 2019.\n\n\n\nimage.png\n\n\n\n\nWhat about mental health ?\nDespite legacy admissions and insider knowledge aiding children, the competition narrows their lives, leaving little room for curiosity or rebellion. This controlled upbringing often robs individuals of the adventurousness seen in past pioneers\n\nThis means that their (students’) lives are way more tightly controlled, in order to compete against everyone else attempting to achieve SUCCESS in the modern era, which is why so many of the most “successful” people according to conventional measurements aren’t very adventurous anymore, there’s not a lot of room for experimentation or much else anymore, since the road to professional success is, for the most part, so very NARROW, and doesn’t tend to reward the inquisitiveness and rebelliousness that many great people of the past had going for them\n\n\n\nDecline in Job Satisfaction Over Time\nIn the mid-1980s, approximately 61% of workers reported being satisfied with their jobs, as shown by studies from NLS and Gallup surveys. However, as of 2021, this percentage has dropped to around 50%, reflecting increasing pressures in the workplace.\nMany of these pressures stem from\n\nshifting expectations and demands,\nexacerbated by the rise of the gig economy,\nrapid technological changes,\nthe COVID-19 pandemic, oppressive hours,\npolitical infighting,\nincreased competition sparked by globalization,\nan “always-on culture” bred by the internet.\n\nThe decline in job satisfaction is noticeable across several industries, especially in sectors that are fast-paced and high- stress, like engineering and finance.\nOne Harvard MBA observed about his Harvard MBA classmate: &gt; “One classmate described having to invest USD 5M a day — which didn’t sound terrible, until he explained that if he put only USD 4M to work on Monday, he had to scramble to place USD 6M on Tuesday, and his co-workers were constantly undermining one another in search of the next promotion. It was insanely stressful work, done among people he didn’t particularly like. He earned about $1.2 million a year and hated going to the office. &gt; ‘I feel like I’m wasting my life,’ he told me.’ When I die, is anyone going to care that I earned an extra percentage point of return? My work feels totally meaningless.’\n\nHe recognized the incredible privilege of his pay and status, but his anguish seemed genuine.\n\n\n‘If you spend 12 hours a day doing work you hate, at some point it doesn’t matter what your paycheck says,’ he told me.\n\n\nThere’s no magic salary at which a bad job becomes good. He had received an offer at a start-up, and he would have loved to take it, but it paid half as much, and he felt locked into a lifestyle that made this pay cut impossible”\n\n\n\n\nimage.png\n\n\nTake David, a stressed engineering manager. Although on paper he holds a managerial role, he feels powerless, with no real authority to make decisions. David, who originally hails from New Zealand, misses his home country and its cultural connection, which further strains his emotional well-being. His day-to-day involves constant delivery pressures, tight deadlines, and an unrelenting push to scale projects. For David, the lack of autonomy and cultural disconnection are significant contributors to his mental health challenges. Despite earning a comfortable salary, David experiences burnout and dissatisfaction, showing that financial reward alone doesn’t guarantee happiness.\nBy contrast, Priya, who runs a small social enterprise in rural Malaysia, has managed to structure her business in ways that align with her personal values and mental well-being. Priya’s business supports indigenous weavers and craftsmen, and while her work can be stressful, the novelty of her enterprise, its connection to community engagement, and the autonomy she enjoys significantly bolster her job satisfaction. Priya’s example illustrates how the five dimensions of job satisfaction—autonomy, novelty, cultural alignment, community engagement, and meaningful work—play pivotal roles in mental well-being. Despite financial challenges, Priya feels fulfilled because her work aligns with her personal values and provides her with control and purpose.\nFor professionals, mental health concerns have intensified over the last decade. The National Bureau of Economic Research (NBER) found that 68% of professionals in high-stress fields like finance and healthcare reported elevated stress and anxiety levels in 2022. Factors such as excessive workload, high expectations, and job insecurity contribute to this decline in mental health. David, as an example, mirrors this growing crisis in the professional world, where job satisfaction diminishes due to the relentless demands of modern work environments.\nIn contrast, professionals like Priya, who have woven community engagement and autonomy into their work, tend to report higher job satisfaction and better mental health outcomes. For Priya, who feels deeply connected to her work, her stress is mitigated by the purpose and authority embedded in her role.\nThe Five Dimensions of Job Satisfaction Research indicates that job satisfaction is most influenced by five dimensions:\n\nAutonomy: The freedom to make decisions and control one’s work. Priya, who runs her own business, enjoys this freedom, while David, despite his managerial role, does not.\nNovelty: Engaging in unique and meaningful work that challenges and stimulates. David’s routine job lacks novelty, while Priya’s socially driven enterprise is constantly evolving.\nCultural Alignment: Feeling connected to one’s values or heritage. David’s detachment from his New Zealand roots impacts his satisfaction, while Priya’s work is intertwined with the culture of rural Malaysia, fostering a sense of belonging.\nCommunity Engagement: Being involved in community-oriented work boosts well-being, as seen with Priya, whose business is centered around helping indigenous communities.\nMeaning and Lower Stress: Finding meaning in one’s work can mitigate stress. Priya’s sense of purpose helps her cope with the stresses of running a business, while David’s lack of meaning leads to burnout.\n\nDoes More Pay Lead to More Happiness? Once basic financial needs are met, additional salary and benefits have diminishing returns on job satisfaction. Studies, including those from NBER, show that salary increases above a certain threshold (around $75,000 annually in the U.S.) no longer significantly affect happiness. Despite receiving a generous salary, David’s discontent stems from the lack of autonomy and personal fulfillment, illustrating that financial compensation alone does not ensure job satisfaction.\n\n\nMental Health of Students: Escalating Concerns\nMental health issues among students have become an alarming trend. Data from the American College Health Association (ACHA) shows that the percentage of students reporting mental health challenges increased from 25% in the early 2000s to nearly 46% by 2020. Anxiety, depression, and other mental health disorders have grown, driven by academic pressures, societal expectations, and increasingly uncertain futures. Over 60% of college students reported experiencing significant anxiety and depression during the COVID-19 pandemic.\nUnlike earlier generations, where students balanced academic stress with social interactions, today’s students face a perfect storm of academic pressure, social media comparisons, and global uncertainties. Financial difficulties also weigh heavily, as tuition fees and student loans add to their stress levels. The frequency at which college students exhibit serious mental health conditions has reached an alarming level. Data from the Healthy Minds Study, an annual survey of US college students, show that the portion of students with a lifetime diagnosis of a mental health condition increased from 22% in 2007 to 36% by 2017.4 According to the Center for Collegiate Mental Health (CCMH) annual surveys, about 60% of students seeking mental health services in 2020 reported prior mental health treatment, compared with 48% in 2012-2013.5 The CCMH 2020 data also indicate that among students who reported that they had registered with a university’s disability office, 42% were for attention-deficit hyperactivity disorder and 32% for a psychological or psychiatric condition.5 Another large-scale survey, the American College Health Association National College Health Assessment II, has revealed an equally sizeable increase in reported mental health concerns among college students over the past 10 years.\nReferences\n\nResponding to the Crisis in College Mental Health: A Call to Action. Patel, Bina Pulkit et al. The Journal of Pediatrics, Volume 257, 113390 https://www.jpeds.com/article/S0022-3476(23)00192-0/fulltext\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Data from the snapshot table\ndata = {\n    \"Year\": [2009, 2014, 2019],\n    \"Felt overwhelming anxiety\": [49.1, 54.0, 65.7],\n    \"Felt so depressed it was difficult to function\": [30.7, 32.6, 45.1],\n    \"Seriously considered suicide\": [6.0, 8.1, 13.3],\n    \"Attempted suicide\": [1.1, 1.3, 2.0],\n    \"Diagnosed with or treated for anxiety\": [10.5, 14.3, 24.3],\n    \"Diagnosed with or treated for depression\": [10.1, 12.0, 20.0]\n}\n\n# Creating a DataFrame\ndf = pd.DataFrame(data)\n\n# Plotting the data\nplt.figure(figsize=(10, 6))\nfor column in df.columns[1:]:\n    plt.plot(df['Year'], df[column], label=column)\n\nplt.title('Trends in Mental Health Among College Students (2009-2019)')\nplt.xlabel('Year')\nplt.ylabel('Percentage (%)')\nplt.legend(loc='upper left', bbox_to_anchor=(1,1))\nplt.grid(True)\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtract Data and Reports on Anxiety and Depression among Students and Early Professionals: Data Sources:\n\n\nNational College Health Assessment (NCHA): The American College Health Association regularly publishes reports on student mental health, including data on anxiety, depression, and other mental health issues.\nWorld Health Organization (WHO): The WHO provides global data on mental health, including anxiety and depression prevalence.\nCenters for Disease Control and Prevention (CDC): The CDC offers data on mental health trends in the U.S., including among young adults.\nNational Institute of Mental Health (NIMH): NIMH provides comprehensive data and reports on the prevalence of anxiety and depression across different age groups, including early professionals.\nPubMed and Google Scholar: Academic studies published on these platforms can offer insights into how anxiety and depression have evolved over time, particularly in students and early professionals.\nReports:\nLook for reports from educational institutions, mental health organizations, and government health departments that discuss trends in mental health issues among students and professionals over the years. Surveys from organizations like the Gallup-Sharecare Well-Being Index or the Mental Health Foundation (UK) might also provide relevant insights.\n\n\nQuantify the Cost of Disease Burden: Economic Burden:\n\n\nDirect Costs: These include medical costs related to the treatment of anxiety and depression, including therapy, medication, and hospitalization.\nIndirect Costs: These encompass lost productivity due to absenteeism, presenteeism (reduced productivity while at work), and the long-term impact of mental health issues on career progression.\nIntangible Costs: These involve the emotional toll on individuals and their families, which can be harder to quantify but is crucial in understanding the full impact of mental health issues.\nSources for Economic Data:\nWHO Global Health Estimates: Provides data on the burden of mental health disorders globally, including the economic impact. Health Economics Studies: Published research papers on the economic burden of mental health disorders often quantify the cost of diseases like anxiety and depression in monetary terms. National Health Expenditure Data: Some countries provide data on national health expenditures, which can include spending on mental health.\nhttps://www.kff.org/mental-health/issue-brief/exploring-the-rise-in-mental-health-care-use-by-demographics-and-insurance-status/\nhttps://www.statnews.com/2017/02/06/mental-health-college-students/\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC10625532/\n\n\nimport matplotlib.pyplot as plt\n\n# Data from the table\nyears = [2019, 2020, 2021, 2022]\nage_18_26 = [18, 22, 22, 26]\nage_27_50 = [19, 20, 23, 25]\nage_51_64 = [20, 21, 21, 23]\nage_65_plus = [19, 19, 19, 20]\n\n# Plotting the data\nplt.figure(figsize=(10, 6))\nplt.plot(years, age_18_26, marker='o', label='Ages 18-26')\nplt.plot(years, age_27_50, marker='o', label='Ages 27-50')\nplt.plot(years, age_51_64, marker='o', label='Ages 51-64')\nplt.plot(years, age_65_plus, marker='o', label='Ages 65+')\n\n# Adding titles and labels\nplt.title('Percentage of Adults Reporting Use of Mental Health Services (2019-2022)')\nplt.xlabel('Year')\nplt.ylabel('Percentage')\nplt.legend(title='Age Groups')\nplt.grid(True)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nNHIS Data\n\ndef categorize_age(age):\n    if 18 &lt;= age &lt;= 26:\n        return '18-26'\n    elif 27 &lt;= age &lt;= 50:\n        return '27-50'\n    elif 51 &lt;= age &lt;= 64:\n        return '51-64'\n    elif age &gt;= 65:\n        return '65+'\n    else:\n        return 'Unknown'\n\n\n# Load your dataset (replace 'nhis_data.csv' with your actual file)\ndf = pd.read_csv('nhis_data/nhis_00001.csv')\n\ndf['Age_Group'] = df['AGE'].apply(categorize_age)\n\ndf = df[df['Age_Group'] != 'Unknown']\n\n# # Define the age group and filter the data\n# age_group = df[(df['AGE'] &gt;= 18) & (df['AGE'] &lt;= 26) & (df['YEAR'] &gt; 2018)]\n\ndf['MENTAL_HEALTH_SERVICE_USE'] = (\n    (df['HEALTHMENT'] == 1) |  # If respondent used mental health services\n    (df['DEPRX'] &gt; 0) |  # If respondent received any prescription for depression\n    (df['DEPFREQ'] &gt; 0)  # Frequency of depressive symptoms, assuming higher means more service use\n).astype(int)\n\ntotal_adults_df = df.groupby(['Age_Group', 'YEAR']).agg({'SAMPWEIGHT': 'sum'}).reset_index()\ntotal_adults_df.rename(columns={'SAMPWEIGHT': 'Total_Adults'}, inplace=True)\n\n# Merge with the original dataframe to add Total_Adults column\ndf = pd.merge(df, total_adults_df, on=['Age_Group', 'YEAR'], how='left')\n\n# Calculate the percentage of adults using mental health services\ndf['MENTAL_HEALTH_SERVICE_USE_PERCENT'] = (df['MENTAL_HEALTH_SERVICE_USE'] / df['Total_Adults']) * 100\n\n# Group by year and age group to calculate the percentage\npercentage_df = df.groupby(['YEAR', 'Age_Group']).agg({\n    'MENTAL_HEALTH_SERVICE_USE_PERCENT': 'mean'\n}).reset_index()\n\n# Calculate the percentage of adults using mental health services\ndf['MENTAL_HEALTH_SERVICE_USE_PERCENT'] = (df['MENTAL_HEALTH_SERVICE_USE'] / df['Total_Adults']) * 100\n\n# Group by year and age group to calculate the percentage\npercentage_df = df.groupby(['YEAR', 'Age_Group']).agg({\n    'MENTAL_HEALTH_SERVICE_USE_PERCENT': 'mean'\n}).reset_index()\n\n# plt.figure(figsize=(10, 6))\n# for age_group in percentage_df['Age_Group'].unique():\n#     subset = percentage_df[percentage_df['Age_Group'] == age_group]\n#     plt.plot(subset['YEAR'], subset['MENTAL_HEALTH_SERVICE_USE_PERCENT'], label=age_group)\n\n# plt.xlabel('Year')\n# plt.ylabel('Percentage of Adults Using Mental Health Services (%)')\n# plt.title('Share of Adults (%) Reporting Use of Mental Health Services by Age Group')\n# plt.legend(title='Age Group')\n# plt.grid(True)\n# plt.show()\n# Pivot the data for a stacked bar plot\npivot_df = percentage_df.pivot(index='YEAR', columns='Age_Group', values='MENTAL_HEALTH_SERVICE_USE_PERCENT')\n\n# Plotting the stacked bar chart\npivot_df.plot(kind='bar', stacked=True, figsize=(10, 6))\n\nplt.xlabel('Year')\nplt.ylabel('Percentage of Adults Using Mental Health Services (%)')\nplt.title('Share of Adults (%) Reporting Use of Mental Health Services by Age Group')\nplt.legend(title='Age Group')\nplt.grid(True)\nplt.show()\n\nDtypeWarning: Columns (4,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv('nhis_data/nhis_00001.csv')\n\n\n\n\n\n\n\n\n\nOvereducation and depressive symptoms: diminishing mental health returns to education (https://sci-hub.se/10.1111/1467-9566.12039) &gt; In general, well-educated people enjoy better mental health than those with less education. As a result, some wonder whether there are limits to the mental health benefits of education. Inspired by the literature on the expansion of tertiary education, this article explores marginal mental health returns to education and studies the mental health status of overeducated people. To enhance the validity of the findings we use two indicators of educational attainment – years of education and ISCED97 categories – and two objective indicators of overeducation (the realised matches method and the job analyst method) in a sample of the working population of 25 European countries (unweighted sample N = 19,089). Depression is measured using an eight-item version of the CES-D scale. We find diminishing mental health returns to education. In addition, overeducated people report more depression symptoms. Both findings hold irrespective of the indicators used. The results must be interpreted in the light of the enduring expansion of education, as our findings show that the discussion of the relevance of the human capital perspective, and the diploma disease view on the relationship between education and modern society, is not obsolete.\n\nISCED 0 = Early childhood education\nISCED 1 = Primary Education\nISCED 2 = Lower Secondary Education\nISCED 3 = Upper Secondary Education\nISCED 4 = Post-secondary non-Tertiary Education\nISCED 5 = Short-cycle tertiary education\nISCED 6 = Bachelors degree or equivalent tertiary education level\nISCED 7 = Masters degree or equivalent tertiary education level\nISCED 8 = Doctoral degree or equivalent tertiary education level\n\n\n\nThe Impact of PhD Studies on Mental Health—A Longitudinal Population Study\nReferences: https://lucris.lub.lu.se/ws/portalfiles/portal/194583123/WP24_5.pdf\n\n\n\nimage.png"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "",
    "text": "A 35-Year Quantitative Analysis of Factor Premiums (1990–2025)"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#high-level-structure-of-the-paper",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#high-level-structure-of-the-paper",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "2. High-Level Structure of the Paper",
    "text": "2. High-Level Structure of the Paper\n\nStory – why this matters (intro + conclusion)\nData – what you measure (markets, factors, liquidity proxies)\nModels – how you measure (liquidity index, regimes, cross-sectional tests)\nPortfolio – what to do with it (conditional allocation"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#abstract-150200-words",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#abstract-150200-words",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "3. Abstract (150–200 words)",
    "text": "3. Abstract (150–200 words)\n\nThe “valuations don’t matter in infinite money printing” meme.\nYour construction of a liquidity index and liquidity regimes.\nKey findings:\nValuations do matter, but conditional on liquidity regimes.\nGrowth dominates in high-liquidity, negative-real-rate regimes.\nValue premia revive after QT / real rate normalization.\n\nPortfolio takeaway: a simple regime-aware factor-tilt model."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#motivation",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#motivation",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "4. Motivation",
    "text": "4. Motivation\n\nNarrative: post-GFC QE, ZIRP, NIRP, 2010–2021 “TINA”, tech bubble 2.0.\n\n“We live in a world of infinite money printing, so valuations don’t matter.”\n\nResearch question: &gt; Do valuations truly die under abundant liquidity, or do they merely become regime-dependent?\nDo valuation spreads (cheap vs expensive deciles) expand under high-liquidity regimes and compress under tight liquidity?\nHow do factor premia (value, momentum, quality, low vol) vary conditional on liquidity regimes?\nCan a macro-liquidity index predict the timing of factor rotations (growth vs value, long-duration vs short-duration equities)?\nDoes a regime-aware factor allocation deliver better risk-adjusted returns than a static factor mix?"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#intuition",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#intuition",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "5. Intuition",
    "text": "5. Intuition\n\nValuation spread: the gap between cheap and expensive stocks (e.g., top vs bottom decile by B/M).\nLiquidity regime: periods of systematically easy vs tight financial conditions driven by monetary and fiscal variables."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#literature-sketch-very-short",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#literature-sketch-very-short",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "6. Literature Sketch (very short)",
    "text": "6. Literature Sketch (very short)\n\nSystemic Strategy: Factor investing: value, momentum, quality.\nRisk Factors and Risk premia: Liquidity\nRegime-switching: macro-conditional factors."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#contributions",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#contributions",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "7. Contributions",
    "text": "7. Contributions\n\nBuild a macro liquidity index from multiple public series.\nUse HMM / Markov regimes to label high vs low liquidity states.\nShow how valuation spreads and factor returns behave across liquidity regimes.\nPropose a simple implementable regime-aware factor strategy."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#data-variables",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#data-variables",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "8. Data & Variables",
    "text": "8. Data & Variables\n\n\n8.1 Timeframe & Frequency\n\nHorizon: 1990–2025, monthly frequency (or weekly if feasible).\nMarket: US equities (CRSP/Compustat-type universe); optional robustness on AUS/EU.\n\n\n\n\n8.2 Equity & Factor Data\n\nIndividual stock returns \\(r_{i,t}\\), market cap, book equity, earnings, etc.\nCharacteristics \\(X_{i,t}\\):\n\n\\[\n\\text{B/M},\\ \\text{E/P},\\ \\text{P/S},\\ \\text{size},\\ \\beta,\\ \\text{12–1 momentum},\\ \\text{profitability},\\ \\text{volatility}.\n\\]\n\nFactor returns \\(f_t\\):\n\n\\[\n\\text{MKT},\\ \\text{HML},\\ \\text{SMB},\\ \\text{MOM},\\ \\text{Quality},\\ \\text{LowVol}.\n\\]\nDefine a valuation spread:\n\\[\nV_t^{\\text{spread}} = \\text{Long top decile of B/M} - \\text{Short bottom decile}.\n\\]\n\n\n\n8.3 Macro & Liquidity Variables\nLet raw macro/liquidity variables at time \\(t\\) be \\(x_{j,t}\\), including:\n\n\nMonetary & Balance Sheet\n\nMoney growth: \\[\\Delta \\log(M2_t)\\]\nFed balance sheet growth: \\[\\Delta \\log(\\text{BS}_t)\\]\n\n\n\n\nRates & Term Structure\n\nShort rate: \\[i_t\\]\n10Y yield: \\[y_{10,t}\\]\nSlope: \\[\\text{TS}_t = y_{10,t} - i_t\\]\nReal short rate: \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\]\n\n\n\n\nRisk & Spreads\n\nCredit spread: \\[\\text{CS}_t = y_{\\text{Baa},t} - y_{\\text{Aaa},t}\\]\nVIX level: \\[\\text{VIX}_t\\]\n\n\n\n\nPlumbing Variables (if available)\n\nON RRP usage\nTGA balance\n\n\nAll variables \\(x_{j,t}\\) will then be standardized (e.g., z-scored) and aggregated into a single liquidity index."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#math",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#math",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "9. Math",
    "text": "9. Math\n\n1. Liquidity Proxies & Standardisation\nLet the raw liquidity indicators be collected in the vector\n\\[\nx_t =\n\\begin{bmatrix}\nx_{1,t} \\\\\nx_{2,t} \\\\\n\\vdots \\\\\nx_{J,t}\n\\end{bmatrix}\n\\in \\mathbb{R}^J.\n\\]\nFor each series \\[x_{j,t}\\], compute the sample mean \\[\n\\mu_j = \\frac{1}{T} \\sum_{t=1}^{T} x_{j,t}\n\\] and variance \\[\n\\sigma_j^2 = \\frac{1}{T-1} \\sum_{t=1}^{T} (x_{j,t} - \\mu_j)^2.\n\\]\nStandardised variables are then \\[\nz_{j,t} = \\frac{x_{j,t} - \\mu_j}{\\sigma_j}, \\qquad j = 1,\\dots,J,\n\\] with stacked vector \\[z_t = (z_{1,t},\\dots,z_{J,t})^\\top\\].\nSeries may be sign-flipped so that higher values of \\[z_{j,t}\\] correspond to easier liquidity; for example, use \\[-r_t^{\\text{real}}\\] instead of \\[r_t^{\\text{real}}\\], \\[-CS_t\\] instead of \\[CS_t\\], and \\[-VIX_t\\] instead of \\[VIX_t\\].\n\n\n\n2. Example Macro / Liquidity Variables\nTypical inputs include \\[\\Delta \\log M2_t\\], \\[\\Delta \\log \\text{BS}_t\\], the term spread \\[\\text{TS}_t = y_{10,t} - i_t\\], the real rate \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\], and the credit spread \\[\\text{CS}_t = y_{Baa,t} - y_{Aaa,t}\\].\n\n\n\n3. Covariance Matrix & PCA Liquidity Index\nDefine the covariance matrix \\[\n\\Sigma = \\frac{1}{T} \\sum_{t=1}^{T} z_t z_t^\\top.\n\\]\nLet \\[(\\lambda_k, v_k)\\] solve \\[\\Sigma v_k = \\lambda_k v_k\\], with eigenvalues ordered \\[\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_J\\]. The first principal-component liquidity index is then \\[L_t = v_1^\\top z_t\\]. Alternatively, one may use a fixed-weight index \\[L_t = w^\\top z_t\\].\n\n\n\n4. HMM Liquidity Regimes\nLet the latent regime variable satisfy \\[s_t \\in \\{1,\\dots,K\\}\\], with regime-conditional dynamics \\[\nL_t \\mid (s_t = k) \\sim \\mathcal{N}(\\mu_k, \\sigma_k^2).\n\\]\nTransition probabilities are \\[\\Pr(s_t = j \\mid s_{t-1} = i) = p_{ij}\\], forming the matrix \\[\nP =\n\\begin{bmatrix}\np_{11} & \\cdots & p_{1K} \\\\\n\\vdots & \\ddots & \\vdots \\\\\np_{K1} & \\cdots & p_{KK}\n\\end{bmatrix},\n\\qquad \\sum_{j=1}^{K} p_{ij} = 1.\n\\]\nFiltered or smoothed regime classification is given by \\[\n\\hat{s}_t = \\arg\\max_{k} \\Pr(s_t = k \\mid L_{1:T}).\n\\]\nDefine the high- and tight-liquidity regimes as \\[k_{\\text{High}} = \\arg\\max_k \\mu_k\\] and \\[k_{\\text{Tight}} = \\arg\\min_k \\mu_k\\], with indicator \\[\nI_t^{\\text{High}} = \\mathbf{1}\\!\\left[\\Pr(s_t = k_{\\text{High}} \\mid L_{1:T}) &gt; 0.5\\right].\n\\]\n\n\n\n5. Multivariate Regime-Switching (Optional)\nIn a multivariate setting, define \\[\ny_t =\n\\begin{bmatrix}\nL_t \\\\\nr_t^{\\text{MKT}} \\\\\nr_t^{\\text{HML}} \\\\\n\\text{VIX}_t \\\\\n\\vdots\n\\end{bmatrix},\n\\] with regime-dependent dynamics \\[y_t = A_k y_{t-1} + \\varepsilon_t^{(k)}\\] and \\[\\varepsilon_t^{(k)} \\sim \\mathcal{N}(0,\\Sigma_k)\\] when \\[s_t = k\\].\n\n\n\n6. Valuation Spreads & Factor Returns by Regime\nLet \\[V_t^{\\text{spread}}\\] denote a valuation-spread series (e.g., top–bottom decile). The regime-conditional mean is \\[\n\\bar{V}^{(k)} = \\mathbb{E}[V_t^{\\text{spread}} \\mid s_t = k],\n\\] with sample estimate \\[\n\\hat{\\bar{V}}^{(k)} =\n\\frac{\\sum_{t=1}^{T} V_t^{\\text{spread}} \\mathbf{1}(\\hat{s}_t = k)}\n{\\sum_{t=1}^{T} \\mathbf{1}(\\hat{s}_t = k)}.\n\\]\nDifferences such as \\[\\hat{\\bar{V}}^{(\\text{High})} - \\hat{\\bar{V}}^{(\\text{Tight})}\\] summarise regime effects. Analogously, for factor returns \\[r_t^{(F)}\\], \\[\n\\bar{r}_F^{(k)} = \\mathbb{E}[r_t^{(F)} \\mid s_t = k],\n\\] with Sharpe ratio \\[\\text{SR}_F^{(k)} = \\hat{\\bar{r}}_F^{(k)} / \\hat{\\sigma}_F^{(k)}\\].\n\n\n\n7. Predictive Regressions with Liquidity\nContinuous-index predictability is tested via \\[\nr_{t+1}^{(F)} = \\alpha + \\beta L_t + \\gamma^\\top c_t + \\varepsilon_{t+1},\n\\] while regime-based predictability uses \\[\nr_{t+1}^{(F)} =\n\\alpha\n+ \\delta_{\\text{High}} I_t^{\\text{High}}\n+ \\delta_{\\text{Tight}} I_t^{\\text{Tight}}\n+ \\delta_{\\text{Neutral}} I_t^{\\text{Neutral}}\n+ \\varepsilon_{t+1}.\n\\]\n\n\n\n8. Cross-Sectional (Fama–MacBeth) by Regime\nAt each time \\[t\\], estimate \\[\nr_{i,t+1} =\n\\alpha_t\n+ \\lambda_{1,t}\\text{Valuation}_{i,t}\n+ \\lambda_{2,t}\\text{Size}_{i,t}\n+ \\lambda_{3,t}\\text{Momentum}_{i,t}\n+ \\dots\n+ \\varepsilon_{i,t+1}.\n\\]\nRegime-conditional slopes satisfy \\[\\bar{\\lambda}_1^{(k)} = \\mathbb{E}[\\lambda_{1,t} \\mid s_t = k]\\], with sample analogue \\[\n\\hat{\\bar{\\lambda}}_1^{(k)} =\n\\frac{\\sum_{t=1}^{T} \\lambda_{1,t} \\mathbf{1}(\\hat{s}_t = k)}\n{\\sum_{t=1}^{T} \\mathbf{1}(\\hat{s}_t = k)}.\n\\]\n\n\n\n9. Regime-Aware Portfolio\nLet \\[f_t \\in \\mathbb{R}^K\\] denote factor returns and \\[w^{(k)} \\in \\mathbb{R}^K\\] the regime-specific weights. The applied weights are \\[w_t = w^{(\\hat{s}_t)}\\], yielding portfolio return \\[\nR_{p,t+1} = w_t^\\top f_{t+1}.\n\\]"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#pca-and-sparse-pca",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#pca-and-sparse-pca",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "PCA and Sparse-PCA",
    "text": "PCA and Sparse-PCA\nStandard PCA solves:\n\\[\n\\max_{v} \\; \\| X v \\|^{2}\n\\quad \\text{s.t.} \\quad \\| v \\|^{2} = 1\n\\]\n\n\n1. PCA wants the direction with maximum variance\nPCA tries to find the direction ( \\(v\\) ) (a unit vector) along which the projected data ( \\(Xv\\) ) has maximum variance.\nVariance of the projection:\n\\[\n\\text{Var}(Xv) = \\frac{1}{n} |Xv|^{2}.\n\\]\nSo maximizing variance is equivalent to maximizing ( \\(|Xv|^{2}\\) ).\n\n\n\n2. Why the constraint ( \\(| v |^{2} = 1\\) )?\nWithout this constraint, the problem would blow up:\n\\[\n|X(\\alpha v)|^{2} = \\alpha^{2} |Xv|^{2},\n\\]\nand the maximum would be infinite by choosing ( \\(\\alpha\\) \\(\\infty\\) ).\nSo PCA forces ( \\(v\\) ) to be a direction, not a magnitude → a unit vector.\n\n\n\n3. Reformulating using the covariance matrix\n\\[\n|Xv|^{2} = v^\\top X^\\top X v.\n\\]\nLet:\n\\[\nS = X^\\top X\n\\]\n(the unnormalized covariance matrix). Then PCA solves:\n\\[\n\\max_{v} ; v^\\top S v\n\\quad\\text{s.t.}\\quad v^\\top v = 1.\n\\]\nThis is exactly the Rayleigh quotient.\n\n\nIntuition\n\nPCA finds the direction (unit vector) along which the data cloud has maximum spread. That direction is exactly the eigenvector of the covariance matrix with the largest eigenvalue."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#sparse-pca",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#sparse-pca",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "Sparse PCA",
    "text": "Sparse PCA\n\n1. Standard PCA\n\nPCA rotates the data into orthogonal directions capturing maximum variance.\nEach principal component is a linear combination of all variables.\nLoadings are typically dense (every variable has some non-zero weight).\nThis makes interpretation difficult:\n\n\n“PC1 of 127 economic multicolinear variables is 127-dimensional mush of everything.”\n\nThis is why PCA is almost never used directly for economic interpretation — PCs are not sparse.\n\n\n\n2. What is Sparse PCA?\nSparse PCA = PCA where most loadings are forced to be zero.\nMathematically:\nStandard PCA solves:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1\n\\]\nSparse PCA adds an L1 penalty (lasso-style sparsity) or constrains the number of non-zero elements:\n\\[\n\\max_{v} \\ |Xv|^2 - \\lambda |v|_1\n\\]\nor equivalently:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1,\\ |v|_1 \\leq c\n\\]\nThis forces:\n\nOnly a small subset of variables to load on each component.\nComponents become interpretable (e.g., PC1 loads on CPI, PCE, core CPI → “inflation”).\n\nThe specific implementation cited (“penalized matrix decomposition” by Witten et al. 2009) solves:\n\\[\n\\max_{u, v} \\ u^\\top X v \\quad\n\\text{s.t. } |u|_2 = 1,\\ |v|_2 = 1,\\ |v|_1 \\le c\n\\]\nThis is a general sparse factor extraction algorithm.\nIntuition: &gt; Sparse PCA forces components to use only the relevant variables, not a smear across 127 series.\n\n\n\nKey Difference\n\nStandard PCA components ≈ dense, uninterpretable mixtures\nSparse PCA components ≈ targeted clusters of interpretable economic variables\n\n\n\n\nWhy sparse PCs become interpretable as macro factors\nTake FRED-MD’s 127 macro series.\nIf you run standard PCA:\n\nPC1 loads on 100+ variables.\nPC2 loads on another 80+.\nInterpreting them is basically hopeless.\n\nSparse PCA, with lasso constraints:\n\nallows PC1 to load only on inflation-related variables\nPC2 to load only on housing-related variables\nPC3 on credit spreads\nPC4 on yields\nPC5 on production\netc.\n\nThis resembles the Stock–Watson macro factor model, but with sparsity for interpretability.\n\n\n\nConcrete example (how sparse PCs isolate macro themes)\nSuppose sparse PCA gives:\n\nSparse PC1 loadings:\n\n\n\nVariable\nLoading\n\n\n\n\nCPI YoY\n0.71\n\n\nCore CPI YoY\n0.68\n\n\nPCE Deflator\n0.66\n\n\nPCE core\n0.64\n\n\nAll other 123 variables\n0\n\n\n\n-&gt; You immediately label PC1 as “inflation factor”.\n\n\nSparse PC2 loadings:\n\n\n\nVariable\nLoading\n\n\n\n\nHousing starts\n0.62\n\n\nBuilding permits\n0.59\n\n\nNew homes sold\n0.61\n\n\nMortgage applications\n0.58\n\n\nEverything else\n0\n\n\n\n-&gt; PC2 = “housing & construction cycle”\n\n\n\nSparse PC3 loadings:\n\n\n\nVariable\nLoading\n\n\n\n\nCorporate spreads (BAA–AAA)\n0.72\n\n\nHigh yield spread\n0.69\n\n\nCommercial paper spread\n0.64\n\n\nOthers\n0\n\n\n\n-&gt; PC3 = “credit stress”\n…and so on.\n\n\nThis does not happen with standard PCA.\nSparse PCA essentially performs dimension reduction + variable selection simultaneously.\n\n\n\nHow sparse PCA yields domain-specific macro factors\nSparse PCA encourages:\n\ngrouping variables that co-move strongly\ndropping variables unrelated to the theme\nchoosing a small number of representative series\n\nGiven economic data naturally clusters (inflation variables co-move, housing variables co-move), sparse PCA isolates these clusters.\nThis is similar to economic intuition:\n\ninflationary variables form a single latent factor\nspreads form a financial stress factor\nyields form a term-structure factor\nemployment variables form a labor factor\n\nSparse PCA “discovers” these clusters automatically.\n\n\n\nWhy does this matter\nSparse PCA → interpretable macro drivers → better regime classification → better factor conditioning.\nEspecially when constructing a macro liquidity index or financial conditions index.\nSparse PCA gives:\n\n\n\nComponent\nInterpretation\n\n\n\n\nPC1\nLiquidity / money conditions\n\n\nPC2\nCredit spreads\n\n\nPC3\nHousing activity\n\n\nPC4\nYield curve / rates\n\n\nPC5\nEmployment\n\n\nPC6\nProduction\n\n\nPC7\nIncome\n\n\nPC8\nMarket stress\n\n\n\nThese are exactly the components McCracken & Ng (2016) find in FRED-MD.\n\n\n\nTL;DR — The clean intuition\n\n\nStandard PCA\n\nDense loadings\nHard to interpret\nPCs are linear mush\n\n\n\nSparse PCA\n\nForces many loadings to zero\nEach PC focuses on a small cluster of variables\nBecomes interpretable as a “macro theme”\nMatches how economists think about the business cycle\nPerfect for regime classification & factor research\n\n\n\nSparse PCs are explainable because:\n\nEconomic data naturally clusters into themes\nSparse PCA forces components to choose only the strongest cluster\nThis matches known macro factors (inflation, employment, credit, etc.)\n\n\n# !pip install pandas pandas_datareader numpy scikit-learn hmmlearn matplotlib\n\nimport pandas as pd\nimport numpy as np\nfrom pandas_datareader import data as pdr\n\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom hmmlearn.hmm import GaussianHMM\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#config-date-range-series",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#config-date-range-series",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "Config: Date Range & Series",
    "text": "Config: Date Range & Series\n\nstart_date = \"1990-01-01\"\nend_date   = \"2025-12-31\"\n\n# --- FRED series codes ---\nFRED_SERIES = {\n    \"M2SL\":   \"M2SL\",      # M2 money stock (monthly, SA)\n    \"FED_BAL\":\"WALCL\",     # Fed balance sheet total assets (weekly)\n    \"TB3M\":   \"TB3MS\",     # 3-Month T-Bill rate (monthly)\n    \"DGS10\":  \"DGS10\",     # 10-Year Treasury yield (daily -&gt; resample monthly)\n    \"BAA\":    \"BAA\",       # Moody's Baa corporate yield (monthly)\n    \"AAA\":    \"AAA\",       # Moody's Aaa corporate yield (monthly)\n    \"CPI\":    \"CPIAUCSL\",  # CPI index (monthly, SA)\n    \"GDP\":    \"GDP\",       # Nominal GDP (quarterly, SAAR)\n}\n\n# For equity factors: Fama-French via pandas_datareader (famafrench)\nFF_FACTORS_DATASET = \"F-F_Research_Data_5_Factors_2x3\""
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#download-macro-liquidity-variables-from-fred",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#download-macro-liquidity-variables-from-fred",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations:",
    "section": "Download Macro & Liquidity Variables from FRED",
    "text": "Download Macro & Liquidity Variables from FRED\n\ndef download_fred_series(series_dict, start, end):\n    \"\"\"\n    Download FRED series into a single monthly DataFrame.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Mapping logical_name -&gt; FRED code.\n    \"\"\"\n    dfs = []\n    for name, code in series_dict.items():\n        print(f\"Downloading {name} ({code}) from FRED...\")\n        s = pdr.DataReader(code, \"fred\", start, end)\n        s = s.rename(columns={code: name})\n        dfs.append(s)\n\n    df = pd.concat(dfs, axis=1)\n    # Ensure monthly freq by end-of-month sampling\n    df = df.resample(\"M\").last()\n    return df\n\nmacro_raw = download_fred_series(FRED_SERIES, start_date, end_date)\nmacro_raw.head()\n\nDownloading M2SL (M2SL) from FRED...\nDownloading FED_BAL (WALCL) from FRED...\nDownloading TB3M (TB3MS) from FRED...\nDownloading DGS10 (DGS10) from FRED...\nDownloading BAA (BAA) from FRED...\nDownloading AAA (AAA) from FRED...\nDownloading CPI (CPIAUCSL) from FRED...\nDownloading GDP (GDP) from FRED...\n\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n1990-01-31\n3166.8\nNaN\n7.64\n8.43\n9.94\n8.99\n127.5\n5872.701\n\n\n1990-02-28\n3179.2\nNaN\n7.74\n8.51\n10.14\n9.22\n128.0\nNaN\n\n\n1990-03-31\n3190.1\nNaN\n7.90\n8.65\n10.21\n9.37\n128.6\nNaN\n\n\n1990-04-30\n3201.6\nNaN\n7.77\n9.04\n10.30\n9.46\n128.9\n5960.028\n\n\n1990-05-31\n3200.6\nNaN\n7.74\n8.60\n10.41\n9.47\n129.1\nNaN\n\n\n\n\n\n\n\n\nmacro_raw.tail()\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2025-08-31\n22108.3\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.4\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.0\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\n22322.4\n6552419.0\n3.78\n4.02\n5.86\n5.26\n325.031\nNaN\n\n\n2025-12-31\nNaN\n6640618.0\n3.59\n4.18\n5.90\n5.31\n326.030\nNaN\n\n\n\n\n\n\n\n\nTransform Raw Macro Series into \\(J\\) Liquidity Proxies \\(x_t\\)\n$ x_t =\n\\[\\begin{bmatrix}\nx_{1,t} \\\nx_{2,t} \\\n\\vdots \\\nx_{J,t}\n\\end{bmatrix}\\]\n\nx_{J,t} \\end{bmatrix} ^J $\n\nMoney & balance sheet growth: \\(\\Delta \\log(\\cdot)\\)\nTerm spread: \\(y_{10} - i_{3m}\\)\nReal short rate: \\(i_{3m} - \\pi_{\\text{year-on-year}}\\)\nCredit spread: \\(\\text{BAA} - \\text{AAA}\\)\netc.​\n\n\ndef build_liquidity_proxies(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Growth of M2 and Fed balance sheet\n    df[\"dlog_M2\"] = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"] = np.log(df[\"FED_BAL\"]).diff()\n\n    # 2. Term structure: 10Y - 3M\n    df[\"term_spread\"] = df[\"DGS10\"] - df[\"TB3M\"]\n\n    # 3. Year-on-year inflation (CPI yoy)\n    df[\"infl_yoy\"] = np.log(df[\"CPI\"]).diff(12)\n\n    # 4. Real short rate: nominal 3M - inflation\n    df[\"real_rate\"] = df[\"TB3M\"] - (100 * df[\"infl_yoy\"])  # TB3M in %, infl_yoy in log -&gt; approx*100\n\n    # 5. Credit spread: BAA - AAA\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # Drop rows with NaNs from diff(12) etc.\n    proxies = df[[\"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\",\n                  \"real_rate\", \"credit_spread\"]].dropna()\n\n    return proxies\n\nliquidity_proxies = build_liquidity_proxies(macro_raw)\n\n\nliquidity_proxies.head()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2003-01-31\n0.005642\n-0.026648\n2.83\n-1.550123\n1.18\n\n\n2003-02-28\n0.006192\n0.012784\n2.54\n-1.927593\n1.11\n\n\n2003-03-31\n0.003448\n0.004200\n2.70\n-1.850353\n1.06\n\n\n2003-04-30\n0.006302\n0.028922\n2.76\n-1.021807\n1.11\n\n\n2003-05-31\n0.010075\n-0.001875\n2.30\n-0.806435\n1.16\n\n\n\n\n\n\n\n\n\nStandardize, Sign-Flip and Stack Liquidity Proxies to Get \\(z_{j,t}\\)\nWe want higher \\(z_{j,t}\\) – to mean easier liquidity.\n\nGrowth of M2 / Fed balance sheet: already “easier = higher value” -&gt; keeping sign\nTerm spread: steeper (more positive) often associated with easier conditions -&gt; keeping sign\nReal rate: easier liquidity when real rates are low -&gt; flipping sign\nCredit spread: easier liquidity when spreads are tight (low) -&gt; flipping sign\n\n\ndef standardize_and_signflip(proxies: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Standardize each column and flip signs so that higher z_{j,t}\n    always corresponds to easier liquidity.\n    \n    Parameters\n    ----------\n    proxies : pd.DataFrame\n        Liquidity proxy variables including flows, stocks, and momentum terms\n    \n    Returns\n    -------\n    pd.DataFrame\n        Standardized and sign-adjusted proxies\n    \"\"\"\n    z = proxies.copy()\n    \n    # Standardize all variables to z-scores\n    scaler = StandardScaler()\n    z_vals = scaler.fit_transform(z)\n    z = pd.DataFrame(z_vals, index=z.index, columns=z.columns)\n    \n    # Sign flips so \"higher\" = easier liquidity\n    sign_flips = {\n        # ==========================================\n        # FLOW VARIABLES (month-over-month changes)\n        # ==========================================\n        \"dlog_M2\": +1,          # Higher M2 growth = easier\n        \"dlog_FED_BAL\": +1,     # Higher Fed BS growth = easier\n        \"term_spread\": +1,      # Steeper yield curve = easier\n        \"real_rate\": -1,        # Higher real rate = TIGHTER → flip to negative\n        \"credit_spread\": -1,    # Higher credit spread = TIGHTER → flip to negative\n        \n        # ==========================================\n        # STOCK/LEVEL VARIABLES (deviations from trend)\n        # ==========================================\n        \"EM\": +1,               # Excess M2 above trend = easier\n        \"EB\": +1,               # Excess Fed BS above trend = easier\n        \"EL_3y\": +1,            # 3-year excess liquidity vs GDP = easier\n        \"ZIRP_dummy\": +1,       # In ZIRP regime = easier\n        \n        # ==========================================\n        # MOMENTUM VARIABLES (rate of change)\n        # ==========================================\n        \"dL_3m\": +1,            # Increasing excess M2 = easier\n        \"dL_12m\": +1,           # 12-month acceleration in excess M2 = easier\n        \"dEB_dt\": +1,           # Accelerating Fed BS expansion = easier\n        \"accel_ZIRP\": +1,       # Entering ZIRP (0→1) = easier; Exiting (1→0) = tighter\n        \n        # Optional: Add second derivatives if included\n        \"d2L_dt2\": +1,          # Positive acceleration = easier\n        \"vol_L\": -1,            # Higher volatility = uncertainty = tighter\n    }\n    \n    # Apply sign flips to all present columns\n    for col, sgn in sign_flips.items():\n        if col in z.columns:\n            z[col] = sgn * z[col]\n        # else:\n        #     # Optionally warn if expected column is missing\n        #     print(f\"Warning: Expected column '{col}' not found in proxies\")\n    \n    return z\n\nz_t = standardize_and_signflip(liquidity_proxies)\n\n\nz_t.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-06-30\n0.054918\n-0.226158\n-1.082298\n-1.208083\n0.782405\n\n\n2025-07-31\n-0.162427\n-0.256413\n-0.996504\n-1.189743\n0.879895\n\n\n2025-08-31\n-0.213553\n-0.325854\n-1.004304\n-1.027790\n0.879895\n\n\n2025-09-30\n-0.035859\n-0.169662\n-0.902911\n-0.890559\n0.953012\n\n\n2025-11-30\n-0.623037\n-0.310671\n-0.902911\n-0.969690\n1.001756\n\n\n\n\n\n\n\n\n\nSparse PCA Liquidity Index \\(L_{t}\\)\n\npd.Series(z_t.index).describe()\n\ncount                              274\nmean     2014-06-15 16:38:32.408759040\nmin                2003-01-31 00:00:00\n25%                2008-10-07 18:00:00\n50%                2014-06-15 00:00:00\n75%                2020-02-21 18:00:00\nmax                2025-11-30 00:00:00\nName: DATE, dtype: object\n\n\n\nz_t.values\n\narray([[ 0.1180247 , -0.81081346,  1.11713917,  0.32626822, -0.41183909],\n       [ 0.20761695,  0.11166407,  0.89095592,  0.51056885, -0.24123275],\n       [-0.23943481, -0.08914208,  1.01574668,  0.47285631, -0.11937107],\n       ...,\n       [-0.21355338, -0.32585415, -1.00430373, -1.0277896 ,  0.87989467],\n       [-0.03585918, -0.1696623 , -0.90291124, -0.89055947,  0.95301167],\n       [-0.62303684, -0.31067073, -0.90291124, -0.96969022,  1.00175634]])\n\n\n\ndef build_sparse_pca_liquidity_index(z_df: pd.DataFrame,\n                                     alpha: float = 1.0,\n                                     random_state: int = 42,\n                                    n_check: int = 3) -&gt; pd.Series:\n    \"\"\"\n    Apply SparsePCA with 1 component to z_{j,t} to obtain L_t.\n    \"\"\"\n    spca = SparsePCA(\n        n_components=1,\n        alpha=alpha,\n        random_state=random_state\n    )\n\n    L_scores = spca.fit_transform(z_df.values)\n    L = pd.Series(L_scores.flatten(), index=z_df.index, name=\"L\")\n\n    # Get loadings\n    loadings = spca.components_[0]\n    \n    print(\"SparsePCA components (loadings):\")\n    loading_dict = {}\n    for coef, col in zip(loadings, z_df.columns):\n        print(f\"  {col}: {coef:+.3f}\")\n        loading_dict[col] = coef\n\n    # ========================================\n    # WEIGHTED VOTING FOR SIGN ENFORCEMENT\n    # ========================================\n    # Get top N variables by absolute loading\n    abs_loadings = np.abs(loadings)\n    top_indices = np.argsort(abs_loadings)[-n_check:][::-1]  # Top n_check, descending\n    \n    print(f\"\\nTop {n_check} variables by absolute loading:\")\n    \n    # Weighted voting: correlation * absolute loading\n    weighted_corr_sum = 0\n    total_weight = 0\n    \n    for idx in top_indices:\n        var = z_df.columns[idx]\n        weight = abs_loadings[idx]\n        corr = np.corrcoef(L, z_df[var])[0, 1]\n        weighted_corr = corr * weight\n        \n        print(f\"  {var}: loading={loadings[idx]:+.3f}, corr={corr:+.3f}, weighted={weighted_corr:+.3f}\")\n        \n        weighted_corr_sum += weighted_corr\n        total_weight += weight\n    \n    # Average weighted correlation\n    avg_weighted_corr = weighted_corr_sum / total_weight\n    \n    print(f\"\\nWeighted average correlation: {avg_weighted_corr:+.3f}\")\n    \n    # Flip if weighted average is negative\n    if avg_weighted_corr &lt; 0:\n        print(\"⚠️  FLIPPING SIGN: L_t weighted correlation negative\")\n        L = -L\n    else:\n        print(\"✅ Sign correct: L_t weighted correlation positive\")\n\n    return L\n\nL_t = build_sparse_pca_liquidity_index(z_t, alpha=0.5)\n\nSparsePCA components (loadings):\n  dlog_M2: +0.429\n  dlog_FED_BAL: +0.524\n  term_spread: +0.493\n  real_rate: +0.390\n  credit_spread: -0.383\n\nTop 3 variables by absolute loading:\n  dlog_FED_BAL: loading=+0.524, corr=+0.698, weighted=+0.366\n  term_spread: loading=+0.493, corr=+0.658, weighted=+0.324\n  dlog_M2: loading=+0.429, corr=+0.577, weighted=+0.248\n\nWeighted average correlation: +0.649\n✅ Sign correct: L_t weighted correlation positive\n\n\n\nL_t.plot(title=\"Sparse PCA Liquidity Index L(t)\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHMM / Markov Regime Detection on \\(L_t\\)\n​Fit a Gaussian HMM on the liquidity index. 2 or 3 regimes ?\n\nfrom hmmlearn.hmm import GaussianHMM\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\ndef fit_hmm_on_liquidity(\n    L: pd.Series,\n    n_states: int = 3,\n    covariance_type: str = \"full\",\n    random_state: int = 42,\n    standardize: bool = True\n):\n    \"\"\"\n    Fit a Gaussian HMM on L(t) and return the model and a DataFrame\n    with inferred regimes and posterior probabilities.\n\n    Parameters\n    ----------\n    L : pd.Series\n        Liquidity index (indexed by date).\n    n_states : int\n        Number of hidden states (regimes).\n    standardize : bool\n        If True, standardize L before fitting the HMM.\n\n    Returns\n    -------\n    model : GaussianHMM\n    df_regime : pd.DataFrame\n        Columns: L, state, p_state_k, state_label\n    \"\"\"\n\n    # 1) Prepare X\n    X = L.values.reshape(-1, 1)\n\n    scaler = None\n    if standardize:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n    # 2) Fit HMM\n    model = GaussianHMM(\n        n_components=n_states,\n        covariance_type=covariance_type,\n        random_state=random_state,\n        n_iter=500\n    )\n    model.fit(X)\n\n    hidden_states = model.predict(X)\n    post_probs = model.predict_proba(X)  # T x n_states\n\n    # 3) Build output DataFrame on original index\n    df_regime = pd.DataFrame(\n        {\"L\": L, \"state\": hidden_states},\n        index=L.index\n    )\n    for k in range(n_states):\n        df_regime[f\"p_state_{k}\"] = post_probs[:, k]\n\n    # 4) Use state means to order regimes\n    means = pd.Series(model.means_.flatten(), index=range(n_states))\n\n    print(\"HMM state means (unsorted, in fitted scale):\")\n    for k, m in means.items():\n        print(f\"  state {k}: mean L = {m:.3f}\")\n\n    # Order states from low liquidity to high liquidity\n    ordering = means.sort_values().index.tolist()\n\n    state_label_map = {}\n    if n_states == 3:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"Neutral\"\n        state_label_map[ordering[2]] = \"High\"\n    elif n_states == 2:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"High\"\n    else:\n        # Generic labels if you ever use more states\n        for i, s in enumerate(ordering):\n            state_label_map[s] = f\"Regime_{i}\"\n\n    df_regime[\"state_label\"] = df_regime[\"state\"].map(state_label_map)\n\n    return model, df_regime\n\n\nn_states = 3\nmodel, regimes_df = fit_hmm_on_liquidity(L_t,n_states)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = 2.689\n  state 1: mean L = 0.379\n  state 2: mean L = -0.861\n\n\n\nregimes_df.tail(1)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-11-30\n-1.620391\n2\n0.000292\n2.068795e-07\n0.999708\nTight\n\n\n\n\n\n\n\n\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nvar_covar = pd.Series(model._covars_.flatten(), index=range(n_states))\n\nmodel_df = pd.DataFrame({'means': means, 'var': var_covar})\nmodel_df\n\n\n\n\n\n\n\n\nmeans\nvar\n\n\n\n\n0\n2.689160\n3.067263\n\n\n1\n0.378674\n0.112808\n\n\n2\n-0.860576\n0.190807\n\n\n\n\n\n\n\n\ndef calculate_regime_summary(regimes_df, L_series, state_col='state_label'):\n    \"\"\"\n    Calculate summary statistics for regime table.\n    \"\"\"\n    \n    summary = []\n    for state in sorted(regimes_df[state_col].unique()):\n        state_mask = regimes_df[state_col] == state\n        L_values = L_series[state_mask]\n        \n        n_months = state_mask.sum()\n        pct_months = 100 * n_months / len(regimes_df)\n        \n        summary.append({\n            'Regime': state,\n            'Mean L(t)': L_values.mean(),\n            'Std Dev': L_values.std(),\n            'Months': n_months,\n            'Percent': pct_months\n        })\n    \n    return pd.DataFrame(summary)\n\nsummary = calculate_regime_summary(regimes_df, L_t)\nprint(summary)\n\n    Regime  Mean L(t)   Std Dev  Months    Percent\n0     High   3.799643  2.379628      12   4.379562\n1  Neutral   0.502652  0.450198     155  56.569343\n2    Tight  -1.154269  0.575142     107  39.051095\n\n\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# -----------------------------\n# TOP PANEL — L(t) + shading\n# -----------------------------\nax[0].plot(regimes_df.index, regimes_df[\"L\"], color=\"black\", lw=1.2)\nax[0].set_title(\"Sparse PCA Liquidity Index L(t) with Regime Shading\")\n\n# Define plain colors for each regime\nregime_colors = {\n    \"Tight\":   \"green\",\n    \"Neutral\": \"gray\",\n    \"High\":    \"crimson\",\n}\n\nshade_alpha = 0.2  # semi-opaque\n\nfor label, color in regime_colors.items():\n    mask = regimes_df[\"state_label\"] == label\n\n    in_region = False\n    start = None\n\n    for i in range(len(mask)):\n        if mask.iloc[i] and not in_region:\n            in_region = True\n            start = regimes_df.index[i]\n\n        elif not mask.iloc[i] and in_region:\n            in_region = False\n            end = regimes_df.index[i]\n            ax[0].axvspan(start, end, color=color, alpha=shade_alpha, linewidth=0)\n\n    # If region extends to the end of the sample\n    if in_region:\n        ax[0].axvspan(start, regimes_df.index[-1], color=color, alpha=shade_alpha, linewidth=0)\n\n# Legend for regimes (shaded colors)\nfrom matplotlib.lines import Line2D\nlegend_patches = [\n    Line2D([0], [0], color=\"green\",   lw=6, alpha=shade_alpha, label=\"Tight\"),\n    Line2D([0], [0], color=\"gray\",    lw=6, alpha=shade_alpha, label=\"Neutral\"),\n    Line2D([0], [0], color=\"crimson\", lw=6, alpha=shade_alpha, label=\"High\"),\n]\nax[0].legend(handles=legend_patches, loc=\"upper left\")\n\n# -----------------------------\n# BOTTOM PANEL — High regime probability\n# -----------------------------\nif \"p_state_0\" in regimes_df.columns:\n    high_state_id = regimes_df.groupby(\"state_label\")[\"state\"].first()[\"High\"]\n    ax[1].plot(regimes_df.index,\n               regimes_df[f\"p_state_{high_state_id}\"],\n               color=\"crimson\", lw=1.5)\n    ax[1].set_title(\"Posterior Probability of High Liquidity Regime\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEquity Factor Data (Fama–French)\n\n\n\nimage.png\n\n\nThese five factors extend the original 3-factor model to better explain cross-sectional stock returns.\nThey are:\n\nRm–Rf — Market excess return\nSMB — Size (Small Minus Big)\nHML — Value (High Book/Market minus Low)\nRMW — Profitability (Robust Minus Weak)\nCMA — Investment (Conservative Minus Aggressive)\n\nLet’s break each down precisely.\n\n\nRm – Rf: Market Excess Return\n\\[\nR_{m}-R_{f}\n\\] The return of the broad market minus the risk-free rate.\n\nPositive → market went up more than cash.\nNegative → market underperformed cash/T-bills.\n\nIn above table:\n\nLast 12 months: +17.54% → very strong bull market year.\nLast 3 months: +7.40% → strong quarter.\nOctober 2025: +1.95% → moderate positive month.\n\n\n\n\nSMB: Size Factor (Small Minus Big)\n\\[\n\\text{SMB} = R_{\\text{small}} - R_{\\text{big}}\n\\]\n\nPositive → small caps outperform large caps.\nNegative → large caps outperform small caps.\n\nIn above table:\n\nLast 12 months: –10.71% → a massive large-cap dominance year.\nThis is consistent with mega-cap tech leadership.\n\n\n\n\nHML: Value Factor (High Minus Low Book-to-Market)\n\\[\n\\text{HML} = R_{\\text{value}} - R_{\\text{growth}}\n\\]\n\nPositive → value outperforms growth.\nNegative → growth outperforms value.\n\nIn above table:\n\nLast 12 months: –2.36% → growth beat value.\nOctober 2025: –3.10% → especially growth-heavy month.\n\nThis aligns with liquidity-driven growth leadership.\n\n\n\nRMW: Profitability (Robust Minus Weak)\n\\[\n\\text{RMW} = R_{\\text{robust}} - R_{\\text{weak}}\n\\]\nRobust = high operating profitability. Weak = low profitability.\n\nPositive → profitable companies outperform weak ones.\nNegative → weak/low-profit companies outperform.\n\nIn above table:\n\nLast 12 months: –13.60% → very unusual.\nThis means unprofitable firms outperformed profitable ones over the year.\n\nThat usually happens in:\n\nearly speculative bubbles\nliquidity-driven rallies\nretail-led tech/small-cap frenzies\nAI/futuristic narrative periods\n\n\n\n\nCMA: Investment Factor (Conservative Minus Aggressive)\n\\[\n\\text{CMA} = R_{\\text{conservative}} - R_{\\text{aggressive}}\n\\]\n\nConservative = firms investing slowly\nAggressive = firms aggressively expanding assets\n\nHigh investment → lower expected returns historically (consistent with q-theory: empire-building destroys value).\n\nPositive → conservative firms outperform heavy spenders.\nNegative → aggressive investment firms outperform.\n\nIn above table:\n\nLast 12 months: –10.46%\nLast 3 months: –4.47%\nOctober 2025: –4.03%\n\nInterpretation: Aggressively investing companies — think AI, R&D, biotech, high-growth tech — massively outperformed low-investment firms.\n\n\n\n\n\n\n\n\nFactor\nSign\nInterpretation\n\n\n\n\nRm–Rf: +\nStrong bull market\n\n\n\nSMB: –\nMega-caps beat small caps massively\n\n\n\nHML: –\nGrowth beat value\n\n\n\nRMW: –\nLow-profit firms beat profitable firms\n\n\n\nCMA: –\nAggressive-investment firms beat conservative ones\n\n\n\n\nThis is the textbook signature of a liquidity-driven growth/tech momentum regime, almost identical to:\n\n1998–1999 Dotcom\n2019–2021 QE wave\n2023–2024 AI mega-cap boom\n\nIt means:\n\n\nInvestors preferred\n\nlarge\ngrowth\nunprofitable\nhigh-spending\nhigh-duration tech/AI/future-theme stocks\n\n\n\nFundamentals (profitability, valuation, conservative investment) were penalized\nThis is exactly the kind of environment where:\n\nValue fails\nSmall cap fails\nProfitability fails\nInvestment works negatively\nGrowth dominates\nMega-caps dominate\n\n\ndef download_ff_factors(start, end):\n    \"\"\"\n    Download monthly Fama-French 5 factors (2x3) using pandas_datareader.\n    \"\"\"\n    print(\"Downloading Fama-French factors (famafrench)...\")\n    ff_raw = pdr.DataReader(\n        name=FF_FACTORS_DATASET,\n        data_source=\"famafrench\",\n        start=start,\n        end=end\n    )[0]  # table [0] contains the data\n\n    ff = (ff_raw\n          .divide(100)  # convert from % to decimal\n          .reset_index(names=\"date\")\n          .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n          .rename(str.lower, axis=\"columns\")\n          .rename(columns={\"mkt-rf\": \"mkt_excess\"}))\n\n    ff = ff.set_index(\"date\")\n    return ff\n\nff_factors = download_ff_factors(start_date, end_date)\n\nDownloading Fama-French factors (famafrench)...\n\n\nFutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n&lt;ipython-input-199-55f7e03990a5&gt;:6: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n\n\n\nff_factors.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-07-01\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-01\n0.0185\n0.0488\n0.0442\n-0.0067\n0.0208\n0.0038\n\n\n2025-09-01\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-10-01\n0.0196\n-0.0131\n-0.0309\n-0.0522\n-0.0403\n0.0037\n\n\n2025-11-01\n-0.0013\n0.0147\n0.0376\n0.0143\n0.0068\n0.0030\n\n\n\n\n\n\n\n\n\nFix Fama–French dates (shift one month backward) to align with regimes_df\n\n# Fix Fama–French dates (shift one month backward)\nff_adj = ff_factors.copy()\n\n### Actual Fama-French Convention:\n#\n# Label          Contains Returns FOR\n# 2020-03-01  →  March 2020 (Mar 1 - Mar 31)\n# 2020-04-01  →  April 2020 (Apr 1 - Apr 30)\n# 2020-05-01  →  May 2020 (May 1 - May 31)\nff_adj.index = ff_adj.index.to_period(\"M\").to_timestamp(\"M\")\n\nff_adj.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-07-31\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-31\n0.0185\n0.0488\n0.0442\n-0.0067\n0.0208\n0.0038\n\n\n2025-09-30\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-10-31\n0.0196\n-0.0131\n-0.0309\n-0.0522\n-0.0403\n0.0037\n\n\n2025-11-30\n-0.0013\n0.0147\n0.0376\n0.0143\n0.0068\n0.0030\n\n\n\n\n\n\n\n\n\nJoin regimes_df with Fama–French factors\n\n# Ensure regimes are month-end\nreg = regimes_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined = reg.join(ff_adj, how=\"inner\")\n\n\ncombined.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2025-06-30\n-1.384971\n2\n1.763643e-10\n3.461616e-08\n1.000000\nTight\n0.0486\n-0.0002\n-0.0161\n-0.0320\n0.0144\n0.0034\n\n\n2025-07-31\n-1.481025\n2\n4.213931e-09\n1.477532e-08\n1.000000\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-31\n-1.480004\n2\n1.745053e-07\n1.492070e-08\n1.000000\nTight\n0.0185\n0.0488\n0.0442\n-0.0067\n0.0208\n0.0038\n\n\n2025-09-30\n-1.248705\n2\n7.336618e-06\n1.134537e-07\n0.999993\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-11-30\n-1.620391\n2\n2.921088e-04\n2.068795e-07\n0.999708\nTight\n-0.0013\n0.0147\n0.0376\n0.0143\n0.0068\n0.0030\n\n\n\n\n\n\n\n\n\n\nFactor returns by liquidity regime\n\ndef calculate_factor_statistics_by_regime_accurate(df, factor_cols, regime_col='state_label_lag1', \n                                                   regime_order=['High', 'Neutral', 'Tight']):\n    \"\"\"\n    Calculate comprehensive factor statistics by regime with accurate annualization.\n    \n    Uses geometric compounding for returns and proper volatility scaling.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        DataFrame with monthly factor returns and regime labels\n    factor_cols : list\n        List of factor column names\n    regime_col : str\n        Column name for regime labels\n    regime_order : list\n        Order of regimes for display\n    \n    Returns:\n    --------\n    pd.DataFrame with all statistics\n    \"\"\"\n    \n    # Calculate statistics by regime\n    stats = {}\n    \n    for regime in regime_order:\n        if regime not in df[regime_col].unique():\n            continue\n            \n        regime_data = df[df[regime_col] == regime][factor_cols]\n        \n        stats[regime] = {\n            'mean': regime_data.mean(),\n            'std': regime_data.std(),\n            'sharpe': regime_data.mean() / regime_data.std(),\n            'n_months': len(regime_data),\n            'data': regime_data  # Store for geometric calculation\n        }\n    \n    # Build results dataframe\n    results = []\n    \n    for factor in factor_cols:\n        row = {'Factor': factor.upper()}\n        \n        for regime in regime_order:\n            if regime in stats:\n                # Monthly statistics (as percentages)\n                monthly_mean = stats[regime]['mean'][factor]\n                monthly_std = stats[regime]['std'][factor]\n                monthly_sharpe = stats[regime]['sharpe'][factor]\n                \n                row[f'{regime}_mean'] = monthly_mean * 100\n                row[f'{regime}_std'] = monthly_std * 100\n                row[f'{regime}_sharpe'] = monthly_sharpe\n                \n                # ACCURATE ANNUALIZATION\n                \n                # 1. Annualized return using geometric compounding\n                # Formula: (1 + r_m)^12 - 1\n                ann_return = (1 + monthly_mean)**12 - 1\n                \n                # 2. Annualized volatility\n                # Under IID assumption: σ_annual = σ_monthly * sqrt(12)\n                ann_std = monthly_std * np.sqrt(12)\n                \n                # 3. Annualized Sharpe ratio (two methods - should be equivalent)\n                # Method 1: Scale monthly Sharpe by sqrt(12)\n                ann_sharpe_method1 = monthly_sharpe * np.sqrt(12)\n                \n                # Method 2: Direct calculation from annualized values\n                ann_sharpe_method2 = ann_return / ann_std if ann_std &gt; 0 else np.nan\n                \n                # Store annualized values\n                row[f'{regime}_mean_ann'] = ann_return * 100\n                row[f'{regime}_std_ann'] = ann_std * 100\n                row[f'{regime}_sharpe_ann'] = ann_sharpe_method1  # Using standard method\n                \n                # Optional: Store geometric return for verification\n                # This calculates actual cumulative return over the period\n                actual_returns = stats[regime]['data'][factor].values\n                cumulative_return = np.prod(1 + actual_returns) - 1\n                n_years = len(actual_returns) / 12\n                if n_years &gt; 0:\n                    geometric_annual = (1 + cumulative_return)**(1/n_years) - 1\n                    row[f'{regime}_geom_ann'] = geometric_annual * 100\n        \n        results.append(row)\n    \n    return pd.DataFrame(results)\n\n\ndef print_formatted_table_accurate(stats_df, regime_order=['High', 'Tight'], show_geometric=False):\n    \"\"\"\n    Print a beautifully formatted table with accurate annualization.\n    \"\"\"\n    \n    print(\"\\n\" + \"=\"*90)\n    print(\"FACTOR RETURNS BY REGIME (Monthly)\")\n    print(\"=\"*90)\n    \n    # Header\n    print(f\"{'':6}\", end='')\n    print(f\"{'Mean Return':&gt;24}  {'Std Dev':&gt;24}  {'Sharpe Ratio':&gt;24}\")\n    \n    print(f\"{'Factor':6}\", end='')\n    for regime in regime_order:\n        print(f\"{regime:&gt;11} \", end='')\n    print(\" \", end='')\n    for regime in regime_order:\n        print(f\"{regime:&gt;11} \", end='')\n    print(\" \", end='')\n    for regime in regime_order:\n        print(f\"{regime:&gt;11} \", end='')\n    print()\n    \n    print(\"-\"*90)\n    \n    # Data rows\n    for _, row in stats_df.iterrows():\n        factor = row['Factor']\n        print(f\"{factor:6}\", end='')\n        \n        # Mean returns\n        for regime in regime_order:\n            val = row[f'{regime}_mean']\n            print(f\"{val:10.2f}% \", end='')\n        print(\" \", end='')\n        \n        # Std dev\n        for regime in regime_order:\n            val = row[f'{regime}_std']\n            print(f\"{val:10.2f}% \", end='')\n        print(\" \", end='')\n        \n        # Sharpe\n        for regime in regime_order:\n            val = row[f'{regime}_sharpe']\n            print(f\"{val:11.3f} \", end='')\n        print()\n    \n    # Annualized section\n    print()\n    print(\"Annualized (geometric compounding: (1+r_m)^12-1; volatility: σ_m*√12)\")\n    print(\"-\"*90)\n    \n    for _, row in stats_df.iterrows():\n        factor = row['Factor']\n        print(f\"{factor:6}\", end='')\n        \n        # Annualized mean returns\n        for regime in regime_order:\n            val = row[f'{regime}_mean_ann']\n            print(f\"{val:10.1f}% \", end='')\n        print(\" \", end='')\n        \n        # Annualized std dev\n        for regime in regime_order:\n            val = row[f'{regime}_std_ann']\n            print(f\"{val:10.1f}% \", end='')\n        print(\" \", end='')\n        \n        # Annualized Sharpe\n        for regime in regime_order:\n            val = row[f'{regime}_sharpe_ann']\n            print(f\"{val:11.3f} \", end='')\n        print()\n    \n    # Optional: Show geometric returns based on actual cumulative performance\n    if show_geometric:\n        print()\n        print(\"Geometric Annual Returns (from actual cumulative performance)\")\n        print(\"-\"*90)\n        \n        for _, row in stats_df.iterrows():\n            factor = row['Factor']\n            print(f\"{factor:6}\", end='')\n            \n            for regime in regime_order:\n                if f'{regime}_geom_ann' in row:\n                    val = row[f'{regime}_geom_ann']\n                    print(f\"{val:10.1f}% \", end='')\n                else:\n                    print(f\"{'N/A':&gt;11} \", end='')\n            print()\n    \n    print(\"=\"*90 + \"\\n\")\n\n\ndf2 = combined.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"mkt_excess\", \"hml\", \"rmw\", \"cma\"])\n\n# Define factors\nfactor_cols = ['mkt_excess', 'hml', 'rmw', 'cma']\n\n# Calculate statistics with ACCURATE annualization\nstats_df = calculate_factor_statistics_by_regime_accurate(\n    df2, \n    factor_cols, \n    regime_col='state_label_lag1',\n    regime_order=['High', 'Neutral', 'Tight']\n)\n\n# Print formatted table (like your image but with accurate formulas)\nprint_formatted_table_accurate(stats_df, regime_order=['High', 'Tight'], show_geometric=True)\n\n\n==========================================================================================\nFACTOR RETURNS BY REGIME (Monthly)\n==========================================================================================\n                   Mean Return                   Std Dev              Sharpe Ratio\nFactor       High       Tight         High       Tight         High       Tight \n------------------------------------------------------------------------------------------\nMKT_EXCESS      1.85%       0.74%        6.96%       4.00%        0.266       0.185 \nHML        -1.75%      -0.53%        4.97%       2.94%       -0.352      -0.179 \nRMW         0.29%       0.03%        1.76%       1.58%        0.164       0.019 \nCMA        -0.28%      -0.35%        1.86%       1.79%       -0.150      -0.197 \n\nAnnualized (geometric compounding: (1+r_m)^12-1; volatility: σ_m*√12)\n------------------------------------------------------------------------------------------\nMKT_EXCESS      24.6%        9.3%        24.1%       13.9%        0.921       0.642 \nHML        -19.1%       -6.1%        17.2%       10.2%       -1.220      -0.620 \nRMW          3.5%        0.4%         6.1%        5.5%        0.569       0.064 \nCMA         -3.3%       -4.1%         6.4%        6.2%       -0.518      -0.681 \n\nGeometric Annual Returns (from actual cumulative performance)\n------------------------------------------------------------------------------------------\nMKT_EXCESS      21.3%        8.2% \nHML        -20.2%       -6.6% \nRMW          3.4%        0.2% \nCMA         -3.5%       -4.3% \n==========================================================================================\n\n\n\n\n\nWhat’s going on ?\nThis is reverse of what you’d expect. In a tight liquidity regime, \\(R^{smb}\\) and \\(R^{hml}\\) should be deeply positive. You’d expect small under-valued companies to outperform big companies with premium valuation.\nLet’s go back to the \\(L_t\\) and understand what’s going on\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L_t.index,\n    y=L_t.values,\n    hue=regimes_df[\"state\"],\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution of L(t) by HMM State\")\nax[1].set_xlabel(\"L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nvar_covar = pd.Series(model._covars_.flatten(), index=range(n_states))\n\nmodel_df = pd.DataFrame({'means': means, 'var': var_covar})\nmodel_df\n\n\n\n\n\n\n\n\nmeans\nvar\n\n\n\n\n0\n2.689160\n3.067263\n\n\n1\n0.378674\n0.112808\n\n\n2\n-0.860576\n0.190807\n\n\n\n\n\n\n\nFrom the KDE (bottom panel):\n\nStates 2 (red) and 1 (yellow) heavily overlap\nOnly state 0 (green) is clearly separated — the “high liquidity spike”\nThis corresponds to crisis/QE events (2008, 2020), where liquidity jumps dramatically\n\nSo the natural clustering is:\n\nOne large cluster (normal-tight-neutral combined)\nOne rare extreme spike cluster\n\nMy HMM is being forced to split the unimodal bulk distribution into two states, resulting in:\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\nI initially thought alpha=0.5 in SparsePCA might be the cause. But the plot shows:\n\nL(t) has a healthy dynamic range across time (≈ −2.5 to +12)\nThe high liquidity cluster is clear and RARE\nThe rest of L(t) is genuinely one blob\n\nIf Sparse PCA was “overshrinking,” L(t) would be compressed; but it is not.\n\n\nQuestion is why high liquidity is rare (look at the KDE of gree state 0) ?\n\nIs Liquidity is fundamentally spiky ?\nWhy ?\nAm I defining Liquidity incorrectly ?\n\nFed/Treasury continuously debased currency throughout 2008-2019 period. Liquidity injection (bailing out banks during GFC/2008, ZIRP era, M2 stimulius of 2019 are visible ones). How would I capture this events as proxy of “liquidity” ?\n\n\nRight now your \\(L_t\\) is basically a short-horizon “flow” liquidity shock index.\n\n\nWhereas I’m looking for slow, structural debasement / regime of easy money (2008–2019, QE, ZIRP, etc.).\n\n\nThose are not the same object.\n\nBecause I use 1-month growth rates \\(\\Delta \\log(\\cdot)\\), my index reacts to spikes / changes, not the level of the monetary stock.\nI need to factor in more factors to the liquidity index such as -\n\nLevel of money / balance sheet relative to trend / GDP, not just monthly change\nProlonged ZIRP / negative real rate period\nExcess liquidity over economic activity\n\n\nLiquidity Vector Augmentation: Two-layer liquidity: Flow vs Stock / Excess\n\n\nAdd level / excess variables\nExamples (all can be pulled from FRED):\nLog level vs pre-2008 trend\nLet \\(m_t = \\log M2_t\\). Fit a linear trend on a pre-QE baseline (say 1985–2007):\n\\(m_t \\approx a + bt \\quad (t \\le 2007)\\)\nThen define excess money stock:\n\\(EM_t = m_t - (a + bt)\\)\nAfter 2008, \\(EM_t\\) becomes increasingly positive if M2 grows above its historical trend.\nSimilarly for the Fed Balance Sheet, let \\(b_t = \\log(\\text{FedBal}_t)\\):\n\\(EB_t = b_t - (\\alpha + \\beta t)\\) (pre-2007 trend)\n\nExcess liquidity versus real economy\nUse real GDP series (e.g., GDPC1) and define:\n\\(EL_t = \\log M2_t - \\log GDP_t\\)\nOr measure multi-year excess growth:\n\\(EL_t(3y) = \\log M2_t - \\log M2_{t-36} - (\\log GDP_t - \\log GDP_{t-36})\\)\n\nZIRP / negative real-rate indicator\n\\(D_t^{ZIRP} = 1(r_t^{real} &lt; 0)\\)\nFrom 2009–2015, this should be mostly 1.\n\n\n\nBuild an augmented feature vector\nInstead of only:\n\\(x_t = \\{ \\Delta \\log M2_t,\\; \\Delta \\log FedBal_t,\\; TS_t,\\; r_t^{real},\\; CS_t \\}\\)\nLet’s expand to:\n\\(x_t^{aug} = \\{\n\\Delta \\log M2_t,\\;\n\\Delta \\log FedBal_t,\\;\nTS_t,\\;\nr_t^{real},\\;\nCS_t,\\;\nEM_t,\\;\nEB_t,\\;\nEL_t,\\;\nEL_t(3y),\\;\nD_t^{ZIRP}\n\\}\\)\nThen standardize and perform PCA / SparsePCA on this.\n\nPC1 (or a combination) loading heavily on \\(EM\\), \\(EB\\), \\(EL\\), \\(D_t^{ZIRP}\\) → structural easy-money regime\n\nPC2 (or your current PC1) loading on short-horizon changes → flow / shock liquidity\n\n\nmacro_raw.tail()\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2025-08-31\n22108.3\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\n30485.729\n\n\n2025-09-30\n22212.4\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\n31095.089\n\n\n2025-10-31\n22298.0\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\n22322.4\n6552419.0\n3.78\n4.02\n5.86\n5.26\n325.031\nNaN\n\n\n2025-12-31\nNaN\n6640618.0\n3.59\n4.18\n5.90\n5.31\n326.030\nNaN\n\n\n\n\n\n\n\n\n# 1. Start from the original GDP values only (drop NaNs)\ngdp_series = macro_raw[\"GDP\"].dropna()\n\n# 2. Collapse to unique quarter-end values\n#    - If GDP is timestamped at quarter *start* (e.g. 2025-04-01),\n#      this will move it to the quarter end (2025-06-30) and give unique labels.\ngdp_q = gdp_series.resample(\"Q\").last()   # quarterly, at quarter-end (e.g. 2025-03-31, 2025-06-30, ...)\n\n# 3. Downsample to monthly and forward-fill within the quarter\ngdp_m = gdp_q.resample(\"M\").ffill()\n\n# 4. Assign back into macro_raw, aligning on the monthly index\nmacro_raw[\"GDP\"] = gdp_m\n\nmacro_raw.tail()\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2025-08-31\n22108.3\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\n30485.729\n\n\n2025-09-30\n22212.4\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\n31095.089\n\n\n2025-10-31\n22298.0\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\n22322.4\n6552419.0\n3.78\n4.02\n5.86\n5.26\n325.031\nNaN\n\n\n2025-12-31\nNaN\n6640618.0\n3.59\n4.18\n5.90\n5.31\n326.030\nNaN\n\n\n\n\n\n\n\n\ndef build_liquidity_proxies_augmented(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Flow proxies\n    df[\"dlog_M2\"]       = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"]  = np.log(df[\"FED_BAL\"]).diff()\n    df[\"term_spread\"]   = df[\"DGS10\"] - df[\"TB3M\"]\n    df[\"infl_yoy\"]      = np.log(df[\"CPI\"]).diff(12)\n    df[\"real_rate\"]     = df[\"TB3M\"] - 100 * df[\"infl_yoy\"]\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # 2. Levels\n    df[\"log_M2\"]      = np.log(df[\"M2SL\"])\n    df[\"log_FED_BAL\"] = np.log(df[\"FED_BAL\"])\n\n    # Use data up to 2007-12-31 to fit trends\n    pre = df.loc[: \"2007-12-31\"].copy()\n\n    # --- Trend for M2 ---\n    pre_m2 = pre[\"log_M2\"].dropna()\n    t_m2   = np.arange(len(pre_m2))\n    coefs_M2 = np.polyfit(t_m2, pre_m2.values, deg=1)\n\n    t_full = np.arange(len(df))\n    trend_M2 = np.polyval(coefs_M2, t_full)\n    df[\"EM\"] = df[\"log_M2\"] - trend_M2\n\n    # --- Trend for Fed balance sheet ---\n    pre_fb = pre[\"log_FED_BAL\"].dropna()\n    t_fb   = np.arange(len(pre_fb))\n    coefs_FB = np.polyfit(t_fb, pre_fb.values, deg=1)\n\n    trend_FB = np.polyval(coefs_FB, t_full)\n    df[\"EB\"] = df[\"log_FED_BAL\"] - trend_FB\n\n    # 3. Excess liquidity vs GDP over 3y\n    df[\"log_GDP\"] = np.log(df[\"GDP\"])\n    df[\"EL_3y\"] = (df[\"log_M2\"] - df[\"log_M2\"].shift(36)) - \\\n                  (df[\"log_GDP\"] - df[\"log_GDP\"].shift(36))\n\n    # 4. ZIRP dummy\n    df[\"ZIRP_dummy\"] = (df[\"real_rate\"] &lt; 0).astype(int)\n\n    # 5. Build augmented proxy matrix\n    cols_aug = [\n        \"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\", \"real_rate\", \"credit_spread\",\n        \"EM\", \"EB\", \"EL_3y\", \"ZIRP_dummy\"\n    ]\n\n    return df[cols_aug].dropna()\n\n\nliquidity_proxies_aug = build_liquidity_proxies_augmented(macro_raw)\n\n\nliquidity_proxies_aug.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n0.196762\n0.724600\n-0.167473\n0\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n0.197696\n0.719428\n-0.150763\n0\n\n\n2025-07-31\n0.003921\n-0.002950\n0.12\n1.554846\n0.65\n0.197296\n0.712962\n-0.146787\n0\n\n\n2025-08-31\n0.003607\n-0.005918\n0.11\n1.223147\n0.65\n0.196583\n0.703528\n-0.141566\n0\n\n\n2025-09-30\n0.004698\n0.000759\n0.24\n0.942084\n0.62\n0.196960\n0.700771\n-0.134480\n0\n\n\n\n\n\n\n\n\nz1_t = standardize_and_signflip(liquidity_proxies_aug)\nL1_t = build_sparse_pca_liquidity_index(z1_t, alpha=0.5)\n\nSparsePCA components (loadings):\n  dlog_M2: +0.135\n  dlog_FED_BAL: +0.166\n  term_spread: +0.406\n  real_rate: +0.508\n  credit_spread: -0.067\n  EM: +0.077\n  EB: +0.101\n  EL_3y: +0.518\n  ZIRP_dummy: +0.491\n\nTop 3 variables by absolute loading:\n  EL_3y: loading=+0.518, corr=+0.886, weighted=+0.460\n  real_rate: loading=+0.508, corr=+0.870, weighted=+0.442\n  ZIRP_dummy: loading=+0.491, corr=+0.841, weighted=+0.413\n\nWeighted average correlation: +0.866\n✅ Sign correct: L_t weighted correlation positive\n\n\n\nn_states=2\nmodel, regimes_aug_df = fit_hmm_on_liquidity(L1_t,n_states=n_states)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = 0.501\n  state 1: mean L = -1.516\n\n\n\n\nRetrospective Validation\n\nsummary_aug = calculate_regime_summary(regimes_aug_df, L1_t)\nprint(summary_aug)\n\n  Regime  Mean L(t)   Std Dev  Months    Percent\n0   High   0.859600  0.900571     205  75.091575\n1  Tight  -2.591442  0.597277      68  24.908425\n\n\n\n# Ensure regimes are month-end\nreg = regimes_aug_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined_aug = reg.join(ff_adj, how=\"inner\")\n\n\n\nCurrent combined_aug structure check\n\nprint(\"=\"*80)\nprint(\"DETAILED DATE ALIGNMENT CHECK: COVID Period (Feb-May 2020)\")\nprint(\"=\"*80)\n\n# Show window around COVID crash\ncovid_window = ['2020-01-31', '2020-02-29', '2020-03-31', '2020-04-30', '2020-05-31']\n\nprint(\"\\n\" + \"-\"*80)\nprint(\"Date         | MKT_EXCESS | HML    | Regime      | What Should This Be?\")\nprint(\"-\"*80)\n\nfor date in covid_window:\n    if date in combined_aug.index:\n        row = combined_aug.loc[date]\n        mkt = row['mkt_excess'] * 100\n        hml = row['hml'] * 100\n        regime = row.get('regime_3state', row.get('state_label', 'Unknown'))\n        \n        # Expected values\n        expected = {\n            '2020-01-31': ('Jan: Normal',          -0.16),\n            '2020-02-29': ('Feb: Start of fear',   -8.23),\n            '2020-03-31': ('Mar: CRASH',          -12.35),\n            '2020-04-30': ('Apr: Recovery',       +12.68),\n            '2020-05-31': ('May: Continued rally', +4.53),\n        }\n        \n        exp_desc, exp_ret = expected.get(date, ('Unknown', 0))\n        \n        # Check if close to expected\n        status = \"✅\" if abs(mkt - exp_ret) &lt; 2.0 else \"❌\"\n        \n        print(f\"{date}   | {mkt:+7.2f}%  | {hml:+6.2f}% | {regime:11s} | {status} {exp_desc} (exp: {exp_ret:+.2f}%)\")\n    else:\n        print(f\"{date} | NOT FOUND IN INDEX\")\n\nprint(\"-\"*80)\n\n================================================================================\nDETAILED DATE ALIGNMENT CHECK: COVID Period (Feb-May 2020)\n================================================================================\n\n--------------------------------------------------------------------------------\nDate         | MKT_EXCESS | HML    | Regime      | What Should This Be?\n--------------------------------------------------------------------------------\n2020-01-31   |   -0.09%  |  -6.22% | High        | ✅ Jan: Normal (exp: -0.16%)\n2020-02-29   |   -8.15%  |  -3.82% | High        | ✅ Feb: Start of fear (exp: -8.23%)\n2020-03-31   |  -13.35%  | -13.83% | High        | ✅ Mar: CRASH (exp: -12.35%)\n2020-04-30   |  +13.58%  |  -1.34% | High        | ✅ Apr: Recovery (exp: +12.68%)\n2020-05-31   |   +5.59%  |  -5.00% | High        | ✅ May: Continued rally (exp: +4.53%)\n--------------------------------------------------------------------------------\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(14, 6))\nax.plot(combined_aug.index, combined_aug['L'], label='L(t)', linewidth=1)\n\n# Color by regime\ncolors = combined_aug['state_label'].map({'High': 'red', 'Tight': 'green'})\nax.scatter(combined_aug.index, combined_aug['L'], c=colors, s=5, alpha=0.5)\n\nax.set_title('Liquidity Index with HMM Regime Colors (Red=High, Green=Tight)')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Check factor performance in specific periods we KNOW the regime\nknown_periods = {\n    '2010-2014 (QE era)': ('2010-01', '2014-12'),\n    '2015-2019 (Late cycle)': ('2015-01', '2019-12'), \n    '2020-2021 (COVID QE)': ('2020-01', '2021-12'),\n    '2022-2024 (QT/hiking)': ('2022-01', '2024-12')\n}\n\nfor label, (start, end) in known_periods.items():\n    period_data = combined_aug.loc[start:end]\n    print(f\"\\n{label}:\")\n    print(f\"  Mean L: {period_data['L'].mean():.2f}\")\n    print(f\"  Dominant regime: {period_data['state_label'].mode()[0]}\")\n    print(f\"  HML: {period_data['hml'].mean():.4f}\")\n    print(f\"  SMB: {period_data['smb'].mean():.4f}\")\n\n\n2010-2014 (QE era):\n  Mean L: 1.13\n  Dominant regime: High\n  HML: -0.0005\n  SMB: 0.0012\n\n2015-2019 (Late cycle):\n  Mean L: -0.15\n  Dominant regime: High\n  HML: -0.0034\n  SMB: -0.0018\n\n2020-2021 (COVID QE):\n  Mean L: 1.86\n  Dominant regime: High\n  HML: -0.0056\n  SMB: 0.0023\n\n2022-2024 (QT/hiking):\n  Mean L: -1.19\n  Dominant regime: Tight\n  HML: 0.0033\n  SMB: -0.0041\n\n\n\n# Define factors\nfactors = ['mkt_excess', 'hml', 'rmw', 'cma']\ndf3 = combined_aug.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf3[\"state_label_lag1\"] = df3[\"state_label\"].shift(-1)\n\n# Calculate statistics with ACCURATE annualization\nstats_df_aug = calculate_factor_statistics_by_regime_accurate(\n    df3, \n    factor_cols, \n    regime_col='state_label_lag1',\n    regime_order=['High', 'Neutral', 'Tight']\n)\n\n# Print formatted table (like your image but with accurate formulas)\nprint_formatted_table_accurate(stats_df_aug, regime_order=['High', 'Tight'], show_geometric=True)\n\n\n==========================================================================================\nFACTOR RETURNS BY REGIME (Monthly)\n==========================================================================================\n                   Mean Return                   Std Dev              Sharpe Ratio\nFactor       High       Tight         High       Tight         High       Tight \n------------------------------------------------------------------------------------------\nMKT_EXCESS      0.88%       0.89%        4.54%       3.69%        0.193       0.240 \nHML         0.06%      -0.29%        3.20%       2.74%        0.018      -0.106 \nRMW         0.28%       0.18%        2.08%       1.51%        0.135       0.118 \nCMA         0.15%      -0.31%        1.93%       1.73%        0.079      -0.181 \n\nAnnualized (geometric compounding: (1+r_m)^12-1; volatility: σ_m*√12)\n------------------------------------------------------------------------------------------\nMKT_EXCESS      11.1%       11.2%        15.7%       12.8%        0.670       0.831 \nHML          0.7%       -3.4%        11.1%        9.5%        0.063      -0.366 \nRMW          3.4%        2.2%         7.2%        5.2%        0.466       0.409 \nCMA          1.8%       -3.7%         6.7%        6.0%        0.272      -0.628 \n\nGeometric Annual Returns (from actual cumulative performance)\n------------------------------------------------------------------------------------------\nMKT_EXCESS       9.7%       10.3% \nHML          0.1%       -3.8% \nRMW          3.1%        2.0% \nCMA          1.6%       -3.9% \n==========================================================================================\n\n\n\n\n\nRoot Cause: Crisis Contamination\nThe problem is 2008-2009 and 2020 dominate my “High” regime but have CRISIS-driven factor dynamics, not QE-driven dynamic\nliquidity index is working perfectly - it correctly identifies:\n2008-2009: Massive liquidity injection (L spikes) 2022-2024: Liquidity withdrawal (L crashes)\nBUT during structural shocks (financial crisis, pandemic):\n\nNormal factor relationships break down\nFlight to quality dominates\nValue/defensive assets outperform despite high liquidity\n\n\nprint(\"2010-2014 QE era:\")\nqe_period = combined_3regime.loc['2010-01':'2014-12']\nprint(f\"  Mean L: {qe_period['L'].mean():.2f}\")\nprint(f\"  Regime mode: {qe_period['regime_3state'].mode()[0]}\")\nprint(f\"  HML: {qe_period['hml'].mean()*100:.2f} bps\")\nprint(f\"  Mean MKT: {qe_period['mkt_excess'].mean()*100:.2f} bps\")\n\nprint(\"\\n2022-2024 QT era:\")\nqt_period = combined_3regime.loc['2022-01':'2024-12']\nprint(f\"  Mean L: {qt_period['L'].mean():.2f}\")\nprint(f\"  Regime mode: {qt_period['regime_3state'].mode()[0]}\")\nprint(f\"  HML: {qt_period['hml'].mean()*100:.2f} bps\")\nprint(f\"  Mean MKT: {qt_period['mkt_excess'].mean()*100:.2f} bps\")\n\nprint(\"\\n2008-09 Crisis:\")\ncrisis_period = combined_3regime.loc['2008-09':'2009-03']\nprint(f\"  Mean L: {crisis_period['L'].mean():.2f}\")\nprint(f\"  Regime mode: {crisis_period['regime_3state'].mode()[0]}\")\nprint(f\"  HML: {crisis_period['hml'].mean()*100:.2f} bps\")\nprint(f\"  Mean MKT: {crisis_period['mkt_excess'].mean()*100:.2f} bps\")\n\n2010-2014 QE era:\n  Mean L: 1.13\n  Regime mode: High\n  HML: -0.12 bps\n  Mean MKT: 1.30 bps\n\n2022-2024 QT era:\n  Mean L: -1.19\n  Regime mode: Tight\n  HML: 0.02 bps\n  Mean MKT: 0.73 bps\n\n2008-09 Crisis:\n  Mean L: 1.08\n  Regime mode: Crisis\n  HML: -2.40 bps\n  Mean MKT: -3.17 bps\n\n\n\ndef analyze_factor_returns_by_regime(combined_aug):\n    \"\"\"\n    Analyze factor returns DURING each regime (contemporaneous).\n    For crisis identification, use date-based approach.\n    \"\"\"\n    df = combined_aug.copy()\n    \n    # ==========================================\n    # STEP 1: Classify regimes (no lag!)\n    # ==========================================\n    # Date-based crisis periods\n    crisis_dates = (\n        ((df.index &gt;= '2008-09') & (df.index &lt;= '2009-03')) |\n        ((df.index &gt;= '2020-02') & (df.index &lt;= '2020-04'))\n    )\n    \n    # Liquidity-based classification\n    df['regime_3state'] = 'Tight'\n    df.loc[df['L'] &gt; 0, 'regime_3state'] = 'High'\n    df.loc[crisis_dates, 'regime_3state'] = 'Crisis'\n    \n    print(\"Regime distribution:\")\n    print(df['regime_3state'].value_counts())\n    print()\n    \n    # ==========================================\n    # STEP 2: Returns DURING each regime\n    # ==========================================\n    factors = ['mkt_excess', 'hml', 'rmw', 'cma']\n    \n    # Monthly returns\n    returns_monthly = df.groupby('regime_3state')[factors].agg(['mean', 'std'])\n    returns_monthly = returns_monthly * 100  # Convert to percentage\n    \n    print(\"=\"*80)\n    print(\"FACTOR RETURNS DURING EACH REGIME (Monthly %)\")\n    print(\"=\"*80)\n    \n    for factor in factors:\n        print(f\"\\n{factor.upper()}:\")\n        for regime in ['Crisis', 'High', 'Tight']:\n            if regime in returns_monthly.index:\n                mean = returns_monthly.loc[regime, (factor, 'mean')]\n                std = returns_monthly.loc[regime, (factor, 'std')]\n                sharpe = mean / std if std &gt; 0 else 0\n                n_months = (df['regime_3state'] == regime).sum()\n                print(f\"  {regime:8s}: {mean:+6.2f}% ± {std:5.2f}%  (Sharpe: {sharpe:+.3f}, N={n_months} months)\")\n    \n    # ==========================================\n    # STEP 3: Annualized returns\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"ANNUALIZED RETURNS (Geometric)\")\n    print(\"=\"*80)\n    \n    for factor in factors:\n        print(f\"\\n{factor.upper()}:\")\n        for regime in ['Crisis', 'High', 'Tight']:\n            if regime in df['regime_3state'].unique():\n                regime_data = df[df['regime_3state'] == regime][factor]\n                \n                # Geometric return: (1+r1)*(1+r2)*...*(1+rN) then annualize\n                cumulative = (1 + regime_data).prod()\n                n_months = len(regime_data)\n                \n                if n_months &gt; 0:\n                    # Annualize: raise to power of (12/n_months)\n                    annual_return = (cumulative ** (12 / n_months) - 1) * 100\n                    print(f\"  {regime:8s}: {annual_return:+6.2f}%\")\n    \n    return df\n\ncombined_3regime = analyze_factor_returns_by_regime(combined_aug)\ncombined_3regime\n\nRegime distribution:\nregime_3state\nHigh      167\nTight      98\nCrisis      8\nName: count, dtype: int64\n\n================================================================================\nFACTOR RETURNS DURING EACH REGIME (Monthly %)\n================================================================================\n\nMKT_EXCESS:\n  Crisis  :  -9.03% ±  5.43%  (Sharpe: -1.662, N=8 months)\n  High    :  +1.33% ±  4.09%  (Sharpe: +0.324, N=167 months)\n  Tight   :  +0.95% ±  3.66%  (Sharpe: +0.261, N=98 months)\n\nHML:\n  Crisis  :  -4.71% ±  6.18%  (Sharpe: -0.761, N=8 months)\n  High    :  +0.25% ±  2.82%  (Sharpe: +0.088, N=167 months)\n  Tight   :  -0.13% ±  2.90%  (Sharpe: -0.045, N=98 months)\n\nRMW:\n  Crisis  :  +0.97% ±  2.17%  (Sharpe: +0.449, N=8 months)\n  High    :  +0.24% ±  2.15%  (Sharpe: +0.110, N=167 months)\n  Tight   :  +0.20% ±  1.53%  (Sharpe: +0.131, N=98 months)\n\nCMA:\n  Crisis  :  +0.10% ±  1.78%  (Sharpe: +0.058, N=8 months)\n  High    :  +0.19% ±  1.99%  (Sharpe: +0.093, N=167 months)\n  Tight   :  -0.25% ±  1.68%  (Sharpe: -0.149, N=98 months)\n\n================================================================================\nANNUALIZED RETURNS (Geometric)\n================================================================================\n\nMKT_EXCESS:\n  Crisis  : -68.47%\n  High    : +16.00%\n  Tight   : +11.19%\n\nHML:\n  Crisis  : -45.16%\n  High    :  +2.56%\n  Tight   :  -2.06%\n\nRMW:\n  Crisis  : +12.08%\n  High    :  +2.61%\n  Tight   :  +2.28%\n\nCMA:\n  Crisis  :  +1.08%\n  High    :  +2.02%\n  Tight   :  -3.14%\n\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\nregime_3state\n\n\n\n\n2003-01-31\n0.869090\n0\n1.000000e+00\n6.190372e-63\nHigh\n-0.0257\n0.0071\n-0.0079\n-0.0093\n0.0069\n0.0010\nHigh\n\n\n2003-02-28\n1.041793\n0\n1.000000e+00\n1.286573e-11\nHigh\n-0.0188\n-0.0087\n-0.0136\n0.0079\n-0.0068\n0.0009\nHigh\n\n\n2003-03-31\n0.957294\n0\n1.000000e+00\n2.965294e-11\nHigh\n0.0109\n0.0077\n-0.0211\n0.0168\n-0.0081\n0.0010\nHigh\n\n\n2003-04-30\n0.917277\n0\n1.000000e+00\n4.386328e-11\nHigh\n0.0818\n0.0114\n0.0122\n-0.0468\n0.0098\n0.0010\nHigh\n\n\n2003-05-31\n0.756118\n0\n1.000000e+00\n2.069103e-10\nHigh\n0.0605\n0.0478\n0.0070\n-0.0694\n0.0307\n0.0009\nHigh\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2025-05-31\n-2.880509\n1\n1.340116e-07\n9.999999e-01\nTight\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\nTight\n\n\n2025-06-30\n-2.709857\n1\n2.610193e-07\n9.999997e-01\nTight\n0.0486\n-0.0002\n-0.0161\n-0.0320\n0.0144\n0.0034\nTight\n\n\n2025-07-31\n-2.689149\n1\n2.839775e-07\n9.999997e-01\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\nTight\n\n\n2025-08-31\n-2.606319\n1\n4.195012e-07\n9.999996e-01\nTight\n0.0185\n0.0488\n0.0442\n-0.0067\n0.0208\n0.0038\nTight\n\n\n2025-09-30\n-2.418377\n1\n4.483466e-05\n9.999552e-01\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\nTight\n\n\n\n\n273 rows × 12 columns\n\n\n\n\ndef classify_three_regimes_hmm(combined_aug):\n    \"\"\"\n    Use HMM states directly instead of L &gt; 0 threshold.\n    \"\"\"\n    df = combined_aug.copy()\n    \n    # Crisis indicator (date-based)\n    crisis = (\n        ((df.index &gt;= '2008-09') & (df.index &lt;= '2009-03')) |\n        ((df.index &gt;= '2020-02') & (df.index &lt;= '2020-04'))\n    )\n    \n    # Use HMM state_label directly\n    # State 1 = High (mean = +1.485)\n    # State 0 = Tight (mean = -0.305)\n    df['regime_3state'] = df['state_label'].copy()\n    \n    # Override crisis\n    df.loc[crisis, 'regime_3state'] = 'Crisis'\n    \n    return df\n\ncombined_3regime = classify_three_regimes_hmm(combined_aug)\n\n# Factor returns by 3 regimes\ndf_3regime = combined_3regime.copy()\ndf_3regime = df_3regime.dropna(subset=[\"regime_3state\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\nmeans_3regime = (\n    df_3regime.groupby(\"regime_3state\")[[\"mkt_excess\", \"hml\", \"rmw\", \"cma\"]]\n    .mean() * 100\n)\n\nprint(\"\\nFactor Returns by 3 Regimes (monthly %):\")\nprint(means_3regime)\n\n\nFactor Returns by 3 Regimes (monthly %):\n               mkt_excess       hml       rmw       cma\nregime_3state                                          \nCrisis          -9.031250 -4.707500  0.975000  0.103750\nHigh             1.190558  0.208934  0.266142  0.135888\nTight            1.184853 -0.182500  0.100147 -0.298235\n\n\n\n# ==========================================\n# DIAGNOSTIC: Known period classification\n# ==========================================\ndef diagnose_regime_classification(combined_df):\n    \"\"\"\n    Check if known historical periods are correctly classified.\n    \"\"\"\n    print(\"=\"*80)\n    print(\"REGIME CLASSIFICATION CHECK: Known Historical Periods\")\n    print(\"=\"*80)\n    \n    known_periods = {\n        '2010-2014 (QE2/QE3)': ('2010-01', '2014-12', 'High'),\n        '2015-2019 (Late Cycle)': ('2015-01', '2019-12', 'High/Mixed'),\n        '2020-2020 (COVID Crisis)': ('2020-02', '2020-04', 'Crisis'),\n        '2020-2021 (COVID QE)': ('2020-05', '2021-12', 'High'),  # Exclude crisis months\n        '2022-2024 (QT)': ('2022-03', '2024-12', 'Tight'),\n    }\n    \n    for label, (start, end, expected) in known_periods.items():\n        period_data = combined_df.loc[start:end]\n        \n        mean_L = period_data['L'].mean()\n        mode_regime = period_data['regime_3state'].mode()[0]\n        regime_dist = period_data['regime_3state'].value_counts()\n        \n        # Factor returns\n        hml = period_data['hml'].mean() * 100\n        cma = period_data['cma'].mean() * 100\n        mkt = period_data['mkt_excess'].mean() * 100\n        \n        print(f\"\\n{label}:\")\n        print(f\"  Mean L: {mean_L:+.2f}\")\n        print(f\"  Classified as: {mode_regime}\")\n        print(f\"  Expected: {expected}\")\n        print(f\"  Distribution: {regime_dist.to_dict()}\")\n        print(f\"  Returns: MKT={mkt:+.2f}%, HML={hml:+.2f}%, CMA={cma:+.2f}%\")\n        \n        # Validation\n        if expected == 'High' and mode_regime != 'High':\n            print(f\"  ⚠️  WARNING: Should be High but classified as {mode_regime}\")\n        elif expected == 'Tight' and mode_regime != 'Tight':\n            print(f\"  ⚠️  WARNING: Should be Tight but classified as {mode_regime}\")\n\n# Run diagnostic\ndiagnose_regime_classification(combined_3regime)\n\n================================================================================\nREGIME CLASSIFICATION CHECK: Known Historical Periods\n================================================================================\n\n2010-2014 (QE2/QE3):\n  Mean L: +1.13\n  Classified as: High\n  Expected: High\n  Distribution: {'High': 60}\n  Returns: MKT=+1.29%, HML=-0.05%, CMA=+0.25%\n\n2015-2019 (Late Cycle):\n  Mean L: -0.15\n  Classified as: High\n  Expected: High/Mixed\n  Distribution: {'High': 49, 'Tight': 11}\n  Returns: MKT=+0.89%, HML=-0.34%, CMA=-0.22%\n\n2020-2020 (COVID Crisis):\n  Mean L: +1.23\n  Classified as: Crisis\n  Expected: Crisis\n  Distribution: {'Crisis': 2, 'High': 1}\n  Returns: MKT=-2.64%, HML=-6.33%, CMA=-0.79%\n\n2020-2021 (COVID QE):\n  Mean L: +2.07\n  Classified as: High\n  Expected: High\n  Distribution: {'High': 20}\n  Returns: MKT=+2.74%, HML=+0.58%, CMA=+0.28%\n\n2022-2024 (QT):\n  Mean L: -1.44\n  Classified as: Tight\n  Expected: Tight\n  Distribution: {'Tight': 21, 'High': 13}\n  Returns: MKT=+0.76%, HML=-0.12%, CMA=-0.27%\n\n\n\nfrom hmmlearn.hmm import GaussianHMM\nfrom scipy.stats import zscore\nimport numpy as np\nimport pandas as pd\n\ndef hierarchical_hmm_regimes(combined_aug):\n    \"\"\"\n    Two-layer HMM for automated regime classification.\n    Layer 1: Crisis vs Normal (based on volatility + returns)\n    Layer 2: High vs Tight (based on liquidity, within Normal)\n    \"\"\"\n    df = combined_aug.copy()\n    \n    # ==========================================\n    # LAYER 1: Crisis Detection HMM\n    # ==========================================\n    # Features: Returns volatility + absolute returns + liquidity vol\n    df['ret_vol'] = df['mkt_excess'].rolling(3).std()\n    df['L_vol'] = df['L'].rolling(3).std()\n    df['abs_ret'] = df['mkt_excess'].abs()\n    \n    # Stack features for crisis detection\n    X_crisis = df[['ret_vol', 'abs_ret', 'L_vol']].dropna()\n    X_crisis_scaled = zscore(X_crisis, nan_policy='omit')\n    \n    # Fit 2-state HMM (Crisis vs Normal)\n    hmm_crisis = GaussianHMM(\n        n_components=2,\n        covariance_type=\"full\",\n        n_iter=500,\n        random_state=42\n    )\n    hmm_crisis.fit(X_crisis_scaled)\n    \n    # Predict crisis states\n    crisis_states = hmm_crisis.predict(X_crisis_scaled)\n    \n    # Identify which state is \"Crisis\" (higher volatility)\n    state_vols = []\n    for s in range(2):\n        mask = crisis_states == s\n        state_vols.append(X_crisis.loc[mask, 'ret_vol'].mean())\n    \n    crisis_state_id = np.argmax(state_vols)\n    \n    df['is_crisis'] = False\n    df.loc[X_crisis.index, 'is_crisis'] = (crisis_states == crisis_state_id)\n    \n    print(f\"Layer 1 (Crisis Detection): {df['is_crisis'].sum()} crisis months identified\")\n    \n    # ==========================================\n    # LAYER 2: High/Tight Liquidity HMM (Normal Periods Only)\n    # ==========================================\n    df_normal = df[~df['is_crisis']].copy()\n    \n    # Use liquidity index for regime classification\n    X_liquidity = df_normal[['L']].values.reshape(-1, 1)\n    \n    # Fit 2-state HMM (High vs Tight)\n    hmm_liquidity = GaussianHMM(\n        n_components=2,\n        covariance_type=\"full\",\n        n_iter=500,\n        random_state=42\n    )\n    hmm_liquidity.fit(X_liquidity)\n    \n    # Predict liquidity states\n    liquidity_states = hmm_liquidity.predict(X_liquidity)\n    \n    # Identify High vs Tight (higher mean L = High)\n    state_means = hmm_liquidity.means_.flatten()\n    high_state_id = np.argmax(state_means)\n    \n    # Assign regimes\n    df['regime_hierarchical'] = 'Tight'  # default\n    df.loc[df_normal.index, 'regime_hierarchical'] = \\\n        np.where(liquidity_states == high_state_id, 'High', 'Tight')\n    df.loc[df['is_crisis'], 'regime_hierarchical'] = 'Crisis'\n    \n    # ==========================================\n    # STATISTICS\n    # ==========================================\n    print(\"\\nLayer 2 (Liquidity Regimes in Normal Periods):\")\n    print(f\"  State means: {state_means}\")\n    print(f\"  High regime: state {high_state_id} (mean L = {state_means[high_state_id]:.3f})\")\n    \n    print(\"\\nFinal regime distribution:\")\n    print(df['regime_hierarchical'].value_counts())\n    \n    return df, hmm_crisis, hmm_liquidity\n\n# Apply hierarchical HMM\ncombined_hier, crisis_model, liquidity_model = hierarchical_hmm_regimes(combined_aug_mom)\n\n# Factor returns by hierarchical regime\nmeans_hier = (\n    combined_hier.groupby(\"regime_hierarchical\")[[\"mkt_excess\", \"hml\", \"rmw\", \"cma\"]]\n    .mean() * 100\n)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"FACTOR RETURNS BY HIERARCHICAL HMM REGIMES (Monthly %)\")\nprint(\"=\"*80)\nprint(means_hier)\n\nLayer 1 (Crisis Detection): 66 crisis months identified\n\nLayer 2 (Liquidity Regimes in Normal Periods):\n  State means: [ 0.64300839 -2.24740945]\n  High regime: state 0 (mean L = 0.643)\n\nFinal regime distribution:\nregime_hierarchical\nHigh      138\nCrisis     66\nTight      57\nName: count, dtype: int64\n\n================================================================================\nFACTOR RETURNS BY HIERARCHICAL HMM REGIMES (Monthly %)\n================================================================================\n                     mkt_excess       hml       rmw       cma\nregime_hierarchical                                          \nCrisis                 0.471667 -0.817879  0.281970  0.050000\nHigh                   0.962971  0.310290  0.408841  0.075000\nTight                  0.901930 -0.021754  0.132105 -0.305965\n\n\n\n# Check which months were classified as crisis\ncrisis_months = combined_hier[combined_hier['regime_hierarchical'] == 'Crisis']\n\nprint(\"Crisis months identified by HMM:\")\nprint(f\"Total: {len(crisis_months)}\")\nprint(\"\\nBreakdown by year:\")\nprint(crisis_months.groupby(crisis_months.index.year).size())\n\nprint(\"\\nSample crisis months:\")\nprint(crisis_months[['L', 'mkt_excess', 'ret_vol']].head(20))\n\n# Check if 2010-2014 QE got misclassified\nqe_period = combined_hier.loc['2010-01':'2014-12']\nprint(\"\\n2010-2014 QE Classification:\")\nprint(qe_period['regime_hierarchical'].value_counts())\n\n# Check actual 2008-09 crisis\ncrash_period = combined_hier.loc['2008-09':'2009-03']\nprint(\"\\n2008-09 Actual Crisis:\")\nprint(crash_period['regime_hierarchical'].value_counts())\nprint(f\"Mean returns: {crash_period['mkt_excess'].mean()*100:.2f}%\")\n\nCrisis months identified by HMM:\nTotal: 66\n\nBreakdown by year:\n2008     7\n2009     9\n2010     6\n2011     4\n2015     2\n2018     3\n2019     7\n2020    11\n2021     1\n2022     9\n2023     7\ndtype: int64\n\nSample crisis months:\n                   L  mkt_excess   ret_vol\n2008-06-30  0.070369     -0.0843  0.068756\n2008-07-31  0.989026     -0.0075  0.053531\n2008-08-31  0.146175      0.0153  0.052183\n2008-09-30  4.564198     -0.0935  0.057378\n2008-10-31  7.816953     -0.1720  0.094058\n2008-11-30  2.246385     -0.0774  0.050614\n2008-12-31  2.540236      0.0177  0.094850\n2009-01-31 -0.491676     -0.0809  0.055944\n2009-02-28  0.724004     -0.1014  0.063675\n2009-03-31  1.745156      0.0901  0.105146\n2009-04-30  0.266637      0.1017  0.114059\n2009-05-31  0.677084      0.0520  0.026001\n2009-06-30 -0.312000      0.0042  0.048753\n2009-07-31 -0.429254      0.0774  0.037167\n2009-11-30  1.132449      0.0558  0.043263\n2009-12-31  0.783644      0.0274  0.041261\n2010-05-31  0.577706     -0.0790  0.072817\n2010-06-30  0.256471     -0.0557  0.051760\n2010-07-31  0.142053      0.0692  0.079693\n2010-08-31  0.097191     -0.0477  0.069916\n\n2010-2014 QE Classification:\nregime_hierarchical\nHigh      50\nCrisis    10\nName: count, dtype: int64\n\n2008-09 Actual Crisis:\nregime_hierarchical\nCrisis    7\nName: count, dtype: int64\nMean returns: -5.96%\n\n\n\n\nTwo Dimesnions - Liquidity and Vol.\n\ndef two_dimensional_regime_classification(combined_aug, n_breakpoints=3):\n    \"\"\"\n    Fully automated regime classification with proper NaN handling.\n    \"\"\"\n    df = combined_aug.copy()\n    \n    # ==========================================\n    # STEP 1: Bai-Perron Structural Break Detection\n    # ==========================================\n    print(\"=\"*80)\n    print(\"STEP 1: BAI-PERRON STRUCTURAL BREAK DETECTION\")\n    print(\"=\"*80)\n    \n    # Compute volatility (creates NaNs at start)\n    df['ret_vol'] = df['mkt_excess'].rolling(6).std()\n    \n    # CRITICAL: Drop NaNs before Bai-Perron\n    signal_data = df[['L', 'ret_vol']].dropna()\n    signal = signal_data.values\n    \n    print(f\"\\nData for break detection: {len(signal)} months (after dropping NaNs)\")\n    \n    # Bai-Perron algorithm\n    algo = rpt.Dynp(model=\"rbf\", min_size=24, jump=1)\n    algo.fit(signal)\n    \n    # Detect breakpoints\n    breakpoints = algo.predict(n_bkps=n_breakpoints)\n    \n    # Map breakpoints back to original dataframe index\n    breakpoint_dates = signal_data.index[breakpoints[:-1]]\n    \n    print(f\"\\nDetected {n_breakpoints} structural breaks:\")\n    for i, date in enumerate(breakpoint_dates):\n        print(f\"  Break {i+1}: {date}\")\n    \n    # Assign periods based on breakpoint dates\n    df['period'] = 0\n    for i, break_date in enumerate(breakpoint_dates):\n        df.loc[df.index &gt;= break_date, 'period'] = i + 1\n    \n    # ==========================================\n    # STEP 2: Characterize Each Period\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"STRUCTURAL PERIOD CHARACTERISTICS\")\n    print(\"=\"*80)\n    \n    for period in sorted(df['period'].unique()):\n        period_data = df[df['period'] == period].dropna(subset=['L', 'ret_vol'])\n        if len(period_data) == 0:\n            continue\n            \n        start = period_data.index[0]\n        end = period_data.index[-1]\n        \n        mean_L = period_data['L'].mean()\n        mean_vol = period_data['ret_vol'].mean()\n        mean_ret = period_data['mkt_excess'].mean() * 100\n        \n        print(f\"\\nPeriod {period}: {start.strftime('%Y-%m')} to {end.strftime('%Y-%m')} ({len(period_data)} months)\")\n        print(f\"  Mean Liquidity: {mean_L:+.2f}\")\n        print(f\"  Mean Volatility: {mean_vol:.4f}\")\n        print(f\"  Mean Return: {mean_ret:+.2f}%\")\n    \n    # ==========================================\n    # STEP 3: 2D HMM on Stable Periods\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 3: TWO-DIMENSIONAL HMM (Liquidity × Volatility)\")\n    print(\"=\"*80)\n    \n    # Exclude highest-volatility period (likely crisis)\n    period_vols = df.groupby('period')['ret_vol'].mean()\n    crisis_period = period_vols.idxmax()\n    \n    print(f\"\\nIdentified crisis period: Period {crisis_period}\")\n    print(f\"  Mean volatility: {period_vols[crisis_period]:.4f}\")\n    \n    df['is_crisis'] = (df['period'] == crisis_period)\n    \n    # Get normal periods and drop NaNs\n    df_normal = df[~df['is_crisis']].copy()\n    df_normal_clean = df_normal[['L', 'ret_vol']].dropna()\n    \n    print(f\"\\nNormal periods for HMM: {len(df_normal_clean)} months (after dropping NaNs)\")\n    \n    # CRITICAL: Verify no NaNs\n    if df_normal_clean.isnull().any().any():\n        print(\"⚠️  WARNING: Still have NaNs after dropna!\")\n        print(df_normal_clean.isnull().sum())\n        # Force drop any remaining NaNs\n        df_normal_clean = df_normal_clean.dropna()\n    \n    # Prepare 2D features\n    X_2d = df_normal_clean.values\n    \n    print(f\"\\nX_2d shape: {X_2d.shape}\")\n    print(f\"Contains NaN: {np.isnan(X_2d).any()}\")\n    print(f\"Contains Inf: {np.isinf(X_2d).any()}\")\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_2d_scaled = scaler.fit_transform(X_2d)\n    \n    # Verify scaled data\n    print(f\"\\nScaled data - Contains NaN: {np.isnan(X_2d_scaled).any()}\")\n    print(f\"Scaled data - Contains Inf: {np.isinf(X_2d_scaled).any()}\")\n    \n    # Fit 4-state HMM\n    print(\"\\nFitting 4-state HMM...\")\n    hmm_2d = GaussianHMM(\n        n_components=4,\n        covariance_type=\"full\",\n        n_iter=1000,\n        random_state=42,\n        tol=1e-4,\n        verbose=False\n    )\n    \n    try:\n        hmm_2d.fit(X_2d_scaled)\n        print(\"✅ HMM fitting successful!\")\n    except Exception as e:\n        print(f\"❌ HMM fitting failed: {e}\")\n        print(\"\\nDiagnostic info:\")\n        print(f\"  X_2d_scaled stats:\")\n        print(f\"    Mean: {X_2d_scaled.mean(axis=0)}\")\n        print(f\"    Std: {X_2d_scaled.std(axis=0)}\")\n        print(f\"    Min: {X_2d_scaled.min(axis=0)}\")\n        print(f\"    Max: {X_2d_scaled.max(axis=0)}\")\n        raise\n    \n    states_2d = hmm_2d.predict(X_2d_scaled)\n    \n    # Transform means back to original scale\n    state_means_original = scaler.inverse_transform(hmm_2d.means_)\n    \n    # Classify states into 2×2 grid\n    median_L = np.median(state_means_original[:, 0])\n    median_vol = np.median(state_means_original[:, 1])\n    \n    state_labels = {}\n    for i in range(4):\n        L_level = \"High_Liq\" if state_means_original[i, 0] &gt; median_L else \"Tight_Liq\"\n        vol_level = \"High_Vol\" if state_means_original[i, 1] &gt; median_vol else \"Low_Vol\"\n        state_labels[i] = f\"{L_level}_{vol_level}\"\n    \n    print(\"\\nHMM State Characteristics (Normal Periods):\")\n    print(f\"{'State':&lt;8} {'Mean L':&lt;10} {'Mean Vol':&lt;12} {'Label':&lt;25}\")\n    print(\"-\" * 60)\n    for i in range(4):\n        print(f\"{i:&lt;8} {state_means_original[i,0]:&gt;+8.2f}  {state_means_original[i,1]:&gt;10.4f}  {state_labels[i]:&lt;25}\")\n    \n    # Assign regime labels (only for rows that were in df_normal_clean)\n    df['regime_2d'] = 'Unknown'\n    df.loc[df_normal_clean.index, 'regime_2d'] = [state_labels[s] for s in states_2d]\n    df.loc[df['is_crisis'], 'regime_2d'] = 'Crisis'\n    \n    print(\"\\nFinal 2D Regime Distribution:\")\n    print(df['regime_2d'].value_counts().sort_index())\n    \n    # ==========================================\n    # STEP 4: Aggregate into Interpretable Regimes\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 4: SIMPLIFIED REGIME CLASSIFICATION\")\n    print(\"=\"*80)\n    \n    regime_map = {\n        'High_Liq_Low_Vol': 'Goldilocks',\n        'High_Liq_High_Vol': 'Intervention',\n        'Tight_Liq_Low_Vol': 'Risk_Off',\n        'Tight_Liq_High_Vol': 'Bear',\n        'Crisis': 'Crisis',\n        'Unknown': 'Unknown'  # For NaN rows\n    }\n    \n    df['regime_simple'] = df['regime_2d'].map(regime_map)\n    \n    print(\"\\nSimplified Regime Distribution:\")\n    print(df['regime_simple'].value_counts())\n    \n    # ==========================================\n    # VALIDATION: Check known periods\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"VALIDATION: Known Historical Periods\")\n    print(\"=\"*80)\n    \n    known = {\n        '2010-2014 QE': ('2010-01', '2014-12'),\n        '2015-2019 Late Cycle': ('2015-01', '2019-12'),\n        '2020-2021 COVID QE': ('2020-05', '2021-12'),\n        '2022-2024 QT': ('2022-03', '2024-12'),\n    }\n    \n    for label, (start, end) in known.items():\n        period = df.loc[start:end]\n        valid_regimes = period[period['regime_simple'] != 'Unknown']\n        \n        if len(valid_regimes) &gt; 0:\n            mode_regime = valid_regimes['regime_simple'].mode()[0]\n            dist = valid_regimes['regime_simple'].value_counts().to_dict()\n            \n            mean_L = valid_regimes['L'].mean()\n            mean_vol = valid_regimes['ret_vol'].mean()\n            \n            print(f\"\\n{label}:\")\n            print(f\"  Primary regime: {mode_regime}\")\n            print(f\"  Distribution: {dist}\")\n            print(f\"  Mean L = {mean_L:+.2f}, Mean Vol = {mean_vol:.4f}\")\n    \n    return df, hmm_2d, scaler, breakpoint_dates\n\n# ==========================================\n# APPLY 2D REGIME CLASSIFICATION\n# ==========================================\ncombined_2d, hmm_2d_model, scaler_2d, breaks = two_dimensional_regime_classification(\n    combined_aug_mom,\n    n_breakpoints=3\n)\n\n# ==========================================\n# FACTOR RETURNS BY 2D REGIME\n# ==========================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"FACTOR RETURNS BY TWO-DIMENSIONAL REGIMES (Monthly %)\")\nprint(\"=\"*80)\n\n# Only analyze regimes with valid data\nvalid_data = combined_2d[combined_2d['regime_simple'] != 'Unknown'].copy()\n\nfor regime in ['Crisis', 'Goldilocks', 'Intervention', 'Risk_Off', 'Bear']:\n    if regime in valid_data['regime_simple'].unique():\n        regime_data = valid_data[valid_data['regime_simple'] == regime]\n        n = len(regime_data)\n        \n        print(f\"\\n{regime} ({n} months):\")\n        \n        for factor in ['mkt_excess', 'hml', 'rmw', 'cma']:\n            mean = regime_data[factor].mean() * 100\n            std = regime_data[factor].std() * 100\n            sharpe = mean / std if std &gt; 0 else 0\n            \n            print(f\"  {factor.upper():12s}: {mean:+6.2f}% ± {std:5.2f}%  SR={sharpe:+.3f}\")\n\n# Summary table\nmeans_2d = valid_data.groupby('regime_simple')[['mkt_excess', 'hml', 'rmw', 'cma']].mean() * 100\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY TABLE\")\nprint(\"=\"*80)\nprint(means_2d.round(2))\n\n================================================================================\nSTEP 1: BAI-PERRON STRUCTURAL BREAK DETECTION\n================================================================================\n\nData for break detection: 256 months (after dropping NaNs)\n\nDetected 3 structural breaks:\n  Break 1: 2007-12-31 00:00:00\n  Break 2: 2020-03-31 00:00:00\n  Break 3: 2022-10-31 00:00:00\n\n================================================================================\nSTRUCTURAL PERIOD CHARACTERISTICS\n================================================================================\n\nPeriod 0: 2004-06 to 2007-11 (42 months)\n  Mean Liquidity: -1.37\n  Mean Volatility: 0.0234\n  Mean Return: +0.58%\n\nPeriod 1: 2007-12 to 2020-02 (147 months)\n  Mean Liquidity: +0.41\n  Mean Volatility: 0.0387\n  Mean Return: +0.72%\n\nPeriod 2: 2020-03 to 2022-09 (31 months)\n  Mean Liquidity: +3.08\n  Mean Volatility: 0.0553\n  Mean Return: +0.91%\n\nPeriod 3: 2022-10 to 2025-09 (36 months)\n  Mean Liquidity: -2.79\n  Mean Volatility: 0.0455\n  Mean Return: +1.54%\n\n================================================================================\nSTEP 3: TWO-DIMENSIONAL HMM (Liquidity × Volatility)\n================================================================================\n\nIdentified crisis period: Period 2\n  Mean volatility: 0.0553\n\nNormal periods for HMM: 225 months (after dropping NaNs)\n\nX_2d shape: (225, 2)\nContains NaN: False\nContains Inf: False\n\nScaled data - Contains NaN: False\nScaled data - Contains Inf: False\n\nFitting 4-state HMM...\n✅ HMM fitting successful!\n\nHMM State Characteristics (Normal Periods):\nState    Mean L     Mean Vol     Label                    \n------------------------------------------------------------\n0           +0.55      0.0296  High_Liq_Low_Vol         \n1           -2.65      0.0319  Tight_Liq_High_Vol       \n2           +0.09      0.0660  High_Liq_High_Vol        \n3           -0.73      0.0296  Tight_Liq_Low_Vol        \n\nFinal 2D Regime Distribution:\nregime_2d\nCrisis                31\nHigh_Liq_High_Vol     43\nHigh_Liq_Low_Vol      99\nTight_Liq_High_Vol    49\nTight_Liq_Low_Vol     34\nUnknown                5\nName: count, dtype: int64\n\n================================================================================\nSTEP 4: SIMPLIFIED REGIME CLASSIFICATION\n================================================================================\n\nSimplified Regime Distribution:\nregime_simple\nGoldilocks      99\nBear            49\nIntervention    43\nRisk_Off        34\nCrisis          31\nUnknown          5\nName: count, dtype: int64\n\n================================================================================\nVALIDATION: Known Historical Periods\n================================================================================\n\n2010-2014 QE:\n  Primary regime: Goldilocks\n  Distribution: {'Goldilocks': 47, 'Intervention': 13}\n  Mean L = +0.89, Mean Vol = 0.0371\n\n2015-2019 Late Cycle:\n  Primary regime: Goldilocks\n  Distribution: {'Goldilocks': 32, 'Risk_Off': 16, 'Intervention': 12}\n  Mean L = -0.26, Mean Vol = 0.0337\n\n2020-2021 COVID QE:\n  Primary regime: Crisis\n  Distribution: {'Crisis': 20}\n  Mean L = +3.37, Mean Vol = 0.0519\n\n2022-2024 QT:\n  Primary regime: Bear\n  Distribution: {'Bear': 21, 'Crisis': 7, 'Intervention': 6}\n  Mean L = -2.07, Mean Vol = 0.0495\n\n================================================================================\nFACTOR RETURNS BY TWO-DIMENSIONAL REGIMES (Monthly %)\n================================================================================\n\nCrisis (31 months):\n  MKT_EXCESS  :  +0.91% ±  6.33%  SR=+0.143\n  HML         :  +0.49% ±  5.38%  SR=+0.091\n  RMW         :  +0.71% ±  2.93%  SR=+0.241\n  CMA         :  +0.60% ±  3.16%  SR=+0.191\n\nGoldilocks (99 months):\n  MKT_EXCESS  :  +1.08% ±  3.13%  SR=+0.346\n  HML         :  +0.18% ±  2.35%  SR=+0.079\n  RMW         :  +0.32% ±  1.73%  SR=+0.183\n  CMA         :  +0.03% ±  1.37%  SR=+0.022\n\nIntervention (43 months):\n  MKT_EXCESS  :  +0.43% ±  6.85%  SR=+0.063\n  HML         :  -0.72% ±  3.94%  SR=-0.183\n  RMW         :  +0.46% ±  1.84%  SR=+0.248\n  CMA         :  +0.29% ±  2.20%  SR=+0.130\n\nRisk_Off (34 months):\n  MKT_EXCESS  :  +0.18% ±  2.85%  SR=+0.062\n  HML         :  -0.48% ±  1.84%  SR=-0.264\n  RMW         :  +0.01% ±  1.23%  SR=+0.010\n  CMA         :  -0.66% ±  1.08%  SR=-0.611\n\nBear (49 months):\n  MKT_EXCESS  :  +1.10% ±  3.26%  SR=+0.338\n  HML         :  +0.03% ±  2.64%  SR=+0.010\n  RMW         :  +0.14% ±  1.71%  SR=+0.083\n  CMA         :  -0.28% ±  1.84%  SR=-0.152\n\n================================================================================\nSUMMARY TABLE\n================================================================================\n               mkt_excess   hml   rmw   cma\nregime_simple                              \nBear                 1.10  0.03  0.14 -0.28\nCrisis               0.91  0.49  0.71  0.60\nGoldilocks           1.08  0.18  0.32  0.03\nIntervention         0.43 -0.72  0.46  0.29\nRisk_Off             0.18 -0.48  0.01 -0.66\n\n\n\n# ==========================================\n# DIAGNOSTIC 1: What did Bai-Perron identify as \"Crisis\"?\n# ==========================================\nprint(\"=\"*80)\nprint(\"DIAGNOSTIC 1: Bai-Perron Crisis Period\")\nprint(\"=\"*80)\n\ncrisis_data = combined_2d[combined_2d['regime_simple'] == 'Crisis']\nprint(f\"\\nTotal crisis months: {len(crisis_data)}\")\n\nif len(crisis_data) &gt; 0:\n    print(\"\\nCrisis months identified:\")\n    print(crisis_data.index.tolist())\n    \n    print(\"\\nCrisis period statistics:\")\n    print(f\"  Mean MKT return: {crisis_data['mkt_excess'].mean()*100:+.2f}%\")\n    print(f\"  Mean L: {crisis_data['L'].mean():+.2f}\")\n    print(f\"  Mean Vol: {crisis_data['ret_vol'].mean():.4f}\")\n    \n    print(\"\\nBreakdown by year:\")\n    print(crisis_data.groupby(crisis_data.index.year).size())\n\n# ==========================================\n# DIAGNOSTIC 2: Check each 2D regime characteristics\n# ==========================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"DIAGNOSTIC 2: 2D Regime Characteristics\")\nprint(\"=\"*80)\n\nfor regime in ['Goldilocks', 'Intervention', 'Risk_Off', 'Bear', 'Crisis']:\n    if regime in combined_2d['regime_simple'].unique():\n        regime_data = combined_2d[combined_2d['regime_simple'] == regime]\n        \n        print(f\"\\n{regime} ({len(regime_data)} months):\")\n        print(f\"  Mean L: {regime_data['L'].mean():+.2f}\")\n        print(f\"  Mean Volatility: {regime_data['ret_vol'].mean():.4f}\")\n        print(f\"  Sample months: {regime_data.index[:5].tolist()}\")\n\n# ==========================================\n# DIAGNOSTIC 3: Known periods - which regime were they assigned?\n# ==========================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"DIAGNOSTIC 3: Known Period Classifications\")\nprint(\"=\"*80)\n\nknown_periods = {\n    '2008-2009 Crisis': ('2008-09', '2009-03'),\n    '2010-2014 QE': ('2010-01', '2014-12'),\n    '2020 COVID Crash': ('2020-02', '2020-04'),\n    '2020-2021 Reopening': ('2020-05', '2021-12'),\n    '2022-2024 QT': ('2022-03', '2024-12'),\n}\n\nfor label, (start, end) in known_periods.items():\n    period = combined_2d.loc[start:end]\n    valid = period[period['regime_simple'] != 'Unknown']\n    \n    if len(valid) &gt; 0:\n        dist = valid['regime_simple'].value_counts()\n        mode = dist.idxmax()\n        \n        mean_ret = valid['mkt_excess'].mean() * 100\n        mean_hml = valid['hml'].mean() * 100\n        \n        print(f\"\\n{label}:\")\n        print(f\"  Classified as: {mode} ({dist[mode]}/{len(valid)} months)\")\n        print(f\"  Full distribution: {dist.to_dict()}\")\n        print(f\"  Actual returns: MKT={mean_ret:+.2f}%, HML={mean_hml:+.2f}%\")\n\n# ==========================================\n# DIAGNOSTIC 4: Check HMM state means\n# ==========================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"DIAGNOSTIC 4: HMM State Assignments\")\nprint(\"=\"*80)\n\n# Show the raw 2D regime (before simplification)\nprint(\"\\nRaw 2D regime distribution:\")\nprint(combined_2d['regime_2d'].value_counts())\n\n# Check state means from HMM\nprint(\"\\nHMM state means (from model):\")\nif 'hmm_2d_model' in globals():\n    state_means = scaler_2d.inverse_transform(hmm_2d_model.means_)\n    for i in range(len(state_means)):\n        print(f\"  State {i}: L={state_means[i,0]:+.2f}, Vol={state_means[i,1]:.4f}\")\n\n================================================================================\nDIAGNOSTIC 1: Bai-Perron Crisis Period\n================================================================================\n\nTotal crisis months: 31\n\nCrisis months identified:\n[Timestamp('2020-03-31 00:00:00'), Timestamp('2020-04-30 00:00:00'), Timestamp('2020-05-31 00:00:00'), Timestamp('2020-06-30 00:00:00'), Timestamp('2020-07-31 00:00:00'), Timestamp('2020-08-31 00:00:00'), Timestamp('2020-09-30 00:00:00'), Timestamp('2020-10-31 00:00:00'), Timestamp('2020-11-30 00:00:00'), Timestamp('2020-12-31 00:00:00'), Timestamp('2021-01-31 00:00:00'), Timestamp('2021-02-28 00:00:00'), Timestamp('2021-03-31 00:00:00'), Timestamp('2021-04-30 00:00:00'), Timestamp('2021-05-31 00:00:00'), Timestamp('2021-06-30 00:00:00'), Timestamp('2021-07-31 00:00:00'), Timestamp('2021-08-31 00:00:00'), Timestamp('2021-09-30 00:00:00'), Timestamp('2021-10-31 00:00:00'), Timestamp('2021-11-30 00:00:00'), Timestamp('2021-12-31 00:00:00'), Timestamp('2022-01-31 00:00:00'), Timestamp('2022-02-28 00:00:00'), Timestamp('2022-03-31 00:00:00'), Timestamp('2022-04-30 00:00:00'), Timestamp('2022-05-31 00:00:00'), Timestamp('2022-06-30 00:00:00'), Timestamp('2022-07-31 00:00:00'), Timestamp('2022-08-31 00:00:00'), Timestamp('2022-09-30 00:00:00')]\n\nCrisis period statistics:\n  Mean MKT return: +0.91%\n  Mean L: +3.08\n  Mean Vol: 0.0553\n\nBreakdown by year:\n2020    10\n2021    12\n2022     9\ndtype: int64\n\n================================================================================\nDIAGNOSTIC 2: 2D Regime Characteristics\n================================================================================\n\nGoldilocks (99 months):\n  Mean L: +0.56\n  Mean Volatility: 0.0295\n  Sample months: [Timestamp('2004-06-30 00:00:00'), Timestamp('2004-07-31 00:00:00'), Timestamp('2004-08-31 00:00:00'), Timestamp('2004-09-30 00:00:00'), Timestamp('2004-10-31 00:00:00')]\n\nIntervention (43 months):\n  Mean L: +0.08\n  Mean Volatility: 0.0658\n  Sample months: [Timestamp('2008-09-30 00:00:00'), Timestamp('2008-10-31 00:00:00'), Timestamp('2008-11-30 00:00:00'), Timestamp('2008-12-31 00:00:00'), Timestamp('2009-01-31 00:00:00')]\n\nRisk_Off (34 months):\n  Mean L: -0.75\n  Mean Volatility: 0.0293\n  Sample months: [Timestamp('2004-12-31 00:00:00'), Timestamp('2005-01-31 00:00:00'), Timestamp('2005-02-28 00:00:00'), Timestamp('2005-03-31 00:00:00'), Timestamp('2005-04-30 00:00:00')]\n\nBear (49 months):\n  Mean L: -2.67\n  Mean Volatility: 0.0319\n  Sample months: [Timestamp('2006-01-31 00:00:00'), Timestamp('2006-02-28 00:00:00'), Timestamp('2006-03-31 00:00:00'), Timestamp('2006-04-30 00:00:00'), Timestamp('2006-05-31 00:00:00')]\n\nCrisis (31 months):\n  Mean L: +3.08\n  Mean Volatility: 0.0553\n  Sample months: [Timestamp('2020-03-31 00:00:00'), Timestamp('2020-04-30 00:00:00'), Timestamp('2020-05-31 00:00:00'), Timestamp('2020-06-30 00:00:00'), Timestamp('2020-07-31 00:00:00')]\n\n================================================================================\nDIAGNOSTIC 3: Known Period Classifications\n================================================================================\n\n2008-2009 Crisis:\n  Classified as: Intervention (7/7 months)\n  Full distribution: {'Intervention': 7}\n  Actual returns: MKT=-5.96%, HML=-2.36%\n\n2010-2014 QE:\n  Classified as: Goldilocks (47/60 months)\n  Full distribution: {'Goldilocks': 47, 'Intervention': 13}\n  Actual returns: MKT=+1.29%, HML=-0.05%\n\n2020 COVID Crash:\n  Classified as: Crisis (2/3 months)\n  Full distribution: {'Crisis': 2, 'Intervention': 1}\n  Actual returns: MKT=-2.64%, HML=-6.33%\n\n2020-2021 Reopening:\n  Classified as: Crisis (20/20 months)\n  Full distribution: {'Crisis': 20}\n  Actual returns: MKT=+2.74%, HML=+0.58%\n\n2022-2024 QT:\n  Classified as: Bear (21/34 months)\n  Full distribution: {'Bear': 21, 'Crisis': 7, 'Intervention': 6}\n  Actual returns: MKT=+0.76%, HML=-0.12%\n\n================================================================================\nDIAGNOSTIC 4: HMM State Assignments\n================================================================================\n\nRaw 2D regime distribution:\nregime_2d\nHigh_Liq_Low_Vol      99\nTight_Liq_High_Vol    49\nHigh_Liq_High_Vol     43\nTight_Liq_Low_Vol     34\nCrisis                31\nUnknown                5\nName: count, dtype: int64\n\nHMM state means (from model):\n  State 0: L=+0.55, Vol=0.0296\n  State 1: L=-2.65, Vol=0.0319\n  State 2: L=+0.09, Vol=0.0660\n  State 3: L=-0.73, Vol=0.0296\n\n\n\ndef improved_two_dimensional_regimes(combined_aug):\n    \"\"\"\n    Improved regime classification with better thresholds.\n    \"\"\"\n    df = combined_aug.copy()\n    \n    # ==========================================\n    # STEP 1: Crisis Detection (same as before)\n    # ==========================================\n    print(\"=\"*80)\n    print(\"STEP 1: CRISIS DETECTION\")\n    print(\"=\"*80)\n    \n    df['ret_vol'] = df['mkt_excess'].rolling(6).std()\n    df['L_vol'] = df['L'].rolling(3).std()\n    df['abs_ret'] = df['mkt_excess'].abs()\n    df['cum_ret_3m'] = (1 + df['mkt_excess']).rolling(3).apply(lambda x: x.prod()) - 1\n    \n    crisis_features = df[['ret_vol', 'abs_ret', 'cum_ret_3m', 'L_vol']].dropna()\n    \n    from sklearn.ensemble import IsolationForest\n    iso_forest = IsolationForest(\n        contamination=0.05,\n        random_state=42,\n        n_estimators=100\n    )\n    \n    outlier_labels = iso_forest.fit_predict(crisis_features.values)\n    \n    df['is_outlier'] = False\n    df.loc[crisis_features.index, 'is_outlier'] = (outlier_labels == -1)\n    df['is_crisis'] = df['is_outlier'] & (df['mkt_excess'] &lt; -0.02)\n    \n    crisis_months = df[df['is_crisis']]\n    print(f\"\\nCrisis months: {len(crisis_months)}\")\n    print(f\"Mean return: {crisis_months['mkt_excess'].mean()*100:.2f}%\")\n    \n    # ==========================================\n    # STEP 2: 2D HMM on Normal Periods\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 2: 2D HMM\")\n    print(\"=\"*80)\n    \n    df_normal = df[~df['is_crisis']].copy()\n    df_clean = df_normal[['L', 'ret_vol']].dropna()\n    \n    from sklearn.preprocessing import StandardScaler\n    vol_scaler = StandardScaler()\n    df_clean['ret_vol_scaled'] = vol_scaler.fit_transform(df_clean[['ret_vol']])\n\n    X_for_hmm = df_clean[['L', 'ret_vol_scaled']].values\n\n    from hmmlearn.hmm import GaussianHMM\n    hmm = GaussianHMM(n_components=4, covariance_type=\"full\", \n                      n_iter=1000, random_state=42)\n    hmm.fit(X_for_hmm)\n    states = hmm.predict(X_for_hmm)\n    \n    state_means = vol_scaler.inverse_transform(hmm.means_)\n    \n    print(\"\\nHMM State Means:\")\n    for i in range(4):\n        print(f\"  State {i}: L={state_means[i,0]:+.2f}, Vol={state_means[i,1]:.4f}\")\n    \n    # ==========================================\n    # STEP 3: IMPROVED STATE LABELING\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 3: IMPROVED STATE LABELING\")\n    print(\"=\"*80)\n    \n    # Use DATA percentiles, not state means median\n    L_threshold = df_clean['L'].median()  # Median of actual data\n    vol_threshold = df_clean['ret_vol'].quantile(0.60)  # 60th percentile\n    \n    print(f\"\\nData-based thresholds:\")\n    print(f\"  Liquidity median: {L_threshold:+.2f}\")\n    print(f\"  Volatility 60th %ile: {vol_threshold:.4f}\")\n    \n    state_labels = {}\n    state_descriptions = {}\n    \n    for i in range(4):\n        L_val = state_means[i, 0]\n        vol_val = state_means[i, 1]\n        \n        # More nuanced classification\n        if L_val &gt; 1.0:  # Very high liquidity\n            if vol_val &gt; vol_threshold:\n                label = \"Intervention\"\n                desc = \"Very High Liq + High Vol\"\n            else:\n                label = \"Goldilocks\"\n                desc = \"Very High Liq + Low Vol\"\n        elif L_val &gt; 0:  # Positive but moderate liquidity\n            if vol_val &gt; vol_threshold:\n                label = \"Goldilocks\"  # Still expansionary despite vol\n                desc = \"Moderate High Liq + High Vol\"\n            else:\n                label = \"Goldilocks\"  # Best regime\n                desc = \"Moderate High Liq + Low Vol\"\n        else:  # Negative liquidity (tight)\n            if vol_val &gt; vol_threshold:\n                label = \"Bear\"\n                desc = \"Tight Liq + High Vol\"\n            else:\n                label = \"Risk_Off\"\n                desc = \"Tight Liq + Low Vol\"\n        \n        state_labels[i] = label\n        state_descriptions[i] = desc\n        \n        print(f\"  State {i}: L={L_val:+.2f}, Vol={vol_val:.4f}\")\n        print(f\"    → {label} ({desc})\")\n    \n    # Assign regimes\n    df['regime_improved'] = 'Unknown'\n    df.loc[df_clean.index, 'regime_improved'] = [state_labels[s] for s in states]\n    df.loc[df['is_crisis'], 'regime_improved'] = 'Crisis'\n    \n    print(\"\\nFinal distribution:\")\n    print(df['regime_improved'].value_counts())\n    \n    # ==========================================\n    # VALIDATION\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"VALIDATION\")\n    print(\"=\"*80)\n    \n    known = {\n        '2008-2009 Crisis': ('2008-09', '2009-03'),\n        '2010-2014 QE': ('2010-01', '2014-12'),\n        '2020 COVID': ('2020-02', '2020-04'),\n        '2020-2021 Reopening': ('2020-05', '2021-12'),\n        '2022-2024 QT': ('2022-03', '2024-12'),\n    }\n    \n    for label, (start, end) in known.items():\n        period = df.loc[start:end]\n        valid = period[period['regime_improved'] != 'Unknown']\n        \n        if len(valid) &gt; 0:\n            mode = valid['regime_improved'].mode()[0]\n            dist = valid['regime_improved'].value_counts().to_dict()\n            \n            mean_ret = valid['mkt_excess'].mean() * 100\n            mean_hml = valid['hml'].mean() * 100\n            mean_L = valid['L'].mean()\n            \n            print(f\"\\n{label}:\")\n            print(f\"  Primary: {mode} ({dist.get(mode, 0)}/{len(valid)})\")\n            print(f\"  Mean L={mean_L:+.2f}, MKT={mean_ret:+.2f}%, HML={mean_hml:+.2f}%\")\n    \n    return df, hmm, vol_scaler\n\n# ==========================================\n# RUN IMPROVED VERSION\n# ==========================================\ncombined_improved, hmm_improved, scaler_improved = improved_two_dimensional_regimes(combined_aug_mom)\n\n# ==========================================\n# FACTOR RETURNS\n# ==========================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"FACTOR RETURNS BY IMPROVED REGIMES\")\nprint(\"=\"*80)\n\nvalid = combined_improved[combined_improved['regime_improved'] != 'Unknown']\n\nsummary = []\nfor regime in ['Crisis', 'Goldilocks', 'Intervention', 'Risk_Off', 'Bear']:\n    if regime in valid['regime_improved'].unique():\n        regime_data = valid[valid['regime_improved'] == regime]\n        n = len(regime_data)\n        \n        row = {'Regime': regime, 'N': n}\n        for factor in ['mkt_excess', 'hml', 'rmw', 'cma']:\n            row[factor.upper()] = regime_data[factor].mean() * 100\n        \n        summary.append(row)\n\nimport pandas as pd\nsummary_df = pd.DataFrame(summary)\nprint(\"\\n\" + summary_df.to_string(index=False))\n\n# Detailed stats\nprint(\"\\n\" + \"=\"*80)\nprint(\"DETAILED STATISTICS\")\nprint(\"=\"*80)\n\nfor regime in ['Crisis', 'Goldilocks', 'Intervention', 'Risk_Off', 'Bear']:\n    if regime in valid['regime_improved'].unique():\n        regime_data = valid[valid['regime_improved'] == regime]\n        n = len(regime_data)\n        \n        print(f\"\\n{regime} ({n} months):\")\n        for factor in ['mkt_excess', 'hml', 'rmw', 'cma']:\n            mean = regime_data[factor].mean() * 100\n            std = regime_data[factor].std() * 100\n            t_stat = mean / (std / np.sqrt(n)) if std &gt; 0 else 0\n            \n            # Rough significance (|t| &gt; 2 for p &lt; 0.05)\n            sig = \"**\" if abs(t_stat) &gt; 2 else \"  \"\n            \n            print(f\"  {factor.upper():12s}: {mean:+6.2f}% ± {std:5.2f}%  t={t_stat:+.2f} {sig}\")\n\n================================================================================\nSTEP 1: CRISIS DETECTION\n================================================================================\n\nCrisis months: 5\nMean return: -11.56%\n\n================================================================================\nSTEP 2: 2D HMM\n================================================================================\n\nHMM State Means:\n  State 0: L=+0.05, Vol=0.0294\n  State 1: L=-0.01, Vol=0.0320\n  State 2: L=+0.06, Vol=0.0572\n  State 3: L=+0.02, Vol=0.0296\n\n================================================================================\nSTEP 3: IMPROVED STATE LABELING\n================================================================================\n\nData-based thresholds:\n  Liquidity median: +0.10\n  Volatility 60th %ile: 0.0380\n  State 0: L=+0.05, Vol=0.0294\n    → Goldilocks (Moderate High Liq + Low Vol)\n  State 1: L=-0.01, Vol=0.0320\n    → Risk_Off (Tight Liq + Low Vol)\n  State 2: L=+0.06, Vol=0.0572\n    → Goldilocks (Moderate High Liq + High Vol)\n  State 3: L=+0.02, Vol=0.0296\n    → Goldilocks (Moderate High Liq + Low Vol)\n\nFinal distribution:\nregime_improved\nGoldilocks    202\nRisk_Off       49\nUnknown         5\nCrisis          5\nName: count, dtype: int64\n\n================================================================================\nVALIDATION\n================================================================================\n\n2008-2009 Crisis:\n  Primary: Crisis (4/7)\n  Mean L=+2.74, MKT=-5.96%, HML=-2.36%\n\n2010-2014 QE:\n  Primary: Goldilocks (60/60)\n  Mean L=+0.89, MKT=+1.29%, HML=-0.05%\n\n2020 COVID:\n  Primary: Goldilocks (2/3)\n  Mean L=+4.40, MKT=-2.64%, HML=-6.33%\n\n2020-2021 Reopening:\n  Primary: Goldilocks (20/20)\n  Mean L=+3.37, MKT=+2.74%, HML=+0.58%\n\n2022-2024 QT:\n  Primary: Risk_Off (21/34)\n  Mean L=-2.07, MKT=+0.76%, HML=-0.12%\n\n================================================================================\nFACTOR RETURNS BY IMPROVED REGIMES\n================================================================================\n\n    Regime   N  MKT_EXCESS       HML      RMW       CMA\n    Crisis   5  -11.556000 -4.574000 1.840000  1.208000\nGoldilocks 202    1.077327  0.044356 0.316584  0.027574\n  Risk_Off  49    1.102041  0.026939 0.141633 -0.280816\n\n================================================================================\nDETAILED STATISTICS\n================================================================================\n\nCrisis (5 months):\n  MKT_EXCESS  : -11.56% ±  3.76%  t=-6.88 **\n  HML         :  -4.57% ±  7.15%  t=-1.43   \n  RMW         :  +1.84% ±  2.33%  t=+1.77   \n  CMA         :  +1.21% ±  1.12%  t=+2.41 **\n\nGoldilocks (202 months):\n  MKT_EXCESS  :  +1.08% ±  4.23%  t=+3.62 **\n  HML         :  +0.04% ±  3.08%  t=+0.20   \n  RMW         :  +0.32% ±  1.89%  t=+2.38 **\n  CMA         :  +0.03% ±  1.92%  t=+0.20   \n\nRisk_Off (49 months):\n  MKT_EXCESS  :  +1.10% ±  3.26%  t=+2.37 **\n  HML         :  +0.03% ±  2.64%  t=+0.07   \n  RMW         :  +0.14% ±  1.71%  t=+0.58   \n  CMA         :  -0.28% ±  1.84%  t=-1.07   \n\n\n\ndef factor_based_regime_discovery(combined_aug):\n    \"\"\"\n    Factor-based regime discovery with CORRECT standardization.\n    \"\"\"\n    df = combined_aug.copy()\n    \n    # ==========================================\n    # STEP 1: Crisis Detection\n    # ==========================================\n    print(\"=\"*80)\n    print(\"STEP 1: CRISIS OUTLIER DETECTION\")\n    print(\"=\"*80)\n    \n    df['ret_vol'] = df['mkt_excess'].rolling(6).std()\n    df['L_vol'] = df['L'].rolling(3).std()\n    df['abs_ret'] = df['mkt_excess'].abs()\n    df['cum_ret_3m'] = (1 + df['mkt_excess']).rolling(3).apply(lambda x: x.prod()) - 1\n    \n    crisis_features = df[['ret_vol', 'abs_ret', 'cum_ret_3m', 'L_vol']].dropna()\n    \n    from sklearn.ensemble import IsolationForest\n    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n    outlier_labels = iso_forest.fit_predict(crisis_features.values)\n    \n    df['is_crisis'] = False\n    df.loc[crisis_features.index, 'is_crisis'] = (outlier_labels == -1) & (df.loc[crisis_features.index, 'mkt_excess'] &lt; -0.02)\n    \n    print(f\"Crisis months: {df['is_crisis'].sum()}\")\n    \n    # ==========================================\n    # STEP 2: Factor-Based Clustering\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"STEP 2: FACTOR RETURN-BASED REGIME CLUSTERING\")\n    print(\"=\"*80)\n    \n    df_normal = df[~df['is_crisis']].copy()\n    \n    feature_cols = ['L', 'mkt_excess', 'hml', 'rmw', 'cma', 'ret_vol']\n    df_features = df_normal[feature_cols].dropna()\n    \n    print(f\"Normal periods: {len(df_features)} months\")\n    \n    # ==========================================\n    # CORRECT STANDARDIZATION\n    # ==========================================\n    print(\"\\nStandardization:\")\n    print(\"  L: Keep as-is (already z-scored)\")\n    print(\"  Others: Standardize\")\n    \n    L_values = df_features[['L']].values\n    other_features = df_features[['mkt_excess', 'hml', 'rmw', 'cma', 'ret_vol']]\n    \n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    other_scaled = scaler.fit_transform(other_features.values)\n    \n    X_scaled = np.hstack([L_values, other_scaled])\n    \n    print(f\"\\nVerification - L unchanged:\")\n    print(f\"  Original L mean: {df_features['L'].mean():.6f}\")\n    print(f\"  X_scaled[:,0] mean: {X_scaled[:,0].mean():.6f}\")\n    \n    # ==========================================\n    # STEP 2.1: Optimal Number via BIC\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"MODEL SELECTION (BIC)\")\n    print(\"=\"*80)\n    \n    from sklearn.mixture import GaussianMixture  # Use regular GMM for BIC\n    \n    bic_scores = []\n    aic_scores = []\n    n_components_range = range(2, 8)\n    \n    for n in n_components_range:\n        gmm = GaussianMixture(\n            n_components=n,\n            covariance_type='full',\n            random_state=42,\n            n_init=10,\n            max_iter=500\n        )\n        gmm.fit(X_scaled)\n        bic_scores.append(gmm.bic(X_scaled))\n        aic_scores.append(gmm.aic(X_scaled))\n    \n    optimal_n_bic = n_components_range[np.argmin(bic_scores)]\n    optimal_n_aic = n_components_range[np.argmin(aic_scores)]\n    \n    print(f\"\\n{'N':&lt;5} {'BIC':&lt;15} {'AIC':&lt;15}\")\n    print(\"-\" * 35)\n    for n, bic, aic in zip(n_components_range, bic_scores, aic_scores):\n        bic_marker = \" ← BIC optimal\" if n == optimal_n_bic else \"\"\n        aic_marker = \" ← AIC optimal\" if n == optimal_n_aic else \"\"\n        marker = bic_marker or aic_marker\n        print(f\"{n:&lt;5} {bic:&lt;15,.1f} {aic:&lt;15,.1f}{marker}\")\n    \n    # Use BIC optimal\n    optimal_n = optimal_n_bic\n    print(f\"\\nSelected: {optimal_n} regimes (BIC criterion)\")\n    \n    # ==========================================\n    # STEP 2.2: Fit Final Model\n    # ==========================================\n    print(f\"\\nFitting final GMM with {optimal_n} components...\")\n    \n    gmm_final = GaussianMixture(\n        n_components=optimal_n,\n        covariance_type='full',\n        random_state=42,\n        n_init=20,  # More initializations for stability\n        max_iter=500\n    )\n    \n    gmm_final.fit(X_scaled)\n    cluster_labels = gmm_final.predict(X_scaled)\n    cluster_probs = gmm_final.predict_proba(X_scaled)\n    \n    print(f\"Converged: {gmm_final.converged_}\")\n    print(f\"Iterations: {gmm_final.n_iter_}\")\n    \n    # ==========================================\n    # STEP 3: Characterize Clusters\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"CLUSTER CHARACTERIZATION\")\n    print(\"=\"*80)\n    \n    cluster_chars = []\n    \n    for cluster in range(optimal_n):\n        mask = cluster_labels == cluster\n        cluster_data = df_features[mask]\n        \n        n_months = mask.sum()\n        \n        if n_months == 0:\n            continue\n        \n        char = {\n            'cluster': cluster,\n            'n_months': n_months,\n            'mean_L': cluster_data['L'].mean(),\n            'std_L': cluster_data['L'].std(),\n            'mean_vol': cluster_data['ret_vol'].mean(),\n            'std_vol': cluster_data['ret_vol'].std(),\n            'mean_mkt': cluster_data['mkt_excess'].mean() * 100,\n            'std_mkt': cluster_data['mkt_excess'].std() * 100,\n            'mean_hml': cluster_data['hml'].mean() * 100,\n            'std_hml': cluster_data['hml'].std() * 100,\n            'mean_rmw': cluster_data['rmw'].mean() * 100,\n            'mean_cma': cluster_data['cma'].mean() * 100,\n        }\n        \n        # Automatic regime naming\n        L = char['mean_L']\n        vol = char['mean_vol']\n        hml = char['mean_hml']\n        rmw = char['mean_rmw']\n        \n        if L &gt; 1.5:  # Very high liquidity\n            if vol &gt; 0.045:\n                regime = \"Intervention\"\n            else:\n                regime = \"Goldilocks\"\n        elif L &gt; 0:  # Moderate positive\n            if hml &lt; -0.3:\n                regime = \"Expansion\"\n            else:\n                regime = \"Goldilocks\"\n        else:  # Negative liquidity\n            if vol &gt; 0.045:\n                regime = \"Bear\"\n            else:\n                regime = \"Tight\"\n        \n        char['regime'] = regime\n        cluster_chars.append(char)\n    \n    # Print table\n    print(f\"\\n{'Cluster':&lt;8} {'Regime':&lt;15} {'N':&lt;5} {'L':&lt;10} {'Vol':&lt;10} {'MKT%':&lt;8} {'HML%':&lt;8} {'RMW%':&lt;8} {'CMA%':&lt;8}\")\n    print(\"-\" * 100)\n    \n    for char in sorted(cluster_chars, key=lambda x: x['mean_L'], reverse=True):\n        print(f\"{char['cluster']:&lt;8} {char['regime']:&lt;15} {char['n_months']:&lt;5} \"\n              f\"{char['mean_L']:&gt;+6.2f}    {char['mean_vol']:&gt;7.4f}   \"\n              f\"{char['mean_mkt']:&gt;+6.2f}  {char['mean_hml']:&gt;+6.2f}  \"\n              f\"{char['mean_rmw']:&gt;+6.2f}  {char['mean_cma']:&gt;+6.2f}\")\n    \n    # ==========================================\n    # STEP 4: Assign to Dataframe\n    # ==========================================\n    cluster_to_regime = {char['cluster']: char['regime'] for char in cluster_chars}\n    \n    df['regime_final'] = 'Unknown'\n    df.loc[df_features.index, 'regime_final'] = [cluster_to_regime[c] for c in cluster_labels]\n    df.loc[df['is_crisis'], 'regime_final'] = 'Crisis'\n    \n    df['regime_prob'] = np.nan\n    for i, idx in enumerate(df_features.index):\n        assigned_cluster = cluster_labels[i]\n        df.loc[idx, 'regime_prob'] = cluster_probs[i, assigned_cluster]\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"REGIME DISTRIBUTION\")\n    print(\"=\"*80)\n    print(df['regime_final'].value_counts().sort_index())\n    \n    # ==========================================\n    # VALIDATION\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"VALIDATION: Known Historical Periods\")\n    print(\"=\"*80)\n    \n    known = {\n        '2008-2009 Crisis': ('2008-09', '2009-03'),\n        '2010-2014 QE': ('2010-01', '2014-12'),\n        '2015-2019 Late Cycle': ('2015-01', '2019-12'),\n        '2020 COVID': ('2020-02', '2020-04'),\n        '2020-2021 Reopening': ('2020-05', '2021-12'),\n        '2022-2024 QT': ('2022-03', '2024-12'),\n    }\n    \n    for label, (start, end) in known.items():\n        period = df.loc[start:end]\n        valid = period[period['regime_final'] != 'Unknown']\n        \n        if len(valid) &gt; 0:\n            dist = valid['regime_final'].value_counts()\n            mode = dist.idxmax() if len(dist) &gt; 0 else 'None'\n            \n            mean_L = valid['L'].mean()\n            mean_vol = valid['ret_vol'].mean()\n            mean_mkt = valid['mkt_excess'].mean() * 100\n            mean_hml = valid['hml'].mean() * 100\n            \n            print(f\"\\n{label}:\")\n            print(f\"  Primary: {mode} ({dist.get(mode, 0)}/{len(valid)})\")\n            print(f\"  Distribution: {dist.to_dict()}\")\n            print(f\"  L={mean_L:+.2f}, Vol={mean_vol:.4f}, MKT={mean_mkt:+.2f}%, HML={mean_hml:+.2f}%\")\n    \n    return df, gmm_final, scaler, cluster_chars\n\n# ==========================================\n# RUN\n# ==========================================\ncombined_final, gmm_model, scaler_final, cluster_info = factor_based_regime_discovery(combined_aug_mom)\n\n# ==========================================\n# RESULTS\n# ==========================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"FACTOR RETURNS BY DATA-DRIVEN REGIMES\")\nprint(\"=\"*80)\n\nvalid = combined_final[combined_final['regime_final'] != 'Unknown']\n\nsummary_rows = []\nfor regime in sorted(valid['regime_final'].unique()):\n    regime_data = valid[valid['regime_final'] == regime]\n    n = len(regime_data)\n    \n    row = {\n        'Regime': regime,\n        'N': n,\n        'Mean_L': regime_data['L'].mean(),\n        'Mean_Vol': regime_data['ret_vol'].mean(),\n    }\n    \n    for factor in ['mkt_excess', 'hml', 'rmw', 'cma']:\n        mean = regime_data[factor].mean() * 100\n        std = regime_data[factor].std() * 100\n        t_stat = mean / (std / np.sqrt(n)) if std &gt; 0 and n &gt; 1 else 0\n        \n        row[factor.upper()] = mean\n        row[f'{factor.upper()}_t'] = t_stat\n    \n    summary_rows.append(row)\n\nsummary_df = pd.DataFrame(summary_rows)\n\nprint(\"\\nSummary Table:\")\nprint(summary_df[['Regime', 'N', 'Mean_L', 'Mean_Vol', 'MKT_EXCESS', 'HML', 'RMW', 'CMA']].to_string(index=False))\n\nprint(\"\\n\\nDetailed Statistics by Regime:\")\nfor _, row in summary_df.iterrows():\n    print(f\"\\n{row['Regime']} ({row['N']} months):\")\n    print(f\"  Liquidity: {row['Mean_L']:+.2f}, Volatility: {row['Mean_Vol']:.4f}\")\n    print(f\"  MKT: {row['MKT_EXCESS']:+.2f}% (t={row['MKT_EXCESS_t']:+.2f})\")\n    print(f\"  HML: {row['HML']:+.2f}% (t={row['HML_t']:+.2f})\")\n    print(f\"  RMW: {row['RMW']:+.2f}% (t={row['RMW_t']:+.2f})\")\n    print(f\"  CMA: {row['CMA']:+.2f}% (t={row['CMA_t']:+.2f})\")\n\n================================================================================\nSTEP 1: CRISIS OUTLIER DETECTION\n================================================================================\nCrisis months: 5\n\n================================================================================\nSTEP 2: FACTOR RETURN-BASED REGIME CLUSTERING\n================================================================================\nNormal periods: 251 months\n\nStandardization:\n  L: Keep as-is (already z-scored)\n  Others: Standardize\n\nVerification - L unchanged:\n  Original L mean: -0.088497\n  X_scaled[:,0] mean: -0.088497\n\n================================================================================\nMODEL SELECTION (BIC)\n================================================================================\n\nN     BIC             AIC            \n-----------------------------------\n2     4,445.5         4,251.6         ← BIC optimal\n3     4,530.3         4,237.6        \n4     4,556.7         4,165.4        \n5     4,631.2         4,141.2        \n6     4,740.1         4,151.3        \n7     4,758.1         4,070.7         ← AIC optimal\n\nSelected: 2 regimes (BIC criterion)\n\nFitting final GMM with 2 components...\nConverged: True\nIterations: 17\n\n================================================================================\nCLUSTER CHARACTERIZATION\n================================================================================\n\nCluster  Regime          N     L          Vol        MKT%     HML%     RMW%     CMA%    \n----------------------------------------------------------------------------------------------------\n0        Goldilocks      107    +0.20     0.0547    +0.85   +0.35   +0.54   +0.26\n1        Tight           144    -0.31     0.0267    +1.26   -0.19   +0.09   -0.25\n\n================================================================================\nREGIME DISTRIBUTION\n================================================================================\nregime_final\nCrisis          5\nGoldilocks    107\nTight         144\nUnknown         5\nName: count, dtype: int64\n\n================================================================================\nVALIDATION: Known Historical Periods\n================================================================================\n\n2008-2009 Crisis:\n  Primary: Crisis (4/7)\n  Distribution: {'Crisis': 4, 'Goldilocks': 3}\n  L=+2.74, Vol=0.0717, MKT=-5.96%, HML=-2.36%\n\n2010-2014 QE:\n  Primary: Tight (42/60)\n  Distribution: {'Tight': 42, 'Goldilocks': 18}\n  L=+0.89, Vol=0.0371, MKT=+1.29%, HML=-0.05%\n\n2015-2019 Late Cycle:\n  Primary: Tight (41/60)\n  Distribution: {'Tight': 41, 'Goldilocks': 19}\n  L=-0.26, Vol=0.0337, MKT=+0.89%, HML=-0.34%\n\n2020 COVID:\n  Primary: Goldilocks (2/3)\n  Distribution: {'Goldilocks': 2, 'Crisis': 1}\n  L=+4.40, Vol=0.0695, MKT=-2.64%, HML=-6.33%\n\n2020-2021 Reopening:\n  Primary: Goldilocks (18/20)\n  Distribution: {'Goldilocks': 18, 'Tight': 2}\n  L=+3.37, Vol=0.0519, MKT=+2.74%, HML=+0.58%\n\n2022-2024 QT:\n  Primary: Goldilocks (26/34)\n  Distribution: {'Goldilocks': 26, 'Tight': 8}\n  L=-2.07, Vol=0.0495, MKT=+0.76%, HML=-0.12%\n\n================================================================================\nFACTOR RETURNS BY DATA-DRIVEN REGIMES\n================================================================================\n\nSummary Table:\n    Regime   N    Mean_L  Mean_Vol  MKT_EXCESS       HML      RMW       CMA\n    Crisis   5  4.111773  0.066251  -11.556000 -4.574000 1.840000  1.208000\nGoldilocks 107  0.204691  0.054710    0.846636  0.347570 0.541402  0.257009\n     Tight 144 -0.306352  0.026653    1.257153 -0.186875 0.090000 -0.247847\n\n\nDetailed Statistics by Regime:\n\nCrisis (5 months):\n  Liquidity: +4.11, Volatility: 0.0663\n  MKT: -11.56% (t=-6.88)\n  HML: -4.57% (t=-1.43)\n  RMW: +1.84% (t=+1.77)\n  CMA: +1.21% (t=+2.41)\n\nGoldilocks (107 months):\n  Liquidity: +0.20, Volatility: 0.0547\n  MKT: +0.85% (t=+1.57)\n  HML: +0.35% (t=+0.92)\n  RMW: +0.54% (t=+2.32)\n  CMA: +0.26% (t=+1.03)\n\nTight (144 months):\n  Liquidity: -0.31, Volatility: 0.0267\n  MKT: +1.26% (t=+6.37)\n  HML: -0.19% (t=-1.10)\n  RMW: +0.09% (t=+0.85)\n  CMA: -0.25% (t=-2.55)\n\n\n\ncombined_aug_mom['L'].describe()\n\n# What period has this extreme value?\nextreme_L = combined_aug_mom[combined_aug_mom['L'] &gt; 5]\nprint(\"Extreme liquidity periods (L &gt; 5):\")\nprint(extreme_L[['L', 'mkt_excess', 'hml']].sort_values('L', ascending=False))\n\n# Distribution check\nprint(\"\\nL value distribution:\")\nprint(f\"Mean: {combined_aug_mom['L'].mean():.2f}\")\nprint(f\"Std:  {combined_aug_mom['L'].std():.2f}\")\nprint(f\"Skew: {combined_aug_mom['L'].skew():.2f}\")\nprint(f\"Kurtosis: {combined_aug_mom['L'].kurtosis():.2f}\")\n\n# Percentiles\nprint(\"\\nPercentiles:\")\nfor p in [1, 5, 10, 25, 50, 75, 90, 95, 99]:\n    val = combined_aug_mom['L'].quantile(p/100)\n    print(f\"  {p:2d}%: {val:+.2f}\")\n\nExtreme liquidity periods (L &gt; 5):\n                   L  mkt_excess     hml\n2020-04-30  8.523441      0.1358 -0.0134\n2008-10-31  7.816953     -0.1720 -0.0189\n2020-05-31  7.224145      0.0559 -0.0500\n2020-03-31  5.207325     -0.1335 -0.1383\n\nL value distribution:\nMean: -0.00\nStd:  1.95\nSkew: 0.69\nKurtosis: 2.22\n\nPercentiles:\n   1%: -3.86\n   5%: -3.22\n  10%: -2.31\n  25%: -1.36\n  50%: +0.14\n  75%: +0.84\n  90%: +2.56\n  95%: +3.05\n  99%: +6.01\n\n\n\ndef cart_optimal_regime_thresholds(combined_aug):\n    \"\"\"\n    Use CART to find OPTIMAL L and Vol thresholds that maximize\n    factor return predictability.\n    \n    Academic approach: Let the data tell us the best thresholds,\n    then validate economically.\n    \"\"\"\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.preprocessing import KBinsDiscretizer\n    import numpy as np\n    import pandas as pd\n    \n    df = combined_aug.copy()\n    \n    # ==========================================\n    # STEP 0: Standardize L\n    # ==========================================\n    df['L_original'] = df['L']\n    df['L_std'] = df['L'] / df['L'].std()\n    \n    print(\"=\"*80)\n    print(\"CART-BASED OPTIMAL THRESHOLD DISCOVERY\")\n    print(\"=\"*80)\n    \n    # ==========================================\n    # STEP 1: Crisis Detection (same)\n    # ==========================================\n    df['ret_vol'] = df['mkt_excess'].rolling(6).std()\n    df['L_vol'] = df['L_std'].rolling(3).std()\n    df['abs_ret'] = df['mkt_excess'].abs()\n    df['cum_ret_3m'] = (1 + df['mkt_excess']).rolling(3).apply(lambda x: x.prod()) - 1\n    \n    crisis_features = df[['ret_vol', 'abs_ret', 'cum_ret_3m', 'L_vol']].dropna()\n    \n    from sklearn.ensemble import IsolationForest\n    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n    outlier_labels = iso_forest.fit_predict(crisis_features.values)\n    \n    df['is_crisis'] = False\n    df.loc[crisis_features.index, 'is_crisis'] = (outlier_labels == -1) & (df.loc[crisis_features.index, 'mkt_excess'] &lt; -0.02)\n    \n    print(f\"Crisis months: {df['is_crisis'].sum()}\")\n    \n    # ==========================================\n    # STEP 2: Create TARGET Variable for CART\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"CREATE TARGET LABELS (HML Quantiles)\")\n    print(\"=\"*80)\n    \n    # Key insight: Regimes should PREDICT factor returns\n    # Create target = HML terciles (Growth/Neutral/Value periods)\n    \n    df_normal = df[~df['is_crisis']].copy()\n    df_clean = df_normal[['L_std', 'ret_vol', 'hml', 'rmw', 'cma']].dropna()\n    \n    # Create HML-based target (which regime are we in based on realized HML?)\n    # This is forward-looking: \"Did growth or value win this month?\"\n    hml_quantiles = pd.qcut(df_clean['hml'], q=3, labels=['Growth', 'Neutral', 'Value'])\n    \n    df_clean['target'] = hml_quantiles\n    \n    print(f\"\\nTarget distribution:\")\n    print(df_clean['target'].value_counts())\n    \n    # ==========================================\n    # STEP 3: Train CART to Find Optimal Thresholds\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"TRAIN CART TO FIND OPTIMAL THRESHOLDS\")\n    print(\"=\"*80)\n    \n    X = df_clean[['L_std', 'ret_vol']].values\n    y = df_clean['target'].values\n    \n    # Train decision tree (this finds optimal splits)\n    cart = DecisionTreeClassifier(\n        max_depth=3,  # Simple tree for interpretability\n        min_samples_leaf=20,  # Minimum regime size = 20 months\n        random_state=42\n    )\n    \n    cart.fit(X, y)\n    \n    # Extract learned thresholds\n    from sklearn.tree import export_text\n    tree_rules = export_text(cart, feature_names=['L_std', 'ret_vol'])\n    \n    print(\"\\nLearned Decision Rules:\")\n    print(tree_rules)\n    \n    # Get predictions\n    regime_predictions = cart.predict(X)\n    \n    # ==========================================\n    # STEP 4: Translate to Economic Regimes\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"TRANSLATE PREDICTIONS TO ECONOMIC REGIMES\")\n    print(\"=\"*80)\n    \n    # Map growth/neutral/value to liquidity regimes\n    df_clean['cart_prediction'] = regime_predictions\n    \n    # Characterize each prediction category by L_std and vol\n    regime_chars = []\n    \n    for pred_class in ['Growth', 'Neutral', 'Value']:\n        if pred_class not in df_clean['cart_prediction'].unique():\n            continue\n            \n        mask = df_clean['cart_prediction'] == pred_class\n        subset = df_clean[mask]\n        \n        mean_L = subset['L_std'].mean()\n        mean_vol = subset['ret_vol'].mean()\n        mean_hml = subset['hml'].mean() * 100\n        mean_rmw = subset['rmw'].mean() * 100\n        mean_cma = subset['cma'].mean() * 100\n        \n        # Economic interpretation\n        if mean_L &gt; 0.3:\n            if mean_vol &gt; subset['ret_vol'].median():\n                regime = \"High_Liq_High_Vol\"\n            else:\n                regime = \"High_Liq_Low_Vol\"\n        elif mean_L &lt; -0.3:\n            if mean_vol &gt; subset['ret_vol'].median():\n                regime = \"Tight_Liq_High_Vol\"\n            else:\n                regime = \"Tight_Liq_Low_Vol\"\n        else:\n            regime = \"Neutral_Liq\"\n        \n        char = {\n            'cart_class': pred_class,\n            'regime': regime,\n            'n': len(subset),\n            'mean_L_std': mean_L,\n            'mean_vol': mean_vol,\n            'mean_hml': mean_hml,\n            'mean_rmw': mean_rmw,\n            'mean_cma': mean_cma,\n        }\n        \n        regime_chars.append(char)\n    \n    # Print characterization\n    print(f\"\\n{'CART Class':&lt;12} {'Regime':&lt;25} {'N':&lt;5} {'L_std':&lt;8} {'Vol':&lt;8} {'HML%':&lt;8} {'RMW%':&lt;8}\")\n    print(\"-\" * 95)\n    \n    for char in sorted(regime_chars, key=lambda x: x['mean_L_std'], reverse=True):\n        print(f\"{char['cart_class']:&lt;12} {char['regime']:&lt;25} {char['n']:&lt;5} \"\n              f\"{char['mean_L_std']:&gt;+6.2f}  {char['mean_vol']:&gt;6.4f}  \"\n              f\"{char['mean_hml']:&gt;+6.2f}  {char['mean_rmw']:&gt;+6.2f}\")\n    \n    # Map to dataframe\n    cart_to_regime = {char['cart_class']: char['regime'] for char in regime_chars}\n    \n    df['regime_final'] = 'Unknown'\n    df.loc[df_clean.index, 'regime_final'] = [cart_to_regime[p] for p in regime_predictions]\n    df.loc[df['is_crisis'], 'regime_final'] = 'Crisis'\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"REGIME DISTRIBUTION\")\n    print(\"=\"*80)\n    print(df['regime_final'].value_counts())\n    \n    # ==========================================\n    # VALIDATION\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"VALIDATION\")\n    print(\"=\"*80)\n    \n    known = {\n        '2008-2009 Crisis': ('2008-09', '2009-03'),\n        '2010-2014 QE': ('2010-01', '2014-12'),\n        '2015-2019 Late Cycle': ('2015-01', '2019-12'),\n        '2020 COVID': ('2020-02', '2020-04'),\n        '2020-2021 Reopening': ('2020-05', '2021-12'),\n        '2022-2024 QT': ('2022-03', '2024-12'),\n    }\n    \n    for label, (start, end) in known.items():\n        period = df.loc[start:end]\n        valid = period[period['regime_final'] != 'Unknown']\n        \n        if len(valid) &gt; 0:\n            dist = valid['regime_final'].value_counts()\n            mode = dist.idxmax()\n            \n            mean_L_std = valid['L_std'].mean()\n            mean_hml = valid['hml'].mean() * 100\n            \n            print(f\"\\n{label}:\")\n            print(f\"  Primary: {mode} ({dist.get(mode, 0)}/{len(valid)})\")\n            print(f\"  L_std={mean_L_std:+.2f}, HML={mean_hml:+.2f}%\")\n\n    for cart_class in ['Growth', 'Neutral', 'Value']:\n        mask = df_clean['cart_prediction'] == cart_class\n        subset = df_clean[mask]\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"{cart_class} Class ({len(subset)} months)\")\n        print(f\"{'='*60}\")\n        \n        # Statistics\n        print(f\"L_std: {subset['L_std'].mean():+.2f} (range: {subset['L_std'].min():+.2f} to {subset['L_std'].max():+.2f})\")\n        print(f\"Vol:   {subset['ret_vol'].mean():.4f}\")\n        print(f\"HML:   {subset['hml'].mean()*100:+.2f}%\")\n        \n        # Time distribution\n        print(f\"\\nYear distribution:\")\n        year_counts = subset.index.year.value_counts().sort_index()\n        for year, count in year_counts.items():\n            pct = count / len(subset) * 100\n            print(f\"  {year}: {count:3d} months ({pct:5.1f}%)\")\n        \n        # Known periods\n        periods_in_class = {\n            '2008-2009': ((subset.index &gt;= '2008-09') & (subset.index &lt;= '2009-03')).sum(),\n            '2010-2014 QE': ((subset.index &gt;= '2010-01') & (subset.index &lt;= '2014-12')).sum(),\n            '2015-2019': ((subset.index &gt;= '2015-01') & (subset.index &lt;= '2019-12')).sum(),\n            '2020-2021 Reopening': ((subset.index &gt;= '2020-05') & (subset.index &lt;= '2021-12')).sum(),\n            '2022-2024 QT': ((subset.index &gt;= '2022-03') & (subset.index &lt;= '2024-12')).sum(),\n        }\n        \n        print(f\"\\nKnown periods in this class:\")\n        for period, count in periods_in_class.items():\n            if count &gt; 0:\n                print(f\"  {period}: {count} months\")\n    \n    return df, cart, regime_chars, X\n\n# RUN CART APPROACH\ncombined_cart, cart_model, cart_chars, X = cart_optimal_regime_thresholds(combined_aug_mom)\n\n================================================================================\nCART-BASED OPTIMAL THRESHOLD DISCOVERY\n================================================================================\nCrisis months: 5\n\n================================================================================\nCREATE TARGET LABELS (HML Quantiles)\n================================================================================\n\nTarget distribution:\ntarget\nGrowth     85\nValue      84\nNeutral    82\nName: count, dtype: int64\n\n================================================================================\nTRAIN CART TO FIND OPTIMAL THRESHOLDS\n================================================================================\n\nLearned Decision Rules:\n|--- ret_vol &lt;= 0.03\n|   |--- ret_vol &lt;= 0.02\n|   |   |--- ret_vol &lt;= 0.02\n|   |   |   |--- class: Neutral\n|   |   |--- ret_vol &gt;  0.02\n|   |   |   |--- class: Value\n|   |--- ret_vol &gt;  0.02\n|   |   |--- class: Neutral\n|--- ret_vol &gt;  0.03\n|   |--- L_std &lt;= -0.56\n|   |   |--- L_std &lt;= -1.29\n|   |   |   |--- class: Growth\n|   |   |--- L_std &gt;  -1.29\n|   |   |   |--- class: Growth\n|   |--- L_std &gt;  -0.56\n|   |   |--- L_std &lt;= 0.64\n|   |   |   |--- class: Neutral\n|   |   |--- L_std &gt;  0.64\n|   |   |   |--- class: Value\n\n\n================================================================================\nTRANSLATE PREDICTIONS TO ECONOMIC REGIMES\n================================================================================\n\nCART Class   Regime                    N     L_std    Vol      HML%     RMW%    \n-----------------------------------------------------------------------------------------------\nValue        High_Liq_High_Vol         58     +0.71  0.0406   +0.60   +0.52\nNeutral      Neutral_Liq               145    +0.06  0.0355   +0.08   +0.24\nGrowth       Tight_Liq_High_Vol        48     -1.26  0.0456   -0.75   +0.13\n\n================================================================================\nREGIME DISTRIBUTION\n================================================================================\nregime_final\nNeutral_Liq           145\nHigh_Liq_High_Vol      58\nTight_Liq_High_Vol     48\nUnknown                 5\nCrisis                  5\nName: count, dtype: int64\n\n================================================================================\nVALIDATION\n================================================================================\n\n2008-2009 Crisis:\n  Primary: Crisis (4/7)\n  L_std=+1.40, HML=-2.36%\n\n2010-2014 QE:\n  Primary: Neutral_Liq (48/60)\n  L_std=+0.46, HML=-0.05%\n\n2015-2019 Late Cycle:\n  Primary: Neutral_Liq (45/60)\n  L_std=-0.13, HML=-0.34%\n\n2020 COVID:\n  Primary: Neutral_Liq (1/3)\n  L_std=+2.25, HML=-6.33%\n\n2020-2021 Reopening:\n  Primary: High_Liq_High_Vol (18/20)\n  L_std=+1.73, HML=+0.58%\n\n2022-2024 QT:\n  Primary: Tight_Liq_High_Vol (23/34)\n  L_std=-1.06, HML=-0.12%\n\n============================================================\nGrowth Class (48 months)\n============================================================\nL_std: -1.26 (range: -2.03 to -0.64)\nVol:   0.0456\nHML:   -0.75%\n\nYear distribution:\n  2005:   3 months (  6.2%)\n  2007:   4 months (  8.3%)\n  2018:   2 months (  4.2%)\n  2019:   8 months ( 16.7%)\n  2022:   1 months (  2.1%)\n  2023:  12 months ( 25.0%)\n  2024:  10 months ( 20.8%)\n  2025:   8 months ( 16.7%)\n\nKnown periods in this class:\n  2015-2019: 10 months\n  2022-2024 QT: 22 months\n\n============================================================\nNeutral Class (145 months)\n============================================================\nL_std: +0.06 (range: -1.42 to +1.40)\nVol:   0.0355\nHML:   +0.08%\n\nYear distribution:\n  2004:   5 months (  3.4%)\n  2005:   9 months (  6.2%)\n  2006:   4 months (  2.8%)\n  2007:   6 months (  4.1%)\n  2008:   8 months (  5.5%)\n  2009:  10 months (  6.9%)\n  2010:  12 months (  8.3%)\n  2011:   5 months (  3.4%)\n  2012:  11 months (  7.6%)\n  2013:   9 months (  6.2%)\n  2014:  11 months (  7.6%)\n  2015:  12 months (  8.3%)\n  2016:  11 months (  7.6%)\n  2017:  11 months (  7.6%)\n  2018:   8 months (  5.5%)\n  2019:   3 months (  2.1%)\n  2020:   1 months (  0.7%)\n  2021:   2 months (  1.4%)\n  2022:   5 months (  3.4%)\n  2024:   1 months (  0.7%)\n  2025:   1 months (  0.7%)\n\nKnown periods in this class:\n  2008-2009: 1 months\n  2010-2014 QE: 47 months\n  2015-2019: 45 months\n  2020-2021 Reopening: 2 months\n  2022-2024 QT: 6 months\n\n============================================================\nValue Class (58 months)\n============================================================\nL_std: +0.71 (range: -1.47 to +4.36)\nVol:   0.0406\nHML:   +0.60%\n\nYear distribution:\n  2004:   2 months (  3.4%)\n  2006:   8 months ( 13.8%)\n  2007:   2 months (  3.4%)\n  2008:   1 months (  1.7%)\n  2009:   1 months (  1.7%)\n  2011:   7 months ( 12.1%)\n  2012:   1 months (  1.7%)\n  2013:   3 months (  5.2%)\n  2014:   1 months (  1.7%)\n  2016:   1 months (  1.7%)\n  2017:   1 months (  1.7%)\n  2018:   2 months (  3.4%)\n  2019:   1 months (  1.7%)\n  2020:  10 months ( 17.2%)\n  2021:  10 months ( 17.2%)\n  2022:   6 months ( 10.3%)\n  2024:   1 months (  1.7%)\n\nKnown periods in this class:\n  2008-2009: 1 months\n  2010-2014 QE: 12 months\n  2015-2019: 4 months\n  2020-2021 Reopening: 17 months\n  2022-2024 QT: 5 months\n\n\n\nfrom sklearn.tree import export_text\n\n# Print the actual decision rules\ntree_rules = export_text(cart_model, feature_names=['L_std', 'ret_vol'])\nprint(\"CART Decision Rules:\")\nprint(tree_rules)\n\n# Also show feature importance\nprint(\"\\nFeature Importance:\")\nprint(f\"  L_std: {cart_model.feature_importances_[0]:.3f}\")\nprint(f\"  ret_vol: {cart_model.feature_importances_[1]:.3f}\")\n\n# Check class distribution\nfrom collections import Counter\nprint(\"\\nCART class predictions:\")\nprint(Counter(cart_model.predict(X)))\n\n# Show sample months from each regime\nfor cart_class in ['Growth', 'Neutral', 'Value']:\n    mask = combined_cart['cart_prediction'] == cart_class\n    sample_months = combined_cart[mask].index[:10].tolist()\n    \n    print(f\"\\n{cart_class} regime - sample months:\")\n    print(sample_months)\n    \n    # Show distribution by year\n    year_dist = combined_cart[mask].index.year.value_counts().sort_index()\n    print(f\"Distribution by year:\")\n    print(year_dist.head(10))\n\nCART Decision Rules:\n|--- ret_vol &lt;= 0.03\n|   |--- ret_vol &lt;= 0.02\n|   |   |--- ret_vol &lt;= 0.02\n|   |   |   |--- class: Neutral\n|   |   |--- ret_vol &gt;  0.02\n|   |   |   |--- class: Value\n|   |--- ret_vol &gt;  0.02\n|   |   |--- class: Neutral\n|--- ret_vol &gt;  0.03\n|   |--- L_std &lt;= -0.56\n|   |   |--- L_std &lt;= -1.29\n|   |   |   |--- class: Growth\n|   |   |--- L_std &gt;  -1.29\n|   |   |   |--- class: Growth\n|   |--- L_std &gt;  -0.56\n|   |   |--- L_std &lt;= 0.64\n|   |   |   |--- class: Neutral\n|   |   |--- L_std &gt;  0.64\n|   |   |   |--- class: Value\n\n\nFeature Importance:\n  L_std: 0.525\n  ret_vol: 0.475\n\nCART class predictions:\nCounter({'Neutral': 145, 'Value': 58, 'Growth': 48})\n\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/anaconda3/envs/py-data/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653, in Index.get_loc(self, key)\n   3652 try:\n-&gt; 3653     return self._engine.get_loc(casted_key)\n   3654 except KeyError as err:\n\nFile ~/anaconda3/envs/py-data/lib/python3.8/site-packages/pandas/_libs/index.pyx:147, in pandas._libs.index.IndexEngine.get_loc()\n\nFile ~/anaconda3/envs/py-data/lib/python3.8/site-packages/pandas/_libs/index.pyx:176, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7080, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'cart_prediction'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[411], line 20\n     18 # Show sample months from each regime\n     19 for cart_class in ['Growth', 'Neutral', 'Value']:\n---&gt; 20     mask = combined_cart['cart_prediction'] == cart_class\n     21     sample_months = combined_cart[mask].index[:10].tolist()\n     23     print(f\"\\n{cart_class} regime - sample months:\")\n\nFile ~/anaconda3/envs/py-data/lib/python3.8/site-packages/pandas/core/frame.py:3761, in DataFrame.__getitem__(self, key)\n   3759 if self.columns.nlevels &gt; 1:\n   3760     return self._getitem_multilevel(key)\n-&gt; 3761 indexer = self.columns.get_loc(key)\n   3762 if is_integer(indexer):\n   3763     indexer = [indexer]\n\nFile ~/anaconda3/envs/py-data/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655, in Index.get_loc(self, key)\n   3653     return self._engine.get_loc(casted_key)\n   3654 except KeyError as err:\n-&gt; 3655     raise KeyError(key) from err\n   3656 except TypeError:\n   3657     # If we have a listlike key, _check_indexing_error will raise\n   3658     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3659     #  the TypeError.\n   3660     self._check_indexing_error(key)\n\nKeyError: 'cart_prediction'\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L1_t.index,\n    y=L1_t.values,\n    hue=regimes_aug_df[\"state\"],\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Augmented Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"Augmented L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_aug_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution/KDE of Augmented L(t) by HMM State\")\nax[1].set_xlabel(\"Augmented L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn KDE of augmented liquidity index \\(L_t^{aug}\\), clusters are clearly separated, unlike last one\n\n\nLiquidity Vector w/ Momentum\n\ndef build_liquidity_proxies_with_momentum(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = build_liquidity_proxies_augmented(macro_df)  # Your existing function\n    \n    # Add momentum features\n    df[\"dL_3m\"] = df[\"EM\"].diff(3)  # 3-month change in excess M2\n    df[\"dL_12m\"] = df[\"EM\"].diff(12)  # 12-month change\n    df[\"dEB_dt\"] = df[\"EB\"].diff()  # Rate of Fed balance sheet expansion\n    df[\"accel_ZIRP\"] = df[\"ZIRP_dummy\"].diff()  # Entry/exit from ZIRP\n    \n    cols_final = [\n        \"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\", \"real_rate\", \"credit_spread\",\n        \"EM\", \"EB\", \"EL_3y\", \"ZIRP_dummy\",\n        \"dL_3m\", \"dL_12m\", \"dEB_dt\", \"accel_ZIRP\"  # Momentum terms\n    ]\n    \n    return df[cols_final].dropna()\n\nliquidity_proxies_with_momentum = build_liquidity_proxies_with_momentum(macro_raw)\nz2_t = standardize_and_signflip(liquidity_proxies_with_momentum)\nL2_t = build_sparse_pca_liquidity_index(z2_t, alpha=0.5)\nn_states=2\nmodel, regimes_aug_mom_df = fit_hmm_on_liquidity(L2_t,n_states=n_states)\n\nreg = regimes_aug_mom_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined_aug_mom = reg.join(ff_adj, how=\"inner\")\ncombined_aug_mom.tail()\n\nSparsePCA components (loadings):\n  dlog_M2: +0.306\n  dlog_FED_BAL: +0.267\n  term_spread: +0.284\n  real_rate: +0.344\n  credit_spread: -0.114\n  EM: +0.080\n  EB: +0.098\n  EL_3y: +0.412\n  ZIRP_dummy: +0.340\n  dL_3m: +0.324\n  dL_12m: +0.380\n  dEB_dt: +0.267\n  accel_ZIRP: +0.025\n\nTop 3 variables by absolute loading:\n  EL_3y: loading=+0.412, corr=+0.802, weighted=+0.331\n  dL_12m: loading=+0.380, corr=+0.741, weighted=+0.281\n  real_rate: loading=+0.344, corr=+0.675, weighted=+0.233\n\nWeighted average correlation: +0.743\n✅ Sign correct: L_t weighted correlation positive\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.305\n  state 1: mean L = 1.485\n\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2025-05-31\n-2.474098\n0\n0.999985\n0.000015\nTight\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-06-30\n-2.179103\n0\n0.999980\n0.000020\nTight\n0.0486\n-0.0002\n-0.0161\n-0.0320\n0.0144\n0.0034\n\n\n2025-07-31\n-2.252183\n0\n0.999981\n0.000019\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-31\n-2.218250\n0\n0.999976\n0.000024\nTight\n0.0185\n0.0488\n0.0442\n-0.0067\n0.0208\n0.0038\n\n\n2025-09-30\n-1.997313\n0\n0.999675\n0.000325\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(14, 6))\nax.plot(combined_aug_mom.index, combined_aug_mom['L'], label='L(t)', linewidth=1)\n\n# Color by regime\ncolors = combined_aug_mom['state_label'].map({'High': 'red', 'Tight': 'green'})\nax.scatter(combined_aug_mom.index, combined_aug_mom['L'], c=colors, s=5, alpha=0.5)\n\nax.set_title('Liquidity Index w/ Momentum Factors with HMM Regime Colors (Red=High, Green=Tight)')\nplt.show()\n\n\n\n\n\n\n\n\n\ndef classify_three_regimes(combined_aug):\n    \"\"\"\n    Three-regime classification:\n    1. Crisis: L spikes + VIX spikes + negative returns\n    2. High: L &gt; threshold, not crisis\n    3. Tight: L &lt; threshold, not crisis\n    \"\"\"\n    df = combined_aug.copy()\n    \n    # Crisis indicator\n    crisis = (\n        ((df.index &gt;= '2008-09') & (df.index &lt;= '2009-03')) |\n        ((df.index &gt;= '2020-02') & (df.index &lt;= '2020-04'))\n    )\n    \n    # Regime classification\n    df['regime_3state'] = 'Tight'  # default\n    df.loc[df['L'] &gt; 0, 'regime_3state'] = 'High'\n    df.loc[crisis, 'regime_3state'] = 'Crisis'\n    \n    return df\n\n# Apply three-regime classification\ncombined_3regime = classify_three_regimes(combined_aug_mom)\n\n# Factor returns by 3 regimes\ndf_3regime = combined_3regime.copy()\ndf_3regime = df_3regime.dropna(subset=[\"regime_3state\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\nmeans_3regime = (\n    df_3regime.groupby(\"regime_3state\")[[\"mkt_excess\", \"hml\", \"rmw\", \"cma\"]]\n    .mean() * 100\n)\n\nprint(\"\\nFactor Returns by 3 Regimes (monthly %):\")\nprint(means_3regime)\n\n\nFactor Returns by 3 Regimes (monthly %):\n               mkt_excess       hml       rmw       cma\nregime_3state                                          \nCrisis          -9.031250 -4.707500  0.975000  0.103750\nHigh             1.375368  0.279191  0.363824  0.127206\nTight            0.860085 -0.108632  0.216068 -0.187350\n\n\n\n# Define factors\nfactors = ['mkt_excess', 'hml', 'rmw', 'cma']\ndf4 = combined_aug_mom.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf4[\"state_label_lag1\"] = df4[\"state_label\"].shift(-1)\n\n# Calculate statistics with ACCURATE annualization\nstats_df_aug_mom = calculate_factor_statistics_by_regime_accurate(\n    df4, \n    factor_cols, \n    regime_col='state_label_lag1',\n    regime_order=['High', 'Neutral', 'Tight']\n)\n\n# Print formatted table (like your image but with accurate formulas)\nprint_formatted_table_accurate(stats_df_aug_mom, regime_order=['High', 'Tight'], show_geometric=True)\n\n\n==========================================================================================\nFACTOR RETURNS BY REGIME (Monthly)\n==========================================================================================\n                   Mean Return                   Std Dev              Sharpe Ratio\nFactor       High       Tight         High       Tight         High       Tight \n------------------------------------------------------------------------------------------\nMKT_EXCESS      0.09%       0.97%        6.40%       3.84%        0.014       0.252 \nHML         0.16%      -0.10%        4.72%       2.73%        0.033      -0.037 \nRMW         1.11%       0.17%        2.56%       1.67%        0.436       0.103 \nCMA         0.59%      -0.15%        2.55%       1.72%        0.231      -0.089 \n\nAnnualized (geometric compounding: (1+r_m)^12-1; volatility: σ_m*√12)\n------------------------------------------------------------------------------------------\nMKT_EXCESS       1.1%       12.3%        22.2%       13.3%        0.049       0.875 \nHML          1.9%       -1.2%        16.4%        9.5%        0.115      -0.128 \nRMW         14.2%        2.1%         8.9%        5.8%        1.511       0.357 \nCMA          7.3%       -1.8%         8.8%        5.9%        0.800      -0.308 \n\nGeometric Annual Returns (from actual cumulative performance)\n------------------------------------------------------------------------------------------\nMKT_EXCESS      -1.4%       11.3% \nHML          0.6%       -1.6% \nRMW         13.8%        1.9% \nCMA          6.9%       -2.0% \n==========================================================================================\n\n\n\n\n\n\nS&P 500 monthly prices\n\n\nCAPE Data from https://shillerdata.com/\n\nimport pandas as pd\nimport numpy as np\n\ndef parse_yyyy_mm_to_date(s: str):\n    \"\"\"\n    Convert 'YYYY.MM' to a proper datetime.\n    Handles the special case where October appears as YYYY.1 instead of YYYY.10.\n    \"\"\"\n    s = str(s).strip()\n    parts = s.split(\".\")\n    if len(parts) != 2:\n        raise ValueError(f\"Unexpected date format: {s}\")\n    \n    year = int(parts[0])\n    mm = parts[1]\n\n    # Fix: \"1\" should be \"10\" (October)\n    if mm == \"1\":\n        month = 10\n    else:\n        month = int(mm)\n\n    return pd.Timestamp(year=year, month=month, day=1)\n\n# --- Load CAPE CSV ---\nsp500_cape_raw = pd.read_csv(\"data/sp500-cape-1990-2025.csv\")\n\n# Clean + parse\nsp500_cape_raw[\"date\"] = sp500_cape_raw[\"Date\"].apply(parse_yyyy_mm_to_date)\n\nsp500_cape_raw = (\n    sp500_cape_raw[[\"date\", \"SP500\", \"CAPE\"]]\n    .set_index(\"date\")\n    .sort_index()\n)\n\n# Convert to month-end aligned CAPE series\nsp500_cape_m = sp500_cape_raw.resample(\"M\").last()\nsp500_cape_m.rename(columns={\"CAPE\": \"cape\", \"SP500\": \"sp500\"}, inplace=True)\n\nsp500_cape_m.tail()\n\n\n\n\n\n\n\n\nsp500\ncape\n\n\ndate\n\n\n\n\n\n\n2025-08-31\n6408.95\n37.85\n\n\n2025-09-30\n6584.02\n38.58\n\n\n2025-10-31\n6735.69\n39.21\n\n\n2025-11-30\n6740.89\n39.12\n\n\n2025-12-31\n6812.63\n39.42\n\n\n\n\n\n\n\n\n\nFrom valuation levels into a spread series\nLong-Term Average/Median: The S&P 500’s historical long-term average CAPE ratio is around 16-18, with a median value of approximately 16.04 (since 1881). This provides a central benchmark.\nHigh Valuation (Expensive): A CAPE ratio that is notably higher than the historical average (e.g., above 25 or 30) is considered indicative of a highly valued or overvalued market. Historically, values exceeding 30 have only occurred during major market peaks like the 1929 crash, the dot-com bubble in the late 1990s (peaking over 44), and the post-pandemic period around 2021. Such high readings have typically been followed by periods of lower-than-average, or even negative, long-term returns.\nLow Valuation (Cheap): A CAPE ratio well below the historical average (e.g., below 15 or 10) suggests an undervalued or cheap market. Historically, such levels have been associated with minimal downside risk and higher average long-term returns.\nCould have calculated\nV_spread = (x - baseline) / baseline # baseline = 16\nProblem is, if the whole world “structurally” shifted (e.g. post-1990 lower inflation), “16” stops being meaningful.\nWe are instead measuring,\n\\[\nV_t = \\frac{CAPE_t - \\mathrm{median}(CAPE)}{\\mathrm{IQR}(CAPE)}\n\\]\n\ndef build_valuation_spread_baseline(\n    val_df: pd.DataFrame,\n    metric: str = \"cape\",\n    baseline: float = None,\n    scale: bool = True,\n) -&gt; pd.Series:\n    x = val_df[metric].astype(float)\n\n    if baseline is None:\n        # robust long-run baseline\n        median = x.median()\n        iqr = x.quantile(0.75) - x.quantile(0.25)\n        baseline = median\n        denom = iqr if scale and iqr &gt; 0 else baseline\n    else:\n        denom = baseline if scale else 1.0\n\n    diff = x - baseline          \n\n    V_spread = diff / denom if scale else diff\n    V_spread.name = \"V_spread\"\n    return V_spread\n\n# Example: baseline at 16.04 (long-run median)\nV_spread_t = build_valuation_spread_baseline(\n    sp500_cape_m,\n    metric=\"cape\",\n    baseline=None,\n    scale=True,   # or False if you prefer \"CAPE points below baseline\"\n)\n\nV_spread_t.tail()\n\ndate\n2025-08-31    1.259380\n2025-09-30    1.338771\n2025-10-31    1.407287\n2025-11-30    1.397499\n2025-12-31    1.430125\nFreq: M, Name: V_spread, dtype: float64\n\n\n\nV_spread_t.plot(figsize=(14, 4), title=\"S&P 500 Valuation Spread (cheap vs own history)\")\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCombine Liquidity Regime with Valuation Spread\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-06-30\n-1.384971\n2\n1.763643e-10\n3.461616e-08\n1.000000\nTight\n\n\n2025-07-31\n-1.481025\n2\n4.213931e-09\n1.477532e-08\n1.000000\nTight\n\n\n2025-08-31\n-1.480004\n2\n1.745053e-07\n1.492070e-08\n1.000000\nTight\n\n\n2025-09-30\n-1.248705\n2\n7.336618e-06\n1.134537e-07\n0.999993\nTight\n\n\n2025-11-30\n-1.620391\n2\n2.921088e-04\n2.068795e-07\n0.999708\nTight\n\n\n\n\n\n\n\n\ncombined_aug.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2025-05-31\n-2.880509\n1\n1.340116e-07\n1.000000\nTight\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-06-30\n-2.709857\n1\n2.610193e-07\n1.000000\nTight\n0.0486\n-0.0002\n-0.0161\n-0.0320\n0.0144\n0.0034\n\n\n2025-07-31\n-2.689149\n1\n2.839775e-07\n1.000000\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-31\n-2.606319\n1\n4.195012e-07\n1.000000\nTight\n0.0185\n0.0488\n0.0442\n-0.0067\n0.0208\n0.0038\n\n\n2025-09-30\n-2.418377\n1\n4.483466e-05\n0.999955\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n\n\n\n\n\n\n# regimes_df: index is month-end\nreg = combined_aug.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Align valuation spread to month-end, then join\nV_spread_m = V_spread_t.resample(\"M\").last()\n\ncombined_aug_val = (\n    reg\n    .join(V_spread_m.to_frame(), how=\"inner\")\n)\n\ncombined_aug_val.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\nV_spread\n\n\n\n\n2025-05-31\n-2.880509\n1\n1.340116e-07\n1.000000\nTight\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n0.958129\n\n\n2025-06-30\n-2.709857\n1\n2.610193e-07\n1.000000\nTight\n0.0486\n-0.0002\n-0.0161\n-0.0320\n0.0144\n0.0034\n1.070147\n\n\n2025-07-31\n-2.689149\n1\n2.839775e-07\n1.000000\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n1.218053\n\n\n2025-08-31\n-2.606319\n1\n4.195012e-07\n1.000000\nTight\n0.0185\n0.0488\n0.0442\n-0.0067\n0.0208\n0.0038\n1.259380\n\n\n2025-09-30\n-2.418377\n1\n4.483466e-05\n0.999955\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n1.338771\n\n\n\n\n\n\n\n\ndef cart_with_valuation_3d(combined_aug, sp500_cape_m):\n    \"\"\"\n    Three-dimensional CART: Liquidity × Volatility × Valuation\n    This should resolve the 2010-2014 and 2023-2024 anomalies.\n    \"\"\"\n    df = combined_aug.copy()\n    df['L_std'] = df['L'] / df['L'].std()\n    \n    # Build valuation spread\n    x = sp500_cape_m['cape'].astype(float)\n    median = x.median()\n    iqr = x.quantile(0.75) - x.quantile(0.25)\n    V_spread = (x - median) / iqr\n    V_spread.name = \"V_spread\"\n    \n    # Align to month-end\n    V_spread_m = V_spread.resample(\"M\").last()\n    \n    # ==========================================\n    # Crisis Detection\n    # ==========================================\n    print(\"=\"*80)\n    print(\"3D CART: Liquidity × Volatility × Valuation\")\n    print(\"=\"*80)\n    \n    df['ret_vol'] = df['mkt_excess'].rolling(6).std()\n    df['L_vol'] = df['L_std'].rolling(3).std()\n    df['abs_ret'] = df['mkt_excess'].abs()\n    df['cum_ret_3m'] = (1 + df['mkt_excess']).rolling(3).apply(lambda x: x.prod()) - 1\n    \n    crisis_features = df[['ret_vol', 'abs_ret', 'cum_ret_3m', 'L_vol']].dropna()\n    \n    from sklearn.ensemble import IsolationForest\n    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n    outlier_labels = iso_forest.fit_predict(crisis_features.values)\n    \n    df['is_crisis'] = False\n    df.loc[crisis_features.index, 'is_crisis'] = (outlier_labels == -1) & (df.loc[crisis_features.index, 'mkt_excess'] &lt; -0.02)\n    \n    print(f\"Crisis months: {df['is_crisis'].sum()}\")\n    \n    # ==========================================\n    # Join Valuation\n    # ==========================================\n    df_normal = df[~df['is_crisis']].copy()\n    df_with_val = df_normal.join(V_spread_m.to_frame(), how='inner')\n    \n    print(f\"\\nData with valuation: {len(df_with_val)} months\")\n    \n    # Clean data\n    df_clean = df_with_val[['L_std', 'ret_vol', 'V_spread', 'hml', 'rmw', 'cma']].dropna()\n    \n    print(f\"Clean data: {len(df_clean)} months\")\n    print(f\"\\nValuation spread stats:\")\n    print(f\"  Mean: {df_clean['V_spread'].mean():+.2f}\")\n    print(f\"  Std:  {df_clean['V_spread'].std():.2f}\")\n    print(f\"  Min:  {df_clean['V_spread'].min():+.2f}\")\n    print(f\"  Max:  {df_clean['V_spread'].max():+.2f}\")\n    \n    # ==========================================\n    # Create Target\n    # ==========================================\n    hml_quantiles = pd.qcut(df_clean['hml'], q=3, labels=['Growth', 'Neutral', 'Value'])\n    df_clean['target'] = hml_quantiles\n    \n    print(f\"\\nTarget distribution:\")\n    print(df_clean['target'].value_counts())\n    \n    # ==========================================\n    # Train 3D CART\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"TRAINING 3D CART\")\n    print(\"=\"*80)\n    \n    # THREE features now: L_std, ret_vol, V_spread\n    X = df_clean[['L_std', 'ret_vol', 'V_spread']].values\n    y = df_clean['target'].values\n    \n    from sklearn.tree import DecisionTreeClassifier, export_text\n    \n    cart_3d = DecisionTreeClassifier(\n        max_depth=4,  # Deeper tree for 3 dimensions\n        min_samples_leaf=20,\n        random_state=42\n    )\n    \n    cart_3d.fit(X, y)\n    \n    print(\"\\nLearned Decision Rules:\")\n    tree_rules = export_text(cart_3d, feature_names=['L_std', 'ret_vol', 'V_spread'])\n    print(tree_rules)\n    \n    print(\"\\nFeature Importance:\")\n    for i, feat in enumerate(['L_std', 'ret_vol', 'V_spread']):\n        print(f\"  {feat:12s}: {cart_3d.feature_importances_[i]:.3f}\")\n    \n    # ==========================================\n    # Predictions\n    # ==========================================\n    predictions = cart_3d.predict(X)\n    df_clean['cart_prediction'] = predictions\n    \n    # ==========================================\n    # Characterize Classes\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"REGIME CHARACTERIZATION (3D)\")\n    print(\"=\"*80)\n    \n    regime_chars = []\n    \n    for pred_class in ['Growth', 'Neutral', 'Value']:\n        mask = df_clean['cart_prediction'] == pred_class\n        subset = df_clean[mask]\n        \n        if len(subset) == 0:\n            continue\n        \n        char = {\n            'class': pred_class,\n            'n': len(subset),\n            'L_std': subset['L_std'].mean(),\n            'vol': subset['ret_vol'].mean(),\n            'V_spread': subset['V_spread'].mean(),\n            'hml': subset['hml'].mean() * 100,\n            'rmw': subset['rmw'].mean() * 100,\n            'cma': subset['cma'].mean() * 100,\n        }\n        \n        # Economic interpretation\n        if char['L_std'] &gt; 0.5:\n            liq = \"High_Liq\"\n        elif char['L_std'] &lt; -0.5:\n            liq = \"Tight_Liq\"\n        else:\n            liq = \"Neutral_Liq\"\n        \n        if char['V_spread'] &gt; 0.5:\n            val = \"Expensive\"\n        elif char['V_spread'] &lt; -0.5:\n            val = \"Cheap\"\n        else:\n            val = \"Fair\"\n        \n        regime = f\"{liq}_{val}\"\n        char['regime'] = regime\n        \n        regime_chars.append(char)\n    \n    # Print table\n    print(f\"\\n{'Class':&lt;10} {'Regime':&lt;25} {'N':&lt;5} {'L_std':&lt;8} {'V_spread':&lt;10} {'HML%':&lt;8}\")\n    print(\"-\" * 85)\n    \n    for char in sorted(regime_chars, key=lambda x: x['hml'], reverse=True):\n        print(f\"{char['class']:&lt;10} {char['regime']:&lt;25} {char['n']:&lt;5} \"\n              f\"{char['L_std']:&gt;+6.2f}  {char['V_spread']:&gt;+8.2f}  {char['hml']:&gt;+6.2f}\")\n    \n    # ==========================================\n    # Validation\n    # ==========================================\n    print(\"\\n\" + \"=\"*80)\n    print(\"VALIDATION: Known Periods\")\n    print(\"=\"*80)\n    \n    known = {\n        '2010-2014 QE': ('2010-01', '2014-12'),\n        '2015-2019 Late Cycle': ('2015-01', '2019-12'),\n        '2020-2021 Reopening': ('2020-05', '2021-12'),\n        '2022-2024 QT': ('2022-03', '2024-12'),\n    }\n    \n    for label, (start, end) in known.items():\n        period = df_clean.loc[start:end]\n        \n        if len(period) == 0:\n            continue\n        \n        dist = period['cart_prediction'].value_counts()\n        mode = dist.idxmax()\n        \n        mean_L = period['L_std'].mean()\n        mean_V = period['V_spread'].mean()\n        mean_hml = period['hml'].mean() * 100\n        \n        print(f\"\\n{label}:\")\n        print(f\"  Classified: {mode} ({dist.get(mode, 0)}/{len(period)})\")\n        print(f\"  L_std={mean_L:+.2f}, V_spread={mean_V:+.2f}, HML={mean_hml:+.2f}%\")\n        print(f\"  Distribution: {dist.to_dict()}\")\n    \n    return df_clean, cart_3d, regime_chars\n\n# RUN 3D CART\ndf_3d, cart_3d_model, chars_3d = cart_with_valuation_3d(combined_aug_mom, sp500_cape_m)\n\n================================================================================\n3D CART: Liquidity × Volatility × Valuation\n================================================================================\nCrisis months: 5\n\nData with valuation: 256 months\nClean data: 251 months\n\nValuation spread stats:\n  Mean: +0.10\n  Std:  0.56\n  Min:  -1.41\n  Max:  +1.34\n\nTarget distribution:\ntarget\nGrowth     85\nValue      84\nNeutral    82\nName: count, dtype: int64\n\n================================================================================\nTRAINING 3D CART\n================================================================================\n\nLearned Decision Rules:\n|--- V_spread &lt;= 0.04\n|   |--- ret_vol &lt;= 0.02\n|   |   |--- class: Value\n|   |--- ret_vol &gt;  0.02\n|   |   |--- ret_vol &lt;= 0.03\n|   |   |   |--- class: Neutral\n|   |   |--- ret_vol &gt;  0.03\n|   |   |   |--- V_spread &lt;= -0.63\n|   |   |   |   |--- class: Growth\n|   |   |   |--- V_spread &gt;  -0.63\n|   |   |   |   |--- class: Value\n|--- V_spread &gt;  0.04\n|   |--- V_spread &lt;= 0.82\n|   |   |--- L_std &lt;= 0.10\n|   |   |   |--- ret_vol &lt;= 0.05\n|   |   |   |   |--- class: Growth\n|   |   |   |--- ret_vol &gt;  0.05\n|   |   |   |   |--- class: Growth\n|   |   |--- L_std &gt;  0.10\n|   |   |   |--- class: Growth\n|   |--- V_spread &gt;  0.82\n|   |   |--- class: Value\n\n\nFeature Importance:\n  L_std       : 0.172\n  ret_vol     : 0.298\n  V_spread    : 0.530\n\n================================================================================\nREGIME CHARACTERIZATION (3D)\n================================================================================\n\nClass      Regime                    N     L_std    V_spread   HML%    \n-------------------------------------------------------------------------------------\nValue      Neutral_Liq_Fair          111    +0.12     +0.14   +0.76\nNeutral    Neutral_Liq_Fair          23     +0.07     -0.18   -0.38\nGrowth     Neutral_Liq_Fair          117    -0.23     +0.12   -0.56\n\n================================================================================\nVALIDATION: Known Periods\n================================================================================\n\n2010-2014 QE:\n  Classified: Value (34/60)\n  L_std=+0.46, V_spread=-0.41, HML=-0.05%\n  Distribution: {'Value': 34, 'Neutral': 14, 'Growth': 12}\n\n2015-2019 Late Cycle:\n  Classified: Growth (45/60)\n  L_std=-0.13, V_spread=+0.25, HML=-0.34%\n  Distribution: {'Growth': 45, 'Value': 14, 'Neutral': 1}\n\n2020-2021 Reopening:\n  Classified: Value (12/20)\n  L_std=+1.73, V_spread=+0.88, HML=+0.58%\n  Distribution: {'Value': 12, 'Growth': 8}\n\n2022-2024 QT:\n  Classified: Growth (25/34)\n  L_std=-1.06, V_spread=+0.57, HML=-0.12%\n  Distribution: {'Growth': 25, 'Value': 9}\n\n\n\ncombined_aug_val.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\nV_spread\n\n\n\n\n2025-05-31\n-2.880509\n1\n1.340116e-07\n1.000000\nTight\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n0.958129\n\n\n2025-06-30\n-2.709857\n1\n2.610193e-07\n1.000000\nTight\n0.0486\n-0.0002\n-0.0161\n-0.0320\n0.0144\n0.0034\n1.070147\n\n\n2025-07-31\n-2.689149\n1\n2.839775e-07\n1.000000\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n1.218053\n\n\n2025-08-31\n-2.606319\n1\n4.195012e-07\n1.000000\nTight\n0.0185\n0.0488\n0.0442\n-0.0067\n0.0208\n0.0038\n1.259380\n\n\n2025-09-30\n-2.418377\n1\n4.483466e-05\n0.999955\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n1.338771\n\n\n\n\n\n\n\n\ndef test_liquidity_predicts_valuation(combined_aug):\n    \"\"\"\n    Test if L(t) predicts V_spread(t+k) using Granger causality.\n    \"\"\"\n    import pandas as pd\n    from scipy import stats\n    \n    print(\"=\"*80)\n    print(\"TEST 1: Does Liquidity Predict Future Valuation?\")\n    print(\"=\"*80)\n    \n    df = combined_aug.copy()\n    \n    # Test different lags\n    for lag in [3, 6, 12, 24]:\n        df[f'V_lead_{lag}m'] = df['V_spread'].shift(-lag)\n        \n        # Clean data\n        valid = df[['L', f'V_lead_{lag}m']].dropna()\n        \n        if len(valid) &lt; 30:\n            continue\n        \n        # Regression: V(t+k) = α + β*L(t) + ε\n        from scipy.stats import pearsonr, linregress\n        \n        slope, intercept, r_value, p_value, std_err = linregress(\n            valid['L'], \n            valid[f'V_lead_{lag}m']\n        )\n        \n        r_squared = r_value ** 2\n        \n        print(f\"\\nLag = {lag} months:\")\n        print(f\"  L(t) → V(t+{lag})\")\n        print(f\"  Coefficient: {slope:+.3f}\")\n        print(f\"  R²: {r_squared:.3f}\")\n        print(f\"  p-value: {p_value:.4f}\")\n        \n        if p_value &lt; 0.05:\n            print(f\"  ✅ Liquidity DOES predict valuation {lag} months ahead!\")\n        else:\n            print(f\"  ❌ No significant relationship\")\n\ntest_liquidity_predicts_valuation(combined_aug_val)\n\n================================================================================\nTEST 1: Does Liquidity Predict Future Valuation?\n================================================================================\n\nLag = 3 months:\n  L(t) → V(t+3)\n  Coefficient: -0.088\n  R²: 0.070\n  p-value: 0.0000\n  ✅ Liquidity DOES predict valuation 3 months ahead!\n\nLag = 6 months:\n  L(t) → V(t+6)\n  Coefficient: -0.089\n  R²: 0.071\n  p-value: 0.0000\n  ✅ Liquidity DOES predict valuation 6 months ahead!\n\nLag = 12 months:\n  L(t) → V(t+12)\n  Coefficient: -0.070\n  R²: 0.040\n  p-value: 0.0012\n  ✅ Liquidity DOES predict valuation 12 months ahead!\n\nLag = 24 months:\n  L(t) → V(t+24)\n  Coefficient: +0.032\n  R²: 0.006\n  p-value: 0.2076\n  ❌ No significant relationship\n\n\n\ndef granger_liquidity_to_valuation(df, max_lag=24):\n    \"\"\"\n    Proper Granger causality test:\n    Does L Granger-cause V_spread?\n    \"\"\"\n\n    import pandas as pd\n    from statsmodels.tsa.stattools import grangercausalitytests\n\n    print(\"=\"*90)\n    print(\"GRANGER CAUSALITY TEST: L → V_spread\")\n    print(\"=\"*90)\n\n    # Keep only required columns\n    data = df[['V_spread', 'L']].dropna()\n\n    # statsmodels expects: [target, predictor]\n    # Test whether L causes V_spread\n    results = grangercausalitytests(\n        data,\n        maxlag=max_lag,\n        verbose=False\n    )\n\n    print(\"\\nLag | F-stat p-value | χ² p-value | Verdict\")\n    print(\"-\"*70)\n\n    for lag in range(1, max_lag + 1):\n        f_pval = results[lag][0]['ssr_ftest'][1]\n        chi2_pval = results[lag][0]['ssr_chi2test'][1]\n\n        verdict = \"✅ YES\" if f_pval &lt; 0.05 else \"❌ NO\"\n\n        print(f\"{lag:&gt;3} | {f_pval:&gt;13.4f} | {chi2_pval:&gt;11.4f} | {verdict}\")\n\n\ngranger_liquidity_to_valuation(combined_aug_val, max_lag=24)\n\n==========================================================================================\nGRANGER CAUSALITY TEST: L → V_spread\n==========================================================================================\n\nLag | F-stat p-value | χ² p-value | Verdict\n----------------------------------------------------------------------\n  1 |        0.0899 |      0.0870 | ❌ NO\n  2 |        0.3760 |      0.3678 | ❌ NO\n  3 |        0.4680 |      0.4547 | ❌ NO\n  4 |        0.5816 |      0.5640 | ❌ NO\n  5 |        0.5879 |      0.5636 | ❌ NO\n  6 |        0.5844 |      0.5522 | ❌ NO\n  7 |        0.7117 |      0.6789 | ❌ NO\n  8 |        0.6999 |      0.6582 | ❌ NO\n  9 |        0.6900 |      0.6385 | ❌ NO\n 10 |        0.8036 |      0.7585 | ❌ NO\n 11 |        0.8664 |      0.8266 | ❌ NO\n 12 |        0.8785 |      0.8352 | ❌ NO\n 13 |        0.8521 |      0.7936 | ❌ NO\n 14 |        0.8598 |      0.7951 | ❌ NO\n 15 |        0.8944 |      0.8348 | ❌ NO\n 16 |        0.9224 |      0.8691 | ❌ NO\n 17 |        0.8087 |      0.6924 | ❌ NO\n 18 |        0.8156 |      0.6886 | ❌ NO\n 19 |        0.8553 |      0.7343 | ❌ NO\n 20 |        0.8645 |      0.7355 | ❌ NO\n 21 |        0.9428 |      0.8657 | ❌ NO\n 22 |        0.9325 |      0.8348 | ❌ NO\n 23 |        0.9342 |      0.8273 | ❌ NO\n 24 |        0.9138 |      0.7700 | ❌ NO\n\n\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/statsmodels/tsa/stattools.py:1545: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n\n\n\n\nRegime-conditional valuation spreads\n\n\nValuation Spreads & Factor Returns by Regime Math\n(Valuation spread series, e.g. top–bottom decile)\n$ V_t^{} $\n(Regime-conditional mean valuation spread)\n$ {V}^{(k)} = $\n$ ^{(k)} = {_{t=1}^T (_t = k)} $\n(Difference in spreads between regimes)\n$ ^{()} - ^{()} $\n(Factor return in regime ( k ))\n$ r_t^{(F)} $\n$ {r}_F^{(k)} = $\n$ F^{(k)} = {{t=1}^T (_t = k)} $\n(Regime-specific Sharpe ratio)\n$ _F^{(k)} = $\n\n# Regime-conditional mean valuation spread:  V̄^(k)\nval_means_by_regime = (\n    combined_aug_val\n    .groupby(\"state_label\")[\"V_spread\"]\n    .mean()\n)\n\nval_stds_by_regime = (\n    combined_aug_val\n    .groupby(\"state_label\")[\"V_spread\"]\n    .std()\n)\n\nprint(\"Regime-conditional mean valuation spread:\")\nprint(val_means_by_regime)\n\nprint(\"\\nRegime-conditional valuation z-score std dev:\")\nprint(val_stds_by_regime)\n\n# Difference in spreads between regimes:  V̂^(High) − V̂^(Tight)\nif {\"High\", \"Tight\"}.issubset(val_means_by_regime.index):\n    spread_diff = (\n        val_means_by_regime[\"High\"] - val_means_by_regime[\"Tight\"]\n    )\n    print(\"\\nDifference in spreads High − Tight:\")\n    print(spread_diff)\n\nRegime-conditional mean valuation spread:\nstate_label\nHigh    -0.049269\nTight    0.419617\nName: V_spread, dtype: float64\n\nRegime-conditional valuation z-score std dev:\nstate_label\nHigh     0.551632\nTight    0.438484\nName: V_spread, dtype: float64\n\nDifference in spreads High − Tight:\n-0.4688859970837499\n\n\n\n\nInsigts\n\nTight liquidity = cheaper markets (value regime),\nHigh liquidity = expensive markets (growth regime).\n\n\nMarkets are ~0.43σ more expensive in High liquidity regimes than Tight liquidity regimes.\n\n\n\nPlot L(t), S&P 500, and V_spread_t together with regime shading,\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# -------------------------------------------------------------------\n# Assume you already have:\n#   L_t              : pd.Series (liquidity index), monthly, name \"L\"\n#   sp500_cape_m     : DataFrame with column \"sp500\" (S&P 500)\n#   V_spread_t       : pd.Series (\"V_spread\"), monthly valuation cheapness\n#   regimes_df       : DataFrame with column \"state_label\"\n# -------------------------------------------------------------------\n\n# 1) Build a common monthly dataframe\ndf_plot = pd.DataFrame(index=L_t.index.union(sp500_cape_m.index).union(V_spread_t.index))\n\ndf_plot[\"L\"]         = L_t.reindex(df_plot.index)\ndf_plot[\"spx_price\"] = sp500_cape_m[\"sp500\"].reindex(df_plot.index)\ndf_plot[\"V_spread\"]  = V_spread_t.reindex(df_plot.index)\n\n# Attach regimes (forward-fill in case of any slight index mismatch)\nreg_states = combined_aug_val[\"state_label\"].reindex(df_plot.index).ffill()\ndf_plot[\"state_label\"] = reg_states\n\n# Optional: drop rows before we have all three series\ndf_plot = df_plot.dropna(subset=[\"L\", \"spx_price\", \"V_spread\", \"state_label\"])\n\n# 2) Helper: find contiguous regime segments for shading\ndef get_regime_segments(states: pd.Series):\n    \"\"\"\n    Given a Series of regime labels indexed by date,\n    return list of (start_date, end_date, label) segments\n    where the label is constant.\n    \"\"\"\n    segments = []\n    prev_label = None\n    start_date = None\n\n    for dt, label in states.items():\n        if prev_label is None:\n            prev_label = label\n            start_date = dt\n            continue\n\n        if label != prev_label:\n            # segment ended at previous date\n            end_date = dt\n            segments.append((start_date, end_date, prev_label))\n            start_date = dt\n            prev_label = label\n\n    # last segment\n    if prev_label is not None and start_date is not None:\n        segments.append((start_date, states.index[-1], prev_label))\n\n    return segments\n\nsegments = get_regime_segments(df_plot[\"state_label\"])\n\n# 3) Colors for regimes\nregime_colors = {\n    \"High\":    \"#ffe0e0\",  # pale red\n    \"Neutral\": \"#f7f7f7\",  # light gray\n    \"Tight\":   \"#d1ffd1\",  # pale green\n}\n\n# 4) Plot: 3 stacked subplots with shared x-axis\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True,\n                         gridspec_kw={\"height_ratios\": [1, 1, 1]})\n\nax_L, ax_spx, ax_val = axes\n\n# --- Shading first so lines render on top ---\nfor (start, end, label) in segments:\n    color = regime_colors.get(label, \"#f0f0f0\")\n    for ax in axes:\n        ax.axvspan(start, end, color=color, alpha=0.3)\n\n# --- Panel 1: Liquidity index L(t) ---\nax_L.plot(df_plot.index, df_plot[\"L\"], label=\"L(t)\", linewidth=1.5)\nax_L.set_ylabel(\"L(t)\")\nax_L.set_title(\"Liquidity Index, S&P 500, and Valuation Spread by Liquidity Regime\")\nax_L.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 2: S&P 500 level ---\nax_spx.plot(df_plot.index, df_plot[\"spx_price\"], label=\"S&P 500\", linewidth=1.5)\nax_spx.set_ylabel(\"S&P 500\")\nax_spx.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 3: Valuation cheapness (V_spread) ---\nax_val.plot(df_plot.index, df_plot[\"V_spread\"], label=\"V_spread (cheap vs baseline)\", linewidth=1.5)\nax_val.axhline(0.0, color=\"black\", linewidth=1, linestyle=\"--\", alpha=0.7)\nax_val.set_ylabel(\"V_spread\")\nax_val.set_xlabel(\"Date\")\nax_val.grid(True, linestyle=\"--\", alpha=0.4)\n\n# Optional legends\nax_L.legend(loc=\"upper left\")\nax_spx.legend(loc=\"upper left\")\nax_val.legend(loc=\"upper left\")\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/mosaic-20-cancer-analysis.html",
    "href": "notebooks/mosaic-20-cancer-analysis.html",
    "title": "Mosaic-20 Cancer Analysis",
    "section": "",
    "text": "image.png"
  },
  {
    "objectID": "notebooks/mosaic-20-cancer-analysis.html#objective",
    "href": "notebooks/mosaic-20-cancer-analysis.html#objective",
    "title": "Mosaic-20 Cancer Analysis",
    "section": "Objective",
    "text": "Objective\nAnalyze potential carcinogenic effects of COVID-19 vaccines through: 1. CDC WONDER mortality data analysis (primary) 2. In-silico whole-body modeling (secondary)"
  },
  {
    "objectID": "notebooks/mosaic-20-cancer-analysis.html#key-hypotheses",
    "href": "notebooks/mosaic-20-cancer-analysis.html#key-hypotheses",
    "title": "Mosaic-20 Cancer Analysis",
    "section": "Key Hypotheses",
    "text": "Key Hypotheses\nTesting 20 mechanisms of potential vaccine-induced carcinogenesis: 1. Increased cancer risk (7 major types) 2. Gene disruption 3. Genome integration 4. Genome instability 5. Tumor immune escape 6. DNA repair suppression 7. Chronic inflammation 8. Immune dysregulation 9. microRNA disruption 10. Oncogenic signaling (MAPK, PI3K/AKT/mTOR) 11. Tumor microenvironment remodeling 12. Dormant cancer reactivation 13. TLR inhibition 14. Frameshift protein errors 15. Immune exhaustion 16. IgG4 class switching 17. SV40 promoter effects 18. RAS signaling dysregulation 19. Microbiome disruption 20. Treatment resistance"
  },
  {
    "objectID": "notebooks/mosaic-20-cancer-analysis.html#primary-finding",
    "href": "notebooks/mosaic-20-cancer-analysis.html#primary-finding",
    "title": "Mosaic-20 Cancer Analysis",
    "section": "Primary Finding",
    "text": "Primary Finding\n\nPhase 1:\nThe Bayesian Structural Time Series (CausalImpact) analysis estimates 1,314,451 cumulative excess cancer deaths in the post-intervention period (April 2021 – Jan 2026), representing a +79.79% relative effect over the counterfactual.\n\n95% Credible Interval: [1,223,525 – 1,405,227]\nAverage weekly excess: 5,216.1 deaths/week\nPosterior P(Effect &gt; 0): 1.000\nPre-intervention training period: 379 weeks (2014 – April 2021)\nPost-intervention prediction period: 252 weeks\nIntervention date: April 10, 2021"
  },
  {
    "objectID": "notebooks/mosaic-20-cancer-analysis.html#analysis-approach",
    "href": "notebooks/mosaic-20-cancer-analysis.html#analysis-approach",
    "title": "Mosaic-20 Cancer Analysis",
    "section": "Analysis Approach",
    "text": "Analysis Approach\n\nPhase 1: CDC WONDER Data Analysis (Weeks 1-6)\n\nWeek 1: Replicate mortality chart\nWeek 2: Test confounders\nWeek 3: Dose-response by state\nWeek 4: Cancer type specificity\nWeek 5: Geographic variation\nWeek 6: Age stratification\n\n\n\nPhase 2: In-Silico Modeling (2-3 months)\nTools: PK-Sim, PhysiCell, COPASI, VCell - Implement 20 mechanisms - Multi-scale integration - Validation against known data"
  },
  {
    "objectID": "notebooks/mosaic-20-cancer-analysis.html#data-sources",
    "href": "notebooks/mosaic-20-cancer-analysis.html#data-sources",
    "title": "Mosaic-20 Cancer Analysis",
    "section": "Data Sources",
    "text": "Data Sources\n\nCDC WONDER (free, public): https://wonder.cdc.gov\nInsurance claims (if budget allows)\nIn-silico platforms (all open-source)"
  },
  {
    "objectID": "notebooks/mosaic-20-cancer-analysis.html#budget-constraints",
    "href": "notebooks/mosaic-20-cancer-analysis.html#budget-constraints",
    "title": "Mosaic-20 Cancer Analysis",
    "section": "Budget Constraints",
    "text": "Budget Constraints\n\nPrefer free/open-source tools\nAvoid government data sources if compromised\nNo IRB requirements (public data + computational)"
  },
  {
    "objectID": "notebooks/mosaic-20-cancer-analysis.html#next-immediate-steps",
    "href": "notebooks/mosaic-20-cancer-analysis.html#next-immediate-steps",
    "title": "Mosaic-20 Cancer Analysis",
    "section": "Next Immediate Steps",
    "text": "Next Immediate Steps\n\nDownload CDC WONDER data (C00-C97, ages 0-54, weekly 2018-2025)\nReplicate chart in Python/R\nBegin systematic confounder analysis"
  },
  {
    "objectID": "notebooks/mosaic-20-cancer-analysis.html#cdc-wonder-data-analysis",
    "href": "notebooks/mosaic-20-cancer-analysis.html#cdc-wonder-data-analysis",
    "title": "Mosaic-20 Cancer Analysis",
    "section": "CDC WONDER Data Analysis",
    "text": "CDC WONDER Data Analysis"
  }
]