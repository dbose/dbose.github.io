[
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "",
    "text": "import yfinance as yf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Calculate annualized return\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\n# Calculate volatility (annualized standard deviation)\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\n# Calculate Sharpe Ratio\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(periods_per_year)\n\n# Calculate Sortino Ratio\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / downside_returns.std() * np.sqrt(periods_per_year)\n\n# Maximum drawdown\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#the-philosophy-investing-in-enablers-not-just-innovators",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#the-philosophy-investing-in-enablers-not-just-innovators",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "The Philosophy: Investing in Enablers, Not Just Innovators",
    "text": "The Philosophy: Investing in Enablers, Not Just Innovators\nThe concept of a K-wave, named after economist Nikolai Kondratiev, describes long-term economic cycles driven by technological innovation. Each wave—spanning roughly 40-60 years—ushers in transformative advancements, from the Industrial Revolution to the Information Age. We’re now entering what many believe is the sixth K-wave, propelled by AI, clean energy, and advanced manufacturing. While companies at the forefront of these technologies (e.g., pure-play AI startups or speculative renewable energy firms) often dominate headlines, their volatility can make them risky bets for long-term investors\nDuring the 1848 California Gold Rush, it wasn’t the gold miners who reaped the most consistent rewards—it was the “shovelmakers,” the suppliers of tools and infrastructure, who built lasting wealth by enabling the frenzy. Today, as we stand on the cusp of a new technological era defined by artificial intelligence, renewable energy, and advanced manufacturing, a similar strategy can guide us toward sustainable growth.\n\n\n\nGold Rush\n\n\nInstead, our mini-fund targets companies that enable these breakthroughs—those building the “picks and shovels” of the modern era. ASML, for instance, powers the semiconductor industry with its photolithography machines, a cornerstone of AI and computing advancements. Tesla, beyond its electric vehicles, drives innovation in energy storage and autonomous driving infrastructure. Microsoft provides cloud computing and AI platforms that underpin countless applications, while Berkshire Hathaway offers stability and diversified exposure to industrial and financial sectors. AbbVie contributes healthcare innovation, a critical pillar of societal progress, and Lockheed Martin strengthens the portfolio with its leadership in aerospace and defense—sectors poised for growth amid geopolitical shifts and technological integration. This blend of high-growth and value stocks creates a portfolio that captures multiple growth trends while maintaining a solid foundation. By avoiding overexposure to speculative “gold miners,” we aim to deliver consistent returns with reduced downside risk—a strategy validated by our backtest results."
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#initial-portfolio-composition",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#initial-portfolio-composition",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Initial Portfolio Composition",
    "text": "Initial Portfolio Composition\nThe original portfolio comprises five stocks — ASML Holding N.V. (ASML), SolarEdge Technologies, Inc. (SEDG), Rockwell Automation, Inc. (ROK), Illumina, Inc. (ILMN), and Lockheed Martin Corporation (LMT) — each allocated an equal weight of 20%.\nThis portfolio was backtested using historical closing prices from January 1, 2000, to January 1, 2024, sourced via Yahoo Finance (yf.download). Daily returns were calculated with pct_change() and cleaned with dropna() to ensure data integrity. The portfolio is benchmarked against the S&P 500 (^GSPC) and Nasdaq-100 (^NDX), reflecting a strategy to measure performance against broad market and technology-focused indices.\nThis portfolio embodies the “shovelmakers of tomorrow” philosophy outlined earlier. Rather than chasing speculative leaders in emerging technologies, it targets companies that provide critical infrastructure, tools, and services enabling the next Kondratiev wave (K-wave) — such as renewable energy, automation, genomics, and aerospace.\nThe equal-weight allocation ensures diversification across the following sectors, balancing growth potential with stability:\n\nSemiconductors\nSolar Energy\nIndustrial Automation\nGenomics\nDefense\n\n\n\nCompanies in the Portfolio\n\n\n\nASML Holding N.V. (ASML)\nSector: Technology (Semiconductors)\nWeight: 20%\nRole:\nASML is the global leader in photolithography systems, essential for manufacturing integrated circuits (microchips). Its extreme ultraviolet (EUV) lithography machines are critical for producing advanced chips used in AI, 5G, and high-performance computing.\nK-Wave Relevance:\nASML is a quintessential shovelmaker, supplying the tools that power the semiconductor industry—a backbone of the sixth K-wave. As demand for AI and IoT grows, ASML’s equipment enables chipmakers like TSMC and Intel to push technological boundaries.\nPortfolio Fit:\nASML contributes high-growth potential, capitalizing on secular trends in technology, while its dominant market position adds resilience.\n\n\n\nSolarEdge Technologies, Inc. (SEDG)\nSector: Renewable Energy (Solar)\nWeight: 20%\nRole:\nSolarEdge specializes in power optimizers, inverters, and monitoring systems for solar photovoltaic (PV) installations. Its solutions maximize energy efficiency and reliability for residential, commercial, and utility-scale solar projects.\nK-Wave Relevance:\nThe transition to clean energy is a defining feature of the next K-wave, with solar power at the forefront. SolarEdge’s technologies enhance the scalability and affordability of solar energy, positioning it as an enabler of the renewable revolution. Unlike solar panel manufacturers, SolarEdge focuses on the tools that optimize energy output, aligning with the shovelmaker strategy.\nPortfolio Fit:\nSEDG introduces exposure to the fast-growing renewable energy sector, offering growth potential tempered by the volatility inherent in clean energy markets.\n\n\n\nRockwell Automation, Inc. (ROK)\nSector: Industrials (Automation)\nWeight: 20%\nRole:\nRockwell Automation provides industrial automation and digital transformation solutions, including programmable logic controllers (PLCs), sensors, and software for smart manufacturing. It serves industries like automotive, food and beverage, and pharmaceuticals.\nK-Wave Relevance:\nAutomation is a cornerstone of the sixth K-wave, driving efficiency in manufacturing and supply chains. Rockwell’s technologies enable “Industry 4.0”—the integration of IoT, AI, and robotics into production—making it a key supplier of tools for industrial innovation. Its focus on software and analytics further aligns with digital transformation trends.\nPortfolio Fit:\nROK adds a value-oriented component, balancing growth with stability. Its diversified client base mitigates sector-specific risks, enhancing portfolio resilience.\n\n\n\nIllumina, Inc. (ILMN)\nSector: Healthcare (Genomics)\nWeight: 20%\nRole:\nIllumina is a leader in DNA sequencing and genomics, providing instruments, reagents, and software for genetic analysis. Its technologies support applications in personalized medicine, cancer research, and agriculture.\nK-Wave Relevance:\nGenomics is poised to transform healthcare, a critical pillar of societal progress in the next K-wave. Illumina’s sequencing platforms are the shovels of this revolution, enabling researchers and clinicians to decode genetic data at scale. Its dominance in sequencing technology ensures long-term growth potential.\nPortfolio Fit:\nILMN brings exposure to healthcare innovation, a sector with defensive qualities and high growth. It diversifies the portfolio away from pure technology, reducing correlation with market cycles.\n\n\n\nLockheed Martin Corporation (LMT)\nSector: Aerospace and Defense\nWeight: 20%\nRole:\nLockheed Martin is a global leader in aerospace, defense, and security, known for products like the F-35 fighter jet, missile systems, and space technologies. It serves government and commercial clients worldwide.\nK-Wave Relevance:\nDefense and aerospace are integral to the next K-wave, driven by geopolitical dynamics and technological advancements (e.g., hypersonics, space exploration). Lockheed Martin’s role as a systems integrator and innovator positions it as an enabler of national security and space infrastructure—a stable shovelmaker in a high-stakes field.\nPortfolio Fit:\nLMT anchors the portfolio with defensive characteristics, offering steady cash flows and dividends. Its low correlation with tech sectors enhances diversification, mitigating downside risk during market downturns. ownside risk during market downturns.\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'SEDG', 'ROK', 'ILMN', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.2]) \n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Mini-Fund Portfolio vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\npd.set_option('display.width', 1000)\nprint(metrics_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.194025    0.269933      0.718391       0.997384     -0.411544\nS&P 500              0.100897    0.184097      0.506164       0.610701     -0.339250\nNasdaq-100           0.168178    0.225889      0.713269       0.905724     -0.355631"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#beginning",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#beginning",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Beginning",
    "text": "Beginning\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\n# stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT']\n#stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n# weights = np.array([0.1639441, 0.2852132, 0.3240187, 0.2268240])\n# weights = np.array([0.14, 0.25, 0.29, 0.20, 0.12]) \n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Portfolio-GV', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(metrics_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.367925    0.272138      1.214923       1.683422     -0.421506\nS&P 500              0.119445    0.174019      0.621038       0.762031     -0.339250\nNasdaq-100           0.181999    0.207497      0.813858       1.043914     -0.355631\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\n\n# Define the stocks and benchmark indices\n# stocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT']\n# stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.2, 0.2, 0.2, 0.2, 0.2,  0.2])\n# weights = np.array([0.1639441, 0.2852132, 0.3240187, 0.2268240])\n# weights = np.array([0.14, 0.25, 0.29, 0.20, 0.12]) \n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O1 vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(metrics_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.325947    0.240479      1.211467       1.630888     -0.371925\nS&P 500              0.113679    0.172243      0.595699       0.721428     -0.339250\nNasdaq-100           0.179487    0.210365      0.795558       1.003233     -0.355631\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\n\n# Define the stocks and benchmark indices\n#stocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\n#stocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT']\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns = returns.dropna()\n\n# Equal weights for portfolio\n# weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n# weights = np.array([0.1639441, 0.2852132, 0.3240187, 0.2268240])\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns = (1 + portfolio_returns).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n\n# Calculate metrics for mini-fund portfolio\nportfolio_metrics = {\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns)\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC'])\n}\n\n# Calculate metrics for Nasdaq-100\nndx_metrics = {\n    'Annualized Return': annualized_return(returns['^NDX']),\n    'Volatility': annualized_volatility(returns['^NDX']),\n    'Sharpe Ratio': sharpe_ratio(returns['^NDX']),\n    'Sortino Ratio': sortino_ratio(returns['^NDX']),\n    'Max Drawdown': max_drawdown(returns['^NDX'])\n}\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns / cumulative_portfolio_returns.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns / cumulative_ndx_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O2 vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics, ndx_metrics], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(metrics_df)\n\n[*********************100%***********************]  8 of 8 completed\n\n\n\n\n\n\n\n\n\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund            0.308158    0.218942      1.246309       1.642653     -0.350050\nS&P 500              0.113679    0.172243      0.595699       0.721428     -0.339250\nNasdaq-100           0.179487    0.210365      0.795558       1.003233     -0.355631\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Plot the density plots for Portfolio, S&P 500, and Nasdaq-100 returns\nplt.figure(figsize=(15, 8))\n\n# Full Density Plot\n\nsns.kdeplot(portfolio_returns, label='Portfolio-GV-O2', color='blue', shade=True, clip=(-0.06, 0.06))\nsns.kdeplot(returns['^GSPC'], label='S&P 500', color='orange', shade=True, clip=(-0.06, 0.06))\nsns.kdeplot(returns['^NDX'], label='Nasdaq-100', color='green', shade=True, clip=(-0.06, 0.06))\n\nplt.title('Zoomed-In Density Plot: Left and Right Tails', fontsize=16)\nplt.xlabel('Daily Return', fontsize=12)\nplt.ylabel('Density', fontsize=12)\nplt.legend(loc='upper right', fontsize=10)\n\n\n# Adjust layout and show plot\nplt.tight_layout()\nplt.grid(True)\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\n&lt;ipython-input-23-b30301a8e790&gt;:28: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(portfolio_returns, label='Mini-Fund Portfolio', color='blue', shade=True, clip=(-0.06, 0.06))\n&lt;ipython-input-23-b30301a8e790&gt;:29: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(returns['^GSPC'], label='S&P 500', color='orange', shade=True, clip=(-0.06, 0.06))\n&lt;ipython-input-23-b30301a8e790&gt;:30: FutureWarning: \n\n`shade` is now deprecated in favor of `fill`; setting `fill=True`.\nThis will become an error in seaborn v0.14.0; please update your code.\n\n  sns.kdeplot(returns['^NDX'], label='Nasdaq-100', color='green', shade=True, clip=(-0.06, 0.06))"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#perf-across-vol-regimes",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#perf-across-vol-regimes",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Perf across vol regimes",
    "text": "Perf across vol regimes\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom hmmlearn.hmm import GaussianHMM\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Fit a 3-state HMM to detect volatility regimes\nmodel = GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=1000)\nmodel.fit(portfolio_returns.values.reshape(-1, 1))\n\n# Predict regimes\nhidden_states = model.predict(portfolio_returns.values.reshape(-1, 1))\n\n# Add hidden state labels to returns DataFrame\nreturns_df = pd.DataFrame({\n    'Portfolio Returns': portfolio_returns,\n    '^GSPC Returns': returns['^GSPC'],\n    '^NDX Returns': returns['^NDX'],\n    'Hidden State': hidden_states\n})\n\n# Calculate average volatility of each regime to determine regime labels\naverage_volatility = returns_df.groupby('Hidden State')['Portfolio Returns'].std()\n\n# Sort the volatilities to label regimes\nvolatility_rank = average_volatility.sort_values().index\nregime_labels = {volatility_rank[0]: 'Low Volatility',\n                 volatility_rank[1]: 'Mid Volatility',\n                 volatility_rank[2]: 'High Volatility'}\n\n# Add regime labels to DataFrame\nreturns_df['Volatility Regime'] = returns_df['Hidden State'].map(regime_labels)\n\n# Define metric functions\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(periods_per_year)\n\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return np.mean(excess_returns) / downside_returns.std() * np.sqrt(periods_per_year)\n\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()\n\n# Calculate metrics for each volatility regime for portfolio, S&P 500, and Nasdaq-100\nmetrics = {}\n\nfor regime in ['Low Volatility', 'Mid Volatility', 'High Volatility']:\n    state_data = returns_df[returns_df['Volatility Regime'] == regime]\n    state_metrics = {}\n    \n    for col in ['Portfolio Returns', '^GSPC Returns', '^NDX Returns']:\n        state_metrics[col] = {\n            'Annualized Return': annualized_return(state_data[col]),\n            'Volatility': annualized_volatility(state_data[col]),\n            'Sharpe Ratio': sharpe_ratio(state_data[col]),\n            'Sortino Ratio': sortino_ratio(state_data[col]),\n            'Max Drawdown': max_drawdown(state_data[col])\n        }\n    \n    metrics[regime] = state_metrics\n\n# Convert metrics to a DataFrame for better visualization\nmetrics_dict = {}\nfor regime, data in metrics.items():\n    for asset, values in data.items():\n        row_key = f\"{regime} - {asset.replace('Portfolio Returns', 'Mini-Fund').replace('^GSPC Returns', 'S&P 500').replace('^NDX Returns', 'Nasdaq-100')}\"\n        metrics_dict[row_key] = values\n\nmetrics_df = pd.DataFrame(metrics_dict).T\nmetrics_df = metrics_df.rename_axis('Regime and Asset').reset_index()\n\n# Group the table display by Low/Mid/High Volatility Regimes to enhance readability\ngrouped_metrics = metrics_df.copy()\n\n# Formatting the DataFrame to show Regime and Metrics without repetition of the regime\ngrouped_metrics['Volatility Regime'] = grouped_metrics['Regime and Asset'].str.extract(r'^(Low|Mid|High) Volatility')\ngrouped_metrics['Asset'] = grouped_metrics['Regime and Asset'].str.replace(r'^(Low|Mid|High) Volatility - ', '')\ngrouped_metrics.drop(columns=['Regime and Asset'], inplace=True)\n\n# Reorder columns for better readability\ngrouped_metrics = grouped_metrics[['Volatility Regime', 'Asset', 'Annualized Return', 'Volatility', 'Sharpe Ratio', 'Sortino Ratio', 'Max Drawdown']]\n\n# Sorting the DataFrame by 'Volatility Regime' to ensure grouping\ngrouped_metrics = grouped_metrics.sort_values(by=['Volatility Regime', 'Asset']).reset_index(drop=True)\n\n# Display in a more readable format\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', '{:.4f}'.format)\npd.set_option('display.width', 1000)\nprint(\"\\nPerformance Metrics Grouped by Volatility Regime:\")\nprint(grouped_metrics)\n\n# Plotting cumulative returns for each regime\nplt.figure(figsize=(15, 8))\n\nfor regime in ['Low Volatility', 'Mid Volatility', 'High Volatility']:\n    state_data = returns_df[returns_df['Volatility Regime'] == regime]\n    cumulative_portfolio_returns = (1 + state_data['Portfolio Returns']).cumprod()\n    cumulative_sp500_returns = (1 + state_data['^GSPC Returns']).cumprod()\n    cumulative_ndx_returns = (1 + state_data['^NDX Returns']).cumprod()\n\n    plt.plot(cumulative_portfolio_returns, label=f'{regime} - Mini-Fund Cumulative Returns', linestyle='-', color=f'C{list(regime_labels.values()).index(regime)}')\n    plt.plot(cumulative_sp500_returns, label=f'{regime} - S&P 500 Cumulative Returns', linestyle='--', color=f'C{list(regime_labels.values()).index(regime)}')\n    plt.plot(cumulative_ndx_returns, label=f'{regime} - Nasdaq-100 Cumulative Returns', linestyle=':', color=f'C{list(regime_labels.values()).index(regime)}')\n\n# Add labels and title\nplt.title('Cumulative Returns of Portfolio-GV-O2, S&P 500, and Nasdaq-100 Across Volatility Regimes', fontsize=16)\nplt.xlabel('Date', fontsize=12)\nplt.ylabel('Cumulative Growth', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\nModel is not converging.  Current: 8306.194727967259 is not greater than 8306.204205929746. Delta is -0.009477962486926117\n\n\n\nPerformance Metrics Grouped by Volatility Regime:\n  Volatility Regime                         Asset  Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\n0              High   High Volatility - Mini-Fund            -0.8314      0.8617       -1.6850        -2.4929       -0.3083\n1              High  High Volatility - Nasdaq-100            -0.7188      0.8288       -1.1669        -2.0536       -0.2297\n2              High     High Volatility - S&P 500            -0.8402      0.8390       -1.8236        -3.2338       -0.3064\n3               Low    Low Volatility - Mini-Fund             0.3906      0.1553        2.0735         3.2254       -0.1154\n4               Low   Low Volatility - Nasdaq-100             0.3058      0.1421        1.8093         2.5148       -0.1106\n5               Low      Low Volatility - S&P 500             0.2072      0.1101        1.5845         2.1873       -0.1028\n6               Mid    Mid Volatility - Mini-Fund             0.1974      0.2961        0.6896         1.0894       -0.3166\n7               Mid   Mid Volatility - Nasdaq-100            -0.0683      0.2950       -0.1602        -0.2455       -0.4023\n8               Mid      Mid Volatility - S&P 500            -0.0423      0.2274       -0.1644        -0.2437       -0.3408\n\n\n\n\n\n\n\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom hmmlearn.hmm import GaussianHMM\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Fit a 3-state HMM to detect volatility regimes\nmodel = GaussianHMM(n_components=3, covariance_type=\"full\", n_iter=1000)\nmodel.fit(portfolio_returns.values.reshape(-1, 1))\n\n# Predict regimes\nhidden_states = model.predict(portfolio_returns.values.reshape(-1, 1))\n\n# Add hidden state labels to returns DataFrame\nreturns_df = pd.DataFrame({\n    'Date': returns.index,\n    'Portfolio Returns': portfolio_returns,\n    'S&P 500 Returns': returns['^GSPC'],\n    'Nasdaq-100 Returns': returns['^NDX'],\n    'Hidden State': hidden_states\n})\n\n# Calculate average volatility of each regime to determine regime labels\naverage_volatility = returns_df.groupby('Hidden State')['Portfolio Returns'].std()\n\n# Sort the volatilities to label regimes\nvolatility_rank = average_volatility.sort_values().index\nregime_labels = {volatility_rank[0]: 'Low Volatility',\n                 volatility_rank[1]: 'Mid Volatility',\n                 volatility_rank[2]: 'High Volatility'}\n\n# Add regime labels to DataFrame\nreturns_df['Volatility Regime'] = returns_df['Hidden State'].map(regime_labels)\n\n# Calculate cumulative returns for each asset and add to DataFrame\nreturns_df['Cumulative Portfolio Returns'] = (1 + returns_df['Portfolio Returns']).cumprod()\nreturns_df['Cumulative S&P 500 Returns'] = (1 + returns_df['S&P 500 Returns']).cumprod()\nreturns_df['Cumulative Nasdaq-100 Returns'] = (1 + returns_df['Nasdaq-100 Returns']).cumprod()\n\n# Melt the DataFrame for seaborn compatibility\ncumulative_returns_df = pd.melt(\n    returns_df,\n    id_vars=['Date', 'Volatility Regime'],\n    value_vars=['Cumulative Portfolio Returns', 'Cumulative S&P 500 Returns', 'Cumulative Nasdaq-100 Returns'],\n    var_name='Asset',\n    value_name='Cumulative Returns'\n)\n\n# Create a faceted plot using seaborn's FacetGrid\ng = sns.FacetGrid(cumulative_returns_df, col='Volatility Regime', hue='Asset', col_wrap=3, height=4, aspect=1.5)\ng.map(plt.plot, 'Date', 'Cumulative Returns').add_legend()\n\n# Adjust plot aesthetics\ng.set_axis_labels('Date', 'Cumulative Growth')\ng.set_titles(col_template='{col_name} Regime')\ng.fig.suptitle('Cumulative Returns Across Volatility Regimes for Portfolio, S&P 500, and Nasdaq-100', y=1.05)\n\nplt.tight_layout()\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\nModel is not converging.  Current: 8305.528897318622 is not greater than 8305.57159532981. Delta is -0.042698011187894735\n\n\n\n\n\n\n\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom tabulate import tabulate\n\nfrom scipy.stats import t\nfrom arch import arch_model  # To use GARCH for volatility forecasting\n\n# Define risk-free rate (approximation)\nrisk_free_rate = 0.02  # 2% annually\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']  # Example portfolio stocks\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Calculate benchmark returns\nmarket_returns = returns['^GSPC']  # S&P 500 as benchmark\n\n# Alpha, Beta, R-Square\ncov_matrix = np.cov(portfolio_returns, market_returns)\nbeta_portfolio = cov_matrix[0, 1] / cov_matrix[1, 1]\nalpha_portfolio = np.mean(portfolio_returns) - beta_portfolio * np.mean(market_returns)\nr_square = 1 - (np.var(portfolio_returns - beta_portfolio * market_returns) / np.var(portfolio_returns))\n\n# Value at Risk (VaR) and Expected Shortfall (ES)\nimport numpy as np\nfrom scipy.stats import norm\n\ndef var_es(returns, confidence_level=0.95):\n    \"\"\"\n    Calculate VaR and Expected Shortfall using the normal distribution.\n\n    Args:\n        returns (pd.Series): A pandas Series of historical returns.\n        confidence_level (float): Confidence level for VaR and ES calculation.\n\n    Returns:\n        Tuple: VaR and ES for 1-month, 6-month, and 1-year periods.\n    \"\"\"\n    # Calculate the mean return and standard deviation (daily)\n    mean_return_daily = returns.mean()\n    std_dev_daily = returns.std()\n\n    # z-score for the given confidence level (left tail)\n    z = norm.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + z * std_dev_period)\n\n        # ES Calculation\n        es_factor = norm.pdf(z) / (1 - confidence_level)\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n\ndef var_es_tf(returns, confidence_level=0.95, forecast_volatility=False):\n    \"\"\"\n    Calculate VaR and Expected Shortfall using Student's t-distribution, with optional GARCH volatility forecasting.\n\n    Args:\n        returns (pd.Series): A pandas Series of historical returns.\n        confidence_level (float): Confidence level for VaR and ES calculation.\n        forecast_volatility (bool): If True, use GARCH(1,1) to forecast volatility.\n\n    Returns:\n        Tuple: VaR and ES for 1-month, 6-month, and 1-year periods.\n    \"\"\"\n    # Calculate the mean return (daily)\n    mean_return_daily = returns.mean()\n\n    # Volatility calculation\n    if forecast_volatility:\n        # Fit GARCH(1,1) model to the return series\n        am = arch_model(returns * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        # Use last forecasted volatility (scaled back to decimal)\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        # Use historical standard deviation\n        std_dev_daily = returns.std()\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n\n\n# Get VaR and Expected Shortfall for the portfolio\n#var_1m, var_6m, var_1y, es_1m, es_6m, es_1y = var_es(portfolio_returns)\nvar_1m, var_6m, var_1y, es_1m, es_6m, es_1y = var_es_tf(portfolio_returns, forecast_volatility=True)\n\n# Calculate standard performance metrics (already in your code)\nportfolio_metrics = {\n    'Alpha': alpha_portfolio,\n    'Beta': beta_portfolio,\n    'R-Square': r_square,\n    'Annualized Return': annualized_return(portfolio_returns),\n    'Volatility': annualized_volatility(portfolio_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_returns),\n    'Max Drawdown': max_drawdown(portfolio_returns),\n    'VaR 1M (95%)': var_1m,\n    'VaR 6M (95%)': var_6m,\n    'VaR 1Y (95%)': var_1y,\n    'ES 1M (95%)': es_1m,\n    'ES 6M (95%)': es_6m,\n    'ES 1Y (95%)': es_1y\n}\n\n# Calculate VaR and ES for S&P 500 benchmark\n#var_1m_sp500, var_6m_sp500, var_1y_sp500, es_1m_sp500, es_6m_sp500, es_1y_sp500 = var_es(returns['^GSPC'])\nvar_1m_sp500, var_6m_sp500, var_1y_sp500, es_1m_sp500, es_6m_sp500, es_1y_sp500 = var_es_tf(returns['^GSPC'], forecast_volatility=True)\n\n# Calculate the same for S&P 500 benchmark\nsp500_metrics = {\n    'Beta': 1,\n    'R-Square': 1,\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC']),\n    'VaR 1M (95%)': var_1m_sp500,\n    'VaR 6M (95%)': var_6m_sp500,\n    'VaR 1Y (95%)': var_1y_sp500,\n    'ES 1M (95%)': es_1m_sp500,\n    'ES 6M (95%)': es_6m_sp500,\n    'ES 1Y (95%)': es_1y_sp500\n}\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics, sp500_metrics], index=['Portfolio-GV-O2', 'S&P 500'])\n\n# Adjust display settings to show all columns without wrapping\npd.set_option('display.max_columns', None)          # Show all columns\npd.set_option('display.expand_frame_repr', False)   # Prevent DataFrame from wrapping\npd.set_option('display.width', 1000)                # Set width to large enough value\npd.set_option('display.colheader_justify', 'center') # Center align the headers\npd.set_option('display.float_format', '{:.6f}'.format)  # Format float to 6 decimal places\n\nprint(metrics_df)\n\n[*********************100%***********************]  8 of 8 completed\n\n\n                  Alpha    Beta    R-Square  Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown  VaR 1M (95%)  VaR 6M (95%)  VaR 1Y (95%)  ES 1M (95%)  ES 6M (95%)  ES 1Y (95%)\nPortfolio-GV-O2 0.000635 1.083823  0.727009      0.308158        0.218942     1.246309      1.642654      -0.350050     -0.105416     -0.344853     -0.573462    -0.140598    -0.431032    -0.695336  \nS&P 500              NaN 1.000000  1.000000      0.113679        0.172243     0.595699      0.721428      -0.339250     -0.068273     -0.203504     -0.323703    -0.093486    -0.265263    -0.411043  \n\n\n\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, t\nfrom arch import arch_model\n\n# Define the stocks, benchmark indices, and TAIL ETF\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\nprotective_etf = ['TAIL']\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices + protective_etf, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Weights for portfolio without TAIL\nweights_without_tail = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns without TAIL\nportfolio_returns_without_tail = (returns[stocks] * weights_without_tail).sum(axis=1)\n\n# Weights for portfolio including TAIL\nweights_with_tail = np.array([0.13, 0.18, 0.22, 0.16, 0.10, 0.11, 0.10])  # Added 10% weight to TAIL\n\n# Calculate portfolio returns with TAIL\nportfolio_returns_with_tail = (returns[stocks + protective_etf] * weights_with_tail).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns_without_tail = (1 + portfolio_returns_without_tail).cumprod()\ncumulative_portfolio_returns_with_tail = (1 + portfolio_returns_with_tail).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n# Define metrics functions\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / returns.std() * np.sqrt(periods_per_year)\n\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / downside_returns.std() * np.sqrt(periods_per_year)\n\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()\n\ndef alpha_beta(returns, benchmark_returns):\n    cov_matrix = np.cov(returns, benchmark_returns)\n    beta = cov_matrix[0, 1] / cov_matrix[1, 1]\n    alpha = returns.mean() - beta * benchmark_returns.mean()\n    return alpha, beta\n\ndef var_es(returns, confidence_level=0.95, forecast_volatility=False):\n    # Calculate the mean return (daily)\n    mean_return_daily = returns.mean()\n\n    # Volatility calculation\n    if forecast_volatility:\n        # Fit GARCH(1,1) model to the return series\n        am = arch_model(returns * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        # Use last forecasted volatility (scaled back to decimal)\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        # Use historical standard deviation\n        std_dev_daily = returns.std()\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n# Calculate Alpha, Beta, VaR, and ES for portfolios and benchmarks\nmarket_returns = returns['^GSPC']\n\nalpha_portfolio_without_tail, beta_portfolio_without_tail = alpha_beta(portfolio_returns_without_tail, market_returns)\nalpha_portfolio_with_tail, beta_portfolio_with_tail = alpha_beta(portfolio_returns_with_tail, market_returns)\n\nvar_1m_without_tail, var_6m_without_tail, var_1y_without_tail, es_1m_without_tail, es_6m_without_tail, es_1y_without_tail = var_es(portfolio_returns_without_tail, forecast_volatility=True)\nvar_1m_with_tail, var_6m_with_tail, var_1y_with_tail, es_1m_with_tail, es_6m_with_tail, es_1y_with_tail = var_es(portfolio_returns_with_tail, forecast_volatility=True)\n\n# Calculate metrics for mini-fund portfolio without TAIL\nportfolio_metrics_without_tail = {\n    'Alpha': alpha_portfolio_without_tail,\n    'Beta': beta_portfolio_without_tail,\n    'Annualized Return': annualized_return(portfolio_returns_without_tail),\n    'Volatility': annualized_volatility(portfolio_returns_without_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_without_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_without_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_without_tail),\n    'VaR 1M (95%)': var_1m_without_tail,\n    'VaR 6M (95%)': var_6m_without_tail,\n    'VaR 1Y (95%)': var_1y_without_tail,\n    'ES 1M (95%)': es_1m_without_tail,\n    'ES 6M (95%)': es_6m_without_tail,\n    'ES 1Y (95%)': es_1y_without_tail\n}\n\n# Calculate metrics for mini-fund portfolio with TAIL\nportfolio_metrics_with_tail = {\n    'Alpha': alpha_portfolio_with_tail,\n    'Beta': beta_portfolio_with_tail,\n    'Annualized Return': annualized_return(portfolio_returns_with_tail),\n    'Volatility': annualized_volatility(portfolio_returns_with_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_with_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_with_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_with_tail),\n    'VaR 1M (95%)': var_1m_with_tail,\n    'VaR 6M (95%)': var_6m_with_tail,\n    'VaR 1Y (95%)': var_1y_with_tail,\n    'ES 1M (95%)': es_1m_with_tail,\n    'ES 6M (95%)': es_6m_with_tail,\n    'ES 1Y (95%)': es_1y_with_tail\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Beta': 1,\n    'R-Square': 1,\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC']),\n    'VaR 1M (95%)': var_es(returns['^GSPC'])[0],\n    'VaR 6M (95%)': var_es(returns['^GSPC'])[1],\n    'VaR 1Y (95%)': var_es(returns['^GSPC'])[2],\n    'ES 1M (95%)': var_es(returns['^GSPC'])[3],\n    'ES 6M (95%)': var_es(returns['^GSPC'])[4],\n    'ES 1Y (95%)': var_es(returns['^GSPC'])[5]\n}\n\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_without_tail_normalized = cumulative_portfolio_returns_without_tail / cumulative_portfolio_returns_without_tail.iloc[0] * 100\ncumulative_portfolio_returns_with_tail_normalized = cumulative_portfolio_returns_with_tail / cumulative_portfolio_returns_with_tail.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio without TAIL\nplt.plot(cumulative_portfolio_returns_without_tail_normalized, label='Portfolio-GV-O2 Portfolio without TAIL', color='red')\n\n# Plot the portfolio with TAIL\nplt.plot(cumulative_portfolio_returns_with_tail_normalized, label='Portfolio-GV-O2 Portfolio with TAIL', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O2 with and without TAIL vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics_without_tail, portfolio_metrics_with_tail, sp500_metrics, ndx_metrics], \n                          index=['Portfolio-GV-O2 without TAIL', 'Portfolio-GV-O2 with TAIL', 'S&P 500', 'Nasdaq-100'])\n\n# Adjust display settings to show all columns without wrapping\npd.set_option('display.max_columns', None)          # Show all columns\npd.set_option('display.expand_frame_repr', False)   # Prevent DataFrame from wrapping\npd.set_option('display.width', 1000)                # Set width to large enough value\npd.set_option('display.colheader_justify', 'center') # Center align the headers\npd.set_option('display.float_format', '{:.6f}'.format)  # Format float to 6 decimal places\n\nprint(metrics_df)\n\n[*********************100%***********************]  9 of 9 completed\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n/Users/dbose/anaconda3/envs/py-data/lib/python3.8/site-packages/arch/__future__/_utility.py:11: FutureWarning: \nThe default for reindex is True. After September 2021 this will change to\nFalse. Set reindex to True or False to silence this message. Alternatively,\nyou can use the import comment\n\nfrom arch.__future__ import reindexing\n\nto globally set reindex to True and silence this warning.\n\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n                               Alpha    Beta    Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown  VaR 1M (95%)  VaR 6M (95%)  VaR 1Y (95%)  ES 1M (95%)  ES 6M (95%)  ES 1Y (95%)  R-Square\nPortfolio-GV-O2 without TAIL 0.000622 1.086780      0.299328        0.243712     1.115379      1.433413      -0.350050     -0.106745     -0.347816     -0.577361    -0.142540    -0.435495    -0.701359         NaN\nPortfolio-GV-O2 with TAIL    0.000560 0.912985      0.262029        0.208124     1.127078      1.462300      -0.300293     -0.093390     -0.304079     -0.504595    -0.124734    -0.380856    -0.613174         NaN\nS&P 500                           NaN 1.000000      0.110526        0.196321     0.530843      0.634252      -0.339250     -0.124550     -0.341836     -0.519811    -0.174143    -0.463314    -0.691607    1.000000\nNasdaq-100                        NaN 1.000000      0.183523        0.241768     0.735701      0.939876      -0.355631     -0.157124     -0.443419     -0.685044    -0.218199    -0.593020    -0.896611    1.000000"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#performance-during-recessions",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#performance-during-recessions",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "Performance during recessions",
    "text": "Performance during recessions\n\nrecession_data = pd.read_csv('recession_indicator.csv', parse_dates=['DATE'])\nrecession_data[\"DATE\"]\n\n0      1854-12-01\n1      1855-01-01\n2      1855-02-01\n3      1855-03-01\n4      1855-04-01\n          ...    \n2033   2024-05-01\n2034   2024-06-01\n2035   2024-07-01\n2036   2024-08-01\n2037   2024-09-01\nName: DATE, Length: 2038, dtype: datetime64[ns]\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Define the stocks and benchmark indices\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Close']\n\n# Calculate daily returns\nreturns = data.pct_change()\nreturns.index = returns.index.tz_localize(None)\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n# Remove time zone information to ensure compatibility\nportfolio_returns.index = portfolio_returns.index.tz_localize(None)\n\n# Load recession periods from uploaded CSV file\nrecession_data = pd.read_csv('recession_indicator.csv', parse_dates=['DATE'])\nrecession_data.set_index('DATE', inplace=True)\nrecession_data.index = recession_data.index.tz_localize(None)\n\nrecession_daily = recession_data.reindex(pd.date_range(start=returns.index.min(), \n                                                       end=returns.index.max(), \n                                                       freq='D')).ffill()\n\n\n# # Align the indices of returns and recession data\n# recession_daily = recession_daily.reindex(returns.index, method='ffill')\n\n# Filter returns for recession periods (only select days when recession is indicated)\nrecession_days_index = recession_daily[recession_daily['VALUE'] == 1].index\n\n# Filter returns for recession periods (only select days when recession is indicated)\nportfolio_recession_returns = portfolio_returns.loc[portfolio_returns.index.intersection(recession_days_index)]\n\n# Define metrics functions\ndef annualized_return(returns):\n    compounded_growth = (1 + returns).prod()\n    n_years = len(returns) / 252\n    return compounded_growth ** (1 / n_years) - 1\n\ndef annualized_volatility(returns):\n    return returns.std() * np.sqrt(252)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02):\n    excess_returns = returns - risk_free_rate / 252\n    return excess_returns.mean() / returns.std() * np.sqrt(252)\n\ndef sortino_ratio(returns, risk_free_rate=0.02):\n    downside_returns = returns[returns &lt; 0]\n    downside_deviation = downside_returns.std() * np.sqrt(252)\n    return (returns.mean() - risk_free_rate / 252) / downside_deviation\n\ndef max_drawdown(returns):\n    cumulative = (1 + returns).cumprod()\n    peak = cumulative.cummax()\n    drawdown = (cumulative - peak) / peak\n    return drawdown.min()\n\n# Calculate metrics for mini-fund portfolio during recession periods\nportfolio_metrics_recession = {\n    'Annualized Return': annualized_return(portfolio_recession_returns),\n    'Volatility': annualized_volatility(portfolio_recession_returns),\n    'Sharpe Ratio': sharpe_ratio(portfolio_recession_returns),\n    'Sortino Ratio': sortino_ratio(portfolio_recession_returns),\n    'Max Drawdown': max_drawdown(portfolio_recession_returns)\n}\n\n# Calculate metrics for S&P 500 during recession periods\nsp500_recession_returns = returns['^GSPC'].loc[returns.index.intersection(recession_days_index)]\nsp500_metrics_recession = {\n    'Annualized Return': annualized_return(sp500_recession_returns),\n    'Volatility': annualized_volatility(sp500_recession_returns),\n    'Sharpe Ratio': sharpe_ratio(sp500_recession_returns),\n    'Sortino Ratio': sortino_ratio(sp500_recession_returns),\n    'Max Drawdown': max_drawdown(sp500_recession_returns)\n}\n\n# Calculate metrics for Nasdaq-100 during recession periods\nndx_recession_returns = returns['^NDX'].loc[returns.index.intersection(recession_days_index)]\nndx_metrics_recession = {\n    'Annualized Return': annualized_return(ndx_recession_returns),\n    'Volatility': annualized_volatility(ndx_recession_returns),\n    'Sharpe Ratio': sharpe_ratio(ndx_recession_returns),\n    'Sortino Ratio': sortino_ratio(ndx_recession_returns),\n    'Max Drawdown': max_drawdown(ndx_recession_returns)\n}\n\n# Combine results into a DataFrame\nmetrics_df_recession = pd.DataFrame([portfolio_metrics_recession, sp500_metrics_recession, ndx_metrics_recession], index=['Mini-Fund', 'S&P 500', 'Nasdaq-100'])\nprint(\"Portfolio Performance During Recession Periods:\")\nprint(metrics_df_recession)\n\n# Plot the cumulative returns during recession periods\ncumulative_portfolio_returns_recession = (1 + portfolio_recession_returns).cumprod()\ncumulative_sp500_returns_recession = (1 + sp500_recession_returns).cumprod()\ncumulative_ndx_returns_recession = (1 + ndx_recession_returns).cumprod()\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_normalized = cumulative_portfolio_returns_recession / cumulative_portfolio_returns_recession.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns_recession / cumulative_sp500_returns_recession.iloc[0] * 100\ncumulative_ndx_returns_normalized = cumulative_ndx_returns_recession / cumulative_ndx_returns_recession.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio\nplt.plot(cumulative_portfolio_returns_normalized, label='Mini-Fund Portfolio (Recession)', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500 (Recession)', linestyle='--', color='orange')\nplt.plot(cumulative_ndx_returns_normalized, label='Nasdaq-100 (Recession)', linestyle='--', color='green')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-O2 vs S&P 500 and Nasdaq-100 During Recession Periods', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n[*********************100%***********************]  8 of 8 completed\n\n\nPortfolio Performance During Recession Periods:\n            Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown\nMini-Fund           -0.031372    0.300771     -0.021338      -0.000109     -0.397179\nS&P 500             -0.193706    0.385337     -0.417644      -0.002256     -0.600109\nNasdaq-100          -0.109028    0.460166     -0.065060      -0.000393     -0.613428\n\n\n\n\n\n\n\n\n\n\nrecession_data\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\n\n# Download historical data for the stocks\nstocks = ['ASML', 'NVDA', 'TSLA', 'BRK-B', 'MSFT']\ndata = yf.download(stocks, start='2010-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Define the function to calculate portfolio performance\ndef portfolio_performance(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n    returns = np.sum(mean_returns * weights) * 252\n    std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n    sharpe_ratio = (returns - risk_free_rate) / std\n    return returns, std, sharpe_ratio\n\n# Define the objective function for optimization (negative Sharpe ratio)\ndef negative_sharpe_ratio(weights, mean_returns, cov_matrix, risk_free_rate=0.02):\n    returns, std, sharpe_ratio = portfolio_performance(weights, mean_returns, cov_matrix, risk_free_rate)\n    return -sharpe_ratio\n\n# Constraints: sum of weights must be 1\ndef check_sum(weights):\n    return np.sum(weights) - 1\n\n# Boundaries: weights between 0 and 1\nbounds = tuple((0, 1) for stock in stocks)\n\n# Initial guess (equal weight distribution)\ninitial_weights = np.array([1/len(stocks)] * len(stocks))\n\n# Mean returns and covariance matrix\nmean_returns = returns.mean()\ncov_matrix = returns.cov()\n\n# Optimization using minimize from scipy.optimize\noptimal_solution = minimize(negative_sharpe_ratio, initial_weights, args=(mean_returns, cov_matrix),\n                            method='SLSQP', bounds=bounds, constraints={'type': 'eq', 'fun': check_sum})\n\noptimal_weights = optimal_solution.x\n\n# Display optimal weights\noptimal_weights_df = pd.DataFrame(optimal_weights, index=stocks, columns=['Optimal Weight'])\n\noptimal_weights_percent = optimal_weights_df * 10000000000\n\n# Display the weights in percentage form\nprint(optimal_weights_percent)\n\n#print(optimal_weights_df)\n\n[*********************100%***********************]  5 of 5 completed\n\n\n       Optimal Weight\nASML     1.639441e+01\nNVDA     2.097931e-15\nTSLA     2.852132e+01\nBRK-B    3.240187e+01\nMSFT     2.268240e+01\n\n\n\n\n\n\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\nfrom arch import arch_model\nfrom arch.__future__ import reindexing\n\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nprotective_etf = ['TAIL']\n\n# Combine stocks and protective ETF\nall_assets = stocks + protective_etf\n\n# Download historical data\ndata = yf.download(all_assets, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Define the function to calculate portfolio performance\ndef portfolio_performance(weights, returns, risk_free_rate=0.02, confidence_level=0.95, forecast_volatility=True):\n    mean_returns = returns.mean()\n    cov_matrix = returns.cov()\n    \n    portfolio_returns = np.dot(mean_returns, weights) * 252\n    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n    sharpe_ratio = (portfolio_returns - risk_free_rate) / portfolio_std\n\n    # Volatility calculation (GARCH(1,1) model)\n    if forecast_volatility:\n        am = arch_model(np.dot(returns, weights) * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        std_dev_daily = np.std(np.dot(returns, weights))\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = portfolio_returns / 252 * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m = var_list\n    es_1m, es_6m = es_list\n\n    return portfolio_returns, portfolio_std, sharpe_ratio, var_1m, var_6m, es_1m, es_6m\n\n# Define the objective function for optimization\ndef objective(weights, returns, risk_free_rate=0.02, confidence_level=0.95):\n    portfolio_returns, portfolio_std, sharpe_ratio, var_1m, var_6m, es_1m, es_6m = portfolio_performance(weights, returns, risk_free_rate, confidence_level)\n    # Weights for multi-objective optimization\n    w_sharpe, w_var, w_es = 1.0, 0.5, 0.5  # Adjust these weights as per preference\n    # Objective value to minimize: negative Sharpe ratio + VaR + ES\n    objective_value = (\n        w_sharpe * -sharpe_ratio +  # Maximizing Sharpe Ratio by minimizing its negative\n        w_var * (var_1m + var_6m) +  # Minimize VaR (1 month and 6 months)\n        w_es * (es_1m + es_6m)       # Minimize ES (1 month and 6 months)\n    )\n    return objective_value\n\n# Constraints: sum of weights must be 1\ndef check_sum(weights):\n    return np.sum(weights) - 1\n\n# Boundaries: weights between 0 and 1\nbounds = tuple((0, 1) for _ in all_assets)\n\n# Initial guess (equal weight distribution)\n# initial_weights = np.array([1 / len(all_assets)] * len(all_assets))\ninitial_weights = np.array([0.13, 0.18, 0.22, 0.16, 0.10, 0.11, 0.10])  # Added 10% weight to TAIL\n\n# Optimization using minimize from scipy.optimize\noptimal_solution = minimize(objective, initial_weights, args=(returns),\n                            method='SLSQP', bounds=bounds, constraints={'type': 'eq', 'fun': check_sum})\n\noptimal_weights = optimal_solution.x\n\n# Display optimal weights\noptimal_weights_df = pd.DataFrame(optimal_weights, index=all_assets, columns=['Optimal Weight'])\n#optimal_weights_percent = optimal_weights_df * 100\n\n# Display the weights in percentage form\nprint(optimal_weights_df)\n\n[*********************100%***********************]  7 of 7 completed\n\n\n       Optimal Weight\nASML      0.107482   \nTSLA      0.182479   \nBRK-B     0.183304   \nMSFT      0.194264   \nABBV      0.083253   \nLMT       0.091662   \nTAIL      0.157556   \n**************************************************************************************\n**************************************************************************************\n\n\n\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\nprotective_etf = ['TAIL']\n\n# Download historical data\ndata = yf.download(stocks + benchmark_indices + protective_etf, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Calculate portfolio returns with TAIL\nportfolio_returns_with_tail = (returns[stocks + protective_etf] * optimal_weights).sum(axis=1)\n\n# Calculate cumulative returns\ncumulative_portfolio_returns_without_tail = (1 + portfolio_returns_without_tail).cumprod()\ncumulative_portfolio_returns_with_tail = (1 + portfolio_returns_with_tail).cumprod()\ncumulative_sp500_returns = (1 + returns['^GSPC']).cumprod()\ncumulative_ndx_returns = (1 + returns['^NDX']).cumprod()\n\n# Define metrics functions\ndef annualized_return(returns, periods_per_year=252):\n    compounded_growth = (1 + returns).prod()\n    n_periods = returns.count()\n    return compounded_growth ** (periods_per_year / n_periods) - 1\n\ndef annualized_volatility(returns, periods_per_year=252):\n    return returns.std() * np.sqrt(periods_per_year)\n\ndef sharpe_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / returns.std() * np.sqrt(periods_per_year)\n\ndef sortino_ratio(returns, risk_free_rate=0.02, periods_per_year=252):\n    downside_returns = returns[returns &lt; 0]\n    excess_returns = returns - risk_free_rate / periods_per_year\n    return excess_returns.mean() / downside_returns.std() * np.sqrt(periods_per_year)\n\ndef max_drawdown(returns):\n    cumulative_returns = (1 + returns).cumprod()\n    peak = cumulative_returns.expanding(min_periods=1).max()\n    drawdown = (cumulative_returns - peak) / peak\n    return drawdown.min()\n\ndef alpha_beta(returns, benchmark_returns):\n    cov_matrix = np.cov(returns, benchmark_returns)\n    beta = cov_matrix[0, 1] / cov_matrix[1, 1]\n    alpha = returns.mean() - beta * benchmark_returns.mean()\n    return alpha, beta\n\ndef var_es(returns, confidence_level=0.95, forecast_volatility=False):\n    # Calculate the mean return (daily)\n    mean_return_daily = returns.mean()\n\n    # Volatility calculation\n    if forecast_volatility:\n        # Fit GARCH(1,1) model to the return series\n        am = arch_model(returns * 100, vol='Garch', p=1, q=1, dist='t')\n        res = am.fit(disp='off')\n        # Use last forecasted volatility (scaled back to decimal)\n        std_dev_daily = res.forecast(horizon=1).variance.values[-1, 0] ** 0.5 / 100\n    else:\n        # Use historical standard deviation\n        std_dev_daily = returns.std()\n\n    # Degrees of freedom for Student's t-distribution\n    df = 5  # Adjust as needed based on data\n    t_dist = t(df)\n\n    # t-score for the given confidence level (left tail)\n    t_score = t_dist.ppf(confidence_level)\n\n    # Time horizons in days\n    periods = {'1m': 21, '6m': 126, '1y': 252}\n\n    var_list = []\n    es_list = []\n\n    for days in periods.values():\n        mean_return_period = mean_return_daily * days\n        std_dev_period = std_dev_daily * np.sqrt(days)\n\n        # VaR Calculation\n        var = - (mean_return_period + t_score * std_dev_period)\n\n        # ES Calculation\n        es_factor = (t_dist.pdf(t_score) / (1 - confidence_level)) * ((df + t_score**2) / (df - 1))\n        es = - (mean_return_period + es_factor * std_dev_period)\n\n        var_list.append(var)\n        es_list.append(es)\n\n    var_1m, var_6m, var_1y = var_list\n    es_1m, es_6m, es_1y = es_list\n\n    return var_1m, var_6m, var_1y, es_1m, es_6m, es_1y\n\n# Calculate Alpha, Beta, VaR, and ES for portfolios and benchmarks\nmarket_returns = returns['^GSPC']\n\nalpha_portfolio_without_tail, beta_portfolio_without_tail = alpha_beta(portfolio_returns_without_tail, market_returns)\nalpha_portfolio_with_tail, beta_portfolio_with_tail = alpha_beta(portfolio_returns_with_tail, market_returns)\n\nvar_1m_without_tail, var_6m_without_tail, var_1y_without_tail, es_1m_without_tail, es_6m_without_tail, es_1y_without_tail = var_es(portfolio_returns_without_tail, forecast_volatility=True)\nvar_1m_with_tail, var_6m_with_tail, var_1y_with_tail, es_1m_with_tail, es_6m_with_tail, es_1y_with_tail = var_es(portfolio_returns_with_tail, forecast_volatility=True)\n\n# Calculate metrics for mini-fund portfolio without TAIL\nportfolio_metrics_without_tail = {\n    'Alpha': alpha_portfolio_without_tail,\n    'Beta': beta_portfolio_without_tail,\n    'Annualized Return': annualized_return(portfolio_returns_without_tail),\n    'Volatility': annualized_volatility(portfolio_returns_without_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_without_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_without_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_without_tail),\n    'VaR 1M (95%)': var_1m_without_tail,\n    'VaR 6M (95%)': var_6m_without_tail,\n    'VaR 1Y (95%)': var_1y_without_tail,\n    'ES 1M (95%)': es_1m_without_tail,\n    'ES 6M (95%)': es_6m_without_tail,\n    'ES 1Y (95%)': es_1y_without_tail\n}\n\n# Calculate metrics for mini-fund portfolio with TAIL\nportfolio_metrics_with_tail = {\n    'Alpha': alpha_portfolio_with_tail,\n    'Beta': beta_portfolio_with_tail,\n    'Annualized Return': annualized_return(portfolio_returns_with_tail),\n    'Volatility': annualized_volatility(portfolio_returns_with_tail),\n    'Sharpe Ratio': sharpe_ratio(portfolio_returns_with_tail),\n    'Sortino Ratio': sortino_ratio(portfolio_returns_with_tail),\n    'Max Drawdown': max_drawdown(portfolio_returns_with_tail),\n    'VaR 1M (95%)': var_1m_with_tail,\n    'VaR 6M (95%)': var_6m_with_tail,\n    'VaR 1Y (95%)': var_1y_with_tail,\n    'ES 1M (95%)': es_1m_with_tail,\n    'ES 6M (95%)': es_6m_with_tail,\n    'ES 1Y (95%)': es_1y_with_tail\n}\n\n# Calculate metrics for S&P 500\nsp500_metrics = {\n    'Beta': 1,\n    'R-Square': 1,\n    'Annualized Return': annualized_return(returns['^GSPC']),\n    'Volatility': annualized_volatility(returns['^GSPC']),\n    'Sharpe Ratio': sharpe_ratio(returns['^GSPC']),\n    'Sortino Ratio': sortino_ratio(returns['^GSPC']),\n    'Max Drawdown': max_drawdown(returns['^GSPC']),\n    'VaR 1M (95%)': var_es(returns['^GSPC'])[0],\n    'VaR 6M (95%)': var_es(returns['^GSPC'])[1],\n    'VaR 1Y (95%)': var_es(returns['^GSPC'])[2],\n    'ES 1M (95%)': var_es(returns['^GSPC'])[3],\n    'ES 6M (95%)': var_es(returns['^GSPC'])[4],\n    'ES 1Y (95%)': var_es(returns['^GSPC'])[5]\n}\n\n\n# Normalize cumulative returns to 100 for easy comparison\ncumulative_portfolio_returns_without_tail_normalized = cumulative_portfolio_returns_without_tail / cumulative_portfolio_returns_without_tail.iloc[0] * 100\ncumulative_portfolio_returns_with_tail_normalized = cumulative_portfolio_returns_with_tail / cumulative_portfolio_returns_with_tail.iloc[0] * 100\ncumulative_sp500_returns_normalized = cumulative_sp500_returns / cumulative_sp500_returns.iloc[0] * 100\n\n# Plot the data\nplt.figure(figsize=(15, 8))\n\n# Plot the portfolio without TAIL\nplt.plot(cumulative_portfolio_returns_without_tail_normalized, label='Portfolio-GV-O2', color='red')\n\n# Plot the portfolio with TAIL\nplt.plot(cumulative_portfolio_returns_with_tail_normalized, label='Portfolio-GV-TAIL-O', color='blue')\n\n# Plot benchmark indices\nplt.plot(cumulative_sp500_returns_normalized, label='S&P 500', linestyle='--', color='orange')\n\n# Add labels and title\nplt.title('Growth of Portfolio-GV-TAIL-O vs Portfolio-GV-O2 (no TAIL) vs S&P 500 and Nasdaq-100 (2000-2024)', fontsize=16)\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Growth (Normalized to 100)', fontsize=12)\nplt.legend(loc='upper left', fontsize=10)\n\n# Show the plot\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Combine results into a DataFrame\nmetrics_df = pd.DataFrame([portfolio_metrics_without_tail, portfolio_metrics_with_tail, sp500_metrics, ndx_metrics], \n                          index=['Portfolio-GV-O2', 'Portfolio-GV-TAIL-O', 'S&P 500', 'Nasdaq-100'])\n\n# Adjust display settings to show all columns without wrapping\npd.set_option('display.max_columns', None)          # Show all columns\npd.set_option('display.expand_frame_repr', False)   # Prevent DataFrame from wrapping\npd.set_option('display.width', 1000)                # Set width to large enough value\npd.set_option('display.colheader_justify', 'center') # Center align the headers\npd.set_option('display.float_format', '{:.6f}'.format)  # Format float to 6 decimal places\n\nprint(metrics_df)\n\n[*********************100%***********************]  9 of 9 completed\n\n\n\n\n\n\n\n\n\n                      Alpha    Beta    Annualized Return  Volatility  Sharpe Ratio  Sortino Ratio  Max Drawdown  VaR 1M (95%)  VaR 6M (95%)  VaR 1Y (95%)  ES 1M (95%)  ES 6M (95%)  ES 1Y (95%)  R-Square\nPortfolio-GV-O2     0.000622 1.086780      0.299328        0.243712     1.115379      1.433413      -0.350050     -0.106745     -0.347816     -0.577361    -0.142540    -0.435495    -0.701359         NaN\nPortfolio-GV-TAIL-O 0.000554 0.837946      0.251487        0.196279     1.139983      1.500943      -0.275660     -0.088748     -0.289508     -0.480820    -0.118467    -0.362305    -0.583771         NaN\nS&P 500                  NaN 1.000000      0.110526        0.196321     0.530843      0.634252      -0.339250     -0.124550     -0.341836     -0.519811    -0.174143    -0.463314    -0.691607    1.000000\nNasdaq-100               NaN 1.000000      0.183523        0.241768     0.735701      0.939876      -0.355631     -0.157124     -0.443419     -0.685044    -0.218199    -0.593020    -0.896611    1.000000"
  },
  {
    "objectID": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#cwarp",
    "href": "notebooks/MiniFund_VolatilityRegime_Backtest_2000_2024.html#cwarp",
    "title": "Mini-Fund Volatility Regime Backtest Analysis (2000–2024)",
    "section": "CWARP",
    "text": "CWARP\nhttps://docsend.com/view/teaqrcewe7ht423x\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport matplotlib.pyplot as plt\n\n# Define a function to calculate Sortino Ratio\n# Rp: Return of Portfolio, Rf: Risk-Free Rate, sigma_dp: Downside Deviation of Portfolio\ndef sortino_ratio(Rp, Rf, sigma_dp):\n    return (Rp - Rf) / sigma_dp\n\n# Define a function to calculate Return to Max Drawdown (RMDD)\n# Rp: Return of Portfolio, Lp: Trough value of Replacement Portfolio, Pp: Peak value of Replacement Portfolio\ndef return_to_max_drawdown(Rp, Lp, Pp):\n    return (Rp - Rf) / ((Lp - Pp) / Pp)\n\n# Define the CWARP Calculation Function\n# Sn: Sortino Ratio of New Portfolio, Sp: Sortino Ratio of Replacement Portfolio\n# RMDDn: Return to Max Drawdown of New Portfolio, RMDDp: Return to Max Drawdown of Replacement Portfolio\ndef cwarp_calculation(Sn, Sp, RMDDn, RMDDp):\n    return (np.sqrt((Sn / Sp) * (RMDDn / RMDDp)) - 1) * 100\n\n# Download historical data for portfolio and benchmarks\nstocks = ['ASML', 'TSLA', 'BRK-B', 'MSFT', 'ABBV', 'LMT']\nbenchmark_indices = ['^GSPC', '^NDX']  # S&P 500 and Nasdaq-100\ndata = yf.download(stocks + benchmark_indices, start='2000-01-01', end='2024-01-01')['Adj Close']\n\n# Calculate daily returns\nreturns = data.pct_change().dropna()\n\n# Equal weights for portfolio\nweights = np.array([0.15, 0.20, 0.25, 0.18, 0.10, 0.12])\n\n# Calculate portfolio returns\nportfolio_returns = (returns[stocks] * weights).sum(axis=1)\n\n# Risk-free rate (assumed)\nRf = 0.02 / 252  # Annual risk-free rate divided by trading days\n\n# Calculate Sortino Ratios for Portfolio, S&P 500, and Nasdaq-100\nportfolio_downside_std = portfolio_returns[portfolio_returns &lt; 0].std()\nsp500_downside_std = returns['^GSPC'][returns['^GSPC'] &lt; 0].std()\nndx_downside_std = returns['^NDX'][returns['^NDX'] &lt; 0].std()\n\nSp_portfolio = sortino_ratio(portfolio_returns.mean(), Rf, portfolio_downside_std)\nSp_sp500 = sortino_ratio(returns['^GSPC'].mean(), Rf, sp500_downside_std)\nSp_ndx = sortino_ratio(returns['^NDX'].mean(), Rf, ndx_downside_std)\n\n# Calculate Return to Max Drawdowns for Portfolio, S&P 500, and Nasdaq-100\nportfolio_peak = portfolio_returns.cummax()\nportfolio_drawdown = (portfolio_returns - portfolio_peak) / portfolio_peak\nLp_portfolio = portfolio_drawdown.min()\nPp_portfolio = portfolio_peak.max()\nRMDD_portfolio = return_to_max_drawdown(portfolio_returns.mean(), Lp_portfolio, Pp_portfolio)\n\nsp500_peak = returns['^GSPC'].cummax()\nsp500_drawdown = (returns['^GSPC'] - sp500_peak) / sp500_peak\nLp_sp500 = sp500_drawdown.min()\nPp_sp500 = sp500_peak.max()\nRMDD_sp500 = return_to_max_drawdown(returns['^GSPC'].mean(), Lp_sp500, Pp_sp500)\n\nndx_peak = returns['^NDX'].cummax()\nndx_drawdown = (returns['^NDX'] - ndx_peak) / ndx_peak\nLp_ndx = ndx_drawdown.min()\nPp_ndx = ndx_peak.max()\nRMDD_ndx = return_to_max_drawdown(returns['^NDX'].mean(), Lp_ndx, Pp_ndx)\n\n# Calculate CWARP for Portfolio vs S&P 500 and Nasdaq-100\ncwarp_portfolio_sp500 = cwarp_calculation(Sp_portfolio, Sp_sp500, RMDD_portfolio, RMDD_sp500)\ncwarp_portfolio_ndx = cwarp_calculation(Sp_portfolio, Sp_ndx, RMDD_portfolio, RMDD_ndx)\n\nprint(f\"CWARP Value (Portfolio vs S&P 500): {cwarp_portfolio_sp500}\")\nprint(f\"CWARP Value (Portfolio vs Nasdaq-100): {cwarp_portfolio_ndx}\")\n\n[*********************100%***********************]  8 of 8 completed\n\n\nCWARP Value (Portfolio vs S&P 500): 162.3071593499393\nCWARP Value (Portfolio vs Nasdaq-100): 78.84851594077611"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Notebooks",
    "section": "",
    "text": "Browse my research notebooks below.\n\n\n\n\n\n\n\n\n\nEmpirical Validation of the 1971 Fiscal Structural Break: Analysis of U.S. Real Individual Income Tax Revenue\n\n\nThis document provides empirical validation for a structural break in U.S. fiscal extraction occurring at the 1971 monetary pivot point. Utilizing an Interrupted Time Series framework and a Chow Test on inflation-adjusted U.S. Individual Income Tax receipts (normalized to 2024 dollars), the analysis confirms a significant shift in the growth trajectory of tax extraction following the transition to a pure fiat regime.\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\nLiquidity Regimes and the Death (and Return) of Valuations\n\n\nThis research utilizes Sparse PCA and Hidden Markov Models to demonstrate that equity factor premiums are strictly regime-dependent, revealing that while growth dominates during expansive liquidity, value and conservative investment factors revive significantly during periods of liquidity tightening. Findings prove that market valuations remain relevant but fluctuate predictably, with the S&P 500 trading approximately 0.43σ more expensive in high-liquidity states compared to tight regimes.\n\n\n\n\n\nJan 2, 2025\n\n\n\n\n\n\n\nPrivate Investment Trends in Aerospace and Defense (2019–2024)\n\n\nThis report analyzes private investment activity in the aerospace and defense sectors across North America and Europe from January 2019 to May 2024. Despite a sectoral investment CAGR of -20% since 2019, the market remains dominated by high-value “scaleup” funding rounds exceeding $100M for industry leaders such as SpaceX, Anduril Industries, and Sierra Space. California serves as the primary geographic epicenter for this capital, with Hawthorne and Costa Mesa leading in total money raised. Key investment drivers include space travel, satellite communications, and advanced manufacturing, backed by prominent lead investors like Valor Equity Partners, Sequoia Capital, and BlackRock. Furthermore, the report highlights the disparity in military spending as a percentage of GDP, with the United States maintaining a significantly higher ratio than its European counterparts, providing a stable backdrop for continued private sector innovation.\n\n\n\n\n\nMay 31, 2024\n\n\n\n\n\n\n\nMini-Fund Volatility Regime Backtest Analysis (2000–2024)\n\n\nThis study evaluates the long-term performance and risk metrics of a high-growth “Mini-Fund” portfolio—consisting of ASML, TSLA, BRK-B, MSFT, ABBV, and LMT—relative to the S&P 500 and Nasdaq-100 benchmarks. Using Hidden Markov Models to categorize market behavior into low, mid, and high volatility regimes, the analysis demonstrates that the optimized “Portfolio-GV-O2” consistently outpaced benchmarks, delivering an annualized return of 30.8% and a Sharpe ratio of 1.25, compared to 11.4% and 0.60 for the S&P 500. The portfolio proved highly resilient during historical recessions, yielding a -2.9% return while the S&P 500 declined by 19.4%. Furthermore, the strategic inclusion of the protective TAIL ETF reduced the maximum drawdown from -35.0% to -27.6%, enhancing downside protection without sacrificing significant growth. CWARP values of 162.3 against the S&P 500 and 78.8 against the Nasdaq-100 further validate the portfolio’s superior risk-adjusted efficiency.\n\n\n\n\n\nMay 22, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/aerospace-defense.html",
    "href": "notebooks/aerospace-defense.html",
    "title": "Private Investment Trends in Aerospace and Defense (2019–2024)",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport adjustText\nCode\n####### WORKING #######\n#!pip install easy-exchange-rates\n#!pip install tqdm\nCode\ndf = pd.read_csv(\"Recipe_CB_Aerospace_and_Defense_Since2019_2.csv\")\nCode\ndf.head(5)\n\n\n\n\n\n\n\n\n\nname\nmoney_raised\nannounced_date\nlocation\nindustries\ndescription\nfunding_type\nlead_investors\ntotal_funding\n\n\n\n\n0\nSpaceX\n$1,724,965,480\nMay 24, 2022\nHawthorne, California, United States, North Am...\nAdvanced Materials, Aerospace, Manufacturing, ...\nSpaceX is an aviation and aerospace company th...\nVenture - Series Unknown\n—\n$9,779,343,846\n\n\n1\nAnduril Industries\n$1,480,000,000\nDec 2, 2022\nCosta Mesa, California, United States, North A...\nAerospace, Government, Military, National Secu...\nAnduril Industries is a defense product compan...\nSeries E\nValor Equity Partners\n$2,171,000,000\n\n\n2\nSierra Space\n$1,400,000,000\nNov 19, 2021\nLouisville, Colorado, United States, North Ame...\nAdvanced Materials, Aerospace, Industrial Manu...\nSierra Space is a commercial space company tha...\nSeries A\nCoatue, General Atlantic, Moore Strategic Vent...\n$1,690,000,000\n\n\n3\nOneWeb\n$1,250,000,000\nMar 18, 2019\nLondon, England, United Kingdom, Europe\nAerospace, Internet, Satellite Communication, ...\nOneWeb is building a space-based communication...\nVenture - Series Unknown\nSoftBank\n$4,700,000,000\n\n\n4\nSpaceX\n$850,000,000\nFeb 16, 2021\nHawthorne, California, United States, North Am...\nAdvanced Materials, Aerospace, Manufacturing, ...\nSpaceX is an aviation and aerospace company th...\nVenture - Series Unknown\nSequoia Capital\n$9,779,343,846"
  },
  {
    "objectID": "notebooks/aerospace-defense.html#plots",
    "href": "notebooks/aerospace-defense.html#plots",
    "title": "Private Investment Trends in Aerospace and Defense (2019–2024)",
    "section": "Plots",
    "text": "Plots\n\n\nCode\nfrom plotnine import ggplot, geom_bar, scale_x_date, scale_y_continuous, aes, stat_smooth, facet_wrap, options, theme_classic, labs, theme, element_text, geom_line, coord_flip, scale_size_continuous, geom_text, geom_label, scale_fill_manual, geom_tile, scale_colour_continuous, theme_bw, scale_colour_manual, scale_color_discrete, geom_point, geom_histogram, after_stat"
  },
  {
    "objectID": "notebooks/aerospace-defense.html#cagr-in-deal-value---preparing-a-derived-dataset",
    "href": "notebooks/aerospace-defense.html#cagr-in-deal-value---preparing-a-derived-dataset",
    "title": "Private Investment Trends in Aerospace and Defense (2019–2024)",
    "section": "CAGR in Deal Value - Preparing a derived dataset",
    "text": "CAGR in Deal Value - Preparing a derived dataset\n\n\nCode\ndf_spacex = df_world_raw[df_world_raw['name'] == 'Hadrian']\n\n\n'Andreessen Horowitz, Lux Capital,Founders Fund'\n\n\n\n\nCode\n# def weighted_average(data):\n#     d = {}\n#     d['d1_wa'] = np.average(data['d1'], weights=data['weights'])\n#     d['d2_wa'] = np.average(data['d2'], weights=data['weights'])\n#     return pd.Series(d)\n# Call the groupby apply method with our custom function:\n\n# df.groupby('group').apply(weighted_average)\n\n#        d1_wa  d2_wa\n# group              \n# a        9.0    2.2\n# b       58.0   13.2\n\ndef first_last_deal_flow(data):\n    d = {}\n    data_sorted = data.sort_values(by = [\"announced_date\"])\n    d['first_deal_value'] = data_sorted.loc[data_sorted.index[0], 'money_raised_usd']\n    d['first_deal_date'] = data_sorted.loc[data_sorted.index[0], 'announced_date']\n    d['last_deal_value'] = data_sorted.loc[data_sorted.index[-1], 'money_raised_usd']\n    d['last_deal_date'] = data_sorted.loc[data_sorted.index[-1], 'announced_date']\n    d['last_deal_year'] = data_sorted.loc[data_sorted.index[-1], 'announced_year']\n    d['deal_span_years'] = round((d['last_deal_date'] - d['first_deal_date'])/pd.Timedelta(days=365),2)\n    d['total_funding_usd'] = data_sorted.loc[data_sorted.index[0], 'total_funding_usd']\n    d['city'] = data_sorted.loc[data_sorted.index[0], 'city']\n    d['country'] = data_sorted.loc[data_sorted.index[0], 'country']\n    d['region'] = data_sorted.loc[data_sorted.index[0], 'region']\n    d['sector'] = data_sorted.loc[data_sorted.index[0], 'sector']\n    d['lead_investors'] = ','.join(pd.Series(data_sorted['lead_investors'].str.split(\",\").explode().unique()).where(lambda x: x != \"—\").dropna())\n    d['industries'] = data_sorted.loc[data_sorted.index[0], 'industries']\n    d['deals'] = len(data_sorted)\n    d['funding_recency'] = \"recent\" if (abs(d['last_deal_year'] - datetime.now().year) &lt;= 2) else \"older\"\n    d['deal_growth_cagr'] = round((d['last_deal_value']/d['first_deal_value'])**(1./d['deal_span_years'])-1,\n                                  2)*100 if d['deal_span_years'] &gt; 0 else 0\n    return pd.Series(d)\n\ndf_world_first_last_deal_values = ( \n    df_world_raw\n    .groupby(['name'])\n    .apply(first_last_deal_flow)\n)\n\n# Ignore outliers - top 2 percentile\ndf_world_first_last_deal_values['deal_growth_cagr'] = df_world_first_last_deal_values['deal_growth_cagr'].clip(upper=df_world_first_last_deal_values['deal_growth_cagr'].quantile(0.98))\n\n# Sort\ndf_world_first_last_deal_values = df_world_first_last_deal_values.sort_values(by = [\"deal_growth_cagr\", \n                                                                                    \"total_funding_usd\", \n                                                                                    \"last_deal_date\"], \n                                                                              ascending=False)\n\ndf_world_first_last_deal_values = df_world_first_last_deal_values.reset_index()\ndf_world_first_last_deal_values.head(10)\n\n\n\n\n\n\n\n\n\nname\nfirst_deal_value\nfirst_deal_date\nlast_deal_value\nlast_deal_date\nlast_deal_year\ndeal_span_years\ntotal_funding_usd\ncity\ncountry\nregion\nsector\nlead_investors\nindustries\ndeals\nfunding_recency\ndeal_growth_cagr\n\n\n\n\n0\nSkyryse\n2500000.0\n2020-05-15\n200000000.0\n2021-10-27\n2021\n1.45\n240500000.0\nEl Segundo\nUnited States\nNorth America\naerospace\nFidelity, Monashee Investment Management\n[Aerospace, Air Transportation, Internet, Transportation]\n2\nolder\n1066.4\n\n\n1\nAeroVanti\n9750000.0\n2022-07-21\n100000000.0\n2022-10-19\n2022\n0.25\n109750000.0\nAnnapolis\nUnited States\nNorth America\naerospace\nNetwork 1 Financial Securities\n[Aerospace, Air Transportation, Transportation]\n2\nrecent\n1066.4\n\n\n2\nBRINC\n2200000.0\n2020-10-29\n55000000.0\n2022-01-01\n2022\n1.18\n82200000.0\nSeattle\nUnited States\nNorth America\naerospace\nSam Altman,Index Ventures,Alameda Research\n[Aerospace, Drones, Law Enforcement, Public Safety, Robotics]\n3\nrecent\n1066.4\n\n\n3\nAKHAN Semiconductor\n1949083.0\n2021-11-08\n20000000.0\n2022-02-17\n2022\n0.28\n37919412.0\nGurnee\nUnited States\nNorth America\naerospace and defense\n\n[Aerospace, Automotive, Consumer Electronics, Manufacturing, Military, Semiconductor, Telecommunications]\n3\nrecent\n1066.4\n\n\n4\nPhantom Space\n875000.0\n2020-09-11\n21630605.0\n2021-11-04\n2021\n1.15\n27655605.0\nTucson\nUnited States\nNorth America\naerospace\nChenel Capital\n[Aerospace, Space Travel, Transportation]\n3\nolder\n1066.4\n\n\n5\nKarman+\n1000000.0\n2022-01-01\n25000000.0\n2023-03-01\n2023\n1.16\n26000000.0\nDenver\nUnited States\nNorth America\naerospace\n\n[Aerospace, Robotics]\n2\nrecent\n1066.4\n\n\n6\nApogee Semiconductor\n468792.0\n2022-07-21\n8606581.0\n2023-04-19\n2023\n0.75\n10373924.0\nPlano\nUnited States\nNorth America\naerospace\n\n[Aerospace, Industrial Manufacturing, Semiconductor]\n2\nrecent\n1066.4\n\n\n7\nRadical\n500000.0\n2023-04-05\n4465000.0\n2024-01-05\n2024\n0.75\n4965000.0\nSeattle\nUnited States\nNorth America\naerospace\nY Combinator,Scout Ventures\n[Aerospace, Internet, Telecommunications]\n2\nrecent\n1066.4\n\n\n8\nBasalt Tech\n500000.0\n2024-04-03\n3500000.0\n2024-05-30\n2024\n0.16\n4000000.0\nSan Francisco\nUnited States\nNorth America\naerospace\nY Combinator,Initialized Capital\n[Aerospace, Industrial Automation, Space Travel]\n2\nrecent\n1066.4\n\n\n9\nThe Exploration Company\n1740300.0\n2021-10-05\n43576000.0\n2023-02-01\n2023\n1.33\n54297360.0\nMunich\nGermany\nEurope\naerospace\nPromus Ventures,EQT Ventures, Red River West\n[Aerospace, Air Transportation, Manufacturing, Space Travel]\n3\nrecent\n1026.0\n\n\n\n\n\n\n\n\n\nCode\noptions.figure_size = (1200 / options.dpi, 780 / options.dpi)\n\n(\n    ggplot(df_world_first_last_deal_values.head(50))\n    + geom_point(aes(x=\"name\",\n                    y=\"deal_growth_cagr\",\n                    size=\"total_funding_usd\",\n                    group=\"region\",\n                    fill=\"factor(funding_recency)\",\n                    colour=\"factor(funding_recency)\"))\n    #+ geom_text(aes(label=\"total_money_raised\"), size=9)\n    + scale_y_continuous(labels = lambda l: ['{}%'.format(v) for v in l])\n    + scale_size_continuous(labels = lambda l: [add_units(v) for v in l])\n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_colour_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue }, guide=False)\n    + labs(\n        x=\"Companies\",\n        y=\"Deal Value Growth - CAGR\",\n        title=\"Private Investment in Aerospace & Defense across World\",\n        size=\"Total Funding\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Top 50 companies by Total Funding and Deal Growth CAGR ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption\n    )\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"}),\n        figure_size=(8, 8)\n    )\n    + coord_flip() \n    + facet_wrap(\"region\")\n    + theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\noptions.figure_size = (1200 / options.dpi, 780 / options.dpi)\n\n(\n    ggplot(df_world_first_last_deal_values[(df_world_first_last_deal_values['total_funding_usd']&gt;10000000) & \n           (df_world_first_last_deal_values['total_funding_usd']&lt;50000000)].head(50))\n    + geom_point(aes(x=\"name\",\n                    y=\"deal_growth_cagr\",\n                    size=\"total_funding_usd\",\n                    group=\"region\",\n                    fill=\"factor(funding_recency)\",\n                    colour=\"factor(funding_recency)\"))\n    #+ geom_text(aes(label=\"total_money_raised\"), size=9)\n    + scale_y_continuous(labels = lambda l: ['{}%'.format(v) for v in l])\n    + scale_size_continuous(labels = lambda l: [add_units(v) for v in l])\n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_colour_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue }, \n                          guide=False)\n    + labs(\n        x=\"Companies\",\n        y=\"Deal Value Growth - CAGR\",\n        title=\"Private Investment in Aerospace & Defense across World\",\n        size=\"Total Funding\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Companies w/ 10-50M total funding, by Total Funding and Deal Growth CAGR ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption\n    )\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"}),\n        figure_size=(8, 8)\n    )\n    + coord_flip() \n    + facet_wrap(\"region\")\n    + theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_stage = (\n    df_world_raw\n    .groupby(['region', 'funding_type', 'funding_recency'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_world_funding_by_stage = df_world_funding_by_stage.reset_index()\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n(\n    ggplot(df_world_funding_by_stage)\n    + geom_bar(aes(x=\"funding_type\", \n                   y=\"total_money_raised\", \n                   group=\"region\",\n                   fill = \"factor(funding_recency)\"),\n               \n               stat=\"identity\") \n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Funding Stages\",\n        y=\"Total Money Raised (in USD)\",\n        title=\"Private Investment in Aerospace & Defense across US & Europe\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Funding by stages ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n    + facet_wrap(\"region\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_countries = (\n    df_world_raw\n    .groupby(['region', 'country', 'funding_recency'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_world_funding_by_countries = df_world_funding_by_countries.reset_index()\ndf_world_funding_by_countries.head(5)\n\n\n\n\n\n\n\n\n\nregion\ncountry\nfunding_recency\ntotal_money_raised\ndeals\n\n\n\n\n0\nEurope\nAustria\nolder\n22542000.0\n1\n\n\n1\nEurope\nAustria\nrecent\n204462.0\n1\n\n\n2\nEurope\nBelgium\nolder\n56542900.0\n4\n\n\n3\nEurope\nBelgium\nrecent\n75873000.0\n3\n\n\n4\nEurope\nBulgaria\nolder\n4453420.0\n2\n\n\n\n\n\n\n\n\nMilitary spending to GDP\nSource: https://data.worldbank.org/indicator/MS.MIL.XPND.GD.ZS?end=2022&start=2022&view=bar\n\n\nCode\nmilitary_to_gdp_df = pd.read_csv(\"Military_to_GDP.csv\")\nmilitary_to_gdp_df = military_to_gdp_df[['Country Name', '2022']]\nmilitary_to_gdp_df.rename(columns = {\"Country Name\": \"country\", \"2022\": \"military_spending_to_gdp\"}, \n                          inplace=True)\nmilitary_to_gdp_df = military_to_gdp_df.dropna()\nmilitary_to_gdp_df.head(5)\n\n\n\n\n\n\n\n\n\ncountry\nmilitary_spending_to_gdp\n\n\n\n\n1\nAfrica Eastern and Southern\n1.001660\n\n\n3\nAfrica Western and Central\n0.975188\n\n\n4\nAngola\n1.328722\n\n\n5\nAlbania\n1.584881\n\n\n7\nArab World\n4.968286\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_and_spending_by_countries = df_world_funding_by_countries.merge(military_to_gdp_df, on = \"country\")\ndf_world_funding_and_spending_by_countries['military_spending_to_gdp'] = pd.to_numeric(df_world_funding_and_spending_by_countries['military_spending_to_gdp'])\ndf_world_funding_and_spending_by_countries.head(5)\n\n\n\n\n\n\n\n\n\nregion\ncountry\nfunding_recency\ntotal_money_raised\ndeals\nmilitary_spending_to_gdp\n\n\n\n\n0\nEurope\nAustria\nolder\n22542000.0\n1\n0.772607\n\n\n1\nEurope\nAustria\nrecent\n204462.0\n1\n0.772607\n\n\n2\nEurope\nBelgium\nolder\n56542900.0\n4\n1.179737\n\n\n3\nEurope\nBelgium\nrecent\n75873000.0\n3\n1.179737\n\n\n4\nEurope\nBulgaria\nolder\n4453420.0\n2\n1.508123\n\n\n\n\n\n\n\n\n\nCode\n# Ordering the bar plot\ndf_world_funding_and_spending_by_countries = df_world_funding_and_spending_by_countries.sort_values(by = \"military_spending_to_gdp\", \n                                                                          ascending=False)\ncountries_by_funding = df_world_funding_and_spending_by_countries[\"country\"].unique()\ndf_world_funding_and_spending_by_countries = df_world_funding_and_spending_by_countries.assign(\n    country_cat=pd.Categorical(df_world_funding_and_spending_by_countries[\"country\"], \n                               categories=countries_by_funding)\n)\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n(\n    ggplot(df_world_funding_and_spending_by_countries[1:])\n    + geom_bar(aes(x=\"country_cat\",\n                   y=\"military_spending_to_gdp\"),\n               fill=boson_blue,\n               stat=\"identity\") \n    #+ scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(breaks = list([x * .5 for x in range(20)]), \n                         labels = lambda l: [\"{}%\".format(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Military Spending to GDP (in %)\",\n        title=\"Military Spending to GDP (US & Europe)\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Source: https://data.worldbank.org/indicator/MS.MIL.XPND.GD.ZS?end=2022&start=2022&view=bar\",\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_and_spending_by_countries.head(2)\n\n\n\n\n\n\n\n\n\nregion\ncountry\nfunding_recency\ntotal_money_raised\ndeals\nmilitary_spending_to_gdp\ncountry_cat\n\n\n\n\n28\nEurope\nUkraine\nolder\n3.500000e+05\n1\n33.546573\nUkraine\n\n\n32\nNorth America\nUnited States\nrecent\n1.026982e+10\n273\n3.454920\nUnited States\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_countries = (\n    df_world_funding_and_spending_by_countries\n    .groupby([\"funding_recency\", \"country\"])\n    .agg(total_money_raised = ('total_money_raised', 'sum'))\n    .sort_values(by=[\"total_money_raised\"], \n                 ascending=False)\n)\ndf_world_funding_by_countries = df_world_funding_by_countries.reset_index()\ndf_world_funding_by_countries.head(5)\n\n\n\n\n\n\n\n\n\nfunding_recency\ncountry\ntotal_money_raised\n\n\n\n\n0\nolder\nUnited States\n1.038508e+10\n\n\n1\nrecent\nUnited States\n1.026982e+10\n\n\n2\nolder\nUnited Kingdom\n3.027402e+09\n\n\n3\nrecent\nGermany\n1.091632e+09\n\n\n4\nolder\nGermany\n7.439148e+08\n\n\n\n\n\n\n\n\n\nCode\n# Ordering the bar plot\ndf_world_funding_by_countries = df_world_funding_by_countries.sort_values(by = \"total_money_raised\", \n                                                                          ascending=False)\ncountries_by_funding = df_world_funding_by_countries[\"country\"].unique()\ndf_world_funding_by_countries = df_world_funding_by_countries.assign(\n    country_cat=pd.Categorical(df_world_funding_by_countries[\"country\"], \n                               categories=countries_by_funding)\n)\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n\n# Annotate &gt;1B countries\ndf_labels = df_world_funding_by_countries[df_world_funding_by_countries['total_money_raised'] &gt; 1000000000]\ndf_labels = (\n    df_labels\n    .groupby([\"country_cat\"])\n    .agg(total_money_raised = ('total_money_raised', 'sum'))\n    .reset_index()\n)\ndf_labels['total_money_raised_format'] = df_labels['total_money_raised'].apply(add_units)\n\n(\n    ggplot(df_world_funding_by_countries)\n    + geom_bar(aes(x=\"country_cat\", \n                   y=\"total_money_raised\",\n                  fill=\"factor(funding_recency)\"),\n               stat=\"identity\") \n    + geom_text(aes(x = \"country_cat\",\n                    y = \"total_money_raised - 300000000\",\n                    label = \"total_money_raised_format\"),\n                color=\"white\",\n                va = \"top\",\n                size = 8,\n               data=df_labels)\n    + scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Total Money Raised (in USD)\",\n        title=\"Private Investment in Aerospace & Defense across US & Europe\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Funding by countries ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_funding_by_countries = (\n    df_world_raw\n    .groupby(['region', 'country', 'announced_year'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_world_funding_by_countries = df_world_funding_by_countries.reset_index()\n\ndf_europe_funding_by_countries = (\n    df_world_funding_by_countries[\n        (df_world_funding_by_countries['region'] == current_region['region']) & \n        ((df_world_funding_by_countries['announced_year'] &gt;= 2022))\n    ]\n    .groupby([\"country\", \"announced_year\"])\n    .agg(total_money_raised = ('total_money_raised', 'sum'))\n    .sort_values(by=[\"total_money_raised\"], \n                 ascending=False)\n)\ndf_europe_funding_by_countries = df_europe_funding_by_countries.reset_index()\n\n# Ordering the bar plot\ndf_europe_funding_by_countries = df_europe_funding_by_countries.sort_values(by = \"total_money_raised\", \n                                                                          ascending=False)\ncountries_by_funding = df_europe_funding_by_countries[\"country\"].unique()\ndf_europe_funding_by_countries = df_europe_funding_by_countries.assign(\n    country_cat=pd.Categorical(df_europe_funding_by_countries[\"country\"], \n                               categories=countries_by_funding)\n)\n\ndf_labels = (\n    df_world_raw[\n        (df_world_raw['region'] == current_region['region']) & \n        (df_world_raw['announced_year'] &gt;= 2022)\n    ]\n    .groupby(['country', 'name'])\n    .agg(total_money_raised = ('money_raised_usd', 'sum'), deals = ('money_raised_usd', 'count'))\n)\ndf_labels = df_labels.reset_index()\ndf_labels = df_labels.assign(\n    country_cat=pd.Categorical(df_labels[\"country\"], \n                               categories=countries_by_funding)\n)\n\n\n(\n    ggplot(df_europe_funding_by_countries)\n    + geom_bar(aes(x=\"country_cat\", \n                   y=\"total_money_raised\"),\n               stat=\"identity\",\n              fill=boson_blue)\n    # + geom_text(aes(x = \"country_cat\",\n    #             y = \"total_money_raised * 1.2 + 1000000\",\n    #             label = \"name\"),\n    #             va = \"bottom\",\n    #             size = 8,\n    #             nudge_y = 0.5,\n    #             colour=\"#780000\",\n    #             #adjust_text=adjust_text_dict,\n    #            data=df_labels[df_labels['total_money_raised'] &gt; 100000000])\n    + scale_y_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Total Money Raised (in USD)\",\n        title=\"Private Investment in Aerospace & Defense across Europe\",\n        subtitle =\"Funding by countries (2022-2024)\",\n        caption=caption,\n    )\n    + coord_flip()\n    + facet_wrap(\"announced_year\", ncol=3)\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_labels.sort_values(by=['total_money_raised'], ascending=False).head(20)\n\n\n\n\n\n\n\n\n\ncountry\nname\ntotal_money_raised\ndeals\ncountry_cat\n\n\n\n\n157\nUnited States\nSpaceX\n1.974965e+09\n2\nUnited States\n\n\n17\nUnited States\nAnduril Industries\n1.480000e+09\n1\nUnited States\n\n\n55\nUnited States\nDivergent\n4.900000e+08\n3\nUnited States\n\n\n195\nUnited States\nWisk Aero\n4.500000e+08\n1\nUnited States\n\n\n37\nUnited States\nBeta Technologies\n3.750000e+08\n1\nUnited States\n\n\n67\nUnited States\nFirefly Aerospace\n3.750000e+08\n2\nUnited States\n\n\n32\nUnited States\nAxiom Space\n3.500000e+08\n1\nUnited States\n\n\n150\nUnited States\nSierra Space\n2.900000e+08\n1\nUnited States\n\n\n62\nUnited States\nEpirus\n2.000000e+08\n1\nUnited States\n\n\n24\nUnited States\nAstranis\n2.000000e+08\n1\nUnited States\n\n\n81\nUnited States\nHadrian\n1.820000e+08\n2\nUnited States\n\n\n77\nUnited States\nGecko Robotics\n1.730000e+08\n2\nUnited States\n\n\n42\nUnited States\nCapella Space\n1.570000e+08\n2\nUnited States\n\n\n202\nUnited States\nZeroAvia\n1.460000e+08\n2\nUnited States\n\n\n124\nUnited States\nOverair\n1.450000e+08\n1\nUnited States\n\n\n178\nUnited States\nUrsa Major\n1.380000e+08\n2\nUnited States\n\n\n173\nUnited States\nTrue Anomaly\n1.330000e+08\n3\nUnited States\n\n\n6\nUnited States\nAeroVanti\n1.097500e+08\n2\nUnited States\n\n\n83\nUnited States\nHermeus\n1.000000e+08\n1\nUnited States\n\n\n165\nUnited States\nStoke Space\n1.000000e+08\n1\nUnited States\n\n\n\n\n\n\n\n\n\nCode\ndf_world_spending_by_countries = (\n    df_world_funding_and_spending_by_countries\n    .groupby([\"country\"])\n    .agg(military_spending_to_gdp = ('military_spending_to_gdp', 'sum'))\n    .sort_values(by=[\"military_spending_to_gdp\"], \n                 ascending=False)\n)\ndf_world_spending_by_countries = df_world_spending_by_countries.reset_index()\n\ncountries_by_spending = df_world_spending_by_countries[\"country\"].unique()\ndf_world_spending_by_countries = df_world_spending_by_countries.assign(\n    country_cat=pd.Categorical(df_world_spending_by_countries[\"country\"], \n                               categories=countries_by_spending)\n)\ndf_world_spending_by_countries.head(5)\n\n\n\n\n\n\n\n\n\ncountry\nmilitary_spending_to_gdp\ncountry_cat\n\n\n\n\n0\nUkraine\n33.546573\nUkraine\n\n\n1\nUnited States\n6.909840\nUnited States\n\n\n2\nLithuania\n5.045247\nLithuania\n\n\n3\nUnited Kingdom\n4.454368\nUnited Kingdom\n\n\n4\nFrance\n3.877447\nFrance\n\n\n\n\n\n\n\n\n\nCode\n(\n    ggplot(df_world_spending_by_countries[1:])\n    + geom_bar(aes(x=\"country_cat\", \n                   y=\"military_spending_to_gdp\"),\n               stat=\"identity\",\n            fill=boson_blue) \n    #+ scale_fill_manual(values = { \"older\": boson_blue_faded, \"recent\": boson_blue })\n    + scale_y_continuous(labels = lambda l: ['{}%'.format(v) for v in l])\n    + labs(\n        x=\"Countries\",\n        y=\"Military Spending to GDP (%()\",\n        title=\"Military Spending to GDP\",\n        fill=\"Funding deals older than two years ?\",\n        subtitle =\"Funding by countries ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_classic()\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf_world_raw.head(2)\n\n\n\n\n\n\n\n\n\nindex\nname\nmoney_raised\nannounced_date\nindustries\ndescription\nfunding_type\nlead_investors\ntotal_funding\ncity\n...\ntotal_funding_currency\ntotal_funding_amount\nmoney_raised_usd\ntotal_funding_usd\naerospace\ndefense\nannounced_date_trunc_month\nannounced_date_trunc_year\nsector\nfunding_recency\n\n\n\n\n0\n0\nSpaceX\n$1724965480\n2022-05-24\nAdvanced Materials, Aerospace, Manufacturing, ...\nSpaceX is an aviation and aerospace company th...\nVenture - Series Unknown\n—\n$9779343846\nHawthorne\n...\n$\n9779343846\n1.724965e+09\n9.779344e+09\nTrue\nFalse\n2022-05-01\n2022-01-01\naerospace\nrecent\n\n\n1\n1\nAnduril Industries\n$1480000000\n2022-12-02\nAerospace, Government, Military, National Secu...\nAnduril Industries is a defense product compan...\nSeries E\nValor Equity Partners\n$2171000000\nCosta Mesa\n...\n$\n2171000000\n1.480000e+09\n2.171000e+09\nTrue\nTrue\n2022-12-01\n2022-01-01\naerospace and defense\nrecent\n\n\n\n\n2 rows × 27 columns\n\n\n\n\n\nCode\ndf_world_raw['industries'] = df_world_raw['industries'].str.split(\",\")\n\n\n\n\nCode\ndf_world_raw_exploded = df_world_raw.explode('industries')\ndf_world_raw_exploded['industries'] = df_world_raw_exploded['industries'].str.strip()\n\ndf_world_raw_exploded_agg_by_industries = (\n    df_world_raw_exploded\n    .groupby(['industries'])\n    .agg(money_raised_usd = ('money_raised_usd', 'sum'))\n)\ndf_world_raw_exploded_agg_by_industries = (\n    df_world_raw_exploded_agg_by_industries\n    .reset_index()\n)\n# df_world_raw_exploded_agg_by_industries.head(2)\n\n\ndf_world_raw_exploded_agg_by_industries['money_raised_usd_format'] = df_world_raw_exploded_agg_by_industries['money_raised_usd'].apply(add_units)\ndf_world_raw_exploded_agg_by_industries.head(5)\n\n\n\n\n\n\n\n\n\nindustries\nmoney_raised_usd\nmoney_raised_usd_format\n\n\n\n\n0\n3D Printing\n6.628812e+08\n663M\n\n\n1\n3D Technology\n1.342485e+09\n1B\n\n\n2\nAdvanced Materials\n6.570686e+09\n7B\n\n\n3\nAerospace\n2.641824e+10\n26B\n\n\n4\nAgTech\n7.591975e+07\n76M\n\n\n\n\n\n\n\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook\"\n\nboson_blue = \"#04024B\"\nboson_royal = \"#1d2b71\"\nboson_yinmin = '#375496'\nboson_glaucous = '#507dbc'\nboson_powder_blue = '#a1c6ea'\nboson_columbia_blue = '#bbd1ea'\nboson_blue_faded = '#dae3e5'\n\nfig = px.treemap(df_world_raw_exploded_agg_by_industries[20:], \n                 path=['industries'],\n                 values='money_raised_usd',\n                 color='money_raised_usd',\n                \n                 # https://coolors.co/generate\n                 # https://coolors.co/04024b-1d2b71-375496-507dbc-a1c6ea-bbd1ea-dae3e5\n                 color_continuous_scale=[(0,boson_blue_faded),\n                                         (0.14, boson_columbia_blue),\n                                         (0.28, boson_powder_blue),\n                                         (0.42, boson_glaucous),\n                                         (0.56, boson_yinmin),\n                                         (0.75, boson_royal),\n                                         (1,boson_blue)],\n\n                 custom_data=['money_raised_usd_format'],\n                 \n                 width=1200, \n                 height=650)\nfig.update_layout(margin = dict(t=20, l=0, r=0, b=0))\nfig.update_traces(marker = dict(\n                    line = dict(width = 0)\n                  ),\n                 tiling = dict(pad = 0))\nfig.data[0].texttemplate = \"&lt;b&gt;%{label}&lt;/b&gt;&lt;br&gt;%{customdata[0]}\"\nfig.show()\n\n\n                                                \n\n\n\n\nCAGR by sub-sectors\n\n\nCode\ndf_world_first_last_deal_values.head(5)\n\n\n\n\n\n\n\n\n\nname\nfirst_deal_value\nfirst_deal_date\nlast_deal_value\nlast_deal_date\nlast_deal_year\ndeal_span_years\ntotal_funding_usd\ncity\ncountry\nregion\nsector\nlead_investors\nindustries\ndeals\nfunding_recency\ndeal_growth_cagr\n\n\n\n\n0\nSkyryse\n2500000.0\n2020-05-15\n200000000.0\n2021-10-27\n2021\n1.45\n240500000.0\nEl Segundo\nUnited States\nNorth America\naerospace\n—\nAerospace, Air Transportation, Internet, Trans...\n2\nolder\n1066.4\n\n\n1\nAeroVanti\n9750000.0\n2022-07-21\n100000000.0\n2022-10-19\n2022\n0.25\n109750000.0\nAnnapolis\nUnited States\nNorth America\naerospace\nNetwork 1 Financial Securities\nAerospace, Air Transportation, Transportation\n2\nrecent\n1066.4\n\n\n2\nBRINC\n2200000.0\n2020-10-29\n55000000.0\n2022-01-01\n2022\n1.18\n82200000.0\nSeattle\nUnited States\nNorth America\naerospace\nSam Altman\nAerospace, Drones, Law Enforcement, Public Saf...\n3\nrecent\n1066.4\n\n\n3\nAKHAN Semiconductor\n1949083.0\n2021-11-08\n20000000.0\n2022-02-17\n2022\n0.28\n37919412.0\nGurnee\nUnited States\nNorth America\naerospace and defense\n—\nAerospace, Automotive, Consumer Electronics, M...\n3\nrecent\n1066.4\n\n\n4\nPhantom Space\n875000.0\n2020-09-11\n21630605.0\n2021-11-04\n2021\n1.15\n27655605.0\nTucson\nUnited States\nNorth America\naerospace\n—\nAerospace, Space Travel, Transportation\n3\nolder\n1066.4\n\n\n\n\n\n\n\n\n\nCode\n#df_world_first_last_deal_values['industries'] = df_world_first_last_deal_values['industries'].str.split(\",\")\n\n\n\n\nCode\ndf_world_first_last_deal_values_exploded = df_world_first_last_deal_values.explode('industries')\ndf_world_first_last_deal_values_exploded['industries'] = df_world_first_last_deal_values_exploded['industries'].str.strip()\n\ndf_world_cagr_exploded_agg_by_industries = (\n    df_world_first_last_deal_values_exploded\n    .groupby(['industries'])\n    .agg(avg_funding_usd = ('total_funding_usd', 'mean'),\n        deal_growth_cagr = ('deal_growth_cagr', 'mean'))\n)\ndf_world_cagr_exploded_agg_by_industries = (\n    df_world_cagr_exploded_agg_by_industries\n    .reset_index()\n)\n#df_world_cagr_exploded_agg_by_industries['deal_growth_cagr'] = df_world_cagr_exploded_agg_by_industries['deal_growth_cagr']*100\n# df_world_cagr_exploded_agg_by_industries.head(20)\n\ndf_world_cagr_exploded_agg_by_industries = df_world_cagr_exploded_agg_by_industries.sort_values(by = \"deal_growth_cagr\",\n                                                                                                ascending=False)\nindustries = df_world_cagr_exploded_agg_by_industries[\"industries\"].unique()\ndf_world_cagr_exploded_agg_by_industries = df_world_cagr_exploded_agg_by_industries.assign(\n    industry_cat=pd.Categorical(df_world_cagr_exploded_agg_by_industries[\"industries\"], \n                               categories=industries)\n)\n\noptions.figure_size = (1024 / options.dpi, 600 / options.dpi)\n(\n    ggplot(df_world_cagr_exploded_agg_by_industries.head(30))\n    + geom_point(aes(x=\"industry_cat\", \n                     y=\"deal_growth_cagr\",\n                     size=\"avg_funding_usd\"),\n               stat=\"identity\",\n              fill=boson_blue) \n    + scale_y_continuous(breaks = list(range(0,800,50)),\n                         labels = lambda l: [\"{}%\".format(v) for v in l])\n    + scale_size_continuous(labels = lambda l: [add_units(v) for v in l])\n    + labs(\n        x=\"Industries\",\n        y=\"Deal Value Growth - CAGR\",\n        title=\"Private Investment in Aerospace & Defense across US & Europe\",\n        size=\"Avg Funding (USD)\",\n        subtitle =\"CAGR and Avg. Funding by industries ({} - {})\".format(start_date.strftime('%b %d, %Y'), end_date.strftime('%b %d, %Y')),\n        caption=caption,\n    )\n    + coord_flip()\n    + theme(\n        # left justify the caption and have one line of space between it and\n        # the x-axis label\n        plot_caption=element_text(ha=\"left\", margin={\"t\": 1, \"units\": \"lines\"})\n    )\n    + theme_bw()\n)\n\n\n\n\n\n\n\n\n\n\n\nSub-industry: Transportation / Air Transportation\n\n\nCode\ndf_world_first_last_deal_values_exploded.head(2)\n\n\n\n\n\n\n\n\n\nname\nfirst_deal_value\nfirst_deal_date\nlast_deal_value\nlast_deal_date\nlast_deal_year\ndeal_span_years\ntotal_funding_usd\ncity\ncountry\nregion\nsector\nlead_investors\nindustries\ndeals\nfunding_recency\ndeal_growth_cagr\n\n\n\n\n0\nSkyryse\n2500000.0\n2020-05-15\n200000000.0\n2021-10-27\n2021\n1.45\n240500000.0\nEl Segundo\nUnited States\nNorth America\naerospace\n—\nAerospace, Air Transportation, Internet, Trans...\n2\nolder\n1066.4\n\n\n1\nAeroVanti\n9750000.0\n2022-07-21\n100000000.0\n2022-10-19\n2022\n0.25\n109750000.0\nAnnapolis\nUnited States\nNorth America\naerospace\nNetwork 1 Financial Securities\nAerospace, Air Transportation, Transportation\n2\nrecent\n1066.4\n\n\n\n\n\n\n\n\n\nCode\ndef make_pretty(styler):\n    #styler.set_caption(caption)\n    #styler.format(recency)\n    styler.background_gradient(axis=None, cmap=\"Blues\")\n    return styler\n\ndf_world_aerospace_transportaion = (\n    df_world_first_last_deal_values_exploded[\n    (\n        (df_world_first_last_deal_values_exploded['industries'] == 'Transportation') |\n        (df_world_first_last_deal_values_exploded['industries'] == 'Air Transportation')\n    ) & \n    (\n        df_world_first_last_deal_values_exploded['deal_growth_cagr']&gt;0\n    )]\n    .sort_values(by=['last_deal_year', 'total_funding_usd', 'deals'], ascending=False)\n)\ndf_world_aerospace_transportaion['total_funding_usd_format'] = df_world_aerospace_transportaion['total_funding_usd'].apply(add_units)\n\n# &gt;10M funding\ndf_world_aerospace_transportaion = df_world_aerospace_transportaion[df_world_aerospace_transportaion['total_funding_usd'] &gt; 10000000] \ndf_world_aerospace_transportaion = df_world_aerospace_transportaion[['name', 'last_deal_year', 'total_funding_usd_format', 'country', 'region', 'deals', 'deal_growth_cagr']].rename(columns={\n    'name': 'Name',\n    'last_deal_year': 'Last Deal Year',\n    'total_funding_usd_format': 'Total Funding (USD)',\n    'country': 'Country',\n    'region': 'Region',\n    'deals': 'Number of Funding Deals',\n    'deal_growth_cagr': 'Deal CAGR'\n})\n\ndf_world_aerospace_transportaion.drop_duplicates().head(20).style.pipe(make_pretty)\n\n\n\n\n\n\n\n \nName\nLast Deal Year\nTotal Funding (USD)\nCountry\nRegion\nNumber of Funding Deals\nDeal CAGR\n\n\n\n\n\n\n\n\n\nPick: Elroy Air\n\n\nCode\n','.join(df_world_raw[df_world_raw['name']=='Elroy Air']['lead_investors'].str.replace('-', ''))\n\n\n'—,Lockheed Martin Ventures, Marlinspike Capital, Prosperity7 Ventures,—,Catapult Ventures'\n\n\n\n\nCode\ndf_world_aerospace_transportaion[df_world_aerospace_transportaion['Region']=='Europe'].drop_duplicates().head(20).style.pipe(make_pretty)\n\n\n\n\n\n\n\n \nName\nLast Deal Year\nTotal Funding (USD)\nCountry\nRegion\nNumber of Funding Deals\nDeal CAGR\n\n\n\n\n\n\n\n\n\nPick: Wingcopter\n\n\nCode\n','.join(df_world_raw[df_world_raw['name']=='Wingcopter']['lead_investors'].str.replace('-', ''))\n\n\n'—,Futury Capital, Xplorer Capital'"
  },
  {
    "objectID": "papers/index.html",
    "href": "papers/index.html",
    "title": "Papers",
    "section": "",
    "text": "PDF papers (preprints, drafts, slide-like PDFs)\nMarkdown/QMD notes that accompany papers (methods, references, appendices)\n\n\n\n\n\n\n\n  \n    \n      markdown\n    \n    Paper A Revised 1971 Shock\n    \n  \n\n\n\n\n\n  \n    \n      pdf\n    \n    AURA Neuro–Symbolic Energy–Efficient Architecture Reusing LLM Infrastructure\n    PDF: AURA-Neuro–Symbolic-Energy–Efficient-Architecture-Reusing-LLM-Infrastructure.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Aerospace defense industry overview investment thesis\n    PDF: Aerospace-defense-industry-overview-investment-thesis.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Hydroverse   Opportunity Report\n    PDF: Hydroverse - Opportunity Report.html\n  \n\n\n\n\n\n  \n    \n      reference\n    \n    Existential Risk and Growth"
  },
  {
    "objectID": "papers/index.html#whats-here",
    "href": "papers/index.html#whats-here",
    "title": "Papers",
    "section": "",
    "text": "PDF papers (preprints, drafts, slide-like PDFs)\nMarkdown/QMD notes that accompany papers (methods, references, appendices)\n\n\n\n\n\n\n\n  \n    \n      markdown\n    \n    Paper A Revised 1971 Shock\n    \n  \n\n\n\n\n\n  \n    \n      pdf\n    \n    AURA Neuro–Symbolic Energy–Efficient Architecture Reusing LLM Infrastructure\n    PDF: AURA-Neuro–Symbolic-Energy–Efficient-Architecture-Reusing-LLM-Infrastructure.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Aerospace defense industry overview investment thesis\n    PDF: Aerospace-defense-industry-overview-investment-thesis.html\n  \n\n\n\n  \n    \n      pdf\n    \n    Hydroverse   Opportunity Report\n    PDF: Hydroverse - Opportunity Report.html\n  \n\n\n\n\n\n  \n    \n      reference\n    \n    Existential Risk and Growth"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Data engineer/scientist and quantitative researcher with 18 years building systems that extract signal from noise.\nMy work spans system engineering, ML, data engineering, portfolio analytics, and economic research—always anchored in first-principles thinking and empirical validation. I don’t chase narratives that don’t survive contact with data."
  },
  {
    "objectID": "about.html#current-work",
    "href": "about.html#current-work",
    "title": "About",
    "section": "Current Work",
    "text": "Current Work\nData Engineering & Analytics\nLeading data platform modernization at Macquarie Group: DBT/Iceberg/Redshift architectures, data governance through DBT & Collier, and observable data pipelines.\nInvestment Research\nDeveloping quantitative frameworks for portfolio construction and long-term portfolio strategy.\nSystems Thinking\nBuilding graph-based intelligence platforms for venture capital analysis and exploring applications of ML/AI to financial domains."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About",
    "section": "Background",
    "text": "Background\nTechnical: Data engineering at scale • DBT, Spark, Redshift, Iceberg • ML/AI (graph networks, probabilistic models) • LLMs & Beyond (JEPA) • Modern development (Python, R, AWS) • Blockchain/decentralized systems\nAnalytical: Quantitative finance • Portfolio analytics • Economic time-series analysis • CFA Level 1 • Data governance (BCBS 239, CPG 235)\nDomain Experience: Financial services • Media/publishing • Healthcare • Renewables/energy • Gaming"
  },
  {
    "objectID": "about.html#previous-roles",
    "href": "about.html#previous-roles",
    "title": "About",
    "section": "Previous Roles",
    "text": "Previous Roles\n\nSenior Manager/Lead Engineer at Boson Research Advisory (2023-2025)\nProduct Manager at Hydroverse •\nVarious roles at Sky News, Fairfax Media, Zynga, Oracle, and startups across Sydney, San Francisco, and Bangalore."
  },
  {
    "objectID": "about.html#published-work",
    "href": "about.html#published-work",
    "title": "About",
    "section": "Published Work",
    "text": "Published Work\nCrusade Against Kidney Disease (2021)\nResearch synthesis on slowing CKD progression through evidence-based interventions. Written after watching family members navigate clinical treatment failures. Not-for-profit."
  },
  {
    "objectID": "about.html#interests",
    "href": "about.html#interests",
    "title": "About",
    "section": "Interests",
    "text": "Interests\nOutside of work: evolutionary economics, self-sustainability, genetic-based health optimization, and long-term systems thinking."
  },
  {
    "objectID": "about.html#site-philosophy",
    "href": "about.html#site-philosophy",
    "title": "About",
    "section": "Site Philosophy",
    "text": "Site Philosophy\nThis site archives my research, notebooks, and projects. It’s organized for reference and discovery, not chronology. Everything here prioritizes:\n\nQuantitative reasoning where possible\nFirst-principles thinking where necessary\n\nSkepticism toward narratives unsupported by data\n\nIf you’re looking for polished marketing material, this isn’t it. If you’re looking for working analysis with code and assumptions made explicit, you’re in the right place.\n\nConnect\nGitHub • LinkedIn\nLocation: Sydney, Australia"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deb Bose",
    "section": "",
    "text": "This site is a working archive — not a feed.\nEverything here is oriented toward truth-seeking through analysis:\nquantitative reasoning where possible, first-principles thinking where necessary, and skepticism toward narratives that don’t survive contact with data."
  },
  {
    "objectID": "index.html#what-youll-find-here",
    "href": "index.html#what-youll-find-here",
    "title": "Deb Bose",
    "section": "What you’ll find here",
    "text": "What you’ll find here\n\nArticles\nEssays, market notes, and technical deep dives — written to clarify, not to perform.\n\n\nNotebooks\nRunnable research with code, charts, and assumptions made explicit.\nExploratory by design, not polished marketing artefacts.\n\n\nPapers\nDrafts, PDFs, and reference material — including work that is incomplete, evolving, or deliberately unresolved.\n\n\nProjects\nPointers to my GitHub work: tools, models, and systems built to answer specific questions."
  },
  {
    "objectID": "index.html#latest-writing",
    "href": "index.html#latest-writing",
    "title": "Deb Bose",
    "section": "Latest writing",
    "text": "Latest writing\n\n→ Go to Papers\n→ Go to Notebooks"
  },
  {
    "objectID": "posts/2026-01-06-my-first-post.html",
    "href": "posts/2026-01-06-my-first-post.html",
    "title": "My first Quarto post",
    "section": "",
    "text": "Some text.\nInline math: \\(I_t = f(1/C_t)\\)\n\nimport math\nmath.sqrt(2)\n\n1.4142135623730951\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html",
    "href": "papers/Paper_A_Revised_1971_Shock.html",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "",
    "text": "Incomplete Paper: Academic Response"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#a-structural-critique-of-the-trammell-aschenbrenner-framework",
    "href": "papers/Paper_A_Revised_1971_Shock.html#a-structural-critique-of-the-trammell-aschenbrenner-framework",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "",
    "text": "Incomplete Paper: Academic Response"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#paper-rs-framework-assumes-technology-is-exogenous",
    "href": "papers/Paper_A_Revised_1971_Shock.html#paper-rs-framework-assumes-technology-is-exogenous",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.1 Paper R’s Framework Assumes Technology is Exogenous",
    "text": "1.1 Paper R’s Framework Assumes Technology is Exogenous\nTrammell & Aschenbrenner model technology level \\(A_t\\) as an exogenous path chosen by a social planner:\n\\[\\max_{a, b} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(1-B_t)) dt\\]\nsubject to: \\[\\dot{S}_t = -\\delta(A_t, B_t) S_t\\]\nThe critical assumption: Technology path \\(a = \\{A_t\\}_{t=0}^{\\infty}\\) is treated as a policy choice independent of monetary or fiscal constraints.\nThis is wrong. Technology advancement is endogenous to: 1. Capital availability (determined by monetary policy) 2. Interest rates (determining which innovations are financially viable) 3. Debt sustainability (constraining long-run growth paths) 4. Fiscal pressure (incentivizing specific types of “growth”)\nWhen monetary regime changes, the entire feasible set of technology paths changes. Paper R’s optimization occurs within a fiat currency regime without acknowledging that the regime itself determines the objective function’s parameters."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.2 The 1971 Structural Break",
    "text": "1.2 The 1971 Structural Break\nAugust 15, 1971: The Nixon Shock\nPresident Nixon announced the USD would no longer be convertible to gold at $35/ounce. This was not merely a U.S. policy change—it was a global monetary regime change.\nPre-1971 (Bretton Woods System): - USD pegged to gold ($35/oz fixed) - All other major currencies pegged to USD at fixed rates - Constraint: No country could print money arbitrarily without losing the peg - Global money supply anchored by gold convertibility\nPost-1971 (Fiat Currency Era): - USD breaks gold peg - All other currencies simultaneously lose their anchor - No constraint: Every central bank can now print money without limit - Global monetary system becomes pure fiat\nCritical observation: There is no control group. Every developed nation went fiat on the same day. This is a natural experiment with universal treatment."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#why-this-invalidates-paper-rs-framework",
    "href": "papers/Paper_A_Revised_1971_Shock.html#why-this-invalidates-paper-rs-framework",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "1.3 Why This Invalidates Paper R’s Framework",
    "text": "1.3 Why This Invalidates Paper R’s Framework\nPaper R’s technology path \\(a(t)\\) conflates: 1. Organic technological advancement (\\(A^{organic}_t\\)): Genuine capability improvements 2. Debt-financed pseudo-innovation (\\(A^{debt}_t\\)): Activity enabled by cheap credit that appears as “growth”\nThe observed path is: \\[A^{observed}_t = A^{organic}_t + A^{debt}_t\\]\nUnder gold-backed currency: - Credit is constrained by gold reserves - \\(A^{debt}_t \\approx 0\\) - Observed growth ≈ Organic growth\nUnder fiat currency: - Credit is constrained only by political will - \\(A^{debt}_t\\) can grow indefinitely (until debt crisis) - Observed growth &gt;&gt; Organic growth\nPaper R treats \\(A^{observed}_t\\) as if it were \\(A^{organic}_t\\) and concludes that accelerating it minimizes risk. But if \\(A^{debt}_t\\) is unsustainable and creates fertility collapse, this optimization is fundamentally flawed."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-complete-causal-structure",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-complete-causal-structure",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.1 The Complete Causal Structure",
    "text": "2.1 The Complete Causal Structure\n1971: Gold Standard Abandoned\n         ↓\nAll Currencies Become Fiat (Unlimited Printing)\n         ↓\nGovernment Debt Can Grow Without Limit\n         ↓\nRising Debt Service Costs Create Fiscal Pressure\n         ↓\nNeed to Expand Tax Base to Service Debt\n         ↓\nPOLICY STRATEGY: Encourage Female Workforce Participation\n         ↓\n         ├→ [Path A: Direct Effect]\n         │   Dual-income households = 2x taxable incomes\n         │        ↓\n         │   Government tax revenue increases\n         │\n         ├→ [Path B: Wage Suppression Effect]\n         │   Labor supply doubles → wages don't rise with productivity\n         │        ↓\n         │   Real wage stagnation makes dual-income NECESSARY (not choice)\n         │\n         └→ [Path C: Cost Inflation Effect]\n             More dual-income households → housing/childcare costs rise\n                  ↓\n             Children become economically prohibitive\n                  \n         ↓\n[All paths converge]\n         ↓\nHome Cooking → Processed Food (BigFood profits)\nChildcare at Home → Institutional Childcare (new industry)\nExtended Family Networks → Nuclear Family Isolation\nCommunity Cohesion → Market Transactions\n         ↓\nFertility Falls Below Replacement (TFR &lt; 2.1)\n         ↓\nPopulation Decline Begins\n         ↓\nEVOLUTIONARY EXTINCTION\n(despite S_∞ &gt; 0 in Paper R's framework)"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#mathematical-formalization",
    "href": "papers/Paper_A_Revised_1971_Shock.html#mathematical-formalization",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.2 Mathematical Formalization",
    "text": "2.2 Mathematical Formalization\n\nThe Debt-Fiscal Pressure Link\nUnder fiat currency, government debt evolves as:\n\\[\\dot{D}_t = r_t D_t + G_t - T_t + M_t\\]\nwhere: - \\(D_t\\) = government debt - \\(r_t\\) = interest rate\n- \\(G_t\\) = government spending - \\(T_t\\) = tax revenue - \\(M_t\\) = monetary expansion (money printing)\nPre-1971 Constraint: \\[M_t \\leq M_t^{max}(Gold_{reserves})\\]\nUnlimited monetary expansion would break the gold peg.\nPost-1971: \\[M_t \\in [0, \\infty)\\]\nNo constraint except political/inflation considerations.\nFiscal Pressure:\nDefine fiscal pressure as debt service burden: \\[FP_t = \\frac{r_t D_t}{GDP_t}\\]\nAs debt grows, fiscal pressure increases, creating incentive to expand tax base.\n\n\nThe Tax Base Expansion Strategy\nTax revenue under single-income households: \\[T_t^{single} = \\tau \\cdot n \\cdot w_t\\]\nwhere \\(n\\) = number of workers, \\(w_t\\) = wage.\nTax revenue under dual-income households: \\[T_t^{dual} = \\tau \\cdot 2n \\cdot w_t^{dual}\\]\nCritical: Even if \\(w_t^{dual} &lt; w_t\\) (wage suppression from labor supply increase), the tax revenue increases:\n\\[T_t^{dual} &gt; T_t^{single} \\iff 2w_t^{dual} &gt; w_t\\]\nThis is satisfied even with substantial wage suppression.\nKey Findings 1:\nHYPOTHESIS: Tax revenue from labor increased disproportionately post-1971\nTEST: Calculate (Labor Income Tax Revenue / GDP) from 1960-2024\nEXPECTED: Structural break upward at 1971\nDATA SOURCE: OECD Tax Revenue Database, IRS Historical Tables\nSTATUS: [VALIDATED]\nTo test the hypothesis of a disproportionate increase in tax extraction following the 1971 monetary regime change, an Interrupted Time Series analysis was performed on U.S. Individual Income Tax receipts. To ensure the analysis isolated structural fiscal shifts from the inherent inflation of the post-1971 fiat era, tax revenue was adjusted to Real Tax values (2024 dollars) using Consumer Price Index (CPI) data.\nA Chow Test was employed to statistically evaluate the presence of a structural break at the 1971 pivot point, comparing the pre-break period (1947–1970) with the post-break period (1971–2025). The results are as follows:\n• Statistical Significance: The analysis yielded a Chow F-statistic of 9.6029 with a p-value of 0.000194. This allows for the rejection of the null hypothesis at the 1% significance level, providing what the sources describe as “strong evidence of a structural break at 1971.” • Trend Acceleration: The regression coefficients reveal a significant divergence in the growth trajectory of real tax revenue extraction. The pre-1971 slope was 27.48, which accelerated to a post-1971 slope of 37.36. This represents a net increase of 9.8787 in the annual growth rate of real tax receipts. • Causal Alignment: These findings provide the empirical foundation for the argument that the removal of gold-convertibility constraints facilitated “hockey-stick growth” in government debt. According to the sources, this necessitated a “Tax Base Expansion Strategy” characterized by increased labor force participation—specifically the transition to dual-income households—to service the expanding debt.\nThis statistically significant break confirms that the 1971 “Nixon Shock” was not merely a monetary adjustment but a fundamental shift in the state’s fiscal extraction apparatus, creating the economic pressure that the sources link to the subsequent civilizational fertility collapse.\n\n\nThe Real Wage Suppression Mechanism\nLabor supply elasticity: \\[\\frac{dN_t}{dw_t} = \\epsilon \\cdot N_t\\]\nWith \\(\\epsilon &gt; 0\\), increasing labor supply (dual incomes) should raise wages. But we observe the opposite: productivity-wage divergence begins exactly at 1971.\nThe Fiat Currency Wage Suppression:\nUnder fiat currency, monetary expansion creates inflation: \\[\\pi_t = f(M_t, V_t)\\]\nNominal wages adjust slowly (sticky wages): \\[\\frac{d w_t^{nominal}}{dt} &lt; \\pi_t\\]\nTherefore real wages decline: \\[w_t^{real} = \\frac{w_t^{nominal}}{P_t} \\searrow\\]\nThis makes dual income necessary rather than optional.\n[DATA VALIDATION PLACEHOLDER 2]:\nHYPOTHESIS: Real wage-productivity divergence begins at 1971\nTEST: Calculate correlation between productivity and real wages\n      Pre-1971: Expect ρ &gt; 0.8\n      Post-1971: Expect ρ → 0\nDATA SOURCE: BLS Productivity and Costs, Real Compensation data\nSPECIFIC TEST: Chow test for structural break at 1971\nSTATUS: [PENDING VALIDATION]\n\n\nThe Fertility Response Function\nFertility depends on: \\[F_t = F(w_t^{real}, C_t^{child}, T_t^{avail}, H_t)\\]\nwhere: - \\(w_t^{real}\\) = real household income - \\(C_t^{child}\\) = cost of raising children (childcare, housing, education) - \\(T_t^{avail}\\) = time available for childrearing\n- \\(H_t\\) = cultural/institutional support for families\nPost-1971 Effects:\n\nReal wage stagnation (\\(\\downarrow w^{real}\\)) → Lower fertility\nDual-income necessity (\\(\\downarrow T^{avail}\\)) → Lower fertility\nHousing cost inflation (\\(\\uparrow C^{child}\\)) → Lower fertility\nInstitutional childcare replacing family (\\(\\downarrow H\\)) → Lower fertility\n\n[DATA VALIDATION PLACEHOLDER 3]:\nHYPOTHESIS: Fertility shows structural break at 1971\nTEST: Interrupted Time Series Analysis\n      Model: TFR_t = α + β₁·Year + β₂·Post1971 + β₃·(Post1971 × Year) + ε\n      Expected: β₃ &lt; 0 (steeper decline post-1971)\nDATA SOURCE: UN World Population Prospects, OECD Family Database\nCOUNTRIES: All OECD nations (n=38)\nSTATISTICAL TEST: Chow test, Quandt-Andrews unknown breakpoint test\nSTATUS: [PENDING VALIDATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-technology-endogeneity",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-technology-endogeneity",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "2.3 The Technology Endogeneity",
    "text": "2.3 The Technology Endogeneity\nWhat Paper R calls “technological advancement” is substantially:\n\\[A_t = A_t^{organic} + \\theta(r_t) \\cdot I_t^{debt}\\]\nwhere \\(\\theta(r_t)\\) is the debt-financed innovation multiplier (increasing as interest rates fall).\nUnder fiat currency: \\[r_t \\to 0 \\implies \\theta(r_t) \\to \\infty\\]\nThis creates an explosion of debt-financed “innovation” that appears as genuine technological progress but is actually: - Venture capital gambling enabled by cheap money - Malinvestment in unsustainable business models\n- Asset bubbles misidentified as innovation - Ponzi schemes (FTX, WeWork, etc.)\n[DATA VALIDATION PLACEHOLDER 4]:\nHYPOTHESIS: VC investment is inversely correlated with interest rates post-1971\nTEST: Regression of VC investment on Federal Funds Rate\n      Expected: β &lt; 0, R² &gt; 0.5\nDATA SOURCE: PitchBook, NVCA Yearbook, Federal Reserve H.15\nTIME PERIOD: 1971-2024\nCONTROL VARIABLES: GDP growth, corporate profits, IPO market\nSTATUS: [PENDING VALIDATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-no-control-group-problem",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-no-control-group-problem",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "3.1 The No-Control-Group Problem",
    "text": "3.1 The No-Control-Group Problem\nStandard causal inference requires a control group: - Difference-in-differences: Treated vs. untreated groups - Synthetic control: Construct counterfactual from untreated units - Regression discontinuity: Compare just above/below treatment threshold\nNone of these work for the 1971 shock because: 1. Every developed nation went fiat simultaneously 2. No country maintained gold-backed currency 3. Treatment was instantaneous and global"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#alternative-empirical-approaches",
    "href": "papers/Paper_A_Revised_1971_Shock.html#alternative-empirical-approaches",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "3.2 Alternative Empirical Approaches",
    "text": "3.2 Alternative Empirical Approaches\n\nApproach 1: Interrupted Time Series Analysis\nMethod: Test for structural breaks at 1971 in outcome variables\nSpecification: \\[Y_{i,t} = \\alpha_i + \\beta_1 \\cdot Year_t + \\beta_2 \\cdot Post1971_t + \\beta_3 \\cdot (Post1971_t \\times Year_t) + \\varepsilon_{i,t}\\]\nwhere: - \\(Y_{i,t}\\) = outcome variable (fertility, debt, wages, etc.) for country \\(i\\) at time \\(t\\) - \\(Post1971_t\\) = 1 if \\(t \\geq 1971\\), 0 otherwise - \\(\\beta_3\\) = change in trend after 1971\nTests: - Chow test: Is there a structural break at 1971? - Quandt-Andrews: What year is the most likely breakpoint? (Should be ~1971)\nOutcomes to test:\n\n\n\nVariable\nExpected Sign of β₃\nRationale\n\n\n\n\nTotal Fertility Rate\nNegative (−)\nFertility decline accelerates\n\n\nGovt Debt/GDP\nPositive (+)\nDebt grows faster\n\n\nFemale Labor Force %\nPositive (+)\nWorkforce participation jumps\n\n\nReal Wage Growth\nNegative (−)\nWage-productivity decoupling\n\n\nCPI Inflation\nPositive (+)\nFiat currency inflation\n\n\nHousing Cost/Income\nPositive (+)\nAsset price inflation\n\n\n\n[DATA VALIDATION PLACEHOLDER 5]:\nDATASET REQUIRED: Panel data for OECD countries, 1950-2024\nVARIABLES:\n  - tfr: Total Fertility Rate (births per woman)\n  - debt_gdp: Government debt as % of GDP\n  - flfp: Female labor force participation rate\n  - real_wage_growth: Annual % change in real wages\n  - productivity_growth: Annual % change in labor productivity\n  - cpi: Consumer Price Index\n  - house_price_income: Median home price / Median household income\n\nSOURCES: \n  - OECD.Stat\n  - World Bank World Development Indicators  \n  - UN Population Division\n  - BIS Property Prices Database\n\nSTATISTICAL TESTS:\n  1. Chow test (known breakpoint at 1971)\n  2. Quandt-Andrews (unknown breakpoint)\n  3. Bai-Perron (multiple breakpoints)\n\nSTATUS: [PENDING DATA COMPILATION]\n\n\nApproach 2: Differential Fiat Currency Exploitation\nWhile all countries went fiat in 1971, they differed in how aggressively they exploited the new regime:\n“Dose” variables: - Cumulative deficit spending 1971-2024 - Average debt/GDP 1971-2024\n- Monetary base expansion rate - Real interest rate suppression (deviation from natural rate)\nHypothesis: Countries that exploited fiat currency more aggressively should show: - Sharper fertility decline - Higher debt accumulation - Larger wage-productivity gaps\nCross-sectional specification: \\[\\Delta TFR_i = \\alpha + \\beta \\cdot FiatExploitation_i + \\gamma \\cdot X_i + \\varepsilon_i\\]\nwhere \\(X_i\\) includes controls (initial TFR, education, urbanization, etc.)\n[DATA VALIDATION PLACEHOLDER 6]:\nHYPOTHESIS: Fertility decline correlates with fiat currency exploitation intensity\nSAMPLE: OECD countries (n ≈ 35)\nDEPENDENT VARIABLE: \n  ΔTFRᵢ = TFR₂₀₂₄ - TFR₁₉₇₁\nINDEPENDENT VARIABLE (Fiat Exploitation Index):\n  FEIᵢ = 0.4·(Avg Debt/GDP)ᵢ + 0.3·(Cum Deficits)ᵢ + 0.3·(Real Rate Suppression)ᵢ\nCONTROL VARIABLES:\n  - Initial TFR (1971)\n  - Female education levels\n  - Urbanization rate\n  - GDP per capita\n  - Religion (% Catholic, Muslim, etc.)\nEXPECTED: β &lt; 0 (more exploitation → larger fertility decline)\nSTATUS: [PENDING ANALYSIS]\n\n\nApproach 3: Mechanism Isolation\nTest each causal pathway separately:\n\nMechanism A: Fiscal Pressure → Tax Policy → Dual Income\nTestable prediction: Countries with higher fiscal pressure (debt service/GDP) should have: - Stronger tax incentives for dual-income households - Faster female labor force participation growth\n[DATA VALIDATION PLACEHOLDER 7]:\nTEST: Panel regression\nSPECIFICATION: \n  FLFPᵢₜ = α + β₁·FiscalPressureᵢₜ + β₂·Xᵢₜ + μᵢ + λₜ + εᵢₜ\n\nWHERE:\n  FiscalPressure = (Interest Payments on Govt Debt) / GDP\n  X = controls (education, GDP, urbanization)\n  μᵢ = country fixed effects\n  λₜ = year fixed effects\n\nEXPECTED: β₁ &gt; 0\nDATA SOURCE: OECD Revenue Statistics, IMF Fiscal Monitor\nSTATUS: [PENDING ANALYSIS]\n\n\nMechanism B: Fiat Currency → Wage Suppression → Dual Income Necessity\nTestable prediction: Real wage growth should decouple from productivity growth post-1971\n[DATA VALIDATION PLACEHOLDER 8]:\nTEST: Cointegration analysis\nPERIOD 1: 1950-1970 (Gold-backed era)\nPERIOD 2: 1971-2024 (Fiat era)\n\nVARIABLES:\n  - Productivity Index (output per hour)\n  - Real Compensation Index\n\nTESTS:\n  Period 1: Expect cointegration (wages track productivity)\n  Period 2: Expect cointegration breakdown\n\nSTATISTICAL TESTS:\n  - Engle-Granger cointegration test\n  - Johansen test\n  - Rolling window correlation\n\nSTATUS: [PENDING ANALYSIS]\n\n\nMechanism C: Cheap Debt → Tech Bubble → Fertility-Hostile Culture\nTestable prediction: Lower interest rates → more “innovation” activity → lower fertility\n[DATA VALIDATION PLACEHOLDER 9]:\nTEST: Instrumental variables regression\nSPECIFICATION:\n  TFRᵢₜ = α + β₁·TechIntensityᵢₜ + β₂·Xᵢₜ + εᵢₜ\n  \nINSTRUMENT for TechIntensity:\n  Real interest rate (rₜ - πₜ)\n  \nRATIONALE:\n  Interest rates affect tech investment but don't directly affect fertility\n  (satisfies exclusion restriction)\n\nEXPECTED: β₁ &lt; 0 (more tech → lower fertility)\nSTATUS: [PENDING ANALYSIS]\n\n\n\nApproach 4: Policy Document Analysis\nHistorical Evidence of Intent:\nSearch for evidence that policymakers explicitly discussed dual-income households as tax revenue strategy.\n[DATA VALIDATION PLACEHOLDER 10]:\nDOCUMENT SEARCH:\n1. FOIA requests to:\n   - U.S. Treasury Department (1970-1980)\n   - Office of Management and Budget (1970-1980)\n   - Council of Economic Advisers (1970-1980)\n\nSEARCH TERMS:\n   - \"female labor force participation\" AND \"tax revenue\"\n   - \"dual-income\" AND \"fiscal policy\"\n   - \"women's employment\" AND \"tax base\"\n   - \"working women\" AND \"government revenue\"\n\n2. Congressional testimony search (1970-1980):\n   - House Ways and Means Committee\n   - Senate Finance Committee\n\n3. Academic literature from era:\n   - Journal of Public Economics\n   - National Tax Journal\n   \nEXPECTED FINDINGS:\n   Explicit discussion of dual-income expansion as fiscal strategy\n   \nSTATUS: [PENDING DOCUMENT REVIEW]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-endogeneity-problem",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-endogeneity-problem",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.1 The Endogeneity Problem",
    "text": "4.1 The Endogeneity Problem\nPaper R’s optimization: \\[\\max_{a, b} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(1-B_t)) dt\\]\ntreats technology path \\(a = \\{A_t\\}\\) as a choice variable.\nBut under fiat currency:\nTechnology path is endogenous to: \\[A_t = A^{organic}_t + h(D_t, r_t, M_t)\\]\nwhere: - \\(D_t\\) = government debt - \\(r_t\\) = interest rate (manipulated by central bank) - \\(M_t\\) = money supply (controlled by central bank)\nThe “choice” of technology path is not free - it’s constrained and determined by monetary regime.\nCorrect formulation: \\[\\max_{a, b, D, r, M} \\int_0^\\infty e^{-\\rho t} S_t u(A_t(D_t, r_t, M_t)(1-B_t)) dt\\]\nsubject to: \\[\\dot{S}_t = -\\delta(A_t, B_t, F_t, D_t) S_t\\] \\[F_t = F(A_t, \\dot{A}_t, w_t^{real}, C_t)\\] \\[\\dot{D}_t = r_t D_t + G_t - T_t + M_t\\]\nThis is a fundamentally different optimization problem."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-missing-fertility-constraint",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-missing-fertility-constraint",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.2 The Missing Fertility Constraint",
    "text": "4.2 The Missing Fertility Constraint\nPaper R’s survival function: \\[S_\\infty = \\lim_{t \\to \\infty} S_t\\]\nignores that population must also survive: \\[N_\\infty = \\lim_{t \\to \\infty} N_t\\]\nPopulation dynamics: \\[\\dot{N}_t = N_t \\cdot \\frac{F_t - 2.1}{30}\\]\nIf \\(F_t &lt; 2.1\\) persistently, then \\(N_\\infty = 0\\) even if \\(S_\\infty &gt; 0\\).\nThe correct survival condition: \\[Survival = (S_\\infty &gt; 0) \\land (N_\\infty &gt; 0)\\]\nPaper R proves \\(S_\\infty &gt; 0\\) is maximized by fast growth, but ignores that fast growth → \\(F_t &lt; 2.1\\) → \\(N_\\infty = 0\\).\nThis is optimizing for extinction."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-debt-sustainability-ignored",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-debt-sustainability-ignored",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.3 The Debt Sustainability Ignored",
    "text": "4.3 The Debt Sustainability Ignored\nPaper R’s hazard function: \\[\\delta(A, B) = \\bar{\\delta} A^\\alpha (1-B)^\\beta\\]\nhas no debt variable.\nBut debt creates systemic risk: \\[\\delta_{total}(A, B, D) = \\delta(A,B) + \\lambda \\cdot g(D/GDP)\\]\nwhere \\(g(\\cdot)\\) is increasing and convex (debt crises become more likely as debt rises).\nSovereign debt crises are existential risks: - Institutional collapse - Social unrest → conflict - Inability to fund safety measures - Economic collapse → famine, disease\n[DATA VALIDATION PLACEHOLDER 11]:\nHYPOTHESIS: Sovereign debt crises increase non-linearly with debt/GDP\nTEST: Logistic regression\nDEPENDENT: Debt crisis dummy (1 if crisis occurred)\nINDEPENDENT: Debt/GDP ratio, lagged 5 years\nDATA SOURCE: Reinhart & Rogoff database, IMF Fiscal Monitor\nSAMPLE: All sovereign debt crises 1950-2024\nEXPECTED: Probability of crisis increases sharply above 90% debt/GDP\nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-kuznets-curve-failure",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-kuznets-curve-failure",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "4.4 The Kuznets Curve Failure",
    "text": "4.4 The Kuznets Curve Failure\nPaper R argues that rich societies spend more on safety (safety is a “luxury good” with \\(\\eta &gt; 1\\)).\nThis fails under fiat currency because:\n\n“Wealth” is illusory if debt-financed\n\nHigh consumption today via debt ≠ genuine wealth\nWhen debt becomes unsustainable, consumption collapses\n\nFiscal pressure crowds out safety spending\n\nDebt service consumes growing fraction of budget\nLess fiscal space for safety measures\nReverses the Kuznets mechanism\n\n\n[DATA VALIDATION PLACEHOLDER 12]:\nHYPOTHESIS: Safety spending (% GDP) shows inverted-U with debt levels\nTEST: Panel regression with quadratic debt term\nSPECIFICATION:\n  SafetySpendingᵢₜ = α + β₁·(Debt/GDP)ᵢₜ + β₂·(Debt/GDP)²ᵢₜ + Xᵢₜ + εᵢₜ\n\nWHERE:\n  SafetySpending = Health + Environmental + Disaster preparedness spending\n  \nEXPECTED: \n  β₁ &gt; 0 (initially increases with debt)\n  β₂ &lt; 0 (decreases at high debt levels)\n  \nDATA SOURCE: OECD Government Spending Database\nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#incorporating-monetary-regime",
    "href": "papers/Paper_A_Revised_1971_Shock.html#incorporating-monetary-regime",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.1 Incorporating Monetary Regime",
    "text": "5.1 Incorporating Monetary Regime\nThe social planner faces:\n\\[\\max_{D_t, M_t, r_t, B_t} \\mathbb{E}\\left[\\int_0^\\infty e^{-\\rho t} S_t N_t u(C_t) dt\\right]\\]\nsubject to:\n1. Survival dynamics: \\[\\dot{S}_t = -\\delta(A_t, B_t, F_t, D_t) S_t\\]\n2. Population dynamics: \\[\\dot{N}_t = N_t \\cdot \\frac{F_t - 2.1}{30}\\]\n3. Fertility function: \\[F_t = F(w_t^{real}, C_t^{housing}, T_t^{available}, A_t, \\dot{A}_t)\\]\n4. Technology endogeneity: \\[A_t = A_t^{organic} + \\theta(r_t, D_t) \\cdot I_t^{debt}\\]\n5. Debt dynamics: \\[\\dot{D}_t = r_t D_t + G_t - T(w_t, N_t^{working}) + M_t\\]\n6. Real wage dynamics: \\[w_t^{real} = \\frac{w_t^{nominal}(N_t^{labor}, A_t)}{P_t(M_t)}\\]\n7. Monetary regime constraint:\nUnder gold standard (pre-1971): \\[M_t \\leq Gold_{reserves} / \\text{reserve ratio}\\]\nUnder fiat currency (post-1971): \\[M_t \\in [0, \\infty) \\text{ subject to political constraints}\\]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#comparative-statics-gold-vs.-fiat",
    "href": "papers/Paper_A_Revised_1971_Shock.html#comparative-statics-gold-vs.-fiat",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.2 Comparative Statics: Gold vs. Fiat",
    "text": "5.2 Comparative Statics: Gold vs. Fiat\nProposition 5.1 (Fertility Under Alternative Regimes):\nLet \\(F^{gold}_\\infty\\) be long-run fertility under gold standard and \\(F^{fiat}_\\infty\\) under fiat currency.\nUnder plausible parameters: \\[F^{fiat}_\\infty &lt; 2.1 &lt; F^{gold}_\\infty\\]\nProof Sketch:\nUnder fiat currency: 1. Government maximizes short-run consumption via debt 2. Debt creates fiscal pressure → dual-income incentives 3. Dual incomes → wage suppression → fertility decline 4. Cheap credit → tech acceleration → fertility decline\nUnder gold standard: 1. Debt constrained → no fiscal pressure for dual incomes 2. Credit constrained → no tech bubble → slower adaptation required 3. Real wages track productivity → single income sufficient 4. Result: Fertility remains above replacement\n[DATA VALIDATION PLACEHOLDER 13]:\nCOMPARISON TEST:\nPERIOD 1: 1950-1970 (Gold-backed)\nPERIOD 2: 1971-2024 (Fiat)\n\nVARIABLES:\n  - Average TFR across OECD\n  - % of countries with TFR &gt; 2.1\n\nEXPECTED:\n  Period 1: Avg TFR &gt; 2.5, most countries &gt; 2.1\n  Period 2: Avg TFR &lt; 1.8, almost no countries &gt; 2.1\n  \nSTATISTICAL TEST:\n  t-test for difference in means\n  \nSTATUS: [PENDING ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-risk-minimizing-growth-rate-under-fiat-currency",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-risk-minimizing-growth-rate-under-fiat-currency",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "5.3 The Risk-Minimizing Growth Rate Under Fiat Currency",
    "text": "5.3 The Risk-Minimizing Growth Rate Under Fiat Currency\nWhen fertility constraint is binding, the risk-minimizing growth rate becomes:\n\\[\\dot{A}^* = \\arg\\min_{\\dot{A}} \\left\\{\\delta_{total}(\\dot{A}) : F(\\dot{A}) \\geq 2.1\\right\\}\\]\nIf fertility is decreasing in growth rate (\\(\\frac{\\partial F}{\\partial \\dot{A}} &lt; 0\\)), and current growth exceeds the level compatible with replacement fertility, then:\n\\[\\dot{A}^* &lt; \\dot{A}_{current}\\]\nOptimal policy is deceleration.\n[DATA VALIDATION PLACEHOLDER 14]:\nESTIMATION TASK:\nEstimate the fertility-growth elasticity: ε = ∂ln(F)/∂ln(Ȧ)\n\nMETHOD: Panel regression\nSPECIFICATION:\n  ln(TFRᵢₜ) = α + ε·ln(TechGrowthᵢₜ) + Xᵢₜ + μᵢ + λₜ + εᵢₜ\n\nWHERE:\n  TechGrowth = VC investment + R&D spending + Patent applications\n  X = controls\n  \nEXPECTED: ε &lt; 0 (faster growth → lower fertility)\n\nIf estimated ε and current Ȧ imply F &lt; 2.1:\n  Then optimal policy is Ȧ* &lt; Ȧ_current (DECELERATE)\n  \nSTATUS: [PENDING ESTIMATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-cross-country-pattern",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-cross-country-pattern",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.1 The Cross-Country Pattern",
    "text": "6.1 The Cross-Country Pattern\nPrediction from our model: All countries that went fiat in 1971 should show: 1. Fertility decline accelerating post-1971 2. Debt/GDP rising post-1971 3. Real wage stagnation beginning post-1971 4. Female labor force participation jumping post-1971\nPrediction from alternative theories: - Contraception hypothesis: Pill approved 1960, effects should appear gradually through 1960s - Education hypothesis: Education expanding throughout 20th century, effects should be smooth - Urbanization hypothesis: Ongoing process, no sharp 1971 break\nThe data will distinguish these hypotheses.\n[DATA VALIDATION PLACEHOLDER 15]:\nCOMPREHENSIVE CROSS-COUNTRY TEST:\n\nSAMPLE: All OECD countries with data availability\n\nDEPENDENT VARIABLES:\n  1. TFR (total fertility rate)\n  2. Govt Debt / GDP\n  3. Real wage growth rate\n  4. Female LFP rate\n\nFOR EACH VARIABLE:\n  - Plot time series 1950-2024 for all countries\n  - Run Chow test for break at 1971\n  - Count: How many countries show significant break at 1971?\n  \nEXPECTED UNDER OUR HYPOTHESIS:\n  &gt;80% of countries show break at 1971 across all variables\n  \nEXPECTED UNDER ALTERNATIVE HYPOTHESES:\n  Breaks should be scattered across different years/countries\n  \nSTATUS: [PENDING COMPREHENSIVE ANALYSIS]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-israel-exception-actually-confirms-the-theory",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-israel-exception-actually-confirms-the-theory",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.2 The “Israel Exception” Actually Confirms the Theory",
    "text": "6.2 The “Israel Exception” Actually Confirms the Theory\nIsrael has TFR = 2.9, seemingly contradicting the tech→low fertility thesis.\nBut decomposing Israel’s fertility:\n\n\n\nPopulation Group\n% of Births\nTFR\nTech Participation\n\n\n\n\nSecular Jews\n~40%\n~2.1\nFull (high-tech sector)\n\n\nReligious Jews\n~35%\n~3.5\nPartial\n\n\nUltra-Orthodox\n~20%\n~7.0\nMinimal\n\n\nArabs\n~5%\n~3.0\nPartial\n\n\n\nWeighted average: \\(0.4(2.1) + 0.35(3.5) + 0.2(7.0) + 0.05(3.0) = 2.9\\)\nKey insight: - Groups fully participating in fiat-tech-dual-income system → TFR ≈ 2.1 (barely replacement) - Groups opting out → TFR &gt;&gt; 2.1\nThis is exactly what the parallel development (Amish) argument predicts.\n[DATA VALIDATION PLACEHOLDER 16]:\nISRAEL DECOMPOSITION:\nDATA SOURCE: \n  - Israel Central Bureau of Statistics\n  - Demographic studies of Israeli subpopulations\n\nVARIABLES:\n  - TFR by religious/ethnic group\n  - Labor force participation by group  \n  - Tech sector employment by group\n  - Housing costs by area (secular vs. religious cities)\n\nHYPOTHESIS:\n  Within-Israel, fertility should be inversely correlated with \n  tech sector participation even as overall TFR remains high\n\nSTATUS: [PENDING DATA COMPILATION]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#frances-success-is-temporary-and-costly",
    "href": "papers/Paper_A_Revised_1971_Shock.html#frances-success-is-temporary-and-costly",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "6.3 France’s “Success” is Temporary and Costly",
    "text": "6.3 France’s “Success” is Temporary and Costly\nFrance has TFR = 1.8, higher than most of Europe (Italy 1.2, Spain 1.2, Germany 1.5).\nBut this “success” requires: - 3.5-4% of GDP spent on family policy (OECD highest) - Massive fiscal transfers ($60+ billion annually) - Still below replacement (TFR = 1.8 &lt; 2.1) - Declining from 2.0 (2010) → 1.8 (2024)\nAnd relies on: - Immigrant fertility (2.5) vs. native French (1.7) - Full-time public childcare system - Extensive parental leave (offsetting dual-income pressure)\nInterpretation: France is fighting the fiat currency fertility collapse with enormous fiscal transfers, barely slowing the decline, at unsustainable fiscal cost.\nThis confirms rather than refutes our model - the “natural” fertility under fiat-tech-dual-income is ~1.2-1.5. France spends 4% of GDP to lift it to 1.8, still below replacement."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-acceleration-trap",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-acceleration-trap",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.1 The Acceleration Trap",
    "text": "7.1 The Acceleration Trap\nPaper R concludes: “efforts to lower x-risk by slowing the development of dangerous AI capabilities may do the opposite on balance unless sufficiently targeted.”\nUnder our framework, this is backwards.\nPaper R’s logic: - Faster growth → less time at risky tech levels → lower cumulative risk - Therefore: Accelerate\nOur logic: - Faster growth is enabled by fiat currency debt - Debt-financed growth creates fiscal pressure - Fiscal pressure → dual-income necessity → fertility collapse - Fertility collapse → \\(N_\\infty = 0\\) (extinction regardless of \\(S_\\infty\\)) - Therefore: Decelerate to sustainable rate compatible with F &gt; 2.1"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#optimizing-the-wrong-objective-function",
    "href": "papers/Paper_A_Revised_1971_Shock.html#optimizing-the-wrong-objective-function",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.2 Optimizing the Wrong Objective Function",
    "text": "7.2 Optimizing the Wrong Objective Function\nPaper R optimizes: \\[\\max S_\\infty\\]\nWe should optimize: \\[\\max (S_\\infty \\cdot N_\\infty \\cdot \\Phi_\\infty)\\]\nwhere: - \\(S_\\infty\\) = probability of avoiding catastrophe - \\(N_\\infty\\) = long-run population size - \\(\\Phi_\\infty\\) = evolutionary fitness\nThese can diverge: - High \\(S_\\infty\\), zero \\(N_\\infty\\): Survive all risks but go extinct via fertility collapse - High \\(S_\\infty\\), low \\(\\Phi_\\infty\\): Survive but with degraded health, cognition, agency\nPaper R’s framework cannot distinguish these outcomes."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-policy-implications",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-policy-implications",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "7.3 The Policy Implications",
    "text": "7.3 The Policy Implications\nIf policymakers accept Paper R’s framework:\n\nAccelerate AI without adequate safety (since speed reduces risk)\nIgnore fertility collapse (not in the model)\nSupport debt-financed tech bubbles (appears as genuine innovation)\nDismantle precautionary regulation (slows beneficial acceleration)\nEliminate parallel development paths (Amish-style communities seen as inefficient)\n\nEach of these increases true existential risk.\nCorrect policy under our framework:\n\nDistinguish organic from debt-financed innovation\n\nSupport genuine capability development\nCurb speculative bubbles enabled by cheap debt\n\nIntegrate fertility into technology policy\n\nAssess fertility impact of major tech deployments\nRequire family-formation support in high-tech sectors\nHousing policy coordinated with tech policy\n\nRespect biological constraints\n\nLimit rate of technological change to sustainable levels\nPreserve low-tech parallel communities (civilizational insurance)\nMaintain traditional knowledge and skills\n\nAddress debt sustainability\n\nConstrain deficit spending\nConsider return to commodity-backed currency\nReduce dependence on debt-driven growth\n\nPrecautionary approach to transformative technologies\n\nWhen institutional capacity is insufficient, delay deployment\nBuild regulatory capacity before technology deployment\nInternational coordination on governance"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#summary-of-the-refutation",
    "href": "papers/Paper_A_Revised_1971_Shock.html#summary-of-the-refutation",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.1 Summary of the Refutation",
    "text": "8.1 Summary of the Refutation\nPaper R’s core claim: Faster technological development minimizes existential risk.\nOur demonstration: This conclusion holds only if: 1. Technology path is exogenous (it’s not - it’s endogenous to monetary regime) 2. Fertility is irrelevant (it’s not - \\(N_\\infty = 0\\) is extinction) 3. Debt doesn’t create systemic risk (it does) 4. Observed growth is organic (it’s substantially debt-financed) 5. Policy responds optimally (it doesn’t - institutions lag badly)\nWhen these false assumptions are corrected, the conclusion reverses:\nThe risk-minimizing growth rate under fiat currency is substantially lower than current rates, and may be negative."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break-1",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-1971-structural-break-1",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.2 The 1971 Structural Break",
    "text": "8.2 The 1971 Structural Break\nThe evidence (pending validation) points to:\nAugust 15, 1971 as the most consequential date in modern economic history: - Global monetary regime change - Enabling unlimited deficit spending - Creating fiscal pressure for dual-income expansion\n- Driving fertility below replacement across all developed nations - Initiating the productivity-wage divergence - Launching the debt super-cycle\nThis is not conspiracy theory - it’s structural political economy."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#the-path-forward",
    "href": "papers/Paper_A_Revised_1971_Shock.html#the-path-forward",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.3 The Path Forward",
    "text": "8.3 The Path Forward\nEmpirical work required:\n\nCompile comprehensive cross-country dataset (1950-2024)\nRun structural break tests for all outcome variables\nEstimate fertility-growth elasticity to find sustainable Ȧ*\nDocument policy mechanisms via archival research\nQuantify debt-innovation relationship via VC/interest rate data\n\n[MASTER DATA VALIDATION PLACEHOLDER]:\nCOMPREHENSIVE EMPIRICAL VALIDATION PLAN:\n\nPHASE 1: Data Compilation (Est. 3-6 months)\n  - Assemble OECD panel dataset 1950-2024\n  - Variables: TFR, debt, wages, productivity, FLFP, tech investment\n  - Sources: OECD, World Bank, UN, BIS, national statistical offices\n\nPHASE 2: Structural Break Analysis (Est. 2-3 months)\n  - Chow tests for 1971 break across all variables\n  - Quandt-Andrews for unknown breakpoint detection\n  - Bai-Perron for multiple breaks\n\nPHASE 3: Cross-Country Variation (Est. 2-3 months)\n  - Construct \"Fiat Exploitation Index\"\n  - Regress fertility decline on FEI\n  - Control for alternative explanations\n\nPHASE 4: Mechanism Tests (Est. 3-4 months)\n  - Fiscal pressure → FLFP relationship\n  - Wage-productivity cointegration breakdown\n  - Interest rates → VC investment → fertility\n\nPHASE 5: Policy Document Review (Est. 2-3 months)\n  - FOIA requests (may take 6-12 months for responses)\n  - Congressional testimony analysis\n  - Academic literature review (1970-1980)\n\nPHASE 6: Synthesis and Publication (Est. 3-4 months)\n  - Integrate all empirical findings\n  - Write comprehensive academic paper\n  - Submit to top journal (Ecological Economics, JEE, QJE)\n\nTOTAL ESTIMATED TIME: 18-24 months\n\nBUDGET REQUIREMENTS:\n  - Data access fees: $5,000-10,000\n  - Research assistant support: $30,000-50,000\n  - FOIA legal support: $10,000-20,000\n  - Total: $45,000-80,000"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#final-provocation",
    "href": "papers/Paper_A_Revised_1971_Shock.html#final-provocation",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "8.4 Final Provocation",
    "text": "8.4 Final Provocation\nA civilization that: - Survives all catastrophes (S_∞ &gt; 0) - But fails to reproduce (N_∞ = 0) - Is equally extinct\nPaper R proves the first while ignoring the second.\nWe are currently on track for: - Avoiding nuclear war ✓ - Avoiding pandemic ✓\n- Avoiding climate catastrophe ✓ - Avoiding AI catastrophe (?) - But guaranteeing demographic extinction ✗\nThe mathematics are clear: With TFR = 0.73 (South Korea), population halves every ~30 years. In 300 years (10 generations): Population = 0.1% of current In 600 years (20 generations): Population = 0.01% of current\nThis is extinction, just slow and invisible to frameworks like Paper R that don’t model fertility.\nThe 1971 monetary regime change may be the actual existential catastrophe - not a sudden collapse, but a slow-motion demographic extinction disguised as economic progress.\nPaper R, by treating technology as exogenous and fertility as irrelevant, cannot see this. Their mathematics optimizes for survival probability while ignoring that the population surviving is approaching zero.\nThis is the fundamental category error."
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-a-data-requirements-summary",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-a-data-requirements-summary",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix A: Data Requirements Summary",
    "text": "Appendix A: Data Requirements Summary\nAll empirical claims in this paper are marked with [DATA VALIDATION PLACEHOLDER X].\nSummary of data requirements:\n\n\n\n\n\n\n\n\n\n\nPlaceholder #\nHypothesis\nData Needed\nSource\nStatus\n\n\n\n\n1\nTax revenue structural break\nLabor income tax / GDP, 1960-2024\nOECD Tax Revenue\nPending\n\n\n2\nWage-productivity divergence\nReal wages, productivity, 1960-2024\nBLS, OECD\nPending\n\n\n3\nFertility structural break\nTFR for all OECD, 1960-2024\nUN Population\nPending\n\n\n4\nVC-interest rate correlation\nVC investment, Fed Funds rate\nPitchBook, Fed\nPending\n\n\n5\nMulti-variable breaks\nPanel: TFR, debt, FLFP, wages\nMultiple\nPending\n\n\n6\nFiat exploitation-fertility\nCross-section debt metrics\nOECD, IMF\nPending\n\n\n7\nFiscal pressure → FLFP\nPanel regression dataset\nOECD, IMF\nPending\n\n\n8\nWage-productivity cointegration\nTime series 1950-2024\nBLS, OECD\nPending\n\n\n9\nTech intensity → fertility IV\nPanel with instruments\nOECD, World Bank\nPending\n\n\n10\nPolicy intent documents\nFOIA, Congressional records\nArchives\nPending\n\n\n11\nDebt crisis probability\nSovereign debt crisis database\nReinhart & Rogoff\nPending\n\n\n12\nSafety spending vs. debt\nGovernment spending by category\nOECD\nPending\n\n\n13\nGold vs. Fiat comparison\nTFR 1950-70 vs. 1971-2024\nUN Population\nPending\n\n\n14\nFertility-growth elasticity\nRegression dataset\nMultiple\nPending\n\n\n15\nComprehensive cross-country\nAll variables, all countries\nMultiple\nPending\n\n\n16\nIsrael subpopulation\nFertility by religious group\nIsrael CBS\nPending"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-b-technical-specifications-for-key-tests",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-b-technical-specifications-for-key-tests",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix B: Technical Specifications for Key Tests",
    "text": "Appendix B: Technical Specifications for Key Tests\n\nB.1 Structural Break Test (Chow Test)\nNull hypothesis: No structural break at 1971 Alternative: Structural break at 1971\nTest statistic: \\[F = \\frac{(RSS_r - RSS_u)/k}{RSS_u/(n-2k)} \\sim F_{k, n-2k}\\]\nwhere: - \\(RSS_r\\) = residual sum of squares (restricted model, no break) - \\(RSS_u\\) = residual sum of squares (unrestricted model, break at 1971) - \\(k\\) = number of parameters - \\(n\\) = number of observations\nReject null if: \\(F &gt; F_{critical}\\) at 5% significance level\n\n\nB.2 Interrupted Time Series Specification\nFull specification: \\[Y_{i,t} = \\alpha_i + \\beta_1 \\cdot Year_t + \\beta_2 \\cdot Post1971_t + \\beta_3 \\cdot (Post1971_t \\times Year_t) + \\gamma \\cdot X_{i,t} + \\varepsilon_{i,t}\\]\nInterpretation: - \\(\\alpha_i\\) = country fixed effects - \\(\\beta_1\\) = pre-1971 trend - \\(\\beta_2\\) = level shift at 1971 - \\(\\beta_3\\) = change in trend post-1971 - \\(X_{i,t}\\) = control variables\nKey test: \\(H_0: \\beta_3 = 0\\) vs. \\(H_A: \\beta_3 \\neq 0\\)\n\n\nB.3 Fiat Exploitation Index Construction\nComponents: 1. Debt Accumulation: \\(\\Delta(Debt/GDP)_{1971-2024}\\) 2. Deficit Intensity: Average (Deficit/GDP) over 1971-2024 3. Monetary Expansion: \\(\\Delta \\log(M2)\\) above GDP growth 4. Real Rate Suppression: Average \\((r_{natural} - r_{actual})\\)\nNormalization: Each component standardized to [0,1]\nAggregation: \\[FEI_i = 0.3 \\cdot Debt_i + 0.3 \\cdot Deficit_i + 0.2 \\cdot Monetary_i + 0.2 \\cdot RateSuppression_i\\]"
  },
  {
    "objectID": "papers/Paper_A_Revised_1971_Shock.html#appendix-c-alternative-explanations-and-how-to-rule-them-out",
    "href": "papers/Paper_A_Revised_1971_Shock.html#appendix-c-alternative-explanations-and-how-to-rule-them-out",
    "title": "Monetary Preconditions of Technological Acceleration",
    "section": "Appendix C: Alternative Explanations and How to Rule Them Out",
    "text": "Appendix C: Alternative Explanations and How to Rule Them Out\n\n\n\n\n\n\n\n\n\nAlternative Hypothesis\nPrediction\nOur Prediction\nDistinguishing Test\n\n\n\n\nThe Pill (contraception)\nGradual effect through 1960s\nSharp break at 1971\nStructural break test\n\n\nEducation expansion\nSmooth decline as education rises\nAcceleration post-1971\nInteraction: education × post1971\n\n\nUrbanization\nOngoing effect (no break)\nBreak at 1971\nChow test controlling for urbanization\n\n\nSecularization\nGradual effect\nBreak at 1971\nControl for religious attendance\n\n\nWomen’s lib movement\nEffect through 1960s-70s\nSpecific break at 1971\nTiming of policy changes\n\n\n\nCombined test: \\[TFR_{i,t} = \\alpha + \\beta_1 Post1971 + \\beta_2 Pill + \\beta_3 Education + \\beta_4 Urban + \\beta_5 Secular + \\varepsilon_{i,t}\\]\nIf our hypothesis is correct: \\(\\beta_1\\) remains significant and large even controlling for all alternatives.\n\nEND OF PAPER"
  },
  {
    "objectID": "notebooks/Empirical Validation of the 1971 Fiscal Structural Break - Analysis of U.S Real Individual Income Tax Revenue.html",
    "href": "notebooks/Empirical Validation of the 1971 Fiscal Structural Break - Analysis of U.S Real Individual Income Tax Revenue.html",
    "title": "Empirical Validation of the 1971 Fiscal Structural Break: Analysis of U.S. Real Individual Income Tax Revenue",
    "section": "",
    "text": "\"\"\"\nSimple script to download US Individual Income Tax / GDP data from FRED\nNo complex dependencies - just pandas and requests\n\"\"\"\n\nimport pandas as pd\nimport requests\nfrom io import StringIO\n\nprint(\"Downloading US Individual Income Tax and GDP data from FRED...\")\nprint(\"=\" * 80)\n\n# FRED series IDs\n# W006RC1Q027SBEA = Individual Income Tax Receipts (quarterly, billions)\n# GDP = Gross Domestic Product (quarterly, billions)\n\ndef download_fred_series(series_id, series_name):\n    \"\"\"Download a single series from FRED as CSV.\"\"\"\n    url = f\"https://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}\"\n    \n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()\n        \n        # Read CSV\n        df = pd.read_csv(StringIO(response.text))\n        df.columns = ['DATE', series_name]\n        df['DATE'] = pd.to_datetime(df['DATE'])\n        df[series_name] = pd.to_numeric(df[series_name], errors='coerce')\n        \n        print(f\"✓ Downloaded {series_name}: {len(df)} observations\")\n        return df\n    \n    except Exception as e:\n        print(f\"✗ Error downloading {series_name}: {e}\")\n        return None\n\n# Download data\nprint(\"\\n1. Downloading Individual Income Tax receipts...\")\ntax_df = download_fred_series('W006RC1Q027SBEA', 'IndividualIncomeTax')\n\nDownloading US Individual Income Tax and GDP data from FRED...\n================================================================================\n\n1. Downloading Individual Income Tax receipts...\n✓ Downloaded IndividualIncomeTax: 315 observations\n\n\n\ntax_df.tail()\n\n\n\n\n\n\n\n\nDATE\nIndividualIncomeTax\n\n\n\n\n310\n2024-07-01\n3141.087\n\n\n311\n2024-10-01\n3202.119\n\n\n312\n2025-01-01\n3246.051\n\n\n313\n2025-04-01\n3455.060\n\n\n314\n2025-07-01\n3599.024\n\n\n\n\n\n\n\n\ntax_df_indexed = tax_df.set_index(\"DATE\")\n\n\nimport pandas as pd\n\ndef to_annual_series(\n    tax_df: pd.DataFrame,\n    date_col: str = \"DATE\",\n    value_col: str = \"IndividualIncomeTax\",\n    agg: str = \"mean\",  # \"mean\" (default) or \"sum\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert a quarterly (or higher frequency) dataframe with a DATE column\n    into an annual dataframe with columns: Year, IndividualIncomeTax_Annual.\n\n    agg:\n      - \"mean\": good for level/rate-like series (e.g., % of GDP, index levels, rates)\n      - \"sum\" : good for flow series (e.g., revenues within year)\n    \"\"\"\n    df = tax_df.copy()\n\n    # If DATE is the index, bring it back as a column\n    if date_col not in df.columns and df.index.name == date_col:\n        df = df.reset_index()\n\n    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n    df = df.dropna(subset=[date_col, value_col])\n\n    df[\"Year\"] = df[date_col].dt.year.astype(int)\n\n    if agg not in (\"mean\", \"sum\", \"median\"):\n        raise ValueError(\"agg must be one of: 'mean', 'sum', 'median'\")\n\n    annual = (\n        df.groupby(\"Year\", as_index=False)[value_col]\n          .agg({f\"{value_col}_Annual\": agg})\n          .sort_values(\"Year\")\n          .reset_index(drop=True)\n    )\n    return annual\n\n\nannual_tax = to_annual_series(tax_df, value_col=\"IndividualIncomeTax\", agg=\"mean\")\nannual_tax.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\n\n\n\n\n74\n2021\n2675.071500\n\n\n75\n2022\n3252.924000\n\n\n76\n2023\n2918.060500\n\n\n77\n2024\n3118.884750\n\n\n78\n2025\n3433.378333\n\n\n\n\n\n\n\n\ncpi_df = download_fred_series('CPIAUCSL', 'CPI')\ncpi_df.tail()\nannual_cpi = to_annual_series(tax_df, value_col=\"CPI\", agg=\"mean\")\nannual_cpi.tail()\n\n✓ Downloaded CPI: 947 observations\n\n\n\n\n\n\n\n\n\nYear\nCPI_Annual\n\n\n\n\n74\n2021\n270.967917\n\n\n75\n2022\n292.625417\n\n\n76\n2023\n304.704167\n\n\n77\n2024\n313.697833\n\n\n78\n2025\n321.577200\n\n\n\n\n\n\n\n\n# Step 2: Merge with your tax data\ndf = pd.merge(annual_tax, annual_cpi_df, on='Year', how='inner')\ndf.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\nCPI_Annual\n\n\n\n\n74\n2021\n2675.071500\n270.967917\n\n\n75\n2022\n3252.924000\n292.625417\n\n\n76\n2023\n2918.060500\n304.704167\n\n\n77\n2024\n3118.884750\n313.697833\n\n\n78\n2025\n3433.378333\n321.577200\n\n\n\n\n\n\n\n\n# Step 3: Calculate Real Tax (in 2024 dollars)\nlatest_cpi = annual_cpi[annual_cpi['Year'] == 2024]['CPI_Annual'].mean()\ndf['Real_Tax'] = (df['IndividualIncomeTax_Annual'] / df['CPI_Annual']) * latest_cpi\ndf.tail()\n\n\n\n\n\n\n\n\nYear\nIndividualIncomeTax_Annual\nCPI_Annual\nReal_Tax\n\n\n\n\n74\n2021\n2675.071500\n270.967917\n3096.913258\n\n\n75\n2022\n3252.924000\n292.625417\n3487.172175\n\n\n76\n2023\n2918.060500\n304.704167\n3004.190151\n\n\n77\n2024\n3118.884750\n313.697833\n3118.884750\n\n\n78\n2025\n3433.378333\n321.577200\n3349.252821\n\n\n\n\n\n\n\n\ndf = df.set_index('Year')\ndf['Real_Tax'].plot()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.api import OLS, add_constant\nfrom scipy import stats\n\ndef chow_test_tax_series(\n    df_annual: pd.DataFrame,\n    y_col: str,\n    break_year: int = 1971,\n    min_obs_each_side: int = 8,\n    save_csv_path: str = None\n):\n    \"\"\"\n    Chow test for a structural break at a known year on an annual series.\n\n    Model: y ~ const + t\n    where t is a normalized year index for numerical stability.\n\n    H0: No structural break (β_pre = β_post)\n    H1: Structural break exists\n    \"\"\"\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"CHOW TEST FOR STRUCTURAL BREAK\")\n    print(\"=\" * 80)\n\n    # Clean and keep what we need\n    df_clean = df_annual[[\"Year\", y_col]].dropna().copy()\n    df_clean = df_clean.sort_values(\"Year\").reset_index(drop=True)\n\n    # Split (match your original: pre &lt; break_year, post &gt;= break_year)\n    df_pre = df_clean[df_clean[\"Year\"] &lt; break_year].copy()\n    df_post = df_clean[df_clean[\"Year\"] &gt;= break_year].copy()\n\n    n_pre, n_post = len(df_pre), len(df_post)\n    n_total = n_pre + n_post\n\n    print(f\"\\nBreak year: {break_year}\")\n    print(f\"Observations pre-break: {n_pre}\")\n    print(f\"Observations post-break: {n_post}\")\n    print(f\"Total observations: {n_total}\")\n\n    if n_pre &lt; min_obs_each_side or n_post &lt; min_obs_each_side:\n        raise ValueError(\n            f\"Not enough annual observations on each side of {break_year}. \"\n            f\"Need at least {min_obs_each_side} each side. Got pre={n_pre}, post={n_post}.\"\n        )\n\n    # Normalize time index for stability (use the same reference for all)\n    base_year = df_clean[\"Year\"].min()\n    df_clean[\"t\"] = df_clean[\"Year\"] - base_year\n    df_pre[\"t\"] = df_pre[\"Year\"] - base_year\n    df_post[\"t\"] = df_post[\"Year\"] - base_year\n\n    # Pooled regression\n    X_pooled = add_constant(df_clean[\"t\"].astype(float))\n    y_pooled = df_clean[y_col].astype(float)\n    model_pooled = OLS(y_pooled, X_pooled).fit()\n    RSS_pooled = float(model_pooled.ssr)\n\n    # Separate regressions\n    X_pre = add_constant(df_pre[\"t\"].astype(float))\n    y_pre = df_pre[y_col].astype(float)\n    model_pre = OLS(y_pre, X_pre).fit()\n    RSS_pre = float(model_pre.ssr)\n\n    X_post = add_constant(df_post[\"t\"].astype(float))\n    y_post = df_post[y_col].astype(float)\n    model_post = OLS(y_post, X_post).fit()\n    RSS_post = float(model_post.ssr)\n\n    RSS_separate = RSS_pre + RSS_post\n\n    # Chow statistic\n    k = 2  # intercept + slope\n    df_num = k\n    df_den = n_total - 2 * k\n\n    F_stat = ((RSS_pooled - RSS_separate) / k) / (RSS_separate / df_den)\n    p_value = 1 - stats.f.cdf(F_stat, df_num, df_den)\n\n    critical_05 = stats.f.ppf(0.95, df_num, df_den)\n    critical_01 = stats.f.ppf(0.99, df_num, df_den)\n\n    print(\"\\n\" + \"-\" * 80)\n    print(\"RESULTS:\")\n    print(\"-\" * 80)\n    print(f\"\\nRSS Pooled (restricted): {RSS_pooled:.4f}\")\n    print(f\"RSS Separate (unrestricted): {RSS_separate:.4f}\")\n    print(f\"  RSS Pre-break: {RSS_pre:.4f}\")\n    print(f\"  RSS Post-break: {RSS_post:.4f}\")\n\n    print(f\"\\nChow F-statistic: {F_stat:.4f}\")\n    print(f\"Degrees of freedom: ({df_num}, {df_den})\")\n    print(f\"P-value: {p_value:.6f}\")\n\n    print(f\"\\nCritical values:\")\n    print(f\"  5% level: {critical_05:.4f}\")\n    print(f\"  1% level: {critical_01:.4f}\")\n\n    print(\"\\nConclusion:\")\n    if p_value &lt; 0.01:\n        result = \"Reject H0 (1%)\"\n        print(f\"  *** REJECT NULL HYPOTHESIS at 1% level ***\")\n        print(f\"  Strong evidence of structural break at {break_year}\")\n    elif p_value &lt; 0.05:\n        result = \"Reject H0 (5%)\"\n        print(f\"  ** REJECT NULL HYPOTHESIS at 5% level **\")\n        print(f\"  Significant evidence of structural break at {break_year}\")\n    elif p_value &lt; 0.10:\n        result = \"Reject H0 (10%)\"\n        print(f\"  * REJECT NULL HYPOTHESIS at 10% level *\")\n        print(f\"  Moderate evidence of structural break at {break_year}\")\n    else:\n        result = \"Fail to Reject H0\"\n        print(f\"  FAIL TO REJECT NULL HYPOTHESIS\")\n        print(f\"  Insufficient evidence of structural break at {break_year}\")\n\n    # Coefficients\n    print(\"\\n\" + \"-\" * 80)\n    print(\"REGRESSION COEFFICIENTS:\")\n    print(\"-\" * 80)\n\n    print(f\"\\nPre-{break_year} period:\")\n    print(f\"  Intercept: {model_pre.params.iloc[0]:.4f} (se: {model_pre.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_pre.params.iloc[1]:.4f} (se: {model_pre.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_pre.rsquared:.4f}\")\n\n    print(f\"\\nPost-{break_year} period:\")\n    print(f\"  Intercept: {model_post.params.iloc[0]:.4f} (se: {model_post.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_post.params.iloc[1]:.4f} (se: {model_post.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_post.rsquared:.4f}\")\n\n    print(f\"\\nPooled (no break):\")\n    print(f\"  Intercept: {model_pooled.params.iloc[0]:.4f} (se: {model_pooled.bse.iloc[0]:.4f})\")\n    print(f\"  Slope:     {model_pooled.params.iloc[1]:.4f} (se: {model_pooled.bse.iloc[1]:.4f})\")\n    print(f\"  R-squared: {model_pooled.rsquared:.4f}\")\n\n    results_dict = {\n        \"Test\": \"Chow Test\",\n        \"Series\": y_col,\n        \"Break Year\": break_year,\n        \"F-statistic\": float(F_stat),\n        \"P-value\": float(p_value),\n        \"Critical Value (5%)\": float(critical_05),\n        \"Critical Value (1%)\": float(critical_01),\n        \"Result\": result,\n        \"Slope Pre\": float(model_pre.params.iloc[1]),\n        \"Slope Post\": float(model_post.params.iloc[1]),\n        \"Slope Change\": float(model_post.params.iloc[1] - model_pre.params.iloc[1]),\n        \"n_pre\": int(n_pre),\n        \"n_post\": int(n_post),\n    }\n\n    if save_csv_path is not None:\n        pd.DataFrame([results_dict]).to_csv(save_csv_path, index=False)\n\n    return results_dict\n\n\n# Chow test at 1971\nres = chow_test_tax_series(\n    df,\n    y_col=\"Real_Tax\",\n    break_year=1971\n)\n\nres\n\n\n================================================================================\nCHOW TEST FOR STRUCTURAL BREAK\n================================================================================\n\nBreak year: 1971\nObservations pre-break: 24\nObservations post-break: 55\nTotal observations: 79\n\n--------------------------------------------------------------------------------\nRESULTS:\n--------------------------------------------------------------------------------\n\nRSS Pooled (restricted): 3811995.5571\nRSS Separate (unrestricted): 3034840.0098\n  RSS Pre-break: 84072.8198\n  RSS Post-break: 2950767.1900\n\nChow F-statistic: 9.6029\nDegrees of freedom: (2, 75)\nP-value: 0.000194\n\nCritical values:\n  5% level: 3.1186\n  1% level: 4.8999\n\nConclusion:\n  *** REJECT NULL HYPOTHESIS at 1% level ***\n  Strong evidence of structural break at 1971\n\n--------------------------------------------------------------------------------\nREGRESSION COEFFICIENTS:\n--------------------------------------------------------------------------------\n\nPre-1971 period:\n  Intercept: 477.9496 (se: 24.4684)\n  Slope:     27.4835 (se: 1.8229)\n  R-squared: 0.9118\n\nPost-1971 period:\n  Intercept: 2.3028 (se: 107.0530)\n  Slope:     37.3621 (se: 2.0042)\n  R-squared: 0.8677\n\nPooled (no break):\n  Intercept: 349.9247 (se: 49.5949)\n  Slope:     31.2690 (se: 1.0978)\n  R-squared: 0.9133\n\n\n{'Test': 'Chow Test',\n 'Series': 'Real_Tax',\n 'Break Year': 1971,\n 'F-statistic': 9.602922372192094,\n 'P-value': 0.00019358783729239715,\n 'Critical Value (5%)': 3.118642128006125,\n 'Critical Value (1%)': 4.899877423111457,\n 'Result': 'Reject H0 (1%)',\n 'Slope Pre': 27.483480442522755,\n 'Slope Post': 37.362148635986124,\n 'Slope Change': 9.878668193463369,\n 'n_pre': 24,\n 'n_post': 55}"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "A 35-Year Quantitative Analysis of Factor Premiums (1990–2025)\n\n\n\nStory – why this matters (intro + conclusion)\nData – what you measure (markets, factors, liquidity proxies)\nModels – how you measure (liquidity index, regimes, cross-sectional tests)\nPortfolio – what to do with it (conditional allocation\n\n\n\n\n\nThe “valuations don’t matter in infinite money printing” meme.\nYour construction of a liquidity index and liquidity regimes.\nKey findings:\nValuations do matter, but conditional on liquidity regimes.\nGrowth dominates in high-liquidity, negative-real-rate regimes.\nValue premia revive after QT / real rate normalization.\n\nPortfolio takeaway: a simple regime-aware factor-tilt model.\n\n\n\n\nNarrative: post-GFC QE, ZIRP, NIRP, 2010–2021 “TINA”, tech bubble 2.0.\n\n“We live in a world of infinite money printing, so valuations don’t matter.”\n\nResearch question: &gt; Do valuations truly die under abundant liquidity, or do they merely become regime-dependent?\nDo valuation spreads (cheap vs expensive deciles) expand under high-liquidity regimes and compress under tight liquidity?\nHow do factor premia (value, momentum, quality, low vol) vary conditional on liquidity regimes?\nCan a macro-liquidity index predict the timing of factor rotations (growth vs value, long-duration vs short-duration equities)?\nDoes a regime-aware factor allocation deliver better risk-adjusted returns than a static factor mix?\n\n\n\n\n\nValuation spread: the gap between cheap and expensive stocks (e.g., top vs bottom decile by B/M).\nLiquidity regime: periods of systematically easy vs tight financial conditions driven by monetary and fiscal variables.\n\n\n\n\n\nSystemic Strategy: Factor investing: value, momentum, quality.\nRisk Factors and Risk premia: Liquidity\nRegime-switching: macro-conditional factors.\n\n\n\n\n\nBuild a macro liquidity index from multiple public series.\nUse HMM / Markov regimes to label high vs low liquidity states.\nShow how valuation spreads and factor returns behave across liquidity regimes.\nPropose a simple implementable regime-aware factor strategy.\n\n\n\n\n\n\n\n\nHorizon: 1990–2025, monthly frequency (or weekly if feasible).\nMarket: US equities (CRSP/Compustat-type universe); optional robustness on AUS/EU.\n\n\n\n\n\n\nIndividual stock returns \\(r_{i,t}\\), market cap, book equity, earnings, etc.\nCharacteristics \\(X_{i,t}\\):\n\n\\[\n\\text{B/M},\\ \\text{E/P},\\ \\text{P/S},\\ \\text{size},\\ \\beta,\\ \\text{12–1 momentum},\\ \\text{profitability},\\ \\text{volatility}.\n\\]\n\nFactor returns \\(f_t\\):\n\n\\[\n\\text{MKT},\\ \\text{HML},\\ \\text{SMB},\\ \\text{MOM},\\ \\text{Quality},\\ \\text{LowVol}.\n\\]\nDefine a valuation spread:\n\\[\nV_t^{\\text{spread}} = \\text{Long top decile of B/M} - \\text{Short bottom decile}.\n\\]\n\n\n\n\nLet raw macro/liquidity variables at time \\(t\\) be \\(x_{j,t}\\), including:\n\n\n\n\nMoney growth: \\[\\Delta \\log(M2_t)\\]\nFed balance sheet growth: \\[\\Delta \\log(\\text{BS}_t)\\]\n\n\n\n\n\n\nShort rate: \\[i_t\\]\n10Y yield: \\[y_{10,t}\\]\nSlope: \\[\\text{TS}_t = y_{10,t} - i_t\\]\nReal short rate: \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\]\n\n\n\n\n\n\nCredit spread: \\[\\text{CS}_t = y_{\\text{Baa},t} - y_{\\text{Aaa},t}\\]\nVIX level: \\[\\text{VIX}_t\\]\n\n\n\n\n\n\nON RRP usage\nTGA balance\n\n\nAll variables \\(x_{j,t}\\) will then be standardized (e.g., z-scored) and aggregated into a single liquidity index.\n\n\n\n\n\n\n\nLet the raw liquidity indicators be collected in the vector\n\\[\nx_t =\n\\begin{bmatrix}\nx_{1,t} \\\\\nx_{2,t} \\\\\n\\vdots \\\\\nx_{J,t}\n\\end{bmatrix}\n\\in \\mathbb{R}^J.\n\\]\nFor each series \\[x_{j,t}\\], compute the sample mean \\[\n\\mu_j = \\frac{1}{T} \\sum_{t=1}^{T} x_{j,t}\n\\] and variance \\[\n\\sigma_j^2 = \\frac{1}{T-1} \\sum_{t=1}^{T} (x_{j,t} - \\mu_j)^2.\n\\]\nStandardised variables are then \\[\nz_{j,t} = \\frac{x_{j,t} - \\mu_j}{\\sigma_j}, \\qquad j = 1,\\dots,J,\n\\] with stacked vector \\[z_t = (z_{1,t},\\dots,z_{J,t})^\\top\\].\nSeries may be sign-flipped so that higher values of \\[z_{j,t}\\] correspond to easier liquidity; for example, use \\[-r_t^{\\text{real}}\\] instead of \\[r_t^{\\text{real}}\\], \\[-CS_t\\] instead of \\[CS_t\\], and \\[-VIX_t\\] instead of \\[VIX_t\\].\n\n\n\n\nTypical inputs include \\[\\Delta \\log M2_t\\], \\[\\Delta \\log \\text{BS}_t\\], the term spread \\[\\text{TS}_t = y_{10,t} - i_t\\], the real rate \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\], and the credit spread \\[\\text{CS}_t = y_{Baa,t} - y_{Aaa,t}\\].\n\n\n\n\nDefine the covariance matrix \\[\n\\Sigma = \\frac{1}{T} \\sum_{t=1}^{T} z_t z_t^\\top.\n\\]\nLet \\[(\\lambda_k, v_k)\\] solve \\[\\Sigma v_k = \\lambda_k v_k\\], with eigenvalues ordered \\[\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_J\\]. The first principal-component liquidity index is then \\[L_t = v_1^\\top z_t\\]. Alternatively, one may use a fixed-weight index \\[L_t = w^\\top z_t\\].\n\n\n\n\nLet the latent regime variable satisfy \\[s_t \\in \\{1,\\dots,K\\}\\], with regime-conditional dynamics \\[\nL_t \\mid (s_t = k) \\sim \\mathcal{N}(\\mu_k, \\sigma_k^2).\n\\]\nTransition probabilities are \\[\\Pr(s_t = j \\mid s_{t-1} = i) = p_{ij}\\], forming the matrix \\[\nP =\n\\begin{bmatrix}\np_{11} & \\cdots & p_{1K} \\\\\n\\vdots & \\ddots & \\vdots \\\\\np_{K1} & \\cdots & p_{KK}\n\\end{bmatrix},\n\\qquad \\sum_{j=1}^{K} p_{ij} = 1.\n\\]\nFiltered or smoothed regime classification is given by \\[\n\\hat{s}_t = \\arg\\max_{k} \\Pr(s_t = k \\mid L_{1:T}).\n\\]\nDefine the high- and tight-liquidity regimes as \\[k_{\\text{High}} = \\arg\\max_k \\mu_k\\] and \\[k_{\\text{Tight}} = \\arg\\min_k \\mu_k\\], with indicator \\[\nI_t^{\\text{High}} = \\mathbf{1}\\!\\left[\\Pr(s_t = k_{\\text{High}} \\mid L_{1:T}) &gt; 0.5\\right].\n\\]\n\n\n\n\nIn a multivariate setting, define \\[\ny_t =\n\\begin{bmatrix}\nL_t \\\\\nr_t^{\\text{MKT}} \\\\\nr_t^{\\text{HML}} \\\\\n\\text{VIX}_t \\\\\n\\vdots\n\\end{bmatrix},\n\\] with regime-dependent dynamics \\[y_t = A_k y_{t-1} + \\varepsilon_t^{(k)}\\] and \\[\\varepsilon_t^{(k)} \\sim \\mathcal{N}(0,\\Sigma_k)\\] when \\[s_t = k\\].\n\n\n\n\nLet \\[V_t^{\\text{spread}}\\] denote a valuation-spread series (e.g., top–bottom decile). The regime-conditional mean is \\[\n\\bar{V}^{(k)} = \\mathbb{E}[V_t^{\\text{spread}} \\mid s_t = k],\n\\] with sample estimate \\[\n\\hat{\\bar{V}}^{(k)} =\n\\frac{\\sum_{t=1}^{T} V_t^{\\text{spread}} \\mathbf{1}(\\hat{s}_t = k)}\n{\\sum_{t=1}^{T} \\mathbf{1}(\\hat{s}_t = k)}.\n\\]\nDifferences such as \\[\\hat{\\bar{V}}^{(\\text{High})} - \\hat{\\bar{V}}^{(\\text{Tight})}\\] summarise regime effects. Analogously, for factor returns \\[r_t^{(F)}\\], \\[\n\\bar{r}_F^{(k)} = \\mathbb{E}[r_t^{(F)} \\mid s_t = k],\n\\] with Sharpe ratio \\[\\text{SR}_F^{(k)} = \\hat{\\bar{r}}_F^{(k)} / \\hat{\\sigma}_F^{(k)}\\].\n\n\n\n\nContinuous-index predictability is tested via \\[\nr_{t+1}^{(F)} = \\alpha + \\beta L_t + \\gamma^\\top c_t + \\varepsilon_{t+1},\n\\] while regime-based predictability uses \\[\nr_{t+1}^{(F)} =\n\\alpha\n+ \\delta_{\\text{High}} I_t^{\\text{High}}\n+ \\delta_{\\text{Tight}} I_t^{\\text{Tight}}\n+ \\delta_{\\text{Neutral}} I_t^{\\text{Neutral}}\n+ \\varepsilon_{t+1}.\n\\]\n\n\n\n\nAt each time \\[t\\], estimate \\[\nr_{i,t+1} =\n\\alpha_t\n+ \\lambda_{1,t}\\text{Valuation}_{i,t}\n+ \\lambda_{2,t}\\text{Size}_{i,t}\n+ \\lambda_{3,t}\\text{Momentum}_{i,t}\n+ \\dots\n+ \\varepsilon_{i,t+1}.\n\\]\nRegime-conditional slopes satisfy \\[\\bar{\\lambda}_1^{(k)} = \\mathbb{E}[\\lambda_{1,t} \\mid s_t = k]\\], with sample analogue \\[\n\\hat{\\bar{\\lambda}}_1^{(k)} =\n\\frac{\\sum_{t=1}^{T} \\lambda_{1,t} \\mathbf{1}(\\hat{s}_t = k)}\n{\\sum_{t=1}^{T} \\mathbf{1}(\\hat{s}_t = k)}.\n\\]\n\n\n\n\nLet \\[f_t \\in \\mathbb{R}^K\\] denote factor returns and \\[w^{(k)} \\in \\mathbb{R}^K\\] the regime-specific weights. The applied weights are \\[w_t = w^{(\\hat{s}_t)}\\], yielding portfolio return \\[\nR_{p,t+1} = w_t^\\top f_{t+1}.\n\\]\n\n\n\n\nStandard PCA solves:\n\\[\n\\max_{v} \\; \\| X v \\|^{2}\n\\quad \\text{s.t.} \\quad \\| v \\|^{2} = 1\n\\]\n\n\n\nPCA tries to find the direction ( \\(v\\) ) (a unit vector) along which the projected data ( \\(Xv\\) ) has maximum variance.\nVariance of the projection:\n\\[\n\\text{Var}(Xv) = \\frac{1}{n} |Xv|^{2}.\n\\]\nSo maximizing variance is equivalent to maximizing ( \\(|Xv|^{2}\\) ).\n\n\n\n\nWithout this constraint, the problem would blow up:\n\\[\n|X(\\alpha v)|^{2} = \\alpha^{2} |Xv|^{2},\n\\]\nand the maximum would be infinite by choosing ( \\(\\alpha\\) \\(\\infty\\) ).\nSo PCA forces ( \\(v\\) ) to be a direction, not a magnitude → a unit vector.\n\n\n\n\n\\[\n|Xv|^{2} = v^\\top X^\\top X v.\n\\]\nLet:\n\\[\nS = X^\\top X\n\\]\n(the unnormalized covariance matrix). Then PCA solves:\n\\[\n\\max_{v} ; v^\\top S v\n\\quad\\text{s.t.}\\quad v^\\top v = 1.\n\\]\nThis is exactly the Rayleigh quotient.\n\n\n\n\nPCA finds the direction (unit vector) along which the data cloud has maximum spread. That direction is exactly the eigenvector of the covariance matrix with the largest eigenvalue.\n\n\n\n\n\n\n\n\n\nPCA rotates the data into orthogonal directions capturing maximum variance.\nEach principal component is a linear combination of all variables.\nLoadings are typically dense (every variable has some non-zero weight).\nThis makes interpretation difficult:\n\n\n“PC1 of 127 economic multicolinear variables is 127-dimensional mush of everything.”\n\nThis is why PCA is almost never used directly for economic interpretation — PCs are not sparse.\n\n\n\n\nSparse PCA = PCA where most loadings are forced to be zero.\nMathematically:\nStandard PCA solves:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1\n\\]\nSparse PCA adds an L1 penalty (lasso-style sparsity) or constrains the number of non-zero elements:\n\\[\n\\max_{v} \\ |Xv|^2 - \\lambda |v|_1\n\\]\nor equivalently:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1,\\ |v|_1 \\leq c\n\\]\nThis forces:\n\nOnly a small subset of variables to load on each component.\nComponents become interpretable (e.g., PC1 loads on CPI, PCE, core CPI → “inflation”).\n\nThe specific implementation cited (“penalized matrix decomposition” by Witten et al. 2009) solves:\n\\[\n\\max_{u, v} \\ u^\\top X v \\quad\n\\text{s.t. } |u|_2 = 1,\\ |v|_2 = 1,\\ |v|_1 \\le c\n\\]\nThis is a general sparse factor extraction algorithm.\nIntuition: &gt; Sparse PCA forces components to use only the relevant variables, not a smear across 127 series.\n\n\n\n\n\nStandard PCA components ≈ dense, uninterpretable mixtures\nSparse PCA components ≈ targeted clusters of interpretable economic variables\n\n\n\n\n\nTake FRED-MD’s 127 macro series.\nIf you run standard PCA:\n\nPC1 loads on 100+ variables.\nPC2 loads on another 80+.\nInterpreting them is basically hopeless.\n\nSparse PCA, with lasso constraints:\n\nallows PC1 to load only on inflation-related variables\nPC2 to load only on housing-related variables\nPC3 on credit spreads\nPC4 on yields\nPC5 on production\netc.\n\nThis resembles the Stock–Watson macro factor model, but with sparsity for interpretability.\n\n\n\n\nSuppose sparse PCA gives:\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCPI YoY\n0.71\n\n\nCore CPI YoY\n0.68\n\n\nPCE Deflator\n0.66\n\n\nPCE core\n0.64\n\n\nAll other 123 variables\n0\n\n\n\n-&gt; You immediately label PC1 as “inflation factor”.\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nHousing starts\n0.62\n\n\nBuilding permits\n0.59\n\n\nNew homes sold\n0.61\n\n\nMortgage applications\n0.58\n\n\nEverything else\n0\n\n\n\n-&gt; PC2 = “housing & construction cycle”\n\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCorporate spreads (BAA–AAA)\n0.72\n\n\nHigh yield spread\n0.69\n\n\nCommercial paper spread\n0.64\n\n\nOthers\n0\n\n\n\n-&gt; PC3 = “credit stress”\n…and so on.\n\n\n\nSparse PCA essentially performs dimension reduction + variable selection simultaneously.\n\n\n\n\nSparse PCA encourages:\n\ngrouping variables that co-move strongly\ndropping variables unrelated to the theme\nchoosing a small number of representative series\n\nGiven economic data naturally clusters (inflation variables co-move, housing variables co-move), sparse PCA isolates these clusters.\nThis is similar to economic intuition:\n\ninflationary variables form a single latent factor\nspreads form a financial stress factor\nyields form a term-structure factor\nemployment variables form a labor factor\n\nSparse PCA “discovers” these clusters automatically.\n\n\n\n\nSparse PCA → interpretable macro drivers → better regime classification → better factor conditioning.\nEspecially when constructing a macro liquidity index or financial conditions index.\nSparse PCA gives:\n\n\n\nComponent\nInterpretation\n\n\n\n\nPC1\nLiquidity / money conditions\n\n\nPC2\nCredit spreads\n\n\nPC3\nHousing activity\n\n\nPC4\nYield curve / rates\n\n\nPC5\nEmployment\n\n\nPC6\nProduction\n\n\nPC7\nIncome\n\n\nPC8\nMarket stress\n\n\n\nThese are exactly the components McCracken & Ng (2016) find in FRED-MD.\n\n\n\n\n\n\n\n\nDense loadings\nHard to interpret\nPCs are linear mush\n\n\n\n\n\nForces many loadings to zero\nEach PC focuses on a small cluster of variables\nBecomes interpretable as a “macro theme”\nMatches how economists think about the business cycle\nPerfect for regime classification & factor research\n\n\n\n\n\nEconomic data naturally clusters into themes\nSparse PCA forces components to choose only the strongest cluster\nThis matches known macro factors (inflation, employment, credit, etc.)\n\n\n# !pip install pandas pandas_datareader numpy scikit-learn hmmlearn matplotlib\n\nimport pandas as pd\nimport numpy as np\nfrom pandas_datareader import data as pdr\n\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom hmmlearn.hmm import GaussianHMM\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\nstart_date = \"1990-01-01\"\nend_date   = \"2025-12-31\"\n\n# --- FRED series codes ---\nFRED_SERIES = {\n    \"M2SL\":   \"M2SL\",      # M2 money stock (monthly, SA)\n    \"FED_BAL\":\"WALCL\",     # Fed balance sheet total assets (weekly)\n    \"TB3M\":   \"TB3MS\",     # 3-Month T-Bill rate (monthly)\n    \"DGS10\":  \"DGS10\",     # 10-Year Treasury yield (daily -&gt; resample monthly)\n    \"BAA\":    \"BAA\",       # Moody's Baa corporate yield (monthly)\n    \"AAA\":    \"AAA\",       # Moody's Aaa corporate yield (monthly)\n    \"CPI\":    \"CPIAUCSL\",  # CPI index (monthly, SA)\n    \"GDP\":    \"GDP\",       # Nominal GDP (quarterly, SAAR)\n}\n\n# For equity factors: Fama-French via pandas_datareader (famafrench)\nFF_FACTORS_DATASET = \"F-F_Research_Data_5_Factors_2x3\"\n\n\n\n\n\ndef download_fred_series(series_dict, start, end):\n    \"\"\"\n    Download FRED series into a single monthly DataFrame.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Mapping logical_name -&gt; FRED code.\n    \"\"\"\n    dfs = []\n    for name, code in series_dict.items():\n        print(f\"Downloading {name} ({code}) from FRED...\")\n        s = pdr.DataReader(code, \"fred\", start, end)\n        s = s.rename(columns={code: name})\n        dfs.append(s)\n\n    df = pd.concat(dfs, axis=1)\n    # Ensure monthly freq by end-of-month sampling\n    df = df.resample(\"M\").last()\n    return df\n\nmacro_raw = download_fred_series(FRED_SERIES, start_date, end_date)\nmacro_raw.head()\n\nDownloading M2SL (M2SL) from FRED...\nDownloading FED_BAL (WALCL) from FRED...\nDownloading TB3M (TB3MS) from FRED...\nDownloading DGS10 (DGS10) from FRED...\nDownloading BAA (BAA) from FRED...\nDownloading AAA (AAA) from FRED...\nDownloading CPI (CPIAUCSL) from FRED...\nDownloading GDP (GDP) from FRED...\n\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n1990-01-31\n3166.8\nNaN\n7.64\n8.43\n9.94\n8.99\n127.5\n5872.701\n\n\n1990-02-28\n3179.2\nNaN\n7.74\n8.51\n10.14\n9.22\n128.0\nNaN\n\n\n1990-03-31\n3190.1\nNaN\n7.90\n8.65\n10.21\n9.37\n128.6\nNaN\n\n\n1990-04-30\n3201.6\nNaN\n7.77\n9.04\n10.30\n9.46\n128.9\n5960.028\n\n\n1990-05-31\n3200.6\nNaN\n7.74\n8.60\n10.41\n9.47\n129.1\nNaN\n\n\n\n\n\n\n\n\nmacro_raw.tail()\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\nNaN\n4.00\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n$ x_t =\n\\[\\begin{bmatrix}\nx_{1,t} \\\nx_{2,t} \\\n\\vdots \\\nx_{J,t}\n\\end{bmatrix}\\]\n\nx_{J,t} \\end{bmatrix} ^J $\n\nMoney & balance sheet growth: \\(\\Delta \\log(\\cdot)\\)\nTerm spread: \\(y_{10} - i_{3m}\\)\nReal short rate: \\(i_{3m} - \\pi_{\\text{year-on-year}}\\)\nCredit spread: \\(\\text{BAA} - \\text{AAA}\\)\netc.​\n\n\ndef build_liquidity_proxies(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Growth of M2 and Fed balance sheet\n    df[\"dlog_M2\"] = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"] = np.log(df[\"FED_BAL\"]).diff()\n\n    # 2. Term structure: 10Y - 3M\n    df[\"term_spread\"] = df[\"DGS10\"] - df[\"TB3M\"]\n\n    # 3. Year-on-year inflation (CPI yoy)\n    df[\"infl_yoy\"] = np.log(df[\"CPI\"]).diff(12)\n\n    # 4. Real short rate: nominal 3M - inflation\n    df[\"real_rate\"] = df[\"TB3M\"] - (100 * df[\"infl_yoy\"])  # TB3M in %, infl_yoy in log -&gt; approx*100\n\n    # 5. Credit spread: BAA - AAA\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # Drop rows with NaNs from diff(12) etc.\n    proxies = df[[\"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\",\n                  \"real_rate\", \"credit_spread\"]].dropna()\n\n    return proxies\n\nliquidity_proxies = build_liquidity_proxies(macro_raw)\n\n\nliquidity_proxies.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n\n\n2025-07-31\n0.003930\n-0.002950\n0.12\n1.554846\n0.65\n\n\n2025-08-31\n0.003607\n-0.005918\n0.11\n1.223147\n0.65\n\n\n2025-09-30\n0.004698\n0.000759\n0.24\n0.942084\n0.62\n\n\n\n\n\n\n\n\n\n\nWe want higher \\(z_{j,t}\\) – to mean easier liquidity.\n\nGrowth of M2 / Fed balance sheet: already “easier = higher value” -&gt; keeping sign\nTerm spread: steeper (more positive) often associated with easier conditions -&gt; keeping sign\nReal rate: easier liquidity when real rates are low -&gt; flipping sign\nCredit spread: easier liquidity when spreads are tight (low) -&gt; flipping sign\n\n\ndef standardize_and_signflip(proxies: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Standardize each column and flip signs so that higher z_{j,t}\n    always corresponds to easier liquidity.\n    \"\"\"\n    z = proxies.copy()\n\n    # Standardize\n    scaler = StandardScaler()\n    z_vals = scaler.fit_transform(z)\n    z = pd.DataFrame(z_vals, index=z.index, columns=z.columns)\n\n    # Sign flips so \"higher\" = easier liquidity\n    # dlog_M2: easier when higher -&gt; keep\n    # dlog_FED_BAL: easier when higher -&gt; keep\n    # term_spread: easier when steeper -&gt; keep (or adjust, depending on your view)\n    # real_rate: easier when more negative -&gt; flip sign\n    # credit_spread: easier when lower -&gt; flip sign\n\n    sign_flips = {\n        \"dlog_M2\": +1,\n        \"dlog_FED_BAL\": +1,\n        \"term_spread\": +1,\n        \"real_rate\": -1,\n        \"credit_spread\": -1,\n    }\n\n    for col, sgn in sign_flips.items():\n        z[col] = sgn * z[col]\n\n    return z\n\nz_t = standardize_and_signflip(liquidity_proxies)\n\n\nz_t.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n-0.377813\n-0.314009\n-0.968292\n-1.362581\n0.639849\n\n\n2025-06-30\n0.052579\n-0.226920\n-1.085244\n-1.211510\n0.786085\n\n\n2025-07-31\n-0.163048\n-0.257126\n-0.999479\n-1.193172\n0.883576\n\n\n2025-08-31\n-0.215601\n-0.326453\n-1.007276\n-1.031235\n0.883576\n\n\n2025-09-30\n-0.038105\n-0.170518\n-0.905917\n-0.894019\n0.956694\n\n\n\n\n\n\n\n\n\n\n\npd.Series(z_t.index).describe()\n\ncount                              273\nmean     2014-05-31 08:42:11.868131840\nmin                2003-01-31 00:00:00\n25%                2008-09-30 00:00:00\n50%                2014-05-31 00:00:00\n75%                2020-01-31 00:00:00\nmax                2025-09-30 00:00:00\nName: DATE, dtype: object\n\n\n\nz_t.values\n\narray([[ 0.11561096, -0.81061174,  1.11346119,  0.322683  , -0.40817503],\n       [ 0.20510346,  0.11034372,  0.8873532 ,  0.50696464, -0.23756644],\n       [-0.24145055, -0.0901311 ,  1.01210244,  0.46925598, -0.11570316],\n       ...,\n       [-0.163048  , -0.25712611, -0.999479  , -1.1931718 ,  0.88357574],\n       [-0.2156006 , -0.32645261, -1.00727583, -1.03123533,  0.88357574],\n       [-0.03810504, -0.17051847, -0.90591708, -0.89401934,  0.95669371]])\n\n\n\ndef build_sparse_pca_liquidity_index(z_df: pd.DataFrame,\n                                     alpha: float = 1.0,\n                                     random_state: int = 42) -&gt; pd.Series:\n    \"\"\"\n    Apply SparsePCA with 1 component to z_{j,t} to obtain L_t.\n    \"\"\"\n    spca = SparsePCA(\n        n_components=1,\n        alpha=alpha,\n        random_state=random_state\n    )\n\n    # Fit on standardized proxies\n    L_scores = spca.fit_transform(z_df.values)  # shape (T, 1)\n    L = pd.Series(L_scores.flatten(), index=z_df.index, name=\"L\")\n\n    # Optional: enforce that L_t is positively correlated with dlog_M2\n    # corr = np.corrcoef(L, z_df[\"dlog_FED_BAL\"])[0, 1]\n    # if corr &lt; 0:\n    #     L = -L\n\n    print(\"SparsePCA components (loadings):\")\n    for coef, col in zip(spca.components_[0], z_df.columns):\n        print(f\"  {col}: {coef:.3f}\")\n\n    return L\n\nL_t = build_sparse_pca_liquidity_index(z_t, alpha=0.5)\n\nSparsePCA components (loadings):\n  dlog_M2: 0.430\n  dlog_FED_BAL: 0.526\n  term_spread: 0.492\n  real_rate: 0.389\n  credit_spread: -0.381\n\n\n\nL_t.head()\n\nDATE\n2003-01-31    0.447153\n2003-02-28    0.861556\n2003-03-31    0.567234\n2003-04-30    0.979006\n2003-05-31    0.696500\nFreq: M, Name: L, dtype: float64\n\n\n\nL_t.plot(title=\"Sparse PCA Liquidity Index L(t)\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n​Fit a Gaussian HMM on the liquidity index. 2 or 3 regimes ?\n\nfrom hmmlearn.hmm import GaussianHMM\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\ndef fit_hmm_on_liquidity(\n    L: pd.Series,\n    n_states: int = 3,\n    covariance_type: str = \"full\",\n    random_state: int = 42,\n    standardize: bool = True\n):\n    \"\"\"\n    Fit a Gaussian HMM on L(t) and return the model and a DataFrame\n    with inferred regimes and posterior probabilities.\n\n    Parameters\n    ----------\n    L : pd.Series\n        Liquidity index (indexed by date).\n    n_states : int\n        Number of hidden states (regimes).\n    standardize : bool\n        If True, standardize L before fitting the HMM.\n\n    Returns\n    -------\n    model : GaussianHMM\n    df_regime : pd.DataFrame\n        Columns: L, state, p_state_k, state_label\n    \"\"\"\n\n    # 1) Prepare X\n    X = L.values.reshape(-1, 1)\n\n    scaler = None\n    if standardize:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n    # 2) Fit HMM\n    model = GaussianHMM(\n        n_components=n_states,\n        covariance_type=covariance_type,\n        random_state=random_state,\n        n_iter=500\n    )\n    model.fit(X)\n\n    hidden_states = model.predict(X)\n    post_probs = model.predict_proba(X)  # T x n_states\n\n    # 3) Build output DataFrame on original index\n    df_regime = pd.DataFrame(\n        {\"L\": L, \"state\": hidden_states},\n        index=L.index\n    )\n    for k in range(n_states):\n        df_regime[f\"p_state_{k}\"] = post_probs[:, k]\n\n    # 4) Use state means to order regimes\n    means = pd.Series(model.means_.flatten(), index=range(n_states))\n\n    print(\"HMM state means (unsorted, in fitted scale):\")\n    for k, m in means.items():\n        print(f\"  state {k}: mean L = {m:.3f}\")\n\n    # Order states from low liquidity to high liquidity\n    ordering = means.sort_values().index.tolist()\n\n    state_label_map = {}\n    if n_states == 3:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"Neutral\"\n        state_label_map[ordering[2]] = \"High\"\n    elif n_states == 2:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"High\"\n    else:\n        # Generic labels if you ever use more states\n        for i, s in enumerate(ordering):\n            state_label_map[s] = f\"Regime_{i}\"\n\n    df_regime[\"state_label\"] = df_regime[\"state\"].map(state_label_map)\n\n    return model, df_regime\n\n\nn_states = 3\nmodel, regimes_df = fit_hmm_on_liquidity(L_t,n_states)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.160\n  state 1: mean L = -0.106\n  state 2: mean L = 2.718\n\n\n\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# -----------------------------\n# TOP PANEL — L(t) + shading\n# -----------------------------\nax[0].plot(regimes_df.index, regimes_df[\"L\"], color=\"black\", lw=1.2)\nax[0].set_title(\"Sparse PCA Liquidity Index L(t) with Regime Shading\")\n\n# Define plain colors for each regime\nregime_colors = {\n    \"Tight\":   \"green\",\n    \"Neutral\": \"gray\",\n    \"High\":    \"crimson\",\n}\n\nshade_alpha = 0.2  # semi-opaque\n\nfor label, color in regime_colors.items():\n    mask = regimes_df[\"state_label\"] == label\n\n    in_region = False\n    start = None\n\n    for i in range(len(mask)):\n        if mask.iloc[i] and not in_region:\n            in_region = True\n            start = regimes_df.index[i]\n\n        elif not mask.iloc[i] and in_region:\n            in_region = False\n            end = regimes_df.index[i]\n            ax[0].axvspan(start, end, color=color, alpha=shade_alpha, linewidth=0)\n\n    # If region extends to the end of the sample\n    if in_region:\n        ax[0].axvspan(start, regimes_df.index[-1], color=color, alpha=shade_alpha, linewidth=0)\n\n# Legend for regimes (shaded colors)\nfrom matplotlib.lines import Line2D\nlegend_patches = [\n    Line2D([0], [0], color=\"green\",   lw=6, alpha=shade_alpha, label=\"Tight\"),\n    Line2D([0], [0], color=\"gray\",    lw=6, alpha=shade_alpha, label=\"Neutral\"),\n    Line2D([0], [0], color=\"crimson\", lw=6, alpha=shade_alpha, label=\"High\"),\n]\nax[0].legend(handles=legend_patches, loc=\"upper left\")\n\n# -----------------------------\n# BOTTOM PANEL — High regime probability\n# -----------------------------\nif \"p_state_0\" in regimes_df.columns:\n    high_state_id = regimes_df.groupby(\"state_label\")[\"state\"].first()[\"High\"]\n    ax[1].plot(regimes_df.index,\n               regimes_df[f\"p_state_{high_state_id}\"],\n               color=\"crimson\", lw=1.5)\n    ax[1].set_title(\"Posterior Probability of High Liquidity Regime\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\nThese five factors extend the original 3-factor model to better explain cross-sectional stock returns.\nThey are:\n\nRm–Rf — Market excess return\nSMB — Size (Small Minus Big)\nHML — Value (High Book/Market minus Low)\nRMW — Profitability (Robust Minus Weak)\nCMA — Investment (Conservative Minus Aggressive)\n\nLet’s break each down precisely.\n\n\n\n\\[\nR_{m}-R_{f}\n\\] The return of the broad market minus the risk-free rate.\n\nPositive → market went up more than cash.\nNegative → market underperformed cash/T-bills.\n\nIn above table:\n\nLast 12 months: +17.54% → very strong bull market year.\nLast 3 months: +7.40% → strong quarter.\nOctober 2025: +1.95% → moderate positive month.\n\n\n\n\n\n\\[\n\\text{SMB} = R_{\\text{small}} - R_{\\text{big}}\n\\]\n\nPositive → small caps outperform large caps.\nNegative → large caps outperform small caps.\n\nIn above table:\n\nLast 12 months: –10.71% → a massive large-cap dominance year.\nThis is consistent with mega-cap tech leadership.\n\n\n\n\n\n\\[\n\\text{HML} = R_{\\text{value}} - R_{\\text{growth}}\n\\]\n\nPositive → value outperforms growth.\nNegative → growth outperforms value.\n\nIn above table:\n\nLast 12 months: –2.36% → growth beat value.\nOctober 2025: –3.10% → especially growth-heavy month.\n\nThis aligns with liquidity-driven growth leadership.\n\n\n\n\n\\[\n\\text{RMW} = R_{\\text{robust}} - R_{\\text{weak}}\n\\]\nRobust = high operating profitability. Weak = low profitability.\n\nPositive → profitable companies outperform weak ones.\nNegative → weak/low-profit companies outperform.\n\nIn above table:\n\nLast 12 months: –13.60% → very unusual.\nThis means unprofitable firms outperformed profitable ones over the year.\n\nThat usually happens in:\n\nearly speculative bubbles\nliquidity-driven rallies\nretail-led tech/small-cap frenzies\nAI/futuristic narrative periods\n\n\n\n\n\n\\[\n\\text{CMA} = R_{\\text{conservative}} - R_{\\text{aggressive}}\n\\]\n\nConservative = firms investing slowly\nAggressive = firms aggressively expanding assets\n\nHigh investment → lower expected returns historically (consistent with q-theory: empire-building destroys value).\n\nPositive → conservative firms outperform heavy spenders.\nNegative → aggressive investment firms outperform.\n\nIn above table:\n\nLast 12 months: –10.46%\nLast 3 months: –4.47%\nOctober 2025: –4.03%\n\nInterpretation: Aggressively investing companies — think AI, R&D, biotech, high-growth tech — massively outperformed low-investment firms.\n\n\n\n\n\n\n\n\nFactor\nSign\nInterpretation\n\n\n\n\nRm–Rf: +\nStrong bull market\n\n\n\nSMB: –\nMega-caps beat small caps massively\n\n\n\nHML: –\nGrowth beat value\n\n\n\nRMW: –\nLow-profit firms beat profitable firms\n\n\n\nCMA: –\nAggressive-investment firms beat conservative ones\n\n\n\n\nThis is the textbook signature of a liquidity-driven growth/tech momentum regime, almost identical to:\n\n1998–1999 Dotcom\n2019–2021 QE wave\n2023–2024 AI mega-cap boom\n\nIt means:\n\n\n\n\nlarge\ngrowth\nunprofitable\nhigh-spending\nhigh-duration tech/AI/future-theme stocks\n\n\n\n\nThis is exactly the kind of environment where:\n\nValue fails\nSmall cap fails\nProfitability fails\nInvestment works negatively\nGrowth dominates\nMega-caps dominate\n\n\ndef download_ff_factors(start, end):\n    \"\"\"\n    Download monthly Fama-French 5 factors (2x3) using pandas_datareader.\n    \"\"\"\n    print(\"Downloading Fama-French factors (famafrench)...\")\n    ff_raw = pdr.DataReader(\n        name=FF_FACTORS_DATASET,\n        data_source=\"famafrench\",\n        start=start,\n        end=end\n    )[0]  # table [0] contains the data\n\n    ff = (ff_raw\n          .divide(100)  # convert from % to decimal\n          .reset_index(names=\"date\")\n          .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n          .rename(str.lower, axis=\"columns\")\n          .rename(columns={\"mkt-rf\": \"mkt_excess\"}))\n\n    ff = ff.set_index(\"date\")\n    return ff\n\nff_factors = download_ff_factors(start_date, end_date)\n\nDownloading Fama-French factors (famafrench)...\n\n\nFutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n&lt;ipython-input-69-55f7e03990a5&gt;:6: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n\n\n\nff_factors.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-06-01\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-07-01\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-01\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-09-01\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-10-01\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Fix Fama–French dates (shift one month backward)\nff_adj = ff_factors.copy()\n\n# FF data comes labeled as YYYY-MM-01 meaning return for previous month.\n# So shift index back to previous month-end.\nff_adj.index = (ff_adj.index.to_period(\"M\") - 1).to_timestamp(\"M\")\n\nff_adj.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined = reg.join(ff_adj, how=\"inner\")\n\n\ncombined.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-12-31\n-1.651021\n1\n0.162158\n0.837762\n0.000079\nTight\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n-1.496425\n0\n0.837514\n0.162233\n0.000253\nTight\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n-1.643180\n1\n0.162188\n0.837738\n0.000075\nTight\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n-1.627975\n0\n0.837473\n0.162258\n0.000269\nTight\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n-1.400432\n1\n0.162220\n0.837714\n0.000067\nTight\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n-1.561611\n0\n0.837459\n0.162284\n0.000258\nTight\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n-1.387057\n1\n0.162245\n0.837689\n0.000066\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n-1.482573\n0\n0.837437\n0.162308\n0.000255\nTight\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n-1.482572\n1\n0.162251\n0.837658\n0.000091\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n-1.251226\n0\n0.836442\n0.162316\n0.001242\nTight\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n\ndf2 = combined.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by *lagged* regime:\")\nprint(means_by_regime)\n\nMean factor returns by *lagged* regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nTight            -0.005439 -0.005360  0.000540 -0.003311\nNeutral           0.003200  0.002938  0.001051 -0.000056\nHigh              0.003327  0.001228  0.005127  0.003575\n\n\n\n\n\nThis is reverse of what you’d expect. In a tight liquidity regime, \\(R^{smb}\\) and \\(R^{hml}\\) should be deeply positive. You’d expect small under-valued companies to outperform big companies with premium valuation.\nLet’s go back to the \\(L_t\\) and understand what’s going on\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L_t.index,\n    y=L_t.values,\n    hue=regimes_df[\"state\"],\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution of L(t) by HMM State\")\nax[1].set_xlabel(\"L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the KDE (bottom panel):\n\nStates 0 (green) and 1 (yellow) heavily overlap\nOnly state 2 (red) is clearly separated — the “high liquidity spike”\nThis corresponds to crisis/QE events (2008, 2020), where liquidity jumps dramatically\n\nSo the natural clustering is:\n\nOne large cluster (normal-tight-neutral combined)\nOne rare extreme spike cluster\n\nYour HMM is being forced to split the unimodal bulk distribution into two states, resulting in:\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\nI initially thought alpha=0.5 in SparsePCA might be the cause. But the plot shows:\n\nL(t) has a healthy dynamic range across time (≈ −2.5 to +12)\nThe high liquidity cluster is clear and RARE\nThe rest of L(t) is genuinely one blob\n\nIf Sparse PCA was “overshrinking,” L(t) would be compressed; but it is not.\n\n\n\n\nIs Liquidity is fundamentally spiky ?\nWhy ?\nAm I defining Liquidity incorrectly ?\n\nFed/Treasury continuously debased currency throughout 2008-2019 period. Liquidity injection (bailing out banks during GFC/2008, ZIRP era, M2 stimulius of 2019 are visible ones). How would I capture this events as proxy of “liquidity” ?\n\n\nRight now your \\(L_t\\) is basically a short-horizon “flow” liquidity shock index.\n\n\nWhereas I’m looking for slow, structural debasement / regime of easy money (2008–2019, QE, ZIRP, etc.).\n\n\nThose are not the same object.\n\nBecause I use 1-month growth rates \\(\\Delta \\log(\\cdot)\\), my index reacts to spikes / changes, not the level of the monetary stock.\nI need to factor in more factors to the liquidity index such as -\n\nLevel of money / balance sheet relative to trend / GDP, not just monthly change\nProlonged ZIRP / negative real rate period\nExcess liquidity over economic activity\n\n\n\n\n\n\nExamples (all can be pulled from FRED):\nLog level vs pre-2008 trend\nLet \\(m_t = \\log M2_t\\). Fit a linear trend on a pre-QE baseline (say 1985–2007):\n\\(m_t \\approx a + bt \\quad (t \\le 2007)\\)\nThen define excess money stock:\n\\(EM_t = m_t - (a + bt)\\)\nAfter 2008, \\(EM_t\\) becomes increasingly positive if M2 grows above its historical trend.\nSimilarly for the Fed Balance Sheet, let \\(b_t = \\log(\\text{FedBal}_t)\\):\n\\(EB_t = b_t - (\\alpha + \\beta t)\\) (pre-2007 trend)\n\nExcess liquidity versus real economy\nUse real GDP series (e.g., GDPC1) and define:\n\\(EL_t = \\log M2_t - \\log GDP_t\\)\nOr measure multi-year excess growth:\n\\(EL_t(3y) = \\log M2_t - \\log M2_{t-36} - (\\log GDP_t - \\log GDP_{t-36})\\)\n\nZIRP / negative real-rate indicator\n\\(D_t^{ZIRP} = 1(r_t^{real} &lt; 0)\\)\nFrom 2009–2015, this should be mostly 1.\n\n\n\n\nInstead of only:\n\\(x_t = \\{ \\Delta \\log M2_t,\\; \\Delta \\log FedBal_t,\\; TS_t,\\; r_t^{real},\\; CS_t \\}\\)\nLet’s expand to:\n\\(x_t^{aug} = \\{\n\\Delta \\log M2_t,\\;\n\\Delta \\log FedBal_t,\\;\nTS_t,\\;\nr_t^{real},\\;\nCS_t,\\;\nEM_t,\\;\nEB_t,\\;\nEL_t,\\;\nEL_t(3y),\\;\nD_t^{ZIRP}\n\\}\\)\nThen standardize and perform PCA / SparsePCA on this.\n\nPC1 (or a combination) loading heavily on \\(EM\\), \\(EB\\), \\(EL\\), \\(D_t^{ZIRP}\\) → structural easy-money regime\n\nPC2 (or your current PC1) loading on short-horizon changes → flow / shock liquidity\n\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\nNaN\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\nNaN\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29511.664\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\nNaN\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\nNaN\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29825.182\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\nNaN\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\nNaN\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n30042.113\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\nNaN\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\nNaN\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30485.729\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\nNaN\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\nNaN\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# 1. Start from the original GDP values only (drop NaNs)\ngdp_series = macro_raw[\"GDP\"].dropna()\n\n# 2. Collapse to unique quarter-end values\n#    - If GDP is timestamped at quarter *start* (e.g. 2025-04-01),\n#      this will move it to the quarter end (2025-06-30) and give unique labels.\ngdp_q = gdp_series.resample(\"Q\").last()   # quarterly, at quarter-end (e.g. 2025-03-31, 2025-06-30, ...)\n\n# 3. Upsample to monthly and forward-fill within the quarter\ngdp_m = gdp_q.resample(\"M\").ffill()\n\n# 4. Assign back into macro_raw, aligning on the monthly index\nmacro_raw[\"GDP\"] = gdp_m\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\n28708.161\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\n29147.044\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29147.044\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\n29147.044\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\n29511.664\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29511.664\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\n29511.664\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\n29825.182\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n29825.182\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\n29825.182\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\n30042.113\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30042.113\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\n30042.113\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\n30485.729\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndef build_liquidity_proxies_augmented(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Flow proxies\n    df[\"dlog_M2\"]       = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"]  = np.log(df[\"FED_BAL\"]).diff()\n    df[\"term_spread\"]   = df[\"DGS10\"] - df[\"TB3M\"]\n    df[\"infl_yoy\"]      = np.log(df[\"CPI\"]).diff(12)\n    df[\"real_rate\"]     = df[\"TB3M\"] - 100 * df[\"infl_yoy\"]\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # 2. Levels\n    df[\"log_M2\"]      = np.log(df[\"M2SL\"])\n    df[\"log_FED_BAL\"] = np.log(df[\"FED_BAL\"])\n\n    # Use data up to 2007-12-31 to fit trends\n    pre = df.loc[: \"2007-12-31\"].copy()\n\n    # --- Trend for M2 ---\n    pre_m2 = pre[\"log_M2\"].dropna()\n    t_m2   = np.arange(len(pre_m2))\n    coefs_M2 = np.polyfit(t_m2, pre_m2.values, deg=1)\n\n    t_full = np.arange(len(df))\n    trend_M2 = np.polyval(coefs_M2, t_full)\n    df[\"EM\"] = df[\"log_M2\"] - trend_M2\n\n    # --- Trend for Fed balance sheet ---\n    pre_fb = pre[\"log_FED_BAL\"].dropna()\n    t_fb   = np.arange(len(pre_fb))\n    coefs_FB = np.polyfit(t_fb, pre_fb.values, deg=1)\n\n    trend_FB = np.polyval(coefs_FB, t_full)\n    df[\"EB\"] = df[\"log_FED_BAL\"] - trend_FB\n\n    # 3. Excess liquidity vs GDP over 3y\n    df[\"log_GDP\"] = np.log(df[\"GDP\"])\n    df[\"EL_3y\"] = (df[\"log_M2\"] - df[\"log_M2\"].shift(36)) - \\\n                  (df[\"log_GDP\"] - df[\"log_GDP\"].shift(36))\n\n    # 4. ZIRP dummy\n    df[\"ZIRP_dummy\"] = (df[\"real_rate\"] &lt; 0).astype(int)\n\n    # 5. Build augmented proxy matrix\n    cols_aug = [\n        \"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\", \"real_rate\", \"credit_spread\",\n        \"EM\", \"EB\", \"EL_3y\", \"ZIRP_dummy\"\n    ]\n\n    return df[cols_aug].dropna()\n\n\nliquidity_proxies_aug = build_liquidity_proxies_augmented(macro_raw)\n\n\nliquidity_proxies_aug.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n0.004077\n-0.006080\n-0.91\n2.316574\n0.74\n0.204864\n0.811903\n-0.194283\n0\n\n\n2024-10-31\n0.002378\n-0.009447\n-0.23\n1.971101\n0.68\n0.202921\n0.798940\n-0.199645\n0\n\n\n2024-11-30\n0.004673\n-0.015569\n-0.24\n1.742012\n0.64\n0.203273\n0.779855\n-0.203197\n0\n\n\n2024-12-31\n0.000775\n-0.002781\n0.31\n1.438113\n0.60\n0.199728\n0.773558\n-0.186129\n0\n\n\n2025-01-31\n0.003164\n-0.009892\n0.37\n1.254690\n0.62\n0.198572\n0.760151\n-0.188367\n0\n\n\n2025-02-28\n0.003354\n-0.007668\n0.02\n1.444603\n0.60\n0.197605\n0.748966\n-0.189520\n0\n\n\n2025-03-31\n0.003338\n-0.003828\n0.03\n1.822893\n0.64\n0.196623\n0.741623\n-0.177663\n0\n\n\n2025-04-30\n0.006165\n-0.004606\n-0.04\n1.903069\n0.73\n0.198467\n0.733501\n-0.172818\n0\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n0.196757\n0.724600\n-0.167478\n0\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n0.197692\n0.719428\n-0.150768\n0\n\n\n\n\n\n\n\n\nz1_t = standardize_and_signflip(liquidity_proxies_aug)\nz1_t.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n-0.139814\n-0.331324\n-1.812889\n-1.577541\n0.673692\n0.888682\n0.786734\n-2.353880\n-1.527525\n\n\n2024-10-31\n-0.414988\n-0.409551\n-1.282837\n-1.408779\n0.819794\n0.868074\n0.764262\n-2.404333\n-1.527525\n\n\n2024-11-30\n-0.043456\n-0.551814\n-1.290631\n-1.296871\n0.917195\n0.871811\n0.731177\n-2.437752\n-1.527525\n\n\n2024-12-31\n-0.674370\n-0.254667\n-0.861913\n-1.148418\n1.014596\n0.834206\n0.720261\n-2.277159\n-1.527525\n\n\n2025-01-31\n-0.287635\n-0.419885\n-0.815143\n-1.058817\n0.965896\n0.821943\n0.697018\n-2.298214\n-1.527525\n\n\n2025-02-28\n-0.256970\n-0.368230\n-1.087964\n-1.151588\n1.014596\n0.811689\n0.677630\n-2.309059\n-1.527525\n\n\n2025-03-31\n-0.259533\n-0.278983\n-1.080169\n-1.336381\n0.917195\n0.801267\n0.664900\n-2.197496\n-1.527525\n\n\n2025-04-30\n0.198084\n-0.297077\n-1.134734\n-1.375546\n0.698042\n0.820833\n0.650819\n-2.151913\n-1.527525\n\n\n2025-05-31\n-0.377318\n-0.315174\n-0.978836\n-1.374952\n0.649342\n0.802693\n0.635389\n-2.101665\n-1.527525\n\n\n2025-06-30\n0.050761\n-0.228533\n-1.095759\n-1.223790\n0.795443\n0.812604\n0.626423\n-1.944440\n-1.527525\n\n\n\n\n\n\n\n\nL1_t = build_sparse_pca_liquidity_index(z1_t, alpha=0.5)\nL1_t.tail(10)\n\nSparsePCA components (loadings):\n  dlog_M2: -0.132\n  dlog_FED_BAL: -0.159\n  term_spread: -0.389\n  real_rate: -0.515\n  credit_spread: 0.046\n  EM: -0.114\n  EB: -0.134\n  EL_3y: -0.518\n  ZIRP_dummy: -0.490\n\n\nDATE\n2024-09-30    3.347080\n2024-10-31    3.143188\n2024-11-30    3.088484\n2024-12-31    2.811365\n2025-01-31    2.736110\n2025-02-28    2.887801\n2025-03-31    2.906425\n2025-04-30    2.856634\n2025-05-31    2.850481\n2025-06-30    2.674924\nFreq: M, Name: L, dtype: float64\n\n\n\nL1_t.plot(title=\"Sparse PCA Liquidity Index L(t) - Augmented w/ Flow & Stock Indicators\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel, regimes_aug_df = fit_hmm_on_liquidity(L1_t,n_states=2)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.484\n  state 1: mean L = 1.536\n\n\n\nregime_aug_df.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_aug_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined_aug = reg.join(ff_adj, how=\"inner\")\n\n\ncombined_aug.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n-0.0100\n-0.0089\n0.0086\n-0.0148\n0.0098\n0.0039\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n0.0649\n0.0459\n0.0015\n-0.0231\n-0.0205\n0.0040\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n-0.0315\n-0.0383\n-0.0300\n0.0189\n-0.0121\n0.0037\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\ndf2 = combined_aug.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by Liquidity Regime:\")\nprint(means_by_regime)\n\nMean factor returns by Liquidity Regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nHigh             -0.004519 -0.001700  0.001564 -0.003006\nTight             0.001896  0.000038  0.002920  0.001302\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\nHigh Liquidity\nTight Liquidity\nInterpretation\n\n\n\n\nSMB\nNegative\nPositive\nSmall caps thrive when liquidity tightens\n\n\nHML\nNegative\nSlightly positive\nValue improves in tight regimes\n\n\nRMW\nPositive\nMore positive\nProfitability is the strongest cross-regime performer\n\n\nCMA\nNegative\nPositive\nConservative investment becomes favored when liquidity tightens\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L1_t.index,\n    y=L1_t.values,\n    hue=regimes_aug_df[\"state\"],\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Augmented Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"Augmented L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_aug_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution/KDE of Augmented L(t) by HMM State\")\nax[1].set_xlabel(\"Augmented L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn KDE of augmented liquidity index \\(L_t^{aug}\\), clusters are clearly separated, unlike last one\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndef parse_yyyy_mm_to_date(s: str):\n    \"\"\"\n    Convert 'YYYY.MM' to a proper datetime.\n    Handles the special case where October appears as YYYY.1 instead of YYYY.10.\n    \"\"\"\n    s = str(s).strip()\n    parts = s.split(\".\")\n    if len(parts) != 2:\n        raise ValueError(f\"Unexpected date format: {s}\")\n    \n    year = int(parts[0])\n    mm = parts[1]\n\n    # Fix: \"1\" should be \"10\" (October)\n    if mm == \"1\":\n        month = 10\n    else:\n        month = int(mm)\n\n    return pd.Timestamp(year=year, month=month, day=1)\n\n# --- Load CAPE CSV ---\nsp500_cape_raw = pd.read_csv(\"data/sp500-cape-1990-2025.csv\")\n\n# Clean + parse\nsp500_cape_raw[\"date\"] = sp500_cape_raw[\"Date\"].apply(parse_yyyy_mm_to_date)\n\nsp500_cape_raw = (\n    sp500_cape_raw[[\"date\", \"SP500\", \"CAPE\"]]\n    .set_index(\"date\")\n    .sort_index()\n)\n\n# Convert to month-end aligned CAPE series\nsp500_cape_m = sp500_cape_raw.resample(\"M\").last()\nsp500_cape_m.rename(columns={\"CAPE\": \"cape\", \"SP500\": \"sp500\"}, inplace=True)\n\nsp500_cape_m.tail()\n\n\n\n\n\n\n\n\nsp500\ncape\n\n\ndate\n\n\n\n\n\n\n2025-08-31\n6408.95\n37.85\n\n\n2025-09-30\n6584.02\n38.58\n\n\n2025-10-31\n6735.69\n39.21\n\n\n2025-11-30\n6740.89\n39.12\n\n\n2025-12-31\n6812.63\n39.42\n\n\n\n\n\n\n\n\n\n\nLong-Term Average/Median: The S&P 500’s historical long-term average CAPE ratio is around 16-18, with a median value of approximately 16.04 (since 1881). This provides a central benchmark.\nHigh Valuation (Expensive): A CAPE ratio that is notably higher than the historical average (e.g., above 25 or 30) is considered indicative of a highly valued or overvalued market. Historically, values exceeding 30 have only occurred during major market peaks like the 1929 crash, the dot-com bubble in the late 1990s (peaking over 44), and the post-pandemic period around 2021. Such high readings have typically been followed by periods of lower-than-average, or even negative, long-term returns.\nLow Valuation (Cheap): A CAPE ratio well below the historical average (e.g., below 15 or 10) suggests an undervalued or cheap market. Historically, such levels have been associated with minimal downside risk and higher average long-term returns.\nCould have calculated\nV_spread = (x - baseline) / baseline # baseline = 16\nProblem is, if the whole world “structurally” shifted (e.g. post-1990 lower inflation), “16” stops being meaningful.\nWe are instead measuring,\n\\[\nV_t = \\frac{CAPE_t - \\mathrm{median}(CAPE)}{\\mathrm{IQR}(CAPE)}\n\\]\n\ndef build_valuation_spread_baseline(\n    val_df: pd.DataFrame,\n    metric: str = \"cape\",\n    baseline: float = None,\n    scale: bool = True,\n) -&gt; pd.Series:\n    x = val_df[metric].astype(float)\n\n    if baseline is None:\n        # robust long-run baseline\n        median = x.median()\n        iqr = x.quantile(0.75) - x.quantile(0.25)\n        baseline = median\n        denom = iqr if scale and iqr &gt; 0 else baseline\n    else:\n        denom = baseline if scale else 1.0\n\n    diff = x - baseline          \n\n    V_spread = diff / denom if scale else diff\n    V_spread.name = \"V_spread\"\n    return V_spread\n\n# Example: baseline at 16.04 (long-run median)\nV_spread_t = build_valuation_spread_baseline(\n    sp500_cape_m,\n    metric=\"cape\",\n    baseline=None,\n    scale=True,   # or False if you prefer \"CAPE points below baseline\"\n)\n\nV_spread_t.tail()\n\ndate\n2025-08-31    1.259380\n2025-09-30    1.338771\n2025-10-31    1.407287\n2025-11-30    1.397499\n2025-12-31    1.430125\nFreq: M, Name: V_spread, dtype: float64\n\n\n\nV_spread_t.plot(figsize=(14, 4), title=\"S&P 500 Valuation Spread (cheap vs own history)\")\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\ncombined_aug.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\n# regimes_df: index is month-end\nreg = combined_aug.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Align valuation spread to month-end, then join\nV_spread_m = V_spread_t.resample(\"M\").last()\n\ncombined = (\n    reg\n    .join(V_spread_m.to_frame(), how=\"inner\")\n)\n\ncombined.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\nV_spread\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n1.187602\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n0.925503\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n0.690593\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n0.958129\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n1.070147\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Valuation spread series, e.g. top–bottom decile)\n$ V_t^{} $\n(Regime-conditional mean valuation spread)\n$ {V}^{(k)} = $\n$ ^{(k)} = {_{t=1}^T (_t = k)} $\n(Difference in spreads between regimes)\n$ ^{()} - ^{()} $\n(Factor return in regime ( k ))\n$ r_t^{(F)} $\n$ {r}_F^{(k)} = $\n$ F^{(k)} = {{t=1}^T (_t = k)} $\n(Regime-specific Sharpe ratio)\n$ _F^{(k)} = $\n\n# Regime-conditional mean valuation spread:  V̄^(k)\nval_means_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .mean()\n)\n\nval_stds_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .std()\n)\n\nprint(\"Regime-conditional mean valuation spread:\")\nprint(val_means_by_regime)\n\nprint(\"\\nRegime-conditional valuation z-score std dev:\")\nprint(val_stds_by_regime)\n\n# Difference in spreads between regimes:  V̂^(High) − V̂^(Tight)\nif {\"High\", \"Tight\"}.issubset(val_means_by_regime.index):\n    spread_diff = (\n        val_means_by_regime[\"High\"] - val_means_by_regime[\"Tight\"]\n    )\n    print(\"\\nDifference in spreads High − Tight:\")\n    print(spread_diff)\n\nRegime-conditional mean valuation spread:\nstate_label\nHigh     0.380274\nTight   -0.049269\nName: V_spread, dtype: float64\n\nRegime-conditional valuation z-score std dev:\nstate_label\nHigh     0.406851\nTight    0.551632\nName: V_spread, dtype: float64\n\nDifference in spreads High − Tight:\n0.42954211798360936\n\n\n\n\n\n\nTight liquidity = cheaper markets (value regime),\nHigh liquidity = expensive markets (growth regime).\n\n\nMarkets are ~0.43σ more expensive in High liquidity regimes than Tight liquidity regimes.\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# -------------------------------------------------------------------\n# Assume you already have:\n#   L_t              : pd.Series (liquidity index), monthly, name \"L\"\n#   sp500_cape_m     : DataFrame with column \"sp500\" (S&P 500)\n#   V_spread_t       : pd.Series (\"V_spread\"), monthly valuation cheapness\n#   regimes_df       : DataFrame with column \"state_label\"\n# -------------------------------------------------------------------\n\n# 1) Build a common monthly dataframe\ndf_plot = pd.DataFrame(index=L_t.index.union(sp500_cape_m.index).union(V_spread_t.index))\n\ndf_plot[\"L\"]         = L_t.reindex(df_plot.index)\ndf_plot[\"spx_price\"] = sp500_cape_m[\"sp500\"].reindex(df_plot.index)\ndf_plot[\"V_spread\"]  = V_spread_t.reindex(df_plot.index)\n\n# Attach regimes (forward-fill in case of any slight index mismatch)\nreg_states = combined[\"state_label\"].reindex(df_plot.index).ffill()\ndf_plot[\"state_label\"] = reg_states\n\n# Optional: drop rows before we have all three series\ndf_plot = df_plot.dropna(subset=[\"L\", \"spx_price\", \"V_spread\", \"state_label\"])\n\n# 2) Helper: find contiguous regime segments for shading\ndef get_regime_segments(states: pd.Series):\n    \"\"\"\n    Given a Series of regime labels indexed by date,\n    return list of (start_date, end_date, label) segments\n    where the label is constant.\n    \"\"\"\n    segments = []\n    prev_label = None\n    start_date = None\n\n    for dt, label in states.items():\n        if prev_label is None:\n            prev_label = label\n            start_date = dt\n            continue\n\n        if label != prev_label:\n            # segment ended at previous date\n            end_date = dt\n            segments.append((start_date, end_date, prev_label))\n            start_date = dt\n            prev_label = label\n\n    # last segment\n    if prev_label is not None and start_date is not None:\n        segments.append((start_date, states.index[-1], prev_label))\n\n    return segments\n\nsegments = get_regime_segments(df_plot[\"state_label\"])\n\n# 3) Colors for regimes\nregime_colors = {\n    \"High\":    \"#ffe0e0\",  # pale red\n    \"Neutral\": \"#f7f7f7\",  # light gray\n    \"Tight\":   \"#d1ffd1\",  # pale green\n}\n\n# 4) Plot: 3 stacked subplots with shared x-axis\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True,\n                         gridspec_kw={\"height_ratios\": [1, 1, 1]})\n\nax_L, ax_spx, ax_val = axes\n\n# --- Shading first so lines render on top ---\nfor (start, end, label) in segments:\n    color = regime_colors.get(label, \"#f0f0f0\")\n    for ax in axes:\n        ax.axvspan(start, end, color=color, alpha=0.3)\n\n# --- Panel 1: Liquidity index L(t) ---\nax_L.plot(df_plot.index, df_plot[\"L\"], label=\"L(t)\", linewidth=1.5)\nax_L.set_ylabel(\"L(t)\")\nax_L.set_title(\"Liquidity Index, S&P 500, and Valuation Spread by Liquidity Regime\")\nax_L.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 2: S&P 500 level ---\nax_spx.plot(df_plot.index, df_plot[\"spx_price\"], label=\"S&P 500\", linewidth=1.5)\nax_spx.set_ylabel(\"S&P 500\")\nax_spx.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 3: Valuation cheapness (V_spread) ---\nax_val.plot(df_plot.index, df_plot[\"V_spread\"], label=\"V_spread (cheap vs baseline)\", linewidth=1.5)\nax_val.axhline(0.0, color=\"black\", linewidth=1, linestyle=\"--\", alpha=0.7)\nax_val.set_ylabel(\"V_spread\")\nax_val.set_xlabel(\"Date\")\nax_val.grid(True, linestyle=\"--\", alpha=0.4)\n\n# Optional legends\nax_L.legend(loc=\"upper left\")\nax_spx.legend(loc=\"upper left\")\nax_val.legend(loc=\"upper left\")\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#high-level-structure-of-the-paper",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#high-level-structure-of-the-paper",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Story – why this matters (intro + conclusion)\nData – what you measure (markets, factors, liquidity proxies)\nModels – how you measure (liquidity index, regimes, cross-sectional tests)\nPortfolio – what to do with it (conditional allocation"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#abstract-150200-words",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#abstract-150200-words",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "The “valuations don’t matter in infinite money printing” meme.\nYour construction of a liquidity index and liquidity regimes.\nKey findings:\nValuations do matter, but conditional on liquidity regimes.\nGrowth dominates in high-liquidity, negative-real-rate regimes.\nValue premia revive after QT / real rate normalization.\n\nPortfolio takeaway: a simple regime-aware factor-tilt model."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#motivation",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#motivation",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Narrative: post-GFC QE, ZIRP, NIRP, 2010–2021 “TINA”, tech bubble 2.0.\n\n“We live in a world of infinite money printing, so valuations don’t matter.”\n\nResearch question: &gt; Do valuations truly die under abundant liquidity, or do they merely become regime-dependent?\nDo valuation spreads (cheap vs expensive deciles) expand under high-liquidity regimes and compress under tight liquidity?\nHow do factor premia (value, momentum, quality, low vol) vary conditional on liquidity regimes?\nCan a macro-liquidity index predict the timing of factor rotations (growth vs value, long-duration vs short-duration equities)?\nDoes a regime-aware factor allocation deliver better risk-adjusted returns than a static factor mix?"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#intuition",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#intuition",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Valuation spread: the gap between cheap and expensive stocks (e.g., top vs bottom decile by B/M).\nLiquidity regime: periods of systematically easy vs tight financial conditions driven by monetary and fiscal variables."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#literature-sketch-very-short",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#literature-sketch-very-short",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Systemic Strategy: Factor investing: value, momentum, quality.\nRisk Factors and Risk premia: Liquidity\nRegime-switching: macro-conditional factors."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#contributions",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#contributions",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Build a macro liquidity index from multiple public series.\nUse HMM / Markov regimes to label high vs low liquidity states.\nShow how valuation spreads and factor returns behave across liquidity regimes.\nPropose a simple implementable regime-aware factor strategy."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#data-variables",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#data-variables",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Horizon: 1990–2025, monthly frequency (or weekly if feasible).\nMarket: US equities (CRSP/Compustat-type universe); optional robustness on AUS/EU.\n\n\n\n\n\n\nIndividual stock returns \\(r_{i,t}\\), market cap, book equity, earnings, etc.\nCharacteristics \\(X_{i,t}\\):\n\n\\[\n\\text{B/M},\\ \\text{E/P},\\ \\text{P/S},\\ \\text{size},\\ \\beta,\\ \\text{12–1 momentum},\\ \\text{profitability},\\ \\text{volatility}.\n\\]\n\nFactor returns \\(f_t\\):\n\n\\[\n\\text{MKT},\\ \\text{HML},\\ \\text{SMB},\\ \\text{MOM},\\ \\text{Quality},\\ \\text{LowVol}.\n\\]\nDefine a valuation spread:\n\\[\nV_t^{\\text{spread}} = \\text{Long top decile of B/M} - \\text{Short bottom decile}.\n\\]\n\n\n\n\nLet raw macro/liquidity variables at time \\(t\\) be \\(x_{j,t}\\), including:\n\n\n\n\nMoney growth: \\[\\Delta \\log(M2_t)\\]\nFed balance sheet growth: \\[\\Delta \\log(\\text{BS}_t)\\]\n\n\n\n\n\n\nShort rate: \\[i_t\\]\n10Y yield: \\[y_{10,t}\\]\nSlope: \\[\\text{TS}_t = y_{10,t} - i_t\\]\nReal short rate: \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\]\n\n\n\n\n\n\nCredit spread: \\[\\text{CS}_t = y_{\\text{Baa},t} - y_{\\text{Aaa},t}\\]\nVIX level: \\[\\text{VIX}_t\\]\n\n\n\n\n\n\nON RRP usage\nTGA balance\n\n\nAll variables \\(x_{j,t}\\) will then be standardized (e.g., z-scored) and aggregated into a single liquidity index."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#math",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#math",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Let the raw liquidity indicators be collected in the vector\n\\[\nx_t =\n\\begin{bmatrix}\nx_{1,t} \\\\\nx_{2,t} \\\\\n\\vdots \\\\\nx_{J,t}\n\\end{bmatrix}\n\\in \\mathbb{R}^J.\n\\]\nFor each series \\[x_{j,t}\\], compute the sample mean \\[\n\\mu_j = \\frac{1}{T} \\sum_{t=1}^{T} x_{j,t}\n\\] and variance \\[\n\\sigma_j^2 = \\frac{1}{T-1} \\sum_{t=1}^{T} (x_{j,t} - \\mu_j)^2.\n\\]\nStandardised variables are then \\[\nz_{j,t} = \\frac{x_{j,t} - \\mu_j}{\\sigma_j}, \\qquad j = 1,\\dots,J,\n\\] with stacked vector \\[z_t = (z_{1,t},\\dots,z_{J,t})^\\top\\].\nSeries may be sign-flipped so that higher values of \\[z_{j,t}\\] correspond to easier liquidity; for example, use \\[-r_t^{\\text{real}}\\] instead of \\[r_t^{\\text{real}}\\], \\[-CS_t\\] instead of \\[CS_t\\], and \\[-VIX_t\\] instead of \\[VIX_t\\].\n\n\n\n\nTypical inputs include \\[\\Delta \\log M2_t\\], \\[\\Delta \\log \\text{BS}_t\\], the term spread \\[\\text{TS}_t = y_{10,t} - i_t\\], the real rate \\[r_t^{\\text{real}} = i_t - \\pi_t^e\\], and the credit spread \\[\\text{CS}_t = y_{Baa,t} - y_{Aaa,t}\\].\n\n\n\n\nDefine the covariance matrix \\[\n\\Sigma = \\frac{1}{T} \\sum_{t=1}^{T} z_t z_t^\\top.\n\\]\nLet \\[(\\lambda_k, v_k)\\] solve \\[\\Sigma v_k = \\lambda_k v_k\\], with eigenvalues ordered \\[\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_J\\]. The first principal-component liquidity index is then \\[L_t = v_1^\\top z_t\\]. Alternatively, one may use a fixed-weight index \\[L_t = w^\\top z_t\\].\n\n\n\n\nLet the latent regime variable satisfy \\[s_t \\in \\{1,\\dots,K\\}\\], with regime-conditional dynamics \\[\nL_t \\mid (s_t = k) \\sim \\mathcal{N}(\\mu_k, \\sigma_k^2).\n\\]\nTransition probabilities are \\[\\Pr(s_t = j \\mid s_{t-1} = i) = p_{ij}\\], forming the matrix \\[\nP =\n\\begin{bmatrix}\np_{11} & \\cdots & p_{1K} \\\\\n\\vdots & \\ddots & \\vdots \\\\\np_{K1} & \\cdots & p_{KK}\n\\end{bmatrix},\n\\qquad \\sum_{j=1}^{K} p_{ij} = 1.\n\\]\nFiltered or smoothed regime classification is given by \\[\n\\hat{s}_t = \\arg\\max_{k} \\Pr(s_t = k \\mid L_{1:T}).\n\\]\nDefine the high- and tight-liquidity regimes as \\[k_{\\text{High}} = \\arg\\max_k \\mu_k\\] and \\[k_{\\text{Tight}} = \\arg\\min_k \\mu_k\\], with indicator \\[\nI_t^{\\text{High}} = \\mathbf{1}\\!\\left[\\Pr(s_t = k_{\\text{High}} \\mid L_{1:T}) &gt; 0.5\\right].\n\\]\n\n\n\n\nIn a multivariate setting, define \\[\ny_t =\n\\begin{bmatrix}\nL_t \\\\\nr_t^{\\text{MKT}} \\\\\nr_t^{\\text{HML}} \\\\\n\\text{VIX}_t \\\\\n\\vdots\n\\end{bmatrix},\n\\] with regime-dependent dynamics \\[y_t = A_k y_{t-1} + \\varepsilon_t^{(k)}\\] and \\[\\varepsilon_t^{(k)} \\sim \\mathcal{N}(0,\\Sigma_k)\\] when \\[s_t = k\\].\n\n\n\n\nLet \\[V_t^{\\text{spread}}\\] denote a valuation-spread series (e.g., top–bottom decile). The regime-conditional mean is \\[\n\\bar{V}^{(k)} = \\mathbb{E}[V_t^{\\text{spread}} \\mid s_t = k],\n\\] with sample estimate \\[\n\\hat{\\bar{V}}^{(k)} =\n\\frac{\\sum_{t=1}^{T} V_t^{\\text{spread}} \\mathbf{1}(\\hat{s}_t = k)}\n{\\sum_{t=1}^{T} \\mathbf{1}(\\hat{s}_t = k)}.\n\\]\nDifferences such as \\[\\hat{\\bar{V}}^{(\\text{High})} - \\hat{\\bar{V}}^{(\\text{Tight})}\\] summarise regime effects. Analogously, for factor returns \\[r_t^{(F)}\\], \\[\n\\bar{r}_F^{(k)} = \\mathbb{E}[r_t^{(F)} \\mid s_t = k],\n\\] with Sharpe ratio \\[\\text{SR}_F^{(k)} = \\hat{\\bar{r}}_F^{(k)} / \\hat{\\sigma}_F^{(k)}\\].\n\n\n\n\nContinuous-index predictability is tested via \\[\nr_{t+1}^{(F)} = \\alpha + \\beta L_t + \\gamma^\\top c_t + \\varepsilon_{t+1},\n\\] while regime-based predictability uses \\[\nr_{t+1}^{(F)} =\n\\alpha\n+ \\delta_{\\text{High}} I_t^{\\text{High}}\n+ \\delta_{\\text{Tight}} I_t^{\\text{Tight}}\n+ \\delta_{\\text{Neutral}} I_t^{\\text{Neutral}}\n+ \\varepsilon_{t+1}.\n\\]\n\n\n\n\nAt each time \\[t\\], estimate \\[\nr_{i,t+1} =\n\\alpha_t\n+ \\lambda_{1,t}\\text{Valuation}_{i,t}\n+ \\lambda_{2,t}\\text{Size}_{i,t}\n+ \\lambda_{3,t}\\text{Momentum}_{i,t}\n+ \\dots\n+ \\varepsilon_{i,t+1}.\n\\]\nRegime-conditional slopes satisfy \\[\\bar{\\lambda}_1^{(k)} = \\mathbb{E}[\\lambda_{1,t} \\mid s_t = k]\\], with sample analogue \\[\n\\hat{\\bar{\\lambda}}_1^{(k)} =\n\\frac{\\sum_{t=1}^{T} \\lambda_{1,t} \\mathbf{1}(\\hat{s}_t = k)}\n{\\sum_{t=1}^{T} \\mathbf{1}(\\hat{s}_t = k)}.\n\\]\n\n\n\n\nLet \\[f_t \\in \\mathbb{R}^K\\] denote factor returns and \\[w^{(k)} \\in \\mathbb{R}^K\\] the regime-specific weights. The applied weights are \\[w_t = w^{(\\hat{s}_t)}\\], yielding portfolio return \\[\nR_{p,t+1} = w_t^\\top f_{t+1}.\n\\]"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#pca-and-sparse-pca",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#pca-and-sparse-pca",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Standard PCA solves:\n\\[\n\\max_{v} \\; \\| X v \\|^{2}\n\\quad \\text{s.t.} \\quad \\| v \\|^{2} = 1\n\\]\n\n\n\nPCA tries to find the direction ( \\(v\\) ) (a unit vector) along which the projected data ( \\(Xv\\) ) has maximum variance.\nVariance of the projection:\n\\[\n\\text{Var}(Xv) = \\frac{1}{n} |Xv|^{2}.\n\\]\nSo maximizing variance is equivalent to maximizing ( \\(|Xv|^{2}\\) ).\n\n\n\n\nWithout this constraint, the problem would blow up:\n\\[\n|X(\\alpha v)|^{2} = \\alpha^{2} |Xv|^{2},\n\\]\nand the maximum would be infinite by choosing ( \\(\\alpha\\) \\(\\infty\\) ).\nSo PCA forces ( \\(v\\) ) to be a direction, not a magnitude → a unit vector.\n\n\n\n\n\\[\n|Xv|^{2} = v^\\top X^\\top X v.\n\\]\nLet:\n\\[\nS = X^\\top X\n\\]\n(the unnormalized covariance matrix). Then PCA solves:\n\\[\n\\max_{v} ; v^\\top S v\n\\quad\\text{s.t.}\\quad v^\\top v = 1.\n\\]\nThis is exactly the Rayleigh quotient.\n\n\n\n\nPCA finds the direction (unit vector) along which the data cloud has maximum spread. That direction is exactly the eigenvector of the covariance matrix with the largest eigenvalue."
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#sparse-pca",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#sparse-pca",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "PCA rotates the data into orthogonal directions capturing maximum variance.\nEach principal component is a linear combination of all variables.\nLoadings are typically dense (every variable has some non-zero weight).\nThis makes interpretation difficult:\n\n\n“PC1 of 127 economic multicolinear variables is 127-dimensional mush of everything.”\n\nThis is why PCA is almost never used directly for economic interpretation — PCs are not sparse.\n\n\n\n\nSparse PCA = PCA where most loadings are forced to be zero.\nMathematically:\nStandard PCA solves:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1\n\\]\nSparse PCA adds an L1 penalty (lasso-style sparsity) or constrains the number of non-zero elements:\n\\[\n\\max_{v} \\ |Xv|^2 - \\lambda |v|_1\n\\]\nor equivalently:\n\\[\n\\max_{v} \\ |Xv|^2 \\quad \\text{s.t. } |v|_2 = 1,\\ |v|_1 \\leq c\n\\]\nThis forces:\n\nOnly a small subset of variables to load on each component.\nComponents become interpretable (e.g., PC1 loads on CPI, PCE, core CPI → “inflation”).\n\nThe specific implementation cited (“penalized matrix decomposition” by Witten et al. 2009) solves:\n\\[\n\\max_{u, v} \\ u^\\top X v \\quad\n\\text{s.t. } |u|_2 = 1,\\ |v|_2 = 1,\\ |v|_1 \\le c\n\\]\nThis is a general sparse factor extraction algorithm.\nIntuition: &gt; Sparse PCA forces components to use only the relevant variables, not a smear across 127 series.\n\n\n\n\n\nStandard PCA components ≈ dense, uninterpretable mixtures\nSparse PCA components ≈ targeted clusters of interpretable economic variables\n\n\n\n\n\nTake FRED-MD’s 127 macro series.\nIf you run standard PCA:\n\nPC1 loads on 100+ variables.\nPC2 loads on another 80+.\nInterpreting them is basically hopeless.\n\nSparse PCA, with lasso constraints:\n\nallows PC1 to load only on inflation-related variables\nPC2 to load only on housing-related variables\nPC3 on credit spreads\nPC4 on yields\nPC5 on production\netc.\n\nThis resembles the Stock–Watson macro factor model, but with sparsity for interpretability.\n\n\n\n\nSuppose sparse PCA gives:\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCPI YoY\n0.71\n\n\nCore CPI YoY\n0.68\n\n\nPCE Deflator\n0.66\n\n\nPCE core\n0.64\n\n\nAll other 123 variables\n0\n\n\n\n-&gt; You immediately label PC1 as “inflation factor”.\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nHousing starts\n0.62\n\n\nBuilding permits\n0.59\n\n\nNew homes sold\n0.61\n\n\nMortgage applications\n0.58\n\n\nEverything else\n0\n\n\n\n-&gt; PC2 = “housing & construction cycle”\n\n\n\n\n\n\n\nVariable\nLoading\n\n\n\n\nCorporate spreads (BAA–AAA)\n0.72\n\n\nHigh yield spread\n0.69\n\n\nCommercial paper spread\n0.64\n\n\nOthers\n0\n\n\n\n-&gt; PC3 = “credit stress”\n…and so on.\n\n\n\nSparse PCA essentially performs dimension reduction + variable selection simultaneously.\n\n\n\n\nSparse PCA encourages:\n\ngrouping variables that co-move strongly\ndropping variables unrelated to the theme\nchoosing a small number of representative series\n\nGiven economic data naturally clusters (inflation variables co-move, housing variables co-move), sparse PCA isolates these clusters.\nThis is similar to economic intuition:\n\ninflationary variables form a single latent factor\nspreads form a financial stress factor\nyields form a term-structure factor\nemployment variables form a labor factor\n\nSparse PCA “discovers” these clusters automatically.\n\n\n\n\nSparse PCA → interpretable macro drivers → better regime classification → better factor conditioning.\nEspecially when constructing a macro liquidity index or financial conditions index.\nSparse PCA gives:\n\n\n\nComponent\nInterpretation\n\n\n\n\nPC1\nLiquidity / money conditions\n\n\nPC2\nCredit spreads\n\n\nPC3\nHousing activity\n\n\nPC4\nYield curve / rates\n\n\nPC5\nEmployment\n\n\nPC6\nProduction\n\n\nPC7\nIncome\n\n\nPC8\nMarket stress\n\n\n\nThese are exactly the components McCracken & Ng (2016) find in FRED-MD.\n\n\n\n\n\n\n\n\nDense loadings\nHard to interpret\nPCs are linear mush\n\n\n\n\n\nForces many loadings to zero\nEach PC focuses on a small cluster of variables\nBecomes interpretable as a “macro theme”\nMatches how economists think about the business cycle\nPerfect for regime classification & factor research\n\n\n\n\n\nEconomic data naturally clusters into themes\nSparse PCA forces components to choose only the strongest cluster\nThis matches known macro factors (inflation, employment, credit, etc.)\n\n\n# !pip install pandas pandas_datareader numpy scikit-learn hmmlearn matplotlib\n\nimport pandas as pd\nimport numpy as np\nfrom pandas_datareader import data as pdr\n\nfrom sklearn.decomposition import SparsePCA\nfrom sklearn.preprocessing import StandardScaler\n\nfrom hmmlearn.hmm import GaussianHMM\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#config-date-range-series",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#config-date-range-series",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "start_date = \"1990-01-01\"\nend_date   = \"2025-12-31\"\n\n# --- FRED series codes ---\nFRED_SERIES = {\n    \"M2SL\":   \"M2SL\",      # M2 money stock (monthly, SA)\n    \"FED_BAL\":\"WALCL\",     # Fed balance sheet total assets (weekly)\n    \"TB3M\":   \"TB3MS\",     # 3-Month T-Bill rate (monthly)\n    \"DGS10\":  \"DGS10\",     # 10-Year Treasury yield (daily -&gt; resample monthly)\n    \"BAA\":    \"BAA\",       # Moody's Baa corporate yield (monthly)\n    \"AAA\":    \"AAA\",       # Moody's Aaa corporate yield (monthly)\n    \"CPI\":    \"CPIAUCSL\",  # CPI index (monthly, SA)\n    \"GDP\":    \"GDP\",       # Nominal GDP (quarterly, SAAR)\n}\n\n# For equity factors: Fama-French via pandas_datareader (famafrench)\nFF_FACTORS_DATASET = \"F-F_Research_Data_5_Factors_2x3\""
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#download-macro-liquidity-variables-from-fred",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#download-macro-liquidity-variables-from-fred",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "def download_fred_series(series_dict, start, end):\n    \"\"\"\n    Download FRED series into a single monthly DataFrame.\n\n    Parameters\n    ----------\n    series_dict : dict\n        Mapping logical_name -&gt; FRED code.\n    \"\"\"\n    dfs = []\n    for name, code in series_dict.items():\n        print(f\"Downloading {name} ({code}) from FRED...\")\n        s = pdr.DataReader(code, \"fred\", start, end)\n        s = s.rename(columns={code: name})\n        dfs.append(s)\n\n    df = pd.concat(dfs, axis=1)\n    # Ensure monthly freq by end-of-month sampling\n    df = df.resample(\"M\").last()\n    return df\n\nmacro_raw = download_fred_series(FRED_SERIES, start_date, end_date)\nmacro_raw.head()\n\nDownloading M2SL (M2SL) from FRED...\nDownloading FED_BAL (WALCL) from FRED...\nDownloading TB3M (TB3MS) from FRED...\nDownloading DGS10 (DGS10) from FRED...\nDownloading BAA (BAA) from FRED...\nDownloading AAA (AAA) from FRED...\nDownloading CPI (CPIAUCSL) from FRED...\nDownloading GDP (GDP) from FRED...\n\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n1990-01-31\n3166.8\nNaN\n7.64\n8.43\n9.94\n8.99\n127.5\n5872.701\n\n\n1990-02-28\n3179.2\nNaN\n7.74\n8.51\n10.14\n9.22\n128.0\nNaN\n\n\n1990-03-31\n3190.1\nNaN\n7.90\n8.65\n10.21\n9.37\n128.6\nNaN\n\n\n1990-04-30\n3201.6\nNaN\n7.77\n9.04\n10.30\n9.46\n128.9\n5960.028\n\n\n1990-05-31\n3200.6\nNaN\n7.74\n8.60\n10.41\n9.47\n129.1\nNaN\n\n\n\n\n\n\n\n\nmacro_raw.tail()\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\nNaN\n4.00\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n$ x_t =\n\\[\\begin{bmatrix}\nx_{1,t} \\\nx_{2,t} \\\n\\vdots \\\nx_{J,t}\n\\end{bmatrix}\\]\n\nx_{J,t} \\end{bmatrix} ^J $\n\nMoney & balance sheet growth: \\(\\Delta \\log(\\cdot)\\)\nTerm spread: \\(y_{10} - i_{3m}\\)\nReal short rate: \\(i_{3m} - \\pi_{\\text{year-on-year}}\\)\nCredit spread: \\(\\text{BAA} - \\text{AAA}\\)\netc.​\n\n\ndef build_liquidity_proxies(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Growth of M2 and Fed balance sheet\n    df[\"dlog_M2\"] = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"] = np.log(df[\"FED_BAL\"]).diff()\n\n    # 2. Term structure: 10Y - 3M\n    df[\"term_spread\"] = df[\"DGS10\"] - df[\"TB3M\"]\n\n    # 3. Year-on-year inflation (CPI yoy)\n    df[\"infl_yoy\"] = np.log(df[\"CPI\"]).diff(12)\n\n    # 4. Real short rate: nominal 3M - inflation\n    df[\"real_rate\"] = df[\"TB3M\"] - (100 * df[\"infl_yoy\"])  # TB3M in %, infl_yoy in log -&gt; approx*100\n\n    # 5. Credit spread: BAA - AAA\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # Drop rows with NaNs from diff(12) etc.\n    proxies = df[[\"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\",\n                  \"real_rate\", \"credit_spread\"]].dropna()\n\n    return proxies\n\nliquidity_proxies = build_liquidity_proxies(macro_raw)\n\n\nliquidity_proxies.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n\n\n2025-07-31\n0.003930\n-0.002950\n0.12\n1.554846\n0.65\n\n\n2025-08-31\n0.003607\n-0.005918\n0.11\n1.223147\n0.65\n\n\n2025-09-30\n0.004698\n0.000759\n0.24\n0.942084\n0.62\n\n\n\n\n\n\n\n\n\n\nWe want higher \\(z_{j,t}\\) – to mean easier liquidity.\n\nGrowth of M2 / Fed balance sheet: already “easier = higher value” -&gt; keeping sign\nTerm spread: steeper (more positive) often associated with easier conditions -&gt; keeping sign\nReal rate: easier liquidity when real rates are low -&gt; flipping sign\nCredit spread: easier liquidity when spreads are tight (low) -&gt; flipping sign\n\n\ndef standardize_and_signflip(proxies: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Standardize each column and flip signs so that higher z_{j,t}\n    always corresponds to easier liquidity.\n    \"\"\"\n    z = proxies.copy()\n\n    # Standardize\n    scaler = StandardScaler()\n    z_vals = scaler.fit_transform(z)\n    z = pd.DataFrame(z_vals, index=z.index, columns=z.columns)\n\n    # Sign flips so \"higher\" = easier liquidity\n    # dlog_M2: easier when higher -&gt; keep\n    # dlog_FED_BAL: easier when higher -&gt; keep\n    # term_spread: easier when steeper -&gt; keep (or adjust, depending on your view)\n    # real_rate: easier when more negative -&gt; flip sign\n    # credit_spread: easier when lower -&gt; flip sign\n\n    sign_flips = {\n        \"dlog_M2\": +1,\n        \"dlog_FED_BAL\": +1,\n        \"term_spread\": +1,\n        \"real_rate\": -1,\n        \"credit_spread\": -1,\n    }\n\n    for col, sgn in sign_flips.items():\n        z[col] = sgn * z[col]\n\n    return z\n\nz_t = standardize_and_signflip(liquidity_proxies)\n\n\nz_t.tail()\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2025-05-31\n-0.377813\n-0.314009\n-0.968292\n-1.362581\n0.639849\n\n\n2025-06-30\n0.052579\n-0.226920\n-1.085244\n-1.211510\n0.786085\n\n\n2025-07-31\n-0.163048\n-0.257126\n-0.999479\n-1.193172\n0.883576\n\n\n2025-08-31\n-0.215601\n-0.326453\n-1.007276\n-1.031235\n0.883576\n\n\n2025-09-30\n-0.038105\n-0.170518\n-0.905917\n-0.894019\n0.956694\n\n\n\n\n\n\n\n\n\n\n\npd.Series(z_t.index).describe()\n\ncount                              273\nmean     2014-05-31 08:42:11.868131840\nmin                2003-01-31 00:00:00\n25%                2008-09-30 00:00:00\n50%                2014-05-31 00:00:00\n75%                2020-01-31 00:00:00\nmax                2025-09-30 00:00:00\nName: DATE, dtype: object\n\n\n\nz_t.values\n\narray([[ 0.11561096, -0.81061174,  1.11346119,  0.322683  , -0.40817503],\n       [ 0.20510346,  0.11034372,  0.8873532 ,  0.50696464, -0.23756644],\n       [-0.24145055, -0.0901311 ,  1.01210244,  0.46925598, -0.11570316],\n       ...,\n       [-0.163048  , -0.25712611, -0.999479  , -1.1931718 ,  0.88357574],\n       [-0.2156006 , -0.32645261, -1.00727583, -1.03123533,  0.88357574],\n       [-0.03810504, -0.17051847, -0.90591708, -0.89401934,  0.95669371]])\n\n\n\ndef build_sparse_pca_liquidity_index(z_df: pd.DataFrame,\n                                     alpha: float = 1.0,\n                                     random_state: int = 42) -&gt; pd.Series:\n    \"\"\"\n    Apply SparsePCA with 1 component to z_{j,t} to obtain L_t.\n    \"\"\"\n    spca = SparsePCA(\n        n_components=1,\n        alpha=alpha,\n        random_state=random_state\n    )\n\n    # Fit on standardized proxies\n    L_scores = spca.fit_transform(z_df.values)  # shape (T, 1)\n    L = pd.Series(L_scores.flatten(), index=z_df.index, name=\"L\")\n\n    # Optional: enforce that L_t is positively correlated with dlog_M2\n    # corr = np.corrcoef(L, z_df[\"dlog_FED_BAL\"])[0, 1]\n    # if corr &lt; 0:\n    #     L = -L\n\n    print(\"SparsePCA components (loadings):\")\n    for coef, col in zip(spca.components_[0], z_df.columns):\n        print(f\"  {col}: {coef:.3f}\")\n\n    return L\n\nL_t = build_sparse_pca_liquidity_index(z_t, alpha=0.5)\n\nSparsePCA components (loadings):\n  dlog_M2: 0.430\n  dlog_FED_BAL: 0.526\n  term_spread: 0.492\n  real_rate: 0.389\n  credit_spread: -0.381\n\n\n\nL_t.head()\n\nDATE\n2003-01-31    0.447153\n2003-02-28    0.861556\n2003-03-31    0.567234\n2003-04-30    0.979006\n2003-05-31    0.696500\nFreq: M, Name: L, dtype: float64\n\n\n\nL_t.plot(title=\"Sparse PCA Liquidity Index L(t)\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n​Fit a Gaussian HMM on the liquidity index. 2 or 3 regimes ?\n\nfrom hmmlearn.hmm import GaussianHMM\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\ndef fit_hmm_on_liquidity(\n    L: pd.Series,\n    n_states: int = 3,\n    covariance_type: str = \"full\",\n    random_state: int = 42,\n    standardize: bool = True\n):\n    \"\"\"\n    Fit a Gaussian HMM on L(t) and return the model and a DataFrame\n    with inferred regimes and posterior probabilities.\n\n    Parameters\n    ----------\n    L : pd.Series\n        Liquidity index (indexed by date).\n    n_states : int\n        Number of hidden states (regimes).\n    standardize : bool\n        If True, standardize L before fitting the HMM.\n\n    Returns\n    -------\n    model : GaussianHMM\n    df_regime : pd.DataFrame\n        Columns: L, state, p_state_k, state_label\n    \"\"\"\n\n    # 1) Prepare X\n    X = L.values.reshape(-1, 1)\n\n    scaler = None\n    if standardize:\n        scaler = StandardScaler()\n        X = scaler.fit_transform(X)\n\n    # 2) Fit HMM\n    model = GaussianHMM(\n        n_components=n_states,\n        covariance_type=covariance_type,\n        random_state=random_state,\n        n_iter=500\n    )\n    model.fit(X)\n\n    hidden_states = model.predict(X)\n    post_probs = model.predict_proba(X)  # T x n_states\n\n    # 3) Build output DataFrame on original index\n    df_regime = pd.DataFrame(\n        {\"L\": L, \"state\": hidden_states},\n        index=L.index\n    )\n    for k in range(n_states):\n        df_regime[f\"p_state_{k}\"] = post_probs[:, k]\n\n    # 4) Use state means to order regimes\n    means = pd.Series(model.means_.flatten(), index=range(n_states))\n\n    print(\"HMM state means (unsorted, in fitted scale):\")\n    for k, m in means.items():\n        print(f\"  state {k}: mean L = {m:.3f}\")\n\n    # Order states from low liquidity to high liquidity\n    ordering = means.sort_values().index.tolist()\n\n    state_label_map = {}\n    if n_states == 3:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"Neutral\"\n        state_label_map[ordering[2]] = \"High\"\n    elif n_states == 2:\n        state_label_map[ordering[0]] = \"Tight\"\n        state_label_map[ordering[1]] = \"High\"\n    else:\n        # Generic labels if you ever use more states\n        for i, s in enumerate(ordering):\n            state_label_map[s] = f\"Regime_{i}\"\n\n    df_regime[\"state_label\"] = df_regime[\"state\"].map(state_label_map)\n\n    return model, df_regime\n\n\nn_states = 3\nmodel, regimes_df = fit_hmm_on_liquidity(L_t,n_states)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.160\n  state 1: mean L = -0.106\n  state 2: mean L = 2.718\n\n\n\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n\n# -----------------------------\n# TOP PANEL — L(t) + shading\n# -----------------------------\nax[0].plot(regimes_df.index, regimes_df[\"L\"], color=\"black\", lw=1.2)\nax[0].set_title(\"Sparse PCA Liquidity Index L(t) with Regime Shading\")\n\n# Define plain colors for each regime\nregime_colors = {\n    \"Tight\":   \"green\",\n    \"Neutral\": \"gray\",\n    \"High\":    \"crimson\",\n}\n\nshade_alpha = 0.2  # semi-opaque\n\nfor label, color in regime_colors.items():\n    mask = regimes_df[\"state_label\"] == label\n\n    in_region = False\n    start = None\n\n    for i in range(len(mask)):\n        if mask.iloc[i] and not in_region:\n            in_region = True\n            start = regimes_df.index[i]\n\n        elif not mask.iloc[i] and in_region:\n            in_region = False\n            end = regimes_df.index[i]\n            ax[0].axvspan(start, end, color=color, alpha=shade_alpha, linewidth=0)\n\n    # If region extends to the end of the sample\n    if in_region:\n        ax[0].axvspan(start, regimes_df.index[-1], color=color, alpha=shade_alpha, linewidth=0)\n\n# Legend for regimes (shaded colors)\nfrom matplotlib.lines import Line2D\nlegend_patches = [\n    Line2D([0], [0], color=\"green\",   lw=6, alpha=shade_alpha, label=\"Tight\"),\n    Line2D([0], [0], color=\"gray\",    lw=6, alpha=shade_alpha, label=\"Neutral\"),\n    Line2D([0], [0], color=\"crimson\", lw=6, alpha=shade_alpha, label=\"High\"),\n]\nax[0].legend(handles=legend_patches, loc=\"upper left\")\n\n# -----------------------------\n# BOTTOM PANEL — High regime probability\n# -----------------------------\nif \"p_state_0\" in regimes_df.columns:\n    high_state_id = regimes_df.groupby(\"state_label\")[\"state\"].first()[\"High\"]\n    ax[1].plot(regimes_df.index,\n               regimes_df[f\"p_state_{high_state_id}\"],\n               color=\"crimson\", lw=1.5)\n    ax[1].set_title(\"Posterior Probability of High Liquidity Regime\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\nThese five factors extend the original 3-factor model to better explain cross-sectional stock returns.\nThey are:\n\nRm–Rf — Market excess return\nSMB — Size (Small Minus Big)\nHML — Value (High Book/Market minus Low)\nRMW — Profitability (Robust Minus Weak)\nCMA — Investment (Conservative Minus Aggressive)\n\nLet’s break each down precisely.\n\n\n\n\\[\nR_{m}-R_{f}\n\\] The return of the broad market minus the risk-free rate.\n\nPositive → market went up more than cash.\nNegative → market underperformed cash/T-bills.\n\nIn above table:\n\nLast 12 months: +17.54% → very strong bull market year.\nLast 3 months: +7.40% → strong quarter.\nOctober 2025: +1.95% → moderate positive month.\n\n\n\n\n\n\\[\n\\text{SMB} = R_{\\text{small}} - R_{\\text{big}}\n\\]\n\nPositive → small caps outperform large caps.\nNegative → large caps outperform small caps.\n\nIn above table:\n\nLast 12 months: –10.71% → a massive large-cap dominance year.\nThis is consistent with mega-cap tech leadership.\n\n\n\n\n\n\\[\n\\text{HML} = R_{\\text{value}} - R_{\\text{growth}}\n\\]\n\nPositive → value outperforms growth.\nNegative → growth outperforms value.\n\nIn above table:\n\nLast 12 months: –2.36% → growth beat value.\nOctober 2025: –3.10% → especially growth-heavy month.\n\nThis aligns with liquidity-driven growth leadership.\n\n\n\n\n\\[\n\\text{RMW} = R_{\\text{robust}} - R_{\\text{weak}}\n\\]\nRobust = high operating profitability. Weak = low profitability.\n\nPositive → profitable companies outperform weak ones.\nNegative → weak/low-profit companies outperform.\n\nIn above table:\n\nLast 12 months: –13.60% → very unusual.\nThis means unprofitable firms outperformed profitable ones over the year.\n\nThat usually happens in:\n\nearly speculative bubbles\nliquidity-driven rallies\nretail-led tech/small-cap frenzies\nAI/futuristic narrative periods\n\n\n\n\n\n\\[\n\\text{CMA} = R_{\\text{conservative}} - R_{\\text{aggressive}}\n\\]\n\nConservative = firms investing slowly\nAggressive = firms aggressively expanding assets\n\nHigh investment → lower expected returns historically (consistent with q-theory: empire-building destroys value).\n\nPositive → conservative firms outperform heavy spenders.\nNegative → aggressive investment firms outperform.\n\nIn above table:\n\nLast 12 months: –10.46%\nLast 3 months: –4.47%\nOctober 2025: –4.03%\n\nInterpretation: Aggressively investing companies — think AI, R&D, biotech, high-growth tech — massively outperformed low-investment firms.\n\n\n\n\n\n\n\n\nFactor\nSign\nInterpretation\n\n\n\n\nRm–Rf: +\nStrong bull market\n\n\n\nSMB: –\nMega-caps beat small caps massively\n\n\n\nHML: –\nGrowth beat value\n\n\n\nRMW: –\nLow-profit firms beat profitable firms\n\n\n\nCMA: –\nAggressive-investment firms beat conservative ones\n\n\n\n\nThis is the textbook signature of a liquidity-driven growth/tech momentum regime, almost identical to:\n\n1998–1999 Dotcom\n2019–2021 QE wave\n2023–2024 AI mega-cap boom\n\nIt means:\n\n\n\n\nlarge\ngrowth\nunprofitable\nhigh-spending\nhigh-duration tech/AI/future-theme stocks\n\n\n\n\nThis is exactly the kind of environment where:\n\nValue fails\nSmall cap fails\nProfitability fails\nInvestment works negatively\nGrowth dominates\nMega-caps dominate\n\n\ndef download_ff_factors(start, end):\n    \"\"\"\n    Download monthly Fama-French 5 factors (2x3) using pandas_datareader.\n    \"\"\"\n    print(\"Downloading Fama-French factors (famafrench)...\")\n    ff_raw = pdr.DataReader(\n        name=FF_FACTORS_DATASET,\n        data_source=\"famafrench\",\n        start=start,\n        end=end\n    )[0]  # table [0] contains the data\n\n    ff = (ff_raw\n          .divide(100)  # convert from % to decimal\n          .reset_index(names=\"date\")\n          .assign(date=lambda x: pd.to_datetime(x[\"date\"].astype(str)))\n          .rename(str.lower, axis=\"columns\")\n          .rename(columns={\"mkt-rf\": \"mkt_excess\"}))\n\n    ff = ff.set_index(\"date\")\n    return ff\n\nff_factors = download_ff_factors(start_date, end_date)\n\nDownloading Fama-French factors (famafrench)...\n\n\nFutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n&lt;ipython-input-69-55f7e03990a5&gt;:6: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n  ff_raw = pdr.DataReader(\n\n\n\nff_factors.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-06-01\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-07-01\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-08-01\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-09-01\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-10-01\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Fix Fama–French dates (shift one month backward)\nff_adj = ff_factors.copy()\n\n# FF data comes labeled as YYYY-MM-01 meaning return for previous month.\n# So shift index back to previous month-end.\nff_adj.index = (ff_adj.index.to_period(\"M\") - 1).to_timestamp(\"M\")\n\nff_adj.tail()\n\n\n\n\n\n\n\n\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined = reg.join(ff_adj, how=\"inner\")\n\n\ncombined.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-12-31\n-1.651021\n1\n0.162158\n0.837762\n0.000079\nTight\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n-1.496425\n0\n0.837514\n0.162233\n0.000253\nTight\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n-1.643180\n1\n0.162188\n0.837738\n0.000075\nTight\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n-1.627975\n0\n0.837473\n0.162258\n0.000269\nTight\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n-1.400432\n1\n0.162220\n0.837714\n0.000067\nTight\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n-1.561611\n0\n0.837459\n0.162284\n0.000258\nTight\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n-1.387057\n1\n0.162245\n0.837689\n0.000066\nTight\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n2025-07-31\n-1.482573\n0\n0.837437\n0.162308\n0.000255\nTight\n0.0185\n0.0488\n0.0442\n-0.0068\n0.0207\n0.0038\n\n\n2025-08-31\n-1.482572\n1\n0.162251\n0.837658\n0.000091\nTight\n0.0339\n-0.0218\n-0.0105\n-0.0203\n-0.0222\n0.0033\n\n\n2025-09-30\n-1.251226\n0\n0.836442\n0.162316\n0.001242\nTight\n0.0195\n-0.0131\n-0.0310\n-0.0522\n-0.0403\n0.0037\n\n\n\n\n\n\n\n\n\n\n\n\ndf2 = combined.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by *lagged* regime:\")\nprint(means_by_regime)\n\nMean factor returns by *lagged* regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nTight            -0.005439 -0.005360  0.000540 -0.003311\nNeutral           0.003200  0.002938  0.001051 -0.000056\nHigh              0.003327  0.001228  0.005127  0.003575\n\n\n\n\n\nThis is reverse of what you’d expect. In a tight liquidity regime, \\(R^{smb}\\) and \\(R^{hml}\\) should be deeply positive. You’d expect small under-valued companies to outperform big companies with premium valuation.\nLet’s go back to the \\(L_t\\) and understand what’s going on\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L_t.index,\n    y=L_t.values,\n    hue=regimes_df[\"state\"],\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"orange\", 2: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution of L(t) by HMM State\")\nax[1].set_xlabel(\"L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the KDE (bottom panel):\n\nStates 0 (green) and 1 (yellow) heavily overlap\nOnly state 2 (red) is clearly separated — the “high liquidity spike”\nThis corresponds to crisis/QE events (2008, 2020), where liquidity jumps dramatically\n\nSo the natural clustering is:\n\nOne large cluster (normal-tight-neutral combined)\nOne rare extreme spike cluster\n\nYour HMM is being forced to split the unimodal bulk distribution into two states, resulting in:\nmeans = pd.Series(model.means_.flatten(), index=range(n_states))\nmeans\n\n0   -0.159797\n1   -0.105787\n2    2.717670\ndtype: float64\nI initially thought alpha=0.5 in SparsePCA might be the cause. But the plot shows:\n\nL(t) has a healthy dynamic range across time (≈ −2.5 to +12)\nThe high liquidity cluster is clear and RARE\nThe rest of L(t) is genuinely one blob\n\nIf Sparse PCA was “overshrinking,” L(t) would be compressed; but it is not.\n\n\n\n\nIs Liquidity is fundamentally spiky ?\nWhy ?\nAm I defining Liquidity incorrectly ?\n\nFed/Treasury continuously debased currency throughout 2008-2019 period. Liquidity injection (bailing out banks during GFC/2008, ZIRP era, M2 stimulius of 2019 are visible ones). How would I capture this events as proxy of “liquidity” ?\n\n\nRight now your \\(L_t\\) is basically a short-horizon “flow” liquidity shock index.\n\n\nWhereas I’m looking for slow, structural debasement / regime of easy money (2008–2019, QE, ZIRP, etc.).\n\n\nThose are not the same object.\n\nBecause I use 1-month growth rates \\(\\Delta \\log(\\cdot)\\), my index reacts to spikes / changes, not the level of the monetary stock.\nI need to factor in more factors to the liquidity index such as -\n\nLevel of money / balance sheet relative to trend / GDP, not just monthly change\nProlonged ZIRP / negative real rate period\nExcess liquidity over economic activity\n\n\n\n\n\n\nExamples (all can be pulled from FRED):\nLog level vs pre-2008 trend\nLet \\(m_t = \\log M2_t\\). Fit a linear trend on a pre-QE baseline (say 1985–2007):\n\\(m_t \\approx a + bt \\quad (t \\le 2007)\\)\nThen define excess money stock:\n\\(EM_t = m_t - (a + bt)\\)\nAfter 2008, \\(EM_t\\) becomes increasingly positive if M2 grows above its historical trend.\nSimilarly for the Fed Balance Sheet, let \\(b_t = \\log(\\text{FedBal}_t)\\):\n\\(EB_t = b_t - (\\alpha + \\beta t)\\) (pre-2007 trend)\n\nExcess liquidity versus real economy\nUse real GDP series (e.g., GDPC1) and define:\n\\(EL_t = \\log M2_t - \\log GDP_t\\)\nOr measure multi-year excess growth:\n\\(EL_t(3y) = \\log M2_t - \\log M2_{t-36} - (\\log GDP_t - \\log GDP_{t-36})\\)\n\nZIRP / negative real-rate indicator\n\\(D_t^{ZIRP} = 1(r_t^{real} &lt; 0)\\)\nFrom 2009–2015, this should be mostly 1.\n\n\n\n\nInstead of only:\n\\(x_t = \\{ \\Delta \\log M2_t,\\; \\Delta \\log FedBal_t,\\; TS_t,\\; r_t^{real},\\; CS_t \\}\\)\nLet’s expand to:\n\\(x_t^{aug} = \\{\n\\Delta \\log M2_t,\\;\n\\Delta \\log FedBal_t,\\;\nTS_t,\\;\nr_t^{real},\\;\nCS_t,\\;\nEM_t,\\;\nEB_t,\\;\nEL_t,\\;\nEL_t(3y),\\;\nD_t^{ZIRP}\n\\}\\)\nThen standardize and perform PCA / SparsePCA on this.\n\nPC1 (or a combination) loading heavily on \\(EM\\), \\(EB\\), \\(EL\\), \\(D_t^{ZIRP}\\) → structural easy-money regime\n\nPC2 (or your current PC1) loading on short-horizon changes → flow / shock liquidity\n\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\nNaN\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\nNaN\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29511.664\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\nNaN\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\nNaN\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29825.182\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\nNaN\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\nNaN\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n30042.113\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\nNaN\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\nNaN\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30485.729\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\nNaN\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\nNaN\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n# 1. Start from the original GDP values only (drop NaNs)\ngdp_series = macro_raw[\"GDP\"].dropna()\n\n# 2. Collapse to unique quarter-end values\n#    - If GDP is timestamped at quarter *start* (e.g. 2025-04-01),\n#      this will move it to the quarter end (2025-06-30) and give unique labels.\ngdp_q = gdp_series.resample(\"Q\").last()   # quarterly, at quarter-end (e.g. 2025-03-31, 2025-06-30, ...)\n\n# 3. Upsample to monthly and forward-fill within the quarter\ngdp_m = gdp_q.resample(\"M\").ffill()\n\n# 4. Assign back into macro_raw, aligning on the monthly index\nmacro_raw[\"GDP\"] = gdp_m\n\nmacro_raw.tail(20)\n\n\n\n\n\n\n\n\nM2SL\nFED_BAL\nTB3M\nDGS10\nBAA\nAAA\nCPI\nGDP\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n2024-05-31\n20989.4\n7284319.0\n5.25\n4.51\n5.95\n5.25\n313.140\n28708.161\n\n\n2024-06-30\n21053.0\n7231163.0\n5.24\n4.36\n5.82\n5.13\n313.131\n29147.044\n\n\n2024-07-31\n21084.2\n7178391.0\n5.20\n4.09\n5.84\n5.12\n313.566\n29147.044\n\n\n2024-08-31\n21171.0\n7123238.0\n5.05\n3.91\n5.60\n4.87\n314.131\n29147.044\n\n\n2024-09-30\n21257.5\n7080059.0\n4.72\n3.81\n5.42\n4.68\n314.851\n29511.664\n\n\n2024-10-31\n21308.1\n7013490.0\n4.51\n4.28\n5.63\n4.95\n315.564\n29511.664\n\n\n2024-11-30\n21407.9\n6905140.0\n4.42\n4.18\n5.78\n5.14\n316.449\n29511.664\n\n\n2024-12-31\n21424.5\n6885963.0\n4.27\n4.58\n5.80\n5.20\n317.603\n29825.182\n\n\n2025-01-31\n21492.4\n6818186.0\n4.21\n4.58\n6.08\n5.46\n319.086\n29825.182\n\n\n2025-02-28\n21564.6\n6766101.0\n4.22\n4.24\n5.92\n5.32\n319.775\n29825.182\n\n\n2025-03-31\n21636.7\n6740253.0\n4.20\n4.23\n5.93\n5.29\n319.615\n30042.113\n\n\n2025-04-30\n21770.5\n6709277.0\n4.21\n4.17\n6.18\n5.45\n320.321\n30042.113\n\n\n2025-05-31\n21827.4\n6673244.0\n4.25\n4.41\n6.29\n5.54\n320.580\n30042.113\n\n\n2025-06-30\n21942.4\n6662200.0\n4.23\n4.24\n6.15\n5.46\n321.500\n30485.729\n\n\n2025-07-31\n22028.8\n6642578.0\n4.25\n4.37\n6.10\n5.45\n322.132\nNaN\n\n\n2025-08-31\n22108.4\n6603384.0\n4.12\n4.23\n6.00\n5.35\n323.364\nNaN\n\n\n2025-09-30\n22212.5\n6608395.0\n3.92\n4.16\n5.83\n5.21\n324.368\nNaN\n\n\n2025-10-31\n22298.1\n6587034.0\n3.82\n4.11\n5.74\n5.13\nNaN\nNaN\n\n\n2025-11-30\nNaN\n6552419.0\n3.78\n4.02\n5.86\n5.26\nNaN\nNaN\n\n\n2025-12-31\nNaN\n6535781.0\nNaN\n4.11\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndef build_liquidity_proxies_augmented(macro_df: pd.DataFrame) -&gt; pd.DataFrame:\n    df = macro_df.copy()\n\n    # 1. Flow proxies\n    df[\"dlog_M2\"]       = np.log(df[\"M2SL\"]).diff()\n    df[\"dlog_FED_BAL\"]  = np.log(df[\"FED_BAL\"]).diff()\n    df[\"term_spread\"]   = df[\"DGS10\"] - df[\"TB3M\"]\n    df[\"infl_yoy\"]      = np.log(df[\"CPI\"]).diff(12)\n    df[\"real_rate\"]     = df[\"TB3M\"] - 100 * df[\"infl_yoy\"]\n    df[\"credit_spread\"] = df[\"BAA\"] - df[\"AAA\"]\n\n    # 2. Levels\n    df[\"log_M2\"]      = np.log(df[\"M2SL\"])\n    df[\"log_FED_BAL\"] = np.log(df[\"FED_BAL\"])\n\n    # Use data up to 2007-12-31 to fit trends\n    pre = df.loc[: \"2007-12-31\"].copy()\n\n    # --- Trend for M2 ---\n    pre_m2 = pre[\"log_M2\"].dropna()\n    t_m2   = np.arange(len(pre_m2))\n    coefs_M2 = np.polyfit(t_m2, pre_m2.values, deg=1)\n\n    t_full = np.arange(len(df))\n    trend_M2 = np.polyval(coefs_M2, t_full)\n    df[\"EM\"] = df[\"log_M2\"] - trend_M2\n\n    # --- Trend for Fed balance sheet ---\n    pre_fb = pre[\"log_FED_BAL\"].dropna()\n    t_fb   = np.arange(len(pre_fb))\n    coefs_FB = np.polyfit(t_fb, pre_fb.values, deg=1)\n\n    trend_FB = np.polyval(coefs_FB, t_full)\n    df[\"EB\"] = df[\"log_FED_BAL\"] - trend_FB\n\n    # 3. Excess liquidity vs GDP over 3y\n    df[\"log_GDP\"] = np.log(df[\"GDP\"])\n    df[\"EL_3y\"] = (df[\"log_M2\"] - df[\"log_M2\"].shift(36)) - \\\n                  (df[\"log_GDP\"] - df[\"log_GDP\"].shift(36))\n\n    # 4. ZIRP dummy\n    df[\"ZIRP_dummy\"] = (df[\"real_rate\"] &lt; 0).astype(int)\n\n    # 5. Build augmented proxy matrix\n    cols_aug = [\n        \"dlog_M2\", \"dlog_FED_BAL\", \"term_spread\", \"real_rate\", \"credit_spread\",\n        \"EM\", \"EB\", \"EL_3y\", \"ZIRP_dummy\"\n    ]\n\n    return df[cols_aug].dropna()\n\n\nliquidity_proxies_aug = build_liquidity_proxies_augmented(macro_raw)\n\n\nliquidity_proxies_aug.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n0.004077\n-0.006080\n-0.91\n2.316574\n0.74\n0.204864\n0.811903\n-0.194283\n0\n\n\n2024-10-31\n0.002378\n-0.009447\n-0.23\n1.971101\n0.68\n0.202921\n0.798940\n-0.199645\n0\n\n\n2024-11-30\n0.004673\n-0.015569\n-0.24\n1.742012\n0.64\n0.203273\n0.779855\n-0.203197\n0\n\n\n2024-12-31\n0.000775\n-0.002781\n0.31\n1.438113\n0.60\n0.199728\n0.773558\n-0.186129\n0\n\n\n2025-01-31\n0.003164\n-0.009892\n0.37\n1.254690\n0.62\n0.198572\n0.760151\n-0.188367\n0\n\n\n2025-02-28\n0.003354\n-0.007668\n0.02\n1.444603\n0.60\n0.197605\n0.748966\n-0.189520\n0\n\n\n2025-03-31\n0.003338\n-0.003828\n0.03\n1.822893\n0.64\n0.196623\n0.741623\n-0.177663\n0\n\n\n2025-04-30\n0.006165\n-0.004606\n-0.04\n1.903069\n0.73\n0.198467\n0.733501\n-0.172818\n0\n\n\n2025-05-31\n0.002610\n-0.005385\n0.16\n1.901852\n0.75\n0.196757\n0.724600\n-0.167478\n0\n\n\n2025-06-30\n0.005255\n-0.001656\n0.01\n1.592409\n0.69\n0.197692\n0.719428\n-0.150768\n0\n\n\n\n\n\n\n\n\nz1_t = standardize_and_signflip(liquidity_proxies_aug)\nz1_t.tail(10)\n\n\n\n\n\n\n\n\ndlog_M2\ndlog_FED_BAL\nterm_spread\nreal_rate\ncredit_spread\nEM\nEB\nEL_3y\nZIRP_dummy\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024-09-30\n-0.139814\n-0.331324\n-1.812889\n-1.577541\n0.673692\n0.888682\n0.786734\n-2.353880\n-1.527525\n\n\n2024-10-31\n-0.414988\n-0.409551\n-1.282837\n-1.408779\n0.819794\n0.868074\n0.764262\n-2.404333\n-1.527525\n\n\n2024-11-30\n-0.043456\n-0.551814\n-1.290631\n-1.296871\n0.917195\n0.871811\n0.731177\n-2.437752\n-1.527525\n\n\n2024-12-31\n-0.674370\n-0.254667\n-0.861913\n-1.148418\n1.014596\n0.834206\n0.720261\n-2.277159\n-1.527525\n\n\n2025-01-31\n-0.287635\n-0.419885\n-0.815143\n-1.058817\n0.965896\n0.821943\n0.697018\n-2.298214\n-1.527525\n\n\n2025-02-28\n-0.256970\n-0.368230\n-1.087964\n-1.151588\n1.014596\n0.811689\n0.677630\n-2.309059\n-1.527525\n\n\n2025-03-31\n-0.259533\n-0.278983\n-1.080169\n-1.336381\n0.917195\n0.801267\n0.664900\n-2.197496\n-1.527525\n\n\n2025-04-30\n0.198084\n-0.297077\n-1.134734\n-1.375546\n0.698042\n0.820833\n0.650819\n-2.151913\n-1.527525\n\n\n2025-05-31\n-0.377318\n-0.315174\n-0.978836\n-1.374952\n0.649342\n0.802693\n0.635389\n-2.101665\n-1.527525\n\n\n2025-06-30\n0.050761\n-0.228533\n-1.095759\n-1.223790\n0.795443\n0.812604\n0.626423\n-1.944440\n-1.527525\n\n\n\n\n\n\n\n\nL1_t = build_sparse_pca_liquidity_index(z1_t, alpha=0.5)\nL1_t.tail(10)\n\nSparsePCA components (loadings):\n  dlog_M2: -0.132\n  dlog_FED_BAL: -0.159\n  term_spread: -0.389\n  real_rate: -0.515\n  credit_spread: 0.046\n  EM: -0.114\n  EB: -0.134\n  EL_3y: -0.518\n  ZIRP_dummy: -0.490\n\n\nDATE\n2024-09-30    3.347080\n2024-10-31    3.143188\n2024-11-30    3.088484\n2024-12-31    2.811365\n2025-01-31    2.736110\n2025-02-28    2.887801\n2025-03-31    2.906425\n2025-04-30    2.856634\n2025-05-31    2.850481\n2025-06-30    2.674924\nFreq: M, Name: L, dtype: float64\n\n\n\nL1_t.plot(title=\"Sparse PCA Liquidity Index L(t) - Augmented w/ Flow & Stock Indicators\", figsize=(14, 6))\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel, regimes_aug_df = fit_hmm_on_liquidity(L1_t,n_states=2)\n\nHMM state means (unsorted, in fitted scale):\n  state 0: mean L = -0.484\n  state 1: mean L = 1.536\n\n\n\nregime_aug_df.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n\n\n\n\n\n\n\n\n# Ensure regimes are month-end\nreg = regimes_aug_df.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Join on month-end date\ncombined_aug = reg.join(ff_adj, how=\"inner\")\n\n\ncombined_aug.tail(10)\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2024-09-30\n3.347080\n1\n7.082075e-08\n1.000000\nHigh\n-0.0100\n-0.0089\n0.0086\n-0.0148\n0.0098\n0.0039\n\n\n2024-10-31\n3.143188\n1\n1.226701e-07\n1.000000\nHigh\n0.0649\n0.0459\n0.0015\n-0.0231\n-0.0205\n0.0040\n\n\n2024-11-30\n3.088484\n1\n1.441515e-07\n1.000000\nHigh\n-0.0315\n-0.0383\n-0.0300\n0.0189\n-0.0121\n0.0037\n\n\n2024-12-31\n2.811365\n1\n3.564580e-07\n1.000000\nHigh\n0.0280\n-0.0123\n0.0163\n-0.0235\n-0.0324\n0.0037\n\n\n2025-01-31\n2.736110\n1\n4.676727e-07\n1.000000\nHigh\n-0.0243\n-0.0491\n0.0491\n0.0108\n0.0306\n0.0033\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\ndf2 = combined_aug.copy()\n\n# Regime at t (end-of-month) predicting factor returns at t+1\ndf2[\"state_label_lag1\"] = df2[\"state_label\"].shift(1)\n\n# Drop first row (no lag)\ndf2 = df2.dropna(subset=[\"state_label_lag1\", \"smb\", \"hml\", \"rmw\", \"cma\"])\n\n# Regime-wise averages (conditional on *previous* month's liquidity)\nmeans_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .mean()\n)\n\nstd_by_regime = (\n    df2.groupby(\"state_label_lag1\")[[\"smb\", \"hml\", \"rmw\", \"cma\"]]\n       .std()\n)\n\nsharpe_by_regime = means_by_regime / std_by_regime\n\nprint(\"Mean factor returns by Liquidity Regime:\")\nprint(means_by_regime)\n\nMean factor returns by Liquidity Regime:\n                       smb       hml       rmw       cma\nstate_label_lag1                                        \nHigh             -0.004519 -0.001700  0.001564 -0.003006\nTight             0.001896  0.000038  0.002920  0.001302"
  },
  {
    "objectID": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#yes",
    "href": "notebooks/Liquidity Regimes and the Death (and Return) of Valuations.html#yes",
    "title": "Liquidity Regimes and the Death (and Return) of Valuations",
    "section": "",
    "text": "Factor\nHigh Liquidity\nTight Liquidity\nInterpretation\n\n\n\n\nSMB\nNegative\nPositive\nSmall caps thrive when liquidity tightens\n\n\nHML\nNegative\nSlightly positive\nValue improves in tight regimes\n\n\nRMW\nPositive\nMore positive\nProfitability is the strongest cross-regime performer\n\n\nCMA\nNegative\nPositive\nConservative investment becomes favored when liquidity tightens\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(2, 1, figsize=(14, 8), sharex=False)\n\n# 1: Time series with regimes\nsns.scatterplot(\n    x=L1_t.index,\n    y=L1_t.values,\n    hue=regimes_aug_df[\"state\"],\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[0],\n    s=15\n)\nax[0].set_title(\"Augmented Liquidity Index L(t) with HMM States\")\nax[0].set_ylabel(\"Augmented L(t)\")\n\n# 2: KDE (distribution) by state\nsns.kdeplot(\n    data=regimes_aug_df,\n    x=\"L\",\n    hue=\"state\",\n    fill=True,\n    palette={0: \"green\", 1: \"red\"},\n    ax=ax[1]\n)\nax[1].set_title(\"Distribution/KDE of Augmented L(t) by HMM State\")\nax[1].set_xlabel(\"Augmented L(t)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn KDE of augmented liquidity index \\(L_t^{aug}\\), clusters are clearly separated, unlike last one\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\ndef parse_yyyy_mm_to_date(s: str):\n    \"\"\"\n    Convert 'YYYY.MM' to a proper datetime.\n    Handles the special case where October appears as YYYY.1 instead of YYYY.10.\n    \"\"\"\n    s = str(s).strip()\n    parts = s.split(\".\")\n    if len(parts) != 2:\n        raise ValueError(f\"Unexpected date format: {s}\")\n    \n    year = int(parts[0])\n    mm = parts[1]\n\n    # Fix: \"1\" should be \"10\" (October)\n    if mm == \"1\":\n        month = 10\n    else:\n        month = int(mm)\n\n    return pd.Timestamp(year=year, month=month, day=1)\n\n# --- Load CAPE CSV ---\nsp500_cape_raw = pd.read_csv(\"data/sp500-cape-1990-2025.csv\")\n\n# Clean + parse\nsp500_cape_raw[\"date\"] = sp500_cape_raw[\"Date\"].apply(parse_yyyy_mm_to_date)\n\nsp500_cape_raw = (\n    sp500_cape_raw[[\"date\", \"SP500\", \"CAPE\"]]\n    .set_index(\"date\")\n    .sort_index()\n)\n\n# Convert to month-end aligned CAPE series\nsp500_cape_m = sp500_cape_raw.resample(\"M\").last()\nsp500_cape_m.rename(columns={\"CAPE\": \"cape\", \"SP500\": \"sp500\"}, inplace=True)\n\nsp500_cape_m.tail()\n\n\n\n\n\n\n\n\nsp500\ncape\n\n\ndate\n\n\n\n\n\n\n2025-08-31\n6408.95\n37.85\n\n\n2025-09-30\n6584.02\n38.58\n\n\n2025-10-31\n6735.69\n39.21\n\n\n2025-11-30\n6740.89\n39.12\n\n\n2025-12-31\n6812.63\n39.42\n\n\n\n\n\n\n\n\n\n\nLong-Term Average/Median: The S&P 500’s historical long-term average CAPE ratio is around 16-18, with a median value of approximately 16.04 (since 1881). This provides a central benchmark.\nHigh Valuation (Expensive): A CAPE ratio that is notably higher than the historical average (e.g., above 25 or 30) is considered indicative of a highly valued or overvalued market. Historically, values exceeding 30 have only occurred during major market peaks like the 1929 crash, the dot-com bubble in the late 1990s (peaking over 44), and the post-pandemic period around 2021. Such high readings have typically been followed by periods of lower-than-average, or even negative, long-term returns.\nLow Valuation (Cheap): A CAPE ratio well below the historical average (e.g., below 15 or 10) suggests an undervalued or cheap market. Historically, such levels have been associated with minimal downside risk and higher average long-term returns.\nCould have calculated\nV_spread = (x - baseline) / baseline # baseline = 16\nProblem is, if the whole world “structurally” shifted (e.g. post-1990 lower inflation), “16” stops being meaningful.\nWe are instead measuring,\n\\[\nV_t = \\frac{CAPE_t - \\mathrm{median}(CAPE)}{\\mathrm{IQR}(CAPE)}\n\\]\n\ndef build_valuation_spread_baseline(\n    val_df: pd.DataFrame,\n    metric: str = \"cape\",\n    baseline: float = None,\n    scale: bool = True,\n) -&gt; pd.Series:\n    x = val_df[metric].astype(float)\n\n    if baseline is None:\n        # robust long-run baseline\n        median = x.median()\n        iqr = x.quantile(0.75) - x.quantile(0.25)\n        baseline = median\n        denom = iqr if scale and iqr &gt; 0 else baseline\n    else:\n        denom = baseline if scale else 1.0\n\n    diff = x - baseline          \n\n    V_spread = diff / denom if scale else diff\n    V_spread.name = \"V_spread\"\n    return V_spread\n\n# Example: baseline at 16.04 (long-run median)\nV_spread_t = build_valuation_spread_baseline(\n    sp500_cape_m,\n    metric=\"cape\",\n    baseline=None,\n    scale=True,   # or False if you prefer \"CAPE points below baseline\"\n)\n\nV_spread_t.tail()\n\ndate\n2025-08-31    1.259380\n2025-09-30    1.338771\n2025-10-31    1.407287\n2025-11-30    1.397499\n2025-12-31    1.430125\nFreq: M, Name: V_spread, dtype: float64\n\n\n\nV_spread_t.plot(figsize=(14, 4), title=\"S&P 500 Valuation Spread (cheap vs own history)\")\nplt.axhline(0, linestyle=\"--\", linewidth=1)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nregimes_df.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\np_state_2\nstate_label\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n2025-05-31\n-1.561611\n0\n0.837436\n0.162306\n0.000258\nTight\n\n\n2025-06-30\n-1.387057\n1\n0.162268\n0.837667\n0.000066\nNeutral\n\n\n2025-07-31\n-1.482573\n0\n0.837414\n0.162331\n0.000255\nTight\n\n\n2025-08-31\n-1.482572\n1\n0.162274\n0.837635\n0.000091\nNeutral\n\n\n2025-09-30\n-1.251226\n0\n0.836419\n0.162339\n0.001242\nTight\n\n\n\n\n\n\n\n\ncombined_aug.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n\n\n\n\n\n\n\n\n# regimes_df: index is month-end\nreg = combined_aug.copy()\nreg.index = reg.index.to_period(\"M\").to_timestamp(\"M\")\n\n# Align valuation spread to month-end, then join\nV_spread_m = V_spread_t.resample(\"M\").last()\n\ncombined = (\n    reg\n    .join(V_spread_m.to_frame(), how=\"inner\")\n)\n\ncombined.tail()\n\n\n\n\n\n\n\n\nL\nstate\np_state_0\np_state_1\nstate_label\nmkt_excess\nsmb\nhml\nrmw\ncma\nrf\nV_spread\n\n\n\n\n2025-02-28\n2.887801\n1\n2.736315e-07\n1.000000\nHigh\n-0.0639\n-0.0149\n0.0290\n0.0211\n-0.0047\n0.0034\n1.187602\n\n\n2025-03-31\n2.906425\n1\n2.569518e-07\n1.000000\nHigh\n-0.0084\n-0.0186\n-0.0340\n-0.0285\n-0.0267\n0.0035\n0.925503\n\n\n2025-04-30\n2.856634\n1\n3.043360e-07\n1.000000\nHigh\n0.0606\n-0.0072\n-0.0288\n0.0129\n0.0251\n0.0038\n0.690593\n\n\n2025-05-31\n2.850481\n1\n3.200660e-07\n1.000000\nHigh\n0.0486\n-0.0002\n-0.0160\n-0.0320\n0.0145\n0.0034\n0.958129\n\n\n2025-06-30\n2.674924\n1\n2.893931e-05\n0.999971\nHigh\n0.0198\n-0.0015\n-0.0127\n-0.0029\n-0.0207\n0.0034\n1.070147\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Valuation spread series, e.g. top–bottom decile)\n$ V_t^{} $\n(Regime-conditional mean valuation spread)\n$ {V}^{(k)} = $\n$ ^{(k)} = {_{t=1}^T (_t = k)} $\n(Difference in spreads between regimes)\n$ ^{()} - ^{()} $\n(Factor return in regime ( k ))\n$ r_t^{(F)} $\n$ {r}_F^{(k)} = $\n$ F^{(k)} = {{t=1}^T (_t = k)} $\n(Regime-specific Sharpe ratio)\n$ _F^{(k)} = $\n\n# Regime-conditional mean valuation spread:  V̄^(k)\nval_means_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .mean()\n)\n\nval_stds_by_regime = (\n    combined\n    .groupby(\"state_label\")[\"V_spread\"]\n    .std()\n)\n\nprint(\"Regime-conditional mean valuation spread:\")\nprint(val_means_by_regime)\n\nprint(\"\\nRegime-conditional valuation z-score std dev:\")\nprint(val_stds_by_regime)\n\n# Difference in spreads between regimes:  V̂^(High) − V̂^(Tight)\nif {\"High\", \"Tight\"}.issubset(val_means_by_regime.index):\n    spread_diff = (\n        val_means_by_regime[\"High\"] - val_means_by_regime[\"Tight\"]\n    )\n    print(\"\\nDifference in spreads High − Tight:\")\n    print(spread_diff)\n\nRegime-conditional mean valuation spread:\nstate_label\nHigh     0.380274\nTight   -0.049269\nName: V_spread, dtype: float64\n\nRegime-conditional valuation z-score std dev:\nstate_label\nHigh     0.406851\nTight    0.551632\nName: V_spread, dtype: float64\n\nDifference in spreads High − Tight:\n0.42954211798360936\n\n\n\n\n\n\nTight liquidity = cheaper markets (value regime),\nHigh liquidity = expensive markets (growth regime).\n\n\nMarkets are ~0.43σ more expensive in High liquidity regimes than Tight liquidity regimes.\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# -------------------------------------------------------------------\n# Assume you already have:\n#   L_t              : pd.Series (liquidity index), monthly, name \"L\"\n#   sp500_cape_m     : DataFrame with column \"sp500\" (S&P 500)\n#   V_spread_t       : pd.Series (\"V_spread\"), monthly valuation cheapness\n#   regimes_df       : DataFrame with column \"state_label\"\n# -------------------------------------------------------------------\n\n# 1) Build a common monthly dataframe\ndf_plot = pd.DataFrame(index=L_t.index.union(sp500_cape_m.index).union(V_spread_t.index))\n\ndf_plot[\"L\"]         = L_t.reindex(df_plot.index)\ndf_plot[\"spx_price\"] = sp500_cape_m[\"sp500\"].reindex(df_plot.index)\ndf_plot[\"V_spread\"]  = V_spread_t.reindex(df_plot.index)\n\n# Attach regimes (forward-fill in case of any slight index mismatch)\nreg_states = combined[\"state_label\"].reindex(df_plot.index).ffill()\ndf_plot[\"state_label\"] = reg_states\n\n# Optional: drop rows before we have all three series\ndf_plot = df_plot.dropna(subset=[\"L\", \"spx_price\", \"V_spread\", \"state_label\"])\n\n# 2) Helper: find contiguous regime segments for shading\ndef get_regime_segments(states: pd.Series):\n    \"\"\"\n    Given a Series of regime labels indexed by date,\n    return list of (start_date, end_date, label) segments\n    where the label is constant.\n    \"\"\"\n    segments = []\n    prev_label = None\n    start_date = None\n\n    for dt, label in states.items():\n        if prev_label is None:\n            prev_label = label\n            start_date = dt\n            continue\n\n        if label != prev_label:\n            # segment ended at previous date\n            end_date = dt\n            segments.append((start_date, end_date, prev_label))\n            start_date = dt\n            prev_label = label\n\n    # last segment\n    if prev_label is not None and start_date is not None:\n        segments.append((start_date, states.index[-1], prev_label))\n\n    return segments\n\nsegments = get_regime_segments(df_plot[\"state_label\"])\n\n# 3) Colors for regimes\nregime_colors = {\n    \"High\":    \"#ffe0e0\",  # pale red\n    \"Neutral\": \"#f7f7f7\",  # light gray\n    \"Tight\":   \"#d1ffd1\",  # pale green\n}\n\n# 4) Plot: 3 stacked subplots with shared x-axis\nfig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True,\n                         gridspec_kw={\"height_ratios\": [1, 1, 1]})\n\nax_L, ax_spx, ax_val = axes\n\n# --- Shading first so lines render on top ---\nfor (start, end, label) in segments:\n    color = regime_colors.get(label, \"#f0f0f0\")\n    for ax in axes:\n        ax.axvspan(start, end, color=color, alpha=0.3)\n\n# --- Panel 1: Liquidity index L(t) ---\nax_L.plot(df_plot.index, df_plot[\"L\"], label=\"L(t)\", linewidth=1.5)\nax_L.set_ylabel(\"L(t)\")\nax_L.set_title(\"Liquidity Index, S&P 500, and Valuation Spread by Liquidity Regime\")\nax_L.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 2: S&P 500 level ---\nax_spx.plot(df_plot.index, df_plot[\"spx_price\"], label=\"S&P 500\", linewidth=1.5)\nax_spx.set_ylabel(\"S&P 500\")\nax_spx.grid(True, linestyle=\"--\", alpha=0.4)\n\n# --- Panel 3: Valuation cheapness (V_spread) ---\nax_val.plot(df_plot.index, df_plot[\"V_spread\"], label=\"V_spread (cheap vs baseline)\", linewidth=1.5)\nax_val.axhline(0.0, color=\"black\", linewidth=1, linestyle=\"--\", alpha=0.7)\nax_val.set_ylabel(\"V_spread\")\nax_val.set_xlabel(\"Date\")\nax_val.grid(True, linestyle=\"--\", alpha=0.4)\n\n# Optional legends\nax_L.legend(loc=\"upper left\")\nax_spx.legend(loc=\"upper left\")\nax_val.legend(loc=\"upper left\")\n\nfig.tight_layout()\nplt.show()"
  }
]